15. Though one [[man]]’s [[idea]] of [[blue]] should be different from [[another]]’s.
[[1989]], [[1988]], [[1987]], [[1986]], [[1985]], [[1984]], [[1983]], [[1982]], [[1981]], [[1980]], [[1979]], ...</text>
[[1999]], [[1998]], [[1997]], [[1996]], [[1995]], [[1994]], [[1993]], [[1992]], [[1991]], [[1990]],
::: 1) Import [[AdaBoost Classification System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[AdaBoostClassifier]]</code>
::: 1) Import [[AdaBoost Regression System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[AdaBoostRegressor]]</code>
::: 1) Import [[Bagging Classification System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[BaggingClassifier]]</code>
::: 1) Import [[Bagging Regression System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[BaggingRegressor]]</code>
::: 1) Import [[Bayesian Ridge Regression]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[BayesianRidge]]</code>
::: 1) Import [[Classification Extra-Trees Learning System]] from [[scikit-learn]] : <code>from [[sklearn.tree]] import [[ExtraTreeClassifier]]</code>
::: 1) Import [[Classification Tree Learning System]] from [[scikit-learn]] : <code>from [[sklearn.tree]] import [[DecisionTreeClassifier]]</code>
::: 1) Import [[ElasticNetCV]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[ElasticNetCV]]</code>
::: 1) Import [[ElasticNet Regression]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[ElasticNet]]</code>
::: 1) Import [[Extremely Randomized Trees Classification System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[ExtraTreesClassifier]]</code>
::: 1) Import [[Extremely Randomized Trees Regression System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[ExtraTreesRegressor]]</code>
::: 1) Import [[Gradient Tree Boosting Classification System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[GradientBoostingClassifier]]</code>
::: 1) Import [[Gradient Tree Boosting Regression System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[GradientBoostingRegressor]]</code>
::: 1) Import [[Huber Regression]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[HuberRegressor]]</code>
::: 1) Import [[Isolation Forest Algorithm]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[IsolationForest]]</code>
::: 1) Import [[LarsCV]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[LarsCV]]</code>
::: 1) Import [[Lars Regression]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[Lars]]</code>
::: 1) Import [[LassoCV]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[LassoCV]]</code>
::: 1) Import [[LassoLarsCV]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[LassoLarsCV]]</code>
::: 1) Import [[LassoLarsIC]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[LassoLarsIC]]</code>
::: 1) Import [[LassoLars]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[LassoLars]]</code>
::: 1) Import [[Linear Regression]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[LinearRegression]]</code>
::: 1) Import [[MLP Classification System]] from [[scikit-learn]] : <code>from [[sklearn.neural_network]] import [[MLPClassifier]]</code>
::: 1) Import [[MLP Regression System]] from [[scikit-learn]] : <code>from [[sklearn.neural_network]] import [[MLPRegressor]]</code>
::: 1) Import [[MultiTaskLasso]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[MultiTaskLasso]]</code>
::: 1) Import [[Random Forest Classification System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[RandomForestClassifier]]</code>
::: 1) Import [[Random Forest Regression System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[RandomForestRegressor]]</code>
::: 1) Import [[RBM Training System]] from [[scikit-learn]] : <code>from [[sklearn.neural_network]] import [[BernoulliRBM]]</code>
::: 1) Import [[Regression Extra-Trees Learning System]] from [[scikit-learn]] : <code>from [[sklearn.tree]] import [[ExtraTreeRegressor]]</code>
::: 1) Import [[Regression Tree Learning System]] from [[scikit-learn]] : <code>from [[sklearn.tree]] import [[DecisionTreeRegressor]]</code>
::: 1) Import the [[Classification System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[VotingClassifier]]</code>
::: 1) Import [[TheilSenRegressor]] model from [[scikit-learn]] : <code>from [[sklearn.linear_model]] import [[TheilSenRegressor]]</code>
::: 1) Import the [[Totally Random Trees Embedding System]] from [[scikit-learn]] : <code>from [[sklearn.ensemble]] import [[RandomTreesEmbedding]]</code>
(1) [[It]] presents a hierarchy of [[relational concept]]s of increasing [[complexity]], using [[relational schema characteristic]]s such as [[cardinality]], and [[derives classes]] of [[aggregation operator]]s that are needed to [[learn]] these [[concept]]s.
1 shows an illustration of [[the model]] which [[learns vector representations]] of [[phrase]]s and [[full sentence]]s as well as their [[hierarchical structure]] from [[unsupervised text]] .
1, the effects of [[within-cluster]] and [[within-period correlation]]s are limited in [[cluster-crossover studi]]es, by design, between the [[experimental]] and [[control group]]s.
1. [[Truth]] and [[falsehood]] properly belong to [[proposition]]s, not to [[idea]]s.
[[2009]], [[2008]], [[2007]], [[2006]], [[2005]], [[2004]], [[2003]], [[2002]], [[2001]], [[2000]],
[[2012]], [[E. Brynjolfsson]] and [[A. McAfee]], [[Race Against The Machine]]:
[[2017 STARDATAAStarCraftAIResearchDat}We]] use [[TorchCraft]] to [[extract and store]] [[the data]], which standardizes the [[data format]] for both [[reading]] from [[replay]]s and [[reading]] directly from the [[game]] .
(2) An in-place [[feature vector]] [[generation algorithm]] with [[linear time complexity]] O(n) regardless of the [[granularity]] of [[sliding target window]] .
2) how to identify the [[social]] [[influence]]s of [[neighboring nodes]] on a particular [[node]]?
2) In [[METIS]], [[product recommendation]] is framed as a [[learning]] to [[rank problem]] .
(2) it [[dynamically]] [[detect]]s [[converged messages]] to efficiently skip unneeded [[update]]s.
2) [[Link prediction]]: The [[compression scheme]] of <i>[[SCMiner]]</i> reveals suspicious [[edge]]s which are probably [[erroneous]] as well as missing [[edge]]s, i.e.
(2) [[OptRank]] learns the best [[feature weight]]s by [[maximizing]] the [[average]] [[AUC (Area Under the ROC Curve)]] of the [[tag recommender]] .
(2) [[OptRank]] successfully improves the results by incorporating [[social network]], [[tag semantics]] and [[item profile]]s.
([[2) Task]] trails provide higher [[web page]] utilities to [[user]]s than other [[source]]s.
3, 993-1022]], in which each [[document]] is [[generated by]] choosing a [[distribution]] over [[topics]] and then choosing each [[word]] in the [[document]] from a [[topic]] selected according to this [[distribution]] .
(3) An [[in-memory caching scheme]] that significantly reduces the [[number]] of [[disk IO]]s to make [[large-scale]] [[learning]] practical.
(3) [[It]] demonstrates empirically on a [[noisy business domain]] that [[more-complex]] [[aggregation method]]s can increase [[generalization performance]] .
[[3-profile]]s are generalizations of [[triangle count]]s that specify the [[number]] of times a [[small graph]] appears as an [[induced subgraph]] of a [[large graph]] .
49 [[journal article]]s on the [[subject published between 1997]] and [[2008]] was analyzed and [[classified]] into four [[categories of financial fraud]] ([[bank fraud]], [[insurance fraud]], [[securiti]]es and [[commodities fraud]], and other related [[financial fraud]]) and six [[classes of data mining techniques (classification]], [[regression]], [[clustering]], [[prediction]], [[outlier detection]], and [[visualization]]).
4) A context that contains [[statement]]s relating the [[concept]] to other [[concept]]s in [[the ontology]],
(4) Highly efficient [[data structures]] and [[sparse representation]]s of [[model]]s and [[data]] to enable [[fast]] [[model]] [[update]]s.
[6] provides a [[list]] of usual [[task]]s on [[bibliographi]]es for different classes of [[user]]s.
[[802.11]], [[wireless network]]s, [[mobile system]]s, [[physical localization]], [[probabilistic analysis]]
:(a) A [[public]] or [[private organization]] located in a [[country]] other than the [[United States]] and its [[territories]] that are subject to the [[law]]s of the [[country]] in which it is located, irrespective of the [[citizenship]] of [[project staff]] or [[place of performance]];<br>
:(a) A strong commitment by [[Federal agency]] and [[non-Federal entity]] [[leadership]] to [[program]] integrity;<br>
A basic job of [[computer ethics]] is to identify these [[policy need]]s, clarify related [[conceptual confusion]]s, formulate appropriate new [[polici]]es, and [[ethically justify]] [[them]] .
A [[benchmark]] is only a [[good tool]] for [[evaluating a system]] if the [[benchmark dataset]] and the [[workload]] are similar to the ones expected in the target [[use case]] ([[Gray, 1993]]; [[Yuanbo Guo et al, 2007]]).
A best [[approximation]] on such [[data]] has a [[minimum set]] of [[inconsistent entries]], i.e., [[mismatches]] between the given [[binary data]] and the [[approximate]] [[matrix]] .
A [[Beta Video Game]] is a [[video game]] that is a [[Beta]] [[software]] .
A better [[understanding]] of their [[progression]] is instrumental in early [[diagnosis]] and [[personalized care]] .
A [[Bibliographic Citation Description Webpage]] is a [[Webpage]] that [[Describe]]s a specific [[Publication]] .
A [[block matrix]] is a [[matrix]] whose entries are themselves [[matrice]]s.
A [[Boolean matrix]] can be expressed as a [[product]] of [[two]] [[Boolean matrice]]s, where the first [[matrix]] [[represents]] a [[set]] of meaningful [[concepts]], and the second [[describes]] how the [[observed data]] can be expressed as [[combinations]] of those [[concepts]] .
A [[bottleneck]] of existing [[approach]]es is that [[implicit]] or [[explicit assessment]]s on [[concept]]s of [[distance]] or [[nearest neighbor]] are deteriorated in [[high-dimensional data]] .
About [[half a billion]] [[years ago]], [[life on earth]] experienced a [[short period]] of very [[rapid diversification]] called the "[[Cambrian Explosion]] ."
About [[one-third]] of [[young New York City worker]]s in [[low-wage industri]]es have a [[bachelor’s degree]] .
A [[broad latent query aspect]] is a set of [[keywords]] that [[succinctly represents]] one particular [[sense]], or one particular [[information need]], that can aid [[users]] in reformulating such [[queri]]es.
[[Absolute Penalty Estimation]], [[Accelerated Lifetime Testing]], [[Acceptance Sampling]], [[Actuarial Method]]s, [[Adaptive Linear Regression]], [[Adaptive Method]]s, [[Adaptive Sampling]], [[Advantages of Bayesian Structuring: Estimating Ranks and Histograms]], [[African Population Censuses]], [[Aggregation Schemes]], [[Agriculture, Statistics in]], [[Akaike’s Information Criterion]], [[Akaike’s Information Criterion: Background, Derivation, Properties, and Refinements]], [[Algebraic Statistics]], [[Almost Sure Convergence of Random Variables]], [[Analysis of Areal and Spatial Interaction Data]], [[Analysis of Covariance]], [[Analysis of Multivariate Agricultural Data]], [[Analysis of Variance]], [[Analysis of Variance Model, Effects of Departures from]], [[Assumptions Underlying]], [[Anderson-Darling Tests of Goodness-of-Fit]], [[Approximations for Densities of Sufficient Estimators]], [[Approximations to Distributions]], [[Association Measures for Nominal Categorical Variables]], [[Astrostatistics]], [[Asymptotic Normality]], [[Asymptotic Relative Efficiency in Estimation]], [[Asymptotic Relative Efficiency in Testing]], [[Asymptotic, Higher Order]], [[Autocorrelation in Regression]], [[Axioms of Probability]], [[Balanced Sampling]], [[Banking, Statistics in]], [[Bartlett and Bartlett-Type Corrections]], [[Bartlett’s Test]], [[Bayes’Theorem]], [[Bayesian Analysis or Evidence Based Statistics?]], [[Bayesian Approach of the Unit Root Test]], [[Bayesian Nonparametric Statistics]], [[Bayesian P-Values]], [[Bayesian Reliability Modeling]], [[Bayesian Semiparametric Regression]], [[Bayesian Statistics]], [[Bayesian Versus Frequentist Statistical Reasoning]], [[Bayesian vs. Classical Point Estimation: A Comparative Overview]], [[Behrens–Fisher Problem]], [[Best Linear Unbiased Estimation in Linear Models]], [[Beta Distribution]], [[Bias Analysis]], [[Bias Correction]], [[Binomial Distribution]], [[Bioinformatics]], [[Biopharmaceutical Research, Statistics in]], [[Biostatistics]], [[Bivariate Distributions]], [[Bootstrap Asymptotics]], [[Bootstrap Method]]s, [[Borel–Cantelli Lemma and Its Generalizations]], [[Box–Cox Transformation]], [[Box–Jenkins Time Series Models]], [[Brownian Motion and Diffusions]], [[Business Forecasting Method]]s, [[Business Intelligence]], [[Business Statistics]], [[Business Surveys]], [[Calibration]], [[Canonical Analysis and Measures of Association]], [[Canonical Correlation Analysis]], [[Careers in Statistics]], [[Case-Control Studies]], [[Categorical Data Analysis]], [[Causal Diagrams]], [[Causation and Causal Inference]], [[Censoring Methodology]], [[Census]], [[Central Limit Theorems]], [[Chaotic Modelling]], [[Characteristic Functions]], [[Chebyshev’s Inequality]], [[Chemometrics]], [[Chernoff Bound]], [[Chernoff Face]]s, [[Chernoff-Savage Theorem]], [[Chi-Square Distribution]], [[Chi-Square Goodness-of-Fit Tests: Drawbacks and Improvements]], [[Chi-Square Test: Analysis of Contingency Tables]], [[Chi-Square Test]]s, [[Clinical Trials, History of]], [[Clinical Trials: An Overview]], [[Clinical Trials: Some Aspects of Public Interest]], [[Cluster Analysis: An Introduction]], [[Cluster Sampling]], [[Coefficient of Variation]], [[Collapsibility]], [[Comparability of Statistics]], [[Complier-Average Causal Effect (CACE) Estimation]], [[Components of Statistics]], [[Composite Indicators]], [[Computational Statistics]], [[Conditional Expectation and Probability]], [[Confidence Distributions]], [[Confidence Interval]], [[Confounding and Confounder Control]], [[Contagious Distributions]], [[Continuity Correction]], [[Control Charts]], [[Convergence of Random Variables]], [[Cook’s Distance]], [[Copulas]], [[Copulas in Finance]], [[Copulas: Distribution Functions and Simulation]], [[Cornish-Fisher Expansions]], [[Correlation Coefficient]], [[Correspondence Analysis]], [[Cp Statistic]], [[Cramér–Rao Inequality]], [[Cramér-VonMises Statistics for Discrete Distributions]], [[Cross Classified and Multiple Membership Multilevel Models]], [[Cross-Covariance Operators]], [[Data Analysis]], [[Data Depth]], [[Data Mining]], [[Data Mining Time Series Data]], [[Data Privacy and Confidentiality]], [[Data Quality (Poor Quality Data: The Fly in the Data Analytics Ointment)]], [[Decision Theory: An Introduction]], [[Decision Theory: An Overview]], [[Decision Trees for the Teaching of Statistical Estimation]], [[Degradation Models in Reliability and Survival Analysis]], [[Degrees of Freedom]], [[Degrees of Freedom in Statistical Inference]], [[Demographic Analysis: A Stochastic Approach]], [[Demography]], [[Density Ratio Model]], [[Design for Six Sigma]], [[Design of Experiments: A Pattern of Progress]], [[Designs for Generalized Linear Models]], [[Detecting Outliers in Time Series Using Simulation]], [[Detection of Turning Points in Business Cycles]], [[Dickey-Fuller Test]]s, [[Discriminant Analysis: An Overview]], [[Discriminant Analysis: Issues and Problems]], [[Dispersion Models]], [[Distance Measures]], [[Distance Sampling]], [[Distributions of Order K]], [[Diversity]], [[Divisible Statistics]], [[Dummy Variables]], [[Durbin–Watson Test]], [[Econometrics]], [[Econometrics: A Failed Science?]], [[Economic Growth andWell-Being: Statistical Perspective]], [[Economic Statistics]], [[Edgeworth Expansion]], [[Effect Modification and Biological Interaction]], [[Effect Size]], [[Eigenvalue, Eigenvector and Eigenspace]], [[Empirical Likelihood Approach to Inference from Sample Survey Data]], [[Empirical Processes]], [[Entropy]], [[Entropy and Cross Entropy as Diversity and Distance Measures]], [[EnvironmentalMonitoring, Statistics Role in]], [[Equivalence Testing]], [[Ergodic Theorem]], [[Erlang’s Formulas]], [[Estimation]], [[Estimation Problems for Random Fields]], [[Estimation: An Overview]], [[Eurostat]], [[Event History Analysis]], [[Exact Goodness-of-Fit Tests Based on Sufficiency]], [[Exact Inference for Categorical Data]], [[Exchangeability]], [[Expected Value]], [[Experimental Design: An Introduction]], [[Expert Systems]], [[Explaining Paradoxes in Nonparametric Statistics]], [[Exploratory Data Analysis]], [[Exponential and Holt-Winters Smoothing]], [[Exponential Family Models]], [[Extreme Value Distributions]], [[Extremes of Gaussian Processes]], [[F Distribution]], [[Factor Analysis and Latent Variable Modelling]], [[Factorial Experiments]], [[False Discovery Rate]], [[Farmer Participatory Research Designs]], [[Federal Statistics in the United States, Some Challenges]], [[Fiducial Inference]], [[Financial Return Distributions]], [[First Exit Time Problem]], [[First-Hitting-Time Based Threshold Regression]], [[Fisher Exact Test]], [[Fisher-Tippett Theorem]], [[Five-Number Summaries]], [[Forecasting Principles]], [[Forecasting with ARIMA Processes]], [[Forecasting: An Overview]], [[Forensic DNA: Statistics in]], [[Foundations of Probability]], [[Frailty Model]], [[Fraud in Statistics]], [[Frequentist Hypothesis Testing: A Defense]], [[Full Bayesian Significant Test (FBST)]], [[Functional Data Analysis]], [[Functional Derivatives in Statistics: Asymptotics and Robustness]], [[Fuzzy Logic in Statistical Data Analysis]], [[Fuzzy Set Theory and Probability Theory:What is the Relationship?]], [[Fuzzy Sets: An Introduction]], [[Gamma Distribution]], [[Gaussian Processes]], [[Gauss-Markov Theorem]], [[General Linear Models]], [[Generalized Extreme Value Family of Probability Distributions]], [[Generalized Hyperbolic Distributions]], [[Generalized Linear Models]], [[Generalized Quasi-Likelihood (GQL) Inference]]s, [[Generalized Rayleigh Distribution]], [[GeneralizedWeibull Distributions]], [[Geometric and Negative Binomial Distributions]], [[Geometric Mean]], [[Geostatistics and Kriging Predictors]], [[Glivenko-Cantelli Theorems]], [[Graphical Analysis of Variance]], [[Graphical Markov Models]], [[Handling with Missing Observations in Simple Random Sampling and Ranked Set Sampling]], [[Harmonic Mean]], [[Hazard Ratio Estimator]], [[Hazard Regression Models]], [[Heavy-Tailed Distributions]], [[Heteroscedastic Time Series]], [[Heteroscedasticity]], [[Hierarchical Clustering]], [[Hodges-Lehmann Estimators]], [[Horvitz–Thompson Estimator]], [[Hotelling’s T. Statistic]], [[Hyperbolic Secant Distributions and Generalizations]], [[Hypergeometric Distribution and Its Application in Statistics]], [[Identifiability]], [[Imprecise Probability]], [[Imprecise Reliability]], [[Imputation]], [[Incomplete Block Designs]], [[IncompleteData in Clinical and Epidemiological Studies]], [[Index Numbers]], [[Industrial Statistics]], [[Inference Under Informative Probability Sampling]], [[Influential Observations]], [[Information Theory and Statistics]], [[Instrumental Variables]], [[Insurance, Statistics in]], [[Integrated Statistical Databases]], [[Interaction]], [[Interactive and Dynamic Statistical Graphics]], [[Internet Survey Methodology: Recent Trends and]], [[Developments]], [[Intervention Analysis in Time Series]], [[Intraclass Correlation Coefficient]], [[Inverse Gaussian Distribution]], [[Inverse Sampling]], [[Inversion of Bayes’ Formula for Events]], [[Itô Integral]], [[Jackknife]], [[James-Stein Estimator]], [[Jarque-Bera Test]], [[Jump Regression Analysis]], [[Kalman Filtering]], [[Kaplan-Meier Estimator]], [[Kappa Coefficient of Agreement]], [[Kendall’s Tau]], [[Khmaladze Transformation]], [[Kolmogorov-Smirnov Test]], [[Kullback-Leibler Divergence]], [[Kurtosis: An Overview]], [[Large Deviations and Applications]], [[Laws of Large Numbers]], [[Learning Statistics in a Foreign Language]], [[Least Absolute Residuals Procedure]], [[Least Squares]], [[Lévy Processes]], [[Life Expectancy]], [[Life Table]], [[Likelihood]], [[Limit Theorems of Probability Theory]], [[Linear Mixed Models]], [[Linear Regression Models]], [[Local Asymptotic Mixed Normal Family]], [[Location-Scale Distributions]], [[Logistic Normal Distribution]], [[Logistic Regression]], [[Logistic Distribution]], [[Lorenz Curve]], [[Loss Function]], [[Margin of Error]], [[Marginal Probability: Its Use in Bayesian Statistics as]], [[Model Evidence]], [[Marine Research, Statistics in]], [[Markov Chain Monte Carlo]], [[Markov Chains]], [[Markov Processes]], [[Martingale Central Limit Theorem]], [[Martingales]], [[Mathematical and Statistical Modeling of Global Warming]], [[Maximum Entropy Method for Estimation of Missing Data]], [[Mean Median and Mode]], [[Mean, Median, Mode: An Introduction]], [[Mean Residual Life]], [[Measure Theory in Probability]], [[Measurement Error Models]], [[Measurement of Economic Progress]], [[Measurement of Uncertainty]], [[Measures of Agreement]], [[Measures of Dependence]], [[Median Filters and Extensions]], [[Medical Research, Statistics in]], [[Medical Statistics]], [[Meta-Analysis]], [[Method Comparison Studies]], [[Methods of Moments Estimation]], [[Minimum Variance Unbiased]], [[Misuse and Misunderstandings of Statistics]], [[Misuse of Statistics]], [[Mixed Membership Models]], [[Mixture Models]], [[Model Selection]], [[Model-Based Geostatistics]], [[Modeling Count Data]], [[Modeling Randomness Using System Dynamics Concepts]], [[Modeling Survival Data]], [[Models for Z+-Valued Time Series Based on Thinning]], [[Moderate Deviations]], [[Moderating and Mediating Variables in Psychological Research]], [[Moment Generating Function]], [[Monte Carlo Methods in Statistics]], [[Monty Hall Problem: Solution]], [[Mood Test]], [[Most Powerful Test]], [[Moving Averages]], [[Multicollinearity]], [[Multicriteria Clustering]], [[Multicriteria Decision Analysis]], [[Multidimensional Scaling]], [[Multidimensional Scaling: An Introduction]], [[Multilevel Analysis]], [[Multinomial Distribution]], [[Multi-Party Inference and Uncongeniality]], [[Multiple Comparison]], [[Multiple Comparisons Testing from a Bayesian Perspective]], [[Multiple Imputation]], [[Multiple Statistical Decision Theory]], [[Multistage Sampling]], [[Multivariable Fractional Polynomial Models]], [[Multivariate Analysis of Variance (MANOVA)]], [[Multivariate Data Analysis: An Overview]], [[Multivariate Normal Distributions]], [[Multivariate Outliers]], [[Multivariate Rank Procedures: Perspectives and Prospectives]], [[Multivariate Reduced-Rank Regression]], [[Multivariate Statistical Analysis]], [[Multivariate Statistical Distributions]], [[Multivariate Statistical Process Control]], [[Multivariate Statistical Simulation]], [[Multivariate Technique: Robustness]], [[National Account Statistics]], [[Network Models in Probability and Statistics]], [[Network Sampling]], [[Neural Networks]], [[Neyman-Pearson Lemma]], [[Nonlinear Mixed EffectsModels]], [[Nonlinear Models]], [[Nonlinear Regression]], [[Nonlinear Time Series Analysis]], [[Nonparametric Density Estimation]], [[Nonparametric Estimation]], [[Nonparametric Estimation Based on Incomplete Observations]], [[NonparametricModels for ANOVA and ANCOVA Designs]], [[Nonparametric Predictive Inference]], [[Nonparametric Rank Test]]s, [[Nonparametric Regression Based on Ranks]], [[Nonparametric Regression Using Kernel and Spline Method]]s, [[Nonparametric Statistical Inference]], [[Non-probability Sampling Survey Method]]s, [[Nonresponse in Surveys]], [[Nonresponse in Web Surveys]], [[Nonsampling Errors in Surveys]], [[Non-UniformRandomVariate Generations]], [[Normal Distribution, Univariate]], [[Normal Scores]], [[Normality Test]]s, [[Normality Tests: Power Comparison]], [[Null-Hypothesis Significance Testing: Misconceptions]], [[Numerical Integration]], [[Numerical Methods for Stochastic Differential Equations]], [[Omnibus Test for Departures from Normality]], [[Online Statistics Education]], [[Optimal Designs for Estimating Slopes]], [[Optimal Regression Design]], [[Optimal Shrinkage Estimation]], [[Optimal Shrinkage Preliminary Test Estimation]], [[Optimal Statistical Inference in Financial Engineering]], [[Optimal Stopping Rules]], [[Optimality and Robustness in Statistical Forecasting]], [[Optimum Experimental Design]], [[Order Statistics]], [[Ordered Statistical Data: Recent Developments]], [[Outliers]], [[Panel Data]], [[Parametric and Nonparametric Reliability Analysis]], [[Parametric Versus Nonparametric Test]]s, [[Pareto Sampling]], [[Partial Least Squares Regression Versus Other Method]]s, [[Pattern Recognition, Aspects of]], [[Permanents in Probability Theory]], [[Permutation Test]]s, [[Pharmaceutical Statistics: Bioequivalence]], [[Philosophical Foundations of Statistics]], [[Philosophy of Probability]], [[Point Processes]], [[Poisson Distribution and Its Application in Statistics]], [[Poisson Processes]], [[Poisson Regression]], [[Population Projections]], [[Portfolio Theory]], [[Posterior Consistency in Bayesian Nonparametrics]], [[Power Analysis]], [[Preprocessing in Data Mining]], [[Presentation of Statistical Testimony]], [[Principal Component Analysis]], [[Principles Underlying Econometric Estimators for]], [[Identifying Causal Effects]], [[Prior Bayes: Rubin’s View of Statistics]], [[Probabilistic Network Models]], [[Probability on Compact Lie Groups]], [[Probability Theory: An Outline]], [[Probability, History of]], [[Probit Analysis]], [[Promoting, Fostering and Development of Statistics in]], [[Developing Countries]], [[Properties of Estimators]], [[Proportions, Inferences, and Comparisons]], [[Psychiatry, Statistics in]], [[Psychological Testing Theory]], [[Psychology, Statistics in]], [[Public Opinion Polls]], [[P-Values]], [[P-Values, Combining of]], [[Pyramid Schemes]], [[Quantitative Risk Management]], [[Questionnaire]], [[Queueing Theory]], [[R Language]], [[Radon–Nikodým Theorem]], [[Random Coefficient Models]], [[Random Field]], [[Random Matrix Theory]], [[Random Permutations and Partition Models]], [[Random Variable]], [[Random Walk]], [[Randomization]], [[Randomization Test]]s, [[Rank Transformations]], [[Ranked Set Sampling]], [[Ranking and Selection Procedures and Related]], [[Inference Problems]], [[Ranks]], [[Rao–Blackwell Theorem]], [[Rating Scales]], [[Record Statistics]], [[Recursive Partitioning]], [[Regression Diagnostics]], [[Regression Models with Increasing Numbers of Unknown Parameters]], [[Regression Models with Symmetrical Errors]], [[Relationship Between Statistical and Engineering]], [[Process Control]], [[Relationships Among Univariate Statistical Distributions]], [[Renewal Processes]], [[Repeated Measures]], [[Representative Samples]], [[Research Designs]], [[Residuals]], [[Response Surface Methodology]], [[Ridge and Surrogate Ridge Regressions]], [[Rise of Statistics in the Twenty First Century]], [[Risk Analysis]], [[Robust Inference]], [[Robust Regression Estimation in Generalized Linear]], [[Models]], [[Robust Statistical Method]]s, [[Robust Statistics]], [[ROC Curves]], [[Role of Statistics]], [[Role of Statistics in Advancing Quantitative Education]], [[Role of Statistics: Developing Country Perspective]], [[Rubin Causal Model]], [[Saddlepoint Approximations]], [[Sample Size Determination]], [[Sample Survey Method]]s, [[Sampling Algorithms]], [[Sampling Distribution]], [[Sampling From Finite Populations]], [[Sampling Problems for Stochastic Processes]], [[Scales of Measurement]], [[Scales of Measurement and Choice of Statistical Method]]s, [[Seasonal Integration and Cointegration in Economic Time Series]], [[Seasonality]], [[Selection of Appropriate Statistical Methods in Developing Countries]], [[Semiparametric Regression Models]], [[Semi-Variance in Finance]], [[Sensitivity Analysis]], [[Sensometrics]], [[Sequential Probability Ratio Test]], [[Sequential Ranks]], [[Sequential Sampling]], [[Sex Ratio at Birth]], [[Sign Test]], [[Significance Testing: An Overview]], [[Significance Tests, History and Logic of]], [[Significance Tests: A Critique]], [[Simes’Test in Multiple Testing]], [[Simple Linear Regression]], [[Simple Random Sample]], [[Simpson’s Paradox]], [[Simulation Based Bayes Procedures for Model]], [[Structures with Non-Elliptical Posteriors]], [[Singular Spectrum Analysis for Time Series]], [[SIPOC and COPIS: Business Flow–Business Optimization Connection in a Six Sigma Context]], [[Six Sigma]], [[Skewness]], [[Skew-Normal Distribution]], [[Skew-Symmetric Families of Distributions]], [[Small Area Estimation]], [[Smoothing Splines]], [[Smoothing Techniques]], [[Social Network Analysis]], [[Social Statistics]], [[Sociology, Statistics in]], [[Spatial Point Pattern]], [[Spatial Statistics]], [[Spectral Analysis]], [[Sport, Statistics in]], [[Spreadsheets in Statistics]], [[Spurious Correlation]], [[St. Petersburg Paradox]], [[Standard Deviation]], [[Statistical Analysis of Drug Release DataWithin the]], [[Pharmaceutical Science]]s, [[Statistical Analysis of Longitudinal and Correlated Data]], [[Statistical Approaches to Protecting Confidentiality in Public Use Data]], [[Statistical Aspects of Hurricane Modeling and Forecasting]], [[Statistical Consulting]], [[Statistical Design of Experiments (DOE)]], [[Statistical Distributions: An Overview]], [[Statistical Ecology]], [[Statistical Estimation of Actuarial Risk Measures for Heavy-Tailed ClaimAmounts]], [[Statistical Evidence]], [[Statistical Fallacies]], [[Statistical Fallacies: Misconceptions, and Myths]], [[Statistical Genetics]], [[Statistical Inference]], [[Statistical Inference for Quantum Systems]], [[Statistical Inference for Stochastic Processes]], [[Statistical Inference in Ecology]], [[Statistical Inference: An Overview]], [[Statistical Literacy, Reasoning, and Thinking]], [[Statistical Methods for Non-Precise Data]], [[Statistical Methods in Epidemiology]], [[Statistical Modeling of Financial Markets]], [[Statistical Modelling in Market Research]], [[Statistical Natural Language Processing]], [[Statistical Pattern Recognition Principles]], [[Statistical Publications, History of]], [[Statistical Quality Control]], [[Statistical Quality Control: Recent Advance]]s, [[Statistical Signal Processing]], [[Statistical Significance]], [[Statistical Software: An Overview]], [[Statistical View of Information Theory]], [[Statistics and Climate Change]], [[Statistics and Gambling]], [[Statistics and the Law]], [[Statistics Education]], [[Statistics of Extremes]], [[Statistics on Ranked Lists]], [[Statistics Targeted Clinical Trials Stratified and]], [[Personalized Medicines]], [[Statistics, History of]], [[Statistics: An Overview]], [[Statistics: Controversies in Practice]], [[Statistics: Nelder’s view]], [[Stem-and-Leaf Plot]], [[Step-Stress Accelerated Life Test]]s, [[Stochastic Difference Equations and Applications]], [[Stochastic Differential Equations]], [[Stochastic Global Optimization]], [[Stochastic Modeling, Recent Advances in]], [[Stochastic Modeling Analysis and Applications]], [[Stochastic Models of Transport Processes]], [[Stochastic Processes]], [[Stochastic Processes: Applications in Finance and]], [[Insurance]], [[Stochastic Processes: Classification]], [[Stratified Sampling]], [[Strong Approximations in Probability and Statistics]], [[Structural Equation Models]], [[Structural Time Series Models]], [[Student’s t-Distribution]], [[Student’s t-Test]]s, [[Sturges’and Scott’s Rules]], [[Sufficient Statistical Information]], [[Sufficient Statistics]], [[Summarizing Data with Boxplots]], [[Superpopulation Models in Survey Sampling]], [[Surveillance]], [[Survival Data]], [[Target Estimation: A New Approach to Parametric Estimation]], [[Telephone Sampling: Frames and Selection Techniques]], [[Testing Exponentiality of Distribution]], [[Testing Variance Components in Mixed Linear Models]], [[Tests for Discriminating Separate or Non-NestedModels]], [[Tests for Homogeneity of Variance]], [[Tests of Fit Based on The Empirical Distribution Function]], [[Tests of Independence]], [[Time Series]], [[Time Series Models to Determine the Death Rate of a Given Disease]], [[Time Series Regression]], [[Total Survey Error]], [[Tourism Statistics]], [[Trend Estimation]], [[Two-Stage Least Squares]], [[Unbiased Estimators and Their Applications]], [[Uniform Distribution in Statistics]], [[Uniform Experimental Design]], [[UniformRandomNumberGenerators]], [[Univariate Discrete Distributions: An Overview]], [[U-Statistics]], [[Validity of Scales]], [[Variables]], [[Variance]], [[Variation for Categorical Variables]], [[Vector Autoregressive Models]], [[Weak Convergence of Probability Measures]], [[Weibull Distribution]], [[Weighted Correlation]], [[Weighted U-Statistics]], [[Wilcoxon–Mann–Whitney Test]], [[Wilcoxon-Signed-Rank Test
[[Abstract Much]] of the [[world's electronic text]] is annotated with [[human-interpretable label]]s, such as [[tag]]s on [[web page]]s and subject codes on [[academic publication]]s.
[[A/B testing]]; [[continuous experimentation]]; [[data science]]; [[customer feedback]]; [[continuous product innovation]]; [[Experimentation Evolution Model]]; [[product value]]; [[Experiment Owner]]
[[A/B testing]] is a [[standard approach]] for evaluating the [[effect]] of [[online experiment]]s; the goal is to [[estimate]] the [[average treatment effect]] of a new [[feature]] or [[condition]] by exposing a [[sample]] of the [[overall population]] to [[it]] .
Abundant [[literature]] has been dedicated to this [[research]] and tremendous progress has been made, ranging from [[efficient]] and [[scalable algorithm]]s for [[frequent itemset mining]] in [[transaction databases]] to numerous [[research frontiers]], such as [[sequential pattern mining]], [[structured pattern mining]], [[correlation mining]], [[associative classification]], and [[frequent pattern-based clustering]], as well as their broad [[applications]] .
A [[business transaction]] is an [[event]] that has a [[direct effect]] on the [[operation]] of an [[economic unit]], is expressed in [[terms of money]], and is [[recorded]] .
[[Access]] to [[publication]]s is provided by [[conventional librari]]es, [[digital librari]]es operated by [[learned societi]]es or [[commercial publisher]]s, and a [[huge number]] of [[web site]]s maintained by the [[scientists themselve]]s or their [[institution]]s.
Accompanying [[William]] and his wife, [[Mary]], from the [[Netherlands]] to [[London]] was the [[philosopher]] [[John Locke]], whose <i>[[Second Treatise on Government]]</i> enunciated the [[principle]] that [[obedience to rule]] should rest on the [[consent]] of [[the governed]] .
Accordingly, [[we]] propose a [[practical solution]] based on the [[iterative substitution]] to jointly [[optimize]] the identification of the [[mapping entiti]]es for the [[Web list item]]s.
Accordingly, [[we]] [[revamp]] two leading [[collaborative filtering recommendation approaches]] .
Accordingly, we [[train]] the generator to [[synthesize sentence]]s which [[confuse the ranker]] so that [[machine-written sentence]]s are [[ranked higher]] than [[human-written sentence]]s in regard to the [[reference]] .
According to [[my definition]], a [[number]] is [[computable]] if its [[decimal]] can be written down by a [[machine]] .
According to the [[user]] [[choice]]s, the [[system]] discards or retains certain [[hypotheses]] on actual [[execution]]s and shows the [[consequent scenario]]s resulting from the corresponding [[re-aggregation]] of the actual [[data]] .
; [[Account]]: A separate [[financial reporting unit]] for [[budget]], [[management]], and/or [[accounting]] [[purpose]]s.
[[Accounting]] is the [[process]] of [[analyzing]], [[classifying]], [[recording]], [[summarizing]], and [[interpreting]] [[business transaction]]s in [[financial or monetary terms]] .
[[Account]]s: [[Accounting entiti]]es by which [[MMARS]] records the status of [[expenditure authorization]]s and [[revenue estimate]]s.
[[Account type]]: There are six main [[account grouping]]s for [[expenditure account]]s, which are designated in [[MMARS]] by the [[indicated code]]s: [[direct appropriation]] (1CN, 1CS), [[retained revenue]] (1RN, 1RS), [[capital]] (2CN), [[trust / other]] (3TN, 3TX), [[federal grant]] (4FN), and [[intragovernmental service]]s (1IN, 1IS).
Accurately [[capturing user preference]]s over [[time]] is a great practical [[challenge]] in [[recommender systems]] .
Accurately predicting [[patient]]'s [[risk-of-readmission]] enables [[care-provider]]s to plan [[resource]]s, perform [[factor analysis]], and improve [[patient]] [[quality of life]] .
[[Accurate prediction]] of [[stroke]] is highly valuable for [[early intervention]] and [[treatment]] .
[[Accurate prediction]] of [[user behavior]]s is important for many [[social media application]]s, including [[social marketing]], [[personalization]] and [[recommendation]], etc. A major [[challenge]] lies in that, the available [[behavior data]] or [[interaction]]s between [[user]]s and [[item]]s in a given [[social network]] are usually very limited and [[sparse]] (e.g., >= 99.9% [[empty]]).
[[Accurate]] [[risk model]]s for [[adverse outcome]]s can provide important input to [[clinical decision-making]] .
A central [[biological question]] is how [[natural organism]]s are so [[evolvable]] ([[capable]] of quickly [[adapting to new environments]]).
A central challenge to many [[fields of science]] and [[engineering]] involves [[minimizing]] [[non-convex error function]]s over [[continuous, high dimensional space]]s.
A central issue in [[performance display advertising]] is [[matching campaign]]s to ad [[impression]]s, which can be formulated as a [[constrained optimization problem]] that [[maximize]]s [[revenue subject]] to [[constraint]]s such as [[budget limit]]s and [[inventory availability]] .
A [[class]] is a set of [[entities]], that are [[instance]]s of that [[class]] (one [[entity]] can be [[instance]] of multiple [[class]]es).
A [[cluster randomization trial]] is one in which intact [[social unit]]s, or [[clusters of individuals]], are [[randomized]] to different [[intervention group]]s.
A [[collaborating]] [[negative correlation]] is a [[negative correlation]] between [[two]] [[sets of variables]] rather than traditionally between a [[pair of variables]] .
A common [[pitfall]] in [[testing]] is to [[check input]] and [[output]] from only a [[single system]] .
A [[common task]] of [[recommender system]]s is to improve [[customer experience]] through [[personalized recommendation]]s based on prior [[implicit feedback]] .
A comparison to baseline [[methods]] shows that the proposed [[algorithm]] [[detects]] [[segments]] from irregular [[states]] with significantly [[high]] [[precision]] and recall.
A [[comparison]] was made of [[vector]]s derived by using ordinary [[co-occurrence statistics]] from [[large text corpora]] and of [[vector]]s derived by measuring the [[interword distance]]s in [[dictionary definition]]s.
A [[compiler]] is a [[software program]] that [[translate]]s a [[high-level source language program]] into a [[form]] ready to [[execute on a computer]] .
A [[complete implementation]] of [[our proposal]] for different [[dialog system]]s and its [[evaluation]] are also detailed.
A comprehensive [[experimental study]] on a [[large collection]] of [[PE file]]s obtained from the [[client]]s of [[anti-malware product]]s of [[Comodo Security Solutions Incorporation]] is performed to compare various [[malware detection approach]]es.
A comprehensive [[set of experiment]]s on [[multiple dataset]]s at three different [[sparsity level]]s indicate that [[the proposed method]]s can handle [[sparse dataset]]s effectively and [[outperform]]s other [[state-of-the-art]] <i>[[top-N recommendation method]]</i>s.
A [[computer]] [[rooted]] to one place is doomed to static iterations, whereas a [[machine]] on the [[prowl]], like a [[mobile organism]], must evolve a richer [[fund of knowledge]] about an [[ever-changing world]] upon which to [[base]] its [[action]]s.
A [[Concept Description Page]] is a [[Webpage]] that [[Describe]]s a specific [[Concept]] .
A [[contestant]] in the [[Winograd Schema Challenge]] is presented with a [[collection]] of one [[sentence]] from each [[pair]], and required to achieve [[human-level accuracy]] in choosing the [[correct disambiguation]] .
A [[co-occurrence matrix]] was built from [[entity pair]]s and [[sequence]]s.
A [[coordinate descent algorithm]] is proposed to improve [[community matching]] and [[outlier detection performance]] [[iteratively]] .
A [[critical component]] for such a [[problem]] is the [[similarity]]/[[metric function]] to compare the [[data sequence]]s.
A critical [[evaluation]] of an existing [[twitter dataset]], as well as the [[D2W task]] in general, provides intuition that [[tweet context expansion]] based on both [[authorship]] and [[TextRank]] based [[clustering]] may enhance the [[disambiguation context]] and improve [[D2W result]]s.
A critical [[problem]] in a [[crisis situation]] is how to [[efficiently]] [[discover]], [[collect]], [[organize]], [[search]] and [[disseminate]] [[real-time]] [[disaster information]] .
A [[critical problem]] in a [[recruiting system]] is how to [[maximally]] satisfy the [[desire]]s of both [[job seeker]]s and [[enterprise]]s with reasonable [[recommendation]]s or [[search result]]s.
A [[cross-bin distance measure]] like [[EMD]] is not affected by [[binning]] [[difference]]s and meaningfully [[match]]es the [[perceptual notion]] of "[[nearness]] ".
[[Active]] and [[semi-supervised learning]] are important [[technique]]s when [[labeled data]] are scarce.
[[Active inference]], [[label acquisition]], [[collective classification]], [[viral marketing]], [[information diffusion]]
[[Active learning (AL)]] is an increasingly popular [[strategy]] for [[mitigating]] the [[amount]] of [[labeled data]] required to [[train classifiers]], thereby reducing [[annotator effort]] .
[[Active learning]] has been proven to be [[effective]] in [[reducing labeling effort]]s for [[supervised learning]] .
[[Active Learning]] is a [[machine learning]] and [[data mining technique]] that selects the most [[informative sample]]s for [[labeling]] and uses them as [[training data]]; it is especially useful when there are [[large amount]] of [[unlabeled data]] and [[labeling]] them is expensive.
[[Active learning]], [[machine learning]], [[class imbalance]], [[human resources]], [[on-line advertising]], [[micro-outsourcing]] .
[[Active Learning]], [[Statistical Relational Learning]], [[Semisupervised Learning]], [[Social Network Analysis]], [[Betweenness Centrality]], [[Closeness Centrality]], [[Community Finding]], [[Clustering]], [[Empirical Risk Minimization]], [[Within-Network Learning]]
[[Active Learning]], [[Text Classification]], [[Multi-label Classification]], [[Support Vector Machine]]s
[[Active research]] has been conducted on [[knowledge discovery in databases]] by the [[researchers]] in our [[group]] for years, with many interesting [[results published]] and a [[prototyped]] [[knowledge discovery system]], [[DBMiner]] (previously called [[DBLearn]]), developed and demonstrated in several [[conference]]s.
[[Active search]] has [[model learning]] and [[exploration]] / [[exploitation feature]]s similar to those encountered in [[active learning]] and [[bandit problem]]s, but [[algorithm]]s for those [[problem]]s do not fit [[active search]] .
[[Activity analysis]] [[disaggregate]]s [[utility consumption]] from [[smart meter]]s into specific usage that associates with [[human activiti]]es.
[[actor]]s, [[plot]], [[visual effect]]s), and expressing [[their sentiment]]s ([[positive or negative]]) on these aspects in their [[review]]s.
[[Acute hospital care]] as performed in the [[intensive care unit (ICU)]] is characterized by its [[frequent]], but [[short-term intervention]]s for [[patient]]s who are [[severely ill]] .
A [[Cyber-Physical System (CPS)]] integrates [[physical]] (i.e., [[sensor]]) devices with [[cyber]] (i.e., [[informational]]) [[component]]s to form a [[context sensitive system]] that responds intelligently to [[dynamic change]]s in [[real-world situation]]s.
[[Adam]] has [[autonomously generated]] [[functional genomics]] [[hypotheses]] about the [[yeast]] [[Saccharomyces cerevisiae]] and experimentally tested these hypotheses by using [[laboratory automation]] .
[[Adaptive resonance theory (ART)]], [[clustering]], [[clustering algorithm]], [[cluster validation]], [[neural networks]], [[proximity]], [[self-organizing feature map (SOFM)]] .
A [[dataset]] is [[imbalanced]] if the [[classification categori]]es are not [[approximately]] [[equally represented]] .
Additionally, [[author]]s have proposed many [[definition]]s for an [[outlier]] with seemingly no [[universally accepted definition]] .
Additionally, a variety of opposing [[allocation type]]s or [[position]]s may be available with which one can [[hedge]] the [[allocation]] to [[alleviate risk]] from [[external change]]s.
Additionally, [[knowledge]] appears in diverse [[representation]]s, such as [[textual]] and [[tabular data]], but also, for example, as [[explicit rule]]s.
Additionally, [[MINDS]] achieves a [[speed-up]] of at least four [[orders of magnitude]] over [[baseline technique]]s.
Additionally, the [[Deep GA]] parallelizes better than [[ES]], [[A3C]], and [[DQN]], and enables a [[state-of-the-art]] [[compact encoding technique]] that can represent [[million-parameter DNN]]s in thousands of [[byte]]s.
Additionally, [[we]] describe the [[process]] of [[dictionary creation]], and our use of [[Mechanical Turk]] to [[check dictionari]]es for [[consistency]] and [[reliability]] .
Additionally, [[we]] develop a [[generative model]] able to [[simulate]] observed [[user activity]] by utilizing a mixture of [[pareto distributions]] .
Additionally, [[we]] explore the [[properti]]es on [[occupancy]] to further prune the [[search space]] for [[high-efficient pattern mining]] .
Additionally, [[we]] perform an [[in-depth analysis]] of the <i>[[annotated interesting response]]s</i>, shedding light on the [[subjectivity]] around the [[selection process]] and the [[perception of interestingness]] .
Additionally, [[we]] propose a new [[family]] of [[algorithm]]s we call [[group boosting]], as an improved [[component]] of [[grouped graphical Granger modeling]] over the existing [[regression method]]s with [[grouped variable selection]] in the literature (e.g [[group]] [[Lasso]]).
Additionally, [[we]] prove that [[tractability]] is lost when [[symmetric role]]s are added: in this case, [[subsumption]] becomes [[ExpTime-hard]] .
Additionally, [[we]] show that [[advertiser]]s can potentially increase their [[return on investment]] significantly by utilizing [[bid increment]]s for [[keyword profile]]s in their [[ad campaign]]s.
Additionally [[we]] use the [[Merchant]], [[Offering Feature]], and [[Function Term]] and avoid the use of a less informative [[NA]] .
Additional tests on [[simulated network]]s show that [[the model]]s [[learned]] by [[GrowCode]] [[generate]] [[distributions of graph]]s with similar [[variance]] as existing [[model]]s for these [[class]]es.
[[Address standardization]] is a very challenging [[task]] in [[data cleansing]] .
Add to that the [[Internet]], [[tiny]], [[low cost]] [[electronic circuitry]], radical advances in [[materials science]] and [[biotech]] and voila!
[[Ad effectiveness]], [[display ads]], [[doubly robust estimation]], [[irrelevant outcome]]s, [[observational data]], [[propensity score]], [[selection bias]]
A [[definitive diagnosis]] of [[AD]] requires [[autopsy confirmation]], thus many [[clinical]] / [[cognitive measure]]s including [[Mini Mental State Examination (MMSE)]] and [[Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-Cog)]] have been designed to [[evaluate]] the [[cognitive status]] of the [[patient]]s and used as important [[criteria]] for [[clinical diagnosis]] of [[probable AD]] .
A [[dialogue]] is a [[sequence of utterances]], produced by [[agent]]s based on their [[belief]]s, in order to reach intended [[states of facts]] by causing other [[agent]]s to change their [[belief]]s and thus also their [[intention]]s.
A [[discriminative model]] is then [[learned]] to [[optimize]] the [[feature weight]]s as well as the [[bandwidth]] of a [[Nadaraya-Watson kernel density estimator]] .
A [[discussion]] of [[this vocabulary]] along with [[RDF example]]s from the [[case studi]]es, appears as [[OWL Vocabulary]] and [[RDF Example]]s in Additional file 1.
A [[discussion]] with [[Pete Hunt]], [[Paul O'Shannessy]], [[Dave Smith]], and [[Terry Coatta]]
[[AD]] is known to be [[heterogeneous]] in the sense that multiple [[set]]s of [[ROI]]s may be related to the [[disease]] in a [[population]] .
A distinctive feature of [[the proposed model]] is that the [[preferential attachment]] of [[item]]s to [[cluster]]s according to [[cluster size]]s, present in [[Dirichlet process]]es, is now driven according to the intensities of [[cluster-wise]] [[self-exciting]] [[temporal point process]]es, the [[Hawkes process]]es.
Adopting a [[psycholinguistic]] approach, this [[thesis]] examines the use of the [[computer]] in [[schools]] as a [[testing tool]], specifically for [[measuring]] [[reading comprehension]] .
Adopting the [[abstraction]] of [[Nash equilibrium point]]s as [[cluster]]s allows for flexible [[definition]] of [[reward function]]s that characterize [[cluster]]s without any modification to the [[underlying algorithm]] .
[[ad selection]], [[ad serving]], [[graph algorithm]]s, [[online advertising]]
[[Ad serving system]]s that assign [[ads]] to [[page]]s on behalf of [[publisher]]s must satisfy these [[contract]]s, but at the same time try to [[maximize]] overall [[quality]] of [[placement]] .
[[Advanced]] and [[effective]] [[collaborative filtering method]]s based on [[explicit feedback]] assume that [[unknown rating]]s do not follow the same [[model]] as the [[observed one]]s (<i>[[not missing at random]]</i>).
Advanced [[manufacturing]] such as [[aerospace]], [[semi-conductor]], and [[flat display device]] often involves [[complex]] [[production process]]es, and [[generate]]s [[large volume]] of [[production data]] .
Advanced technology in [[GPS]] and [[sensor]]s enables us to [[track]] [[physical event]]s, such as [[human movement]]s and [[facility usage]] .
[[Advance]]s in [[artificial intelligence]] and [[robotics]] will have significant implications for evolving [[economic system]]s.
Advances in [[biotechnology]] have made available [[multitude]]s of [[heterogeneous proteomic]] and [[genomic data]] .
[[Advance]]s in [[data collection]] and [[storage capacity]] have made it increasingly possible to collect highly [[volatile]] [[graph data]] for [[analysis]] .
Advancing [[previous work]], [[we]] incorporate the [[mechanism]] of [[trust propagation into the model]] .
[[Advertiser]]s and [[advertising network]]s (hereafter, [[advertiser]]s) would like a [[rating system]] that [[estimate]]s whether a [[web page]] or [[web site]] [[display]]s certain [[objectionable content]] .
[[Advertiser]]s in [[NGD]] sell a [[large portion]] of their ad [[campaign]]s using [[performance dependent pricing model]]s such as [[cost-per-click (CPC)]] and [[cost-per-action (CPA)]] .
[[Advertising]], [[Advertising Response]], [[Data Mining]], [[Modeling Media Consumption]] .
[[Advertising compani]]es [[bid]] for these [[spot]]s on behalf of a [[company]] or [[brand]] in order to purchase these [[spot]]s to display [[banner advertisement]]s.
[[Advertising]], [[R&D]], [[new-product development]], [[training]], [[knowledge creation]], [[software project]]s, and so forth are almost always expensed on a "What can we afford?" basis.
A [[earch engine]] with a lower [[pSkip]] is regarded as having a better [[ranking]] [[quality]] .
[[AESOP]] leverages [[locality-sensitive hashing]] to [[measure]] the strength of these [[inter-file relationship]]s to construct a [[graph]], on which it performs [[large scale inference]] by [[propagating information]] from the [[labeled file]]s (as [[benign or malicious]]) to the [[preponderance of unlabeled file]]s.
A [[feature selection algorithm]] should ideally satisfy four [[condition]]s: [[reliably extract relevant feature]]s; be able to identify [[non-linear feature interaction]]s; [[scale linearly]] with the number of [[feature]]s and [[dimension]]s; allow the [[incorporation]] of known [[sparsity structure]] .
A few [[recent work]]s focus on [[this task]], while almost all the [[method]]s need that each [[event]] to be [[recommended]] should have been [[register]]ed by some [[user]]s to [[attend]] .
[[Affinity Propagation]] is a [[clustering algorithm]] used in many [[application]]s.
A [[Finite Value]] is a [[numerical value]] that is [[real number]] or [[natural number]] .
A [[first-order knowledge base (KB)]] is a set of [[sentence]]s or [[formula]]s in [[first-order logic]] [37].
A first [[set of]] [[unsupervised]], [[pattern-based]] [[clustering experiment]]s was performed to [[detect semantic relations]] .
A focus on [[employment relations]] forms the foundation of [[theori]]es of the [[institution]]s and [[structure]]s that generate [[precarious work]] and the [[cultural]] and [[individual factor]]s that influence [[people's response]]s to [[uncertainty]] .
:::(a) For [[grant]]s and [[cooperative agreement]]s, [[Federal Financial Assistance]] means [[assistance]] that [[non-Federal entities]] receive or administer in the form of:<br>
:(a) For [[grant]]s and [[cooperative agreement]]s, [[Federal Financial Assistance]] means [[assistance]] that [[non-Federal entities]] receive or administer in the form of:<br>
A [[formulation]] of [[learning]] for [[ranking creation]] is given in the [[statistical learning framework]] .
A [[four-factor structure]] -- [[work/employment]], [[patriotic/religion-based belief system]], [[specific social-moral]], and [[independent courage or family based]] -- was found.
After analyzing the way traditional [[latent variable model]]s ([[LSA]], [[PLSA]] / [[LDA]]) handle missing words, [[we]] decide to [[model sentence]]s using a [[weighted matrix factorization approach]] ([[Srebro and Jaakkola, 2003]]), which allows us to treat observed [[word]]s and missing words differently.
After assigning an [[element]] to a [[cluster]], [[we]] remove their [[overlapping]] [[features]] from the [[element]] .
After a [[successful]] [[repair]] has been [[discovered]], it is minimized using [[structural differencing algorithm]]s and [[delta debugging]] .
After describing the [[model]] in detail, [[we]] evaluate it [[qualitatively]] by analyzing the [[learned n-gram vector representation]]s and compare [[quantitatively]] against other methods on [[standard dataset]]s and the [[EP dataset]] .
:: After [[fitting]] ([[training]]), the model can [[predict]] [[label]]s for new samples:
After his departure from [[Buffalo]], [[Saban]] returned to [[coach]] [[college football team]]s including [[Miami]], [[Army]] and [[UCF]] .
After providing an overview of the [[mobile]] and [[social media]] landscapes in [[emerging market]]s, [[we]] discuss a [[system]] that [[implements poll]]s & [[mobile subscription compensation]] .
After reviewing the [[empirical data]] that motivated the recent interest in [[networks]], [[the authors]] discuss the main [[models]] and [[analytical tools]], covering [[random graphs]], [[small-world]] and [[scale-free networks]], the emerging [[theory of evolving networks]] [[, and the interplay between [[topology]] and the [[network]]’s [[robustness]] against [[failures]] and [[attacks]] .
After surveying prior work, [[we]] present [[NetKit]], a [[modular toolkit]] for [[classification in networked data]], and a [[case-study]] of its application to [[networked data]] used in prior [[machine learning research]] .
[[After tax rate of return]] versus [[growth rate]] at the [[world level]], from [[Antiquity]] until [[2100]]
[[After tax rate of return]] versus [[growth rate]] at the [[world level]], from [[Antiquity]] until 2200
After [[text pre-processing]] and [[application]] of appropriate [[distance metric]]s, we apply different [[graph-theoretic approach]]es to [[cluster the alert]]s and [[incident ticket]]s, based on their [[semi-structured]] and [[unstructured text]] respectively.
After that, for each [[thread]], [[we]] employ a [[regression model]] to [[predict]] the [[time]] when the next [[post]] arrives.
After that, we further modify [[PRUB]] to propose a [[heuristic]], [[Algorithm PRUB + IF]], for obtaining [[feasible]] [[solution]]s more [[efficiently]] on [[larger instance]]s.
After that, [[we]] refine the [[workflow]] by verifying it with all [[training]] [[event trace]]s.
After the initial [[set-up phase]], [[the algorithm]]s do not need any further [[communication]] or [[synchronization]] .
After training [[our model]], [[we]] [[extract]] and [[evaluate]] our [[vector representation}vector]]s with [[linear model]]s on 8 [[task]]s: [[semantic relatedness]], [[paraphrase detection]], [[image-sentence ranking]], [[question-type classification]] and 4 [[benchmark sentiment]] and [[subjectivity dataset]]s.
[[After training]], [[TM-LDA]] is thus able to accurately [[predict]] the [[expected topic distribution]] in [[future post]]s.
After two [[downturn]]s and subsequent [[recoveri]]es that failed to reach down the [[economic ladder]], the [[number of people]] living below the [[federal]] [[poverty line]] ($23,492 for a [[family of four]] in [[2012]]) remains stubbornly stuck at [[record level]]s.
Afterwards, the [[bottleneck layer]] and an additional [[layer]] are added and the whole [[network]] is fine-tuned to [[predict]] [[target]] [[phoneme state]]s.
A [[Function Format]] is a [[Software Function]] that transforms [[data]] to a specific [[format]]
A fundamental [[data-analysis task]] for these [[dataset]]s is <i>[[matrix completion]] </i>, where the [[goal]] is to [[accurately infer]] the [[entries]] missing from the [[matrix]] .
Again, ranking of [[features]] with popular [[feature ranking algorithm]]s shows that a [[small]] [[subset]] of [[features]] always plays a significant role in [[link prediction]] .
A [[game]] is a [[system]] in which [[player]]s engage in an [[artificial conflict]], defined by [[rules]], that results in a [[quantifiable outcome]] .
A [[Gated Recurrent Unit (GRU) Network]] is a [[Recurrent Neural Network]] with [[Gated Recurrent Unit]]s (a [[reset gate]] and an [[update gate]]) but no [[output gate]]).
A [[gazetteer]] is a [[list of names]] belonging to a particular category, such as [[place]]s, [[persons]], [[companies]], [[genes]], and so on.
A [[general ontological model]] for typical [[indoor environment]]s has been first developed, followed by [[its]] [[specialization]] using an [[implementation perspective]] .
[[Aggregate number]]s and [[physical quantiti]]es are [[trusted]], while local [[knowledge]]s and [[experiences of scarcity]] that [[identify problem]]s more accurately and specifically are ignored.
[[Aggregating estimate]]s at the [[village level]], [[we]] produce a [[ranked list]] from which [[top village]]s are included in [[GiveDirectly]]'s [[planned distribution]] of [[cash transfer]]s.
[[aggregation]], [[parallelization]], [[spatial classification]], [[spatial data mining]]
[[Aggregator website]]s typically present [[document]]s in the form of [[representative cluster]]s.
A given [[entity]] – representing a [[person]], a [[location]] or an [[organization]] – may be [[mention]]ed in [[text]] in multiple, ambiguous ways.
A [[Glycosidic Bond]] is a [[molecular bond]] that joins [[carbohydrate molecule]] to another [[molecular group]] .
A [[good example]] is a [[web-blogging community]] with the [[daily]] [[blog posts]] and a [[social network]] of [[bloggers]] .
A good way to alleviate [[email overload]] is to [[automatic]]ally [[prioritize]] received [[messages]] according to the [[priorities]] of each [[user]] .
A grand [[challenge]] in [[machine learning]] is the development of [[computational algorithm]]s that [[match]] or [[outperform]] [[human]]s in [[perceptual inference task]]s such as [[visual object]] and [[speech recognition]] .
A growing [[body of work]] has used [[seed expansion]] as a [[scalable]] means of [[detecting]] [[overlapping communiti]]es.
A growing body of works address [[automated mining]] of [[biochemical knowledge]] from [[digital repositories]] of [[scientific literature]], such as [[MEDLINE]] .
A hallmark of the [[new economy]] is the ability of [[organization]]s to realize [[economic value]] from their [[collection]] of [[knowledge asset]]s as well as their [[assets of information]], [[production distribution]], and [[affiliation]] .
A [[hazard function]], commonly used in [[statistics]] for [[survival analysis]], is used to [[capture]] the [[rate]] at which a [[user returns to an artist]] as a [[function]] of [[exposure to the artist]] .
A [[heterogeneous information network]] is an [[information network]] composed of multiple [[type]]s of [[object]]s.
A [[hidden-state Conditional Random Field framework]] [[learns]] a [[set]] of [[latent variables]] [[conditioned]] on [[local features]] .
A high [[bounce rate]] can lead to poor [[advertiser]] [[return on investment]], and suggests [[search engine]] [[user]]s may be having a poor [[experience]] following the [[click]] .
A [[highly nonlinear]] [[multi-layered]] [[embedding function]] is used to capture the [[complex interaction]]s between the [[heterogeneous data]] in a [[network]] .
A [[high quality]] [[hierarchical organization]] of the [[concept]]s in a [[dataset]] at different [[level]]s of [[granularity]] has many valuable [[application]]s such as [[search]], [[summarization]], and [[content browsing]] .
A [[human]] [[manually annotates examples]] of a [[word]] with [[tags]] that indicates the [[intended sense]] in each [[context]] .
:: A [[hypothesis test]] examines two opposing [[hypotheses]] about a [[population]]: the [[null hypothesis]] and the [[alternative hypothesis]] .
A [[hypothetical]] [[skid resistance]] / [[crash]] [[risk curve]] is developed for each [[road segment]], driven by the [[model deployed]] in a novel [[regression tree extrapolation method]] .
:: [[AIC]] is the [[Akaike information criterion]] and [[BIC]] is the [[Bayes Information criterion]] .
A <i>[[discourse]]</i> is considered to be a [[coherent]] [[sequence]] of [[clauses]], [[sentence]]s or [[propositions]] .
:(a) [[Improper payment]] means any [[payment]] that should not have been made or that was made in an incorrect amount (including [[overpayment]]s and [[underpayment]]s) under [[statutory]], [[contractual]], [[administrative]], or other legally applicable [[requirement]]s; and<br>
(a) In [[social media]], the [[context]] of [[user action]]s is constantly changing and co-evolving; hence the [[social context]] contains [[time-evolving]] [[multi-dimensional]] [[relations]] .
A <i>[[number]]</i> may be [[nought]], or may be [[one more]] than a [[number]] .
:(a) Is operated primarily for [[scientific]], [[educational]], [[service]], [[charitable]], or similar purposes in the [[public interest]];<br>
:::(a) Is used to enter into a [[relationship]] the [[principal]] purpose of which is to [[transfer]] anything of [[value]] from the [[Federal awarding agency]] or [[pass-through entity]] to the [[non-Federal entity]] to carry out a [[public]] purpose authorized by a [[law]] of the [[United States]] (see 31 U.S.C. 6101(3)); and not to [[acquire]] [[property]] or [[service]]s for the [[Federal awarding agency]] or [[pass-through entity]]'s direct [[benefit]] or use;<br>
:(a) Is used to enter into a [[relationship]] the [[principal]] purpose of which is to [[transfer]] anything of [[value]] from the [[Federal awarding agency]] or [[pass-through entity]] to the [[non-Federal entity]] to carry out a [[public]] purpose authorized by a [[law]] of the [[United States]] (see 31 U.S.C. 6101(3)); and not to [[acquire]] [[property]] or [[service]]s for the [[Federal awarding agency]] or [[pass-through entity]]'s direct [[benefit]] or use;<br>
[[AI system]]s promise to help make sense of several new types of [[data]]: measurements from the 'omics' such as [[genomics]], [[proteomics]] and [[metabolomics]]; [[electronic health record]]s; and [[digital-sensor monitoring]] of [[health sign]]s.
A key characteristic of [[durable good]]s is the [[long duration of time]] between successive [[purchase]]s within [[item categori]]es whereas this duration for [[nondurable good]]s is much [[shorter]], or even [[negligible]] .
A key design [[criterion]] is <i>[[rapid exploration]]</i> of [[model]]s, hence [[the system]] is [[interactiv]]e and primarily [[single-user]] .
A key [[design]] goal of [[pSkip]] is to integrate the findings from two [[set]]s of [[user]] [[studies]] that utilize [[eye-tracking device]]s to track [[user]]s [[browsing]] [[pattern]]s on the [[search result page]]s, and that use specially instrumented [[browser]]s to [[actively solicit]] [[user]]s [[explicit]] [[judgment]]s on their [[search activities]] .
A key observation of [[the paper]] is that [[journalist]]s and [[expert commentator]]s overstate the extent of [[machine substitution for human labor]] and ignore the [[strong]] [[complementariti]]es.
A key property of [[our approach]] is the ability to [[efficiently reason]] about the [[long-term value]] of [[exploration]], which enables collectively balancing the [[exploration]] /[[exploitation trade-off]] for entire [[fleets of vehicle]]s.
A key subproblem in the [[construction]] of [[location-aware system]]s is the [[determination]] of the [[position]] of a [[mobile device]] .
A key [[task]] in [[analyzing]] [[social network]]s and other [[complex network]]s is [[role analysis]]: describing and [[categorizing node]]s by how they [[interact with]] other [[node]]s.
A [[knowledge base (KB)]] is a [[collection]] of [[class]]es, [[individual]]s, [[frame]]s, [[slot]]s, [[slot value]]s, [[facet]]s, [[facet value]]s, [[frame-slot association]]s, and [[frame-slot-facet association]]s.
:(a) [[Land]], [[building]]s ([[facilities]]), [[equipment]], and [[intellectual property]] (including [[software]]) whether [[acquired]] by [[purchase]], [[construction]], [[manufacture]], [[lease-purchase]], [[exchange]], or through [[capital lease]]s; and<br>
[[Alan Turing]] first addressed the issue of [[computational intelligence]] in an [[unpublished]] [[1948]] [[paper]] titled "[[Intelligent Machinery]]", where he argued that machines can achieve intelligence.
A [[large literature]] has explored the [[link]]s between [[culture]] and [[human progress]] from various [[perspective]]s.
A [[large service provider]] needs to handle, on a [[daily basis]], [[thousands]] of [[tickets]] that report various [[types of problems]] .
A [[large source collection]] of [[newswire]] and [[web document]]s is provided from which [[system]]s are to [[discover information]] .
[[ALE]] presents significant research challenges for [[reinforcement learning]], [[model learning]], [[model-based planning]], [[imitation learning]], [[transfer learning]], and [[intrinsic motivation]] .
[[Alert]]s [[fire automatically]] when [[experiment]]s hurt the [[user experience]], or [[interact with]] other [[experiment]]s.
A [[lexeme]] pairs a particular [[spelling]] or [[pronunciation]] with a particular [[meaning]], i.e., a [[lexeme]] is a [[conjunction]] of a [[word]] and a [[synset]] .
A [[lexicon]] is a [[source of information]] about different forms or [[grammatical properti]]es of [[word]]s.
A [[lexicon]] organizes words as a conventional [[inventory of concept]]s, while an [[ontology]] [[formalize]]s [[concept]]s and their [[logical relation]]s.
[[Algorithm]] [[Hydra]] captures the [[user]] [[perception]] of [[diverse]] [[similarity]] [[notion]]s from millions of [[Web page]]s and [[disambiguates]] different senses using [[feature]]-based [[subspace]] [[locality measure]]s.
[[Algorithm]]s and representation]]s for [[dynamic graph processing]] provide the ability to [[scale]] as needed for [[enterprise-level deployment]]s on [[real-time data stream]]s.
[[Algorithm]]s based on [[simulating]] [[stochastic]] [[flow]]s are a [[simple]] and [[natural solution]] for the [[problem of clustering graphs]], but their widespread use has been hampered by their lack of [[scalability]] and [[fragmentation]] of [[output]] .
[[Algorithms based on upper confidence bounds]] for balancing [[exploration]] and [[exploitation]] are gaining popularity since they are [[easy to implement]], [[efficient]] and [[effective]] .
[[algorithms]]; [[design]]; [[information extraction]]; [[natural language processing]]; [[relation extraction]]; [[text mining]] .
A [[linear path]] refers to a [[path]] [[represented by]] a [[line]] with a single [[dimensional]] [[spatial]] [[coordinate]] marking an [[observation point]] .
A [[literature survey]] shows that the [[interaction]]s identified by [[FHIM]] play important [[role]]s in [[cancer development]] .
All [[algorithm]]s are compared in terms of [[computational complexity]] by [[experiment]]s on [[large data set]]s.
All [[classes]] are [[contiguous sequences of words]] where [[local context]] usually contains enough [[information]] to [[identify]] the [[term]] as a [[class member]] .
All [[methods]] are evaluated using [[accuracy]] and [[F-measure]] on a [[set]] of [[benchmark]] [[data sets]] .
All [[method]]s work on [[coreset]]s of [[closed subgraph]]s, [[compressed representation]]s of [[graph set]]s, and maintain these [[set]]s in a [[batch-incremental manner]], but use different approaches to address [[potential concept drift]] .
All [[Netflix Prize]] [[algorithm]]s proposed so far are [[prohibitively costly]] for [[large-scale]] [[production system]]s.
[[Allocation]] of indirect [[cost]]s also has significant implications for [[understanding]] and comparing the [[cost]] of [[local government service delivery]] .
[[Allocation]] of very scarce [[medical intervention]]s such as [[organ]]s and [[vaccine]]s is a persistent [[ethical challenge]] .
[[Allomorph]]s are formally analyzed here as [[ordered triple]]s, consisting of the [[surface]], the [[category]] and the [[semantic representation]] .
All [[prediction]]s were [[supported]], although the [[pattern of effects]] differed for [[positive]] and [[negative event]]s.
All these [[optimization methods]] are [[iterative procedures]], which generate a [[sequence]] <math>\{\mathbf{w}^k\}^\infty_{k=1}</math> [[converging]] to the [[optimal solution]] of (2).
All these [[pattern]]s violate fundamental [[assumption]]s of traditional [[recommendation system]]s.
All these [[strategies]] are based on repeated [[acquisition]]s of additional [[training examples]] until a [[utility]] [[decrease]] is [[observed]] .
All three are provided by [[Robohub.org]], of which [[I]] am [[co-founder]] and [[president]] .
All [[topic]]s are [[copiously illustrated]] with [[color image]]s and [[worked example]]s drawn from such [[application domain]]s as [[biology]], [[text processing]], [[computer vision]], and [[robotic]]s.
Almost every modern [[social problem]] - [[poor health]], [[violence]], [[lack of community life]], [[teen pregnancy]], [[mental illness]] - is [[more likely to]] occur in a [[less-equal society]] .
A [[longstanding and well-established fact]] in [[labor economics]] is that the [[labor supply]] of [[prime-age]] and [[older adult]]s has been essentially [[acyclical]] throughout the [[postwar period]], while that of [[teenager]]s has been moderately [[procyclical]]; cf.
A long-standing [[goal]] of [[AI]] is to build an [[autonomous agent]] that can [[read]] and [[understand text]] .
A [[long-standing goal]] of [[artificial intelligence]] is an [[algorithm that learns, tabula rasa]], [[superhuman proficiency]] in [[challenging domain]]s.
Along this line, [[we]] develop [[two]] [[cost-aware latent factor model]]s to [[recommend travel package]]s by considering both the [[travel cost]] and the [[tourist's interest]]s.
A [[loss function]] that is [[convex]] has [[nice mathematical properti]]es.
A lot of different [[text clustering algorithm]]s have been proposed in the literature, including [[Scatter/Gather]] ([[Cutting et al., 2002]]), [[SuffixTree Clustering]] ([[Zamir & Etzioni, 1998]]) and [[bisecting k-means]] ([[Steinbach et al.,2000]]).
[[AlphaGo]] becomes its own teacher: a [[neural network]] is trained to [[predict]] [[AlphaGo’]]s own [[move selection]]s and also the [[winner]] of [[AlphaGo’s game]]s.
Also, if one of [[the graph]]s is [[anonymized]], how can [[we]] use [[information]] in one to [[de-anonymize]] the other?
Also inspired by [[our elementary view]], [[we]] propose [[modeling]] <i> [[intermediate orders of interaction]] </i>, such as 1.5-way [[FM]]s.
Also, [[SQL]] — the [[lingua franca]] of [[relational database]]s — is commonly known among [[programmer]]s.
Also, the [[analytics]] [[needs]] to be <i>[[repeatable]]</i>, in that the delivered [[insights]] should not depend heavily on the [[expertise]] of specific [[analysts]] .
Also, the [[conductance]] of [[neighborhood communiti]]es shows [[similar behavior]] as the [[network community profile]] [[computed]] with a [[personalized PageRank community detection method]] .
Also, [[we]] illustrate some interesting [[phenotype]]s derived from the [[data]] .
Also, [[we]] introduce both [[absolute]] and [[relative error bound]]s for our [[approximation]] to guarantee the [[reliability]] of our [[estimation algorithm]] .
Also, [[we]] use a [[linear model]] to fuse these three [[influential factor]]s and [[predict]] [[estate investment value]]s.
Also, [[we]] use these [[performance model]]s to build a [[scalability optimizer]] that [[efficiently]] determines the [[optimal system configuration]] that [[minimize]]s [[DNN training time]] .
[[Alternative Clustering]], [[Information Theoretic Learning]], [[Parzen-window technique]]
Although [[active learning]] presents an attractive solution to [[this problem]], [[previous approach]]es [[minimize]] the [[misclassification rate]] ([[0-1 loss]]) of the [[classifier]], which is an unsuitable [[metric]] for [[entity matching]] due to [[class imbalance]] (i.e., many more [[non-duplicate]] [[pair]]s than [[duplicate pair]]s).
Although an [[ensemble]] is usually more [[accurate]] than a [[single learner]], existing [[ensemble methods]] often tend to construct unnecessarily [[large]] [[ensembles]], which increases the [[memory consumption]] and [[computational cost]] .
Although a significant part of current [[text mining effort]]s focuses on the [[analysis]] of [[document]]s related to [[molecular biology]], the use of [[lexical]], [[terminological]] and [[ontological resource]]s is mentioned in [[research system]]s developed for the analysis of [[clinical narrative]]s (e.g., [[MedSyndikate]] [4]) or the [[biological literature]] (e.g., [[BioRAT]] [5], [[GeneScene]] [6], [[EMPathIE]] [7] and [[PASTA]] [7]).
Although being fast, [[easy-to-implement]], and [[parameter free]], [[this algorithm]] tends to produce a relatively [[large number]] of [[cluster]]s.
Although by their [[nature]], [[frequent itemset]]s are [[aggregate]]s over many [[individual]]s and would not seem to pose a [[privacy threat]], an [[attacker]] with strong [[background information]] can [[learn private individual information]] from [[frequent itemset]]s.
Although [[corporate residence fraud]] is known to be a major [[factor]], [[data availability]] and [[high sensitivity]] have caused this [[domain]] to be [[largely untouched]] by [[academia]] .
Although current [[graph model]]s can capture several important [[characteristic]]s of [[social network graph]]s (e.g., [[degree]], [[path length]]s), many of them do not [[generate]] [[graph]]s with sufficient <i>[[variation]]</i> to reflect the [[natural variability]] in [[real world graph domain]]s.
Although [[deep learning]] has historical roots going back decades, neither the [[term "deep learning"]] nor the [[approach]] was popular just over five years ago, when [[the field]] was reignited by papers such as [[Krizhevsky]], [[Sutskever]] and [[Hinton]]'s now classic ([[2012]]) [[deep network model]] of [[Imagenet]] .
Although [[distributed graph engine]]s such as [[GBase]] and [[Pregel]] handle [[billion-scale]] [[graph]]s, the [[user]] needs to be skilled at [[managing]] and [[tuning]] a [[distributed system]] in a [[cluster]], which is a [[nontrivial job]] for the [[ordinary user]] .
Although effective and superior in many ways, [[we]] also report a [[series]] of [[experiments]] that show [[pSkip]] may be influenced by [[system]] issues that are not directly [[related]] to [[relevance]] [[ranking]], suggesting that [[measurement]]s complementary to [[pSkip]] are still needed in order to form a [[holistic]] and [[accurate]] [[characterization]] of the [[ranking]] [[quality]] .
Although [[efficient]], [[convex model]]s usually deliver [[sub-optimal performance]] while [[nonconvex model]]s on the other hand require significantly more [[computational effort]] .
Although frequent enough to worry [[advertisers]], [[objectionable categories]] are [[rare]] in the [[distribution]] of [[impressions]] encountered by most [[on-line advertisers]] -- so [[rare]] that traditional [[sampling technique]]s do not find enough [[positive examples]] to train [[effective models]] .
Although [[hashing technique]]s have been popular for the [[large scale]] [[similarity search problem]], most of the [[existing method]]s for designing [[optimal hash function]]s focus on [[homogeneous similarity assessment]], i.e., the [[data entiti]]es to be [[indexed]] are of the same [[type]] .
… Although [[Java]] is the primary [[language]] used with [[Lucene]], the [[index]] [[format]] is language netural.
Although many [[methods]] have been [[developed]] to [[identify]] [[statistically significant]] [[anomalies]], a more [[difficult task]] is to [[identify anomalies]] that are both [[interesting]] and [[statistically significant]] .
Although [[millennials]] entered the [[labor market]] at various stages of the [[recession]] and the [[recovery]], the [[decline]] in [[average real earnings held]] for every age between 18 and 29, suggesting a fundamental [[deterioration]] of the [[earnings opportunities]] available to [[young workers]] in [[New York City]] .
Although most [[hypotheses]] assume [[indirect selection]] for [[evolvability]], here [[we]] demonstrate that the ubiquitous, [[direct selection pressure]] to [[reduce the cost of connection]]s between [[network node]]s causes the emergence of [[modular network]]s.
Although [[non-statistician]]s will often refer to the “[[likelihood of an observation]]”, in [[statistics]], [[we]] use the term “[[likelihood]]” to refer to a [[model]] or a [[distribution given observations]] .
Although not written as an [[intellectual autobiography]], [[Killing Time]] sketches the [[people]], [[idea]]s, and [[conflict]]s of sixty year]]s.
Although [[previous studi]]es have addressed the [[problem of task identification]], little is known about the advantage of using [[task]] over [[session]] and [[query]] for [[search application]]s.
Although [[relationships]] between [[entities]] are [[rich]], it is impractical to [[manually]] devise [[complete characterizations]] of these [[relationships]] for every [[pair]] of [[entities]] on [[large]], [[real-world]] [[corpora]] .
Although several previous [[studi]]es have addressed [[information network clustering]], [[shortcoming]]s remain.
Although [[tagging]] has become increasingly popular in [[online image]] and [[video sharing systems]], [[tags]] are known to be [[noisy]], [[ambiguous]], [[incomplete]] and [[subjective]] .
Although the [[low dimensional representations]] [[learned]] are applicable to many different [[task]]s, they are not particularly [[tuned]] for any [[task]] .
Although the problem is [[NP-hard]], [[we]] are able to [[identify case]]s where there exist [[efficient algorithm]]s with [[provable approximation guarantee]]s.
Although the role of [[culture]] in [[economic progress]] is [[unquestioned]], [[interpreting]] this role in the context of other [[influence]]s and isolating the [[independent influence]] of [[culture]] is challenging.
Although the [[SDP formulation]] enjoys attractive [[theoretical properti]]es with respect to [[global optimization]], it is [[computationally intractable]] for [[large problem size]]s.
Although these [[rule]]s can be rationally designed or negotiated, [[norm-following behavior]] is usually grounded not in [[reason]] but in [[emotion]]s like [[pride]], [[guilt]], [[anger]], and [[shame]] .
Although the underlying [[numerical problem]] is a [[linear program]], [[traditional]] [[algorithm]]s are known to suffer poor [[scalability]] for [[large-scale application]]s.
Although this [[optimization problem]] is [[intractable]] in general, [[we]] develop effective [[relaxation]]s that can be efficiently solved by existing [[method]]s.
Although [[this system]] again uses a [[reference set]] for [[extraction]], [[it]] differs in that it does a [[keyword look-up]] into the [[lexicon]] .
Although [[this term]] is appearing quite often in [[popular]] and [[technical article]]s describing [[application]]s such as [[electronic mail]], [[multimedia distributed system]]s, and [[electronic office document]]s, the distinction between [[filtering]] and related [[process]]es such as [[retrieval]], [[routing]], [[categorization]], and [[extraction]] is often not clear.
Although [[users]] of [[online communication tools]] rarely [[categorize]] their [[contacts]] into [[groups]] such as "[[family]]", "[[co-workers]]", or "[[jogging buddies]]", they nonetheless [[implicitly]] [[cluster]] [[contacts]], by virtue of their [[interactions]] with them, forming [[implicit]] [[groups]] .
Although [[we]] show the [[RACP problem]] to be [[NP-hard]], [[we]] propose a general and [[efficient algorithmic solution]] .
A major contributor to evolvability is the fact that many [[biological entiti]]es are [[modular]], especially the many [[biological process]]es and [[structure]]s that can be [[modelled as networks]], such as [[metabolic pathway]]s, [[gene regulation]], [[protein interaction]]s and [[animal brain]]s [1–7].
A major limitation of [[previous work]] on [[LARA]] is the [[assumption]] of [[pre-specified aspect]]s by [[keyword]]s.
A major [[source]] of [[information]] (often the most crucial and informative part) in [[scholarly article]]s from [[scientific journal]]s, [[proceeding]]s and [[book]]s are the [[figure]]s that directly provide [[image]]s and other [[graphical illustration]]s of key [[experimental result]]s and other [[scientific]] [[content]]s.
A major [[source of revenue]] shrink in [[retail store]]s is the [[intentional]] or [[unintentional failure]] of proper [[checking out of item]]s by the [[cashier]] .
A [[managed]] [[analytics service]] is centered around a [[business analyst]] who acts as a [[liaison]] between the [[business]] and the [[technology]] .
A [[many-to-many situation]] can be broken down into two [[one-to-many situation]]s with the [[insertion]] of an "[[intersection table]] ", and the [[one-to-one]] is a [[simplification]] of the [[one-to-many]] .
A [[<math>\sigma-algebra</math>]] is an [[algebra]] that is [[closed under union countable]].)
[[Ambiguous queri]]es, which are typical on [[search engine]]s and [[recommendation system]]s, often return a [[large number]] of [[result]]s from multiple [[interpretation]]s.
A [[memex]] is a [[device]] in which an [[individual]] stores all his [[book]]s, [[record]]s, and [[communication]]s, and which is mechanized so that it may be consulted with exceeding [[speed]] and [[flexibility]] .
[[AMM model]] consists of a [[set]] of [[hyperplane]]s ([[weight]]s), each assigned to one of the multiple [[class]]es, and [[predict]]s based on the associated [[class]] of the [[weight]] that provides the largest [[prediction]] .
A [[mobile social network]] plays an essential [[role]] as the [[spread of information]] and [[influence]] in the form of "[[word-of-mouth]]".
A [[model]] is [[fit]] and a [[parameter estimated]], or a [[decision is made under uncertainty]], and the result is [[communicated]] to the relevant [[stakeholder]]s.
Among [[collaborative recommendation approach]]es, [[methods based on nearest-neighbors]] still enjoy a huge amount of [[popularity]], due to their [[simplicity]], their [[efficiency]], and their ability to produce [[accurate]] and [[personalized recommendation]]s.
[[Among communiti]]es, [[we]] notice that some of them are <i>[[magnetic]]</i> to [[people]] .
Among other [[task]]s, [[matrix factorization]]s are often used to separate [[global structure]] from [[noise]] .
Among [[sequence labeling tasks]] in [[language processing]], [[shallow parsing]] has received much attention, with the development of [[standard evaluation datasets]] and extensive [[comparison]] among [[methods]] .
Among the [[innovative feature]]s in [[this system]], we [[design]] [[COPE]] [[updating]] to combine the multiple [[elicited distribution]]s following a [[Bayesian scheme]] .
Among the [[query languages for RDF]], [[SPARQL]] allows querying [[RDF]] through [[graph pattern]]s, i.e., [[RDF graph]]s involving variables.
A more [[sensitive]] [[approach]] is required, which can make better [[distinctions]] between [[transient effects]] and [[long term]] [[patterns]] .
A [[multiword expression (MWE)]] is an [[expression]] for which the [[syntactic]] or [[semantic properties]] of the whole [[expression]] cannot be derived from its [[parts]] .
An [[accurate prediction]] of the [[probability]] that [[users click]] on [[ad]]s is a crucial [[task]] in [[NGD advertising]] because [[this value]] is required to compute the [[expected revenue]] .
An [[actor]] can [[connect]] to another [[actor]] for different reasons, e.g., [[alumni]], [[colleague]]s, living in the same [[city]], sharing similar interests, etc.
::# an adjective that points out which [[person]], [[object]] or [[concept]] is being [[referred]] to; whether it is [[singular]] or [[plural]]; and whether it is [[near]] or [[far]] from the [[speaker]] or [[writer]]
An advantage of developing [[application]]s in [[Fatafat]] is that it is already [[integrate]]d with [[Hadoop]], [[Kafka]] for [[real-time]] [[data streaming]], [[HBase]] and [[Cassandra]] for [[NoSQL]] [[data storage]] .
An [[affirmative answer]] would allow [[advertiser]]s and [[search engine]]s to [[predict]] the [[effectiveness]] and [[quality]] of [[advertisement]]s before they are [[shown]] .
An [[AKA]] [[Webpage]] is a [[Webpage]] that contains a [[Hyperlink]] to the [[Concept Description Page]]s that contain the term as an [[AKA]] .
[[Analogizer]]s learn by [[extrapolating]] from [[similarity judgment]]s and are influenced by [[psychology]] and [[mathematical optimization]] .
Analogously, [[we]] use [[median]](<math>x_t</math>) for the [[sample median]] and [[gmean]](<math>x_t</math>) for the [[geometric mean]] .
An alternative approach, arising from both branches of [[AI]] and from [[statistics]], combines the [[syntactic]] and [[semantic device]]s of [[logic]] ([[composable function symbol]]s, [[logical variable]]s, [[quantifier]]s) with the [[compositional semantics]] of [[Bayes net]]s.
An alternative way to deploy [[human resource]]s for [[training-data acquisition]] is to have them "[[guide]]" the [[learning]] by [[searching]] explicitly for [[training examples]] of each [[class]] .
[[Analysis of predictive language]] can provide rich [[portrait]]s of the [[mental life]] associated with [[trait]]s.
[[Analyzing functional interaction]]s between small [[compound]]s and [[protein]]s is indispensable in [[genomic drug discovery]] .
[[Analyzing]] [[large volume]] of [[patent data]] can uncover the potential [[competitive]] or [[collaborative relation]]s among companies in certain [[area]]s, which can provide valuable [[information]] to develop [[strategi]]es for [[intellectual property (IP)]], [[R&D]], and [[marketing]] .
[[Analyzing]] [[pairwise measurements]] with [[probabilistic model]]s requires special assumptions, since the usual [[independence]] or [[exchangeability assumption]]s no longer hold.
An [[analysis]] of [[millennials]] in [[New York City]] finds the [[generation]] that [[came of age]] as [[the nation]] began to shed nearly 9 million [[jobs]] faces the following obstacles to a bright [[economic future]]: [[higher]] [[unemployment]], [[lower]] [[real wage]]s, more [[low-wage job]]s, and increased [[under-employment]] .
An [[anomalous]] [[MRI image]] may indicate presence of [[malignant tumor]]s ([[Spence et al. 2001]]] .
An attempt to [[reduce false]] hits by [[indexing]] more [[feature]]s exacerbates the [[curse of dimensionality]], and [[vice versa]] .
A [[natural extension]] to [[relative comparison]]s is the combination of two [[comparison]]s defined on the same [[set]] of three [[instance]]s.
An [[author]] is [[represented by]] a [[probability distribution]] over [[topic]]s, and each [[topic]] is [[represented as]] a [[probability distribution]] over [[word]]s.
An [[author]] may have multiple [[names]] and multiple [[author]]s may [[share]] the same [[name]] simply due to [[name]] [[abbreviation]]s, [[identical]] [[name]]s, or [[name]] [[misspelling]]s in [[publication]]s or [[bibliographi]]es 1.
And [[I]] found my first [[PhD student]] through his [[nanomedicine]] [[blog]] .
And many more [[dreary]], [[annoying]], seemingly [[meaningless routine]]s besides.
And of course, [[inference]] and [[learning]] remain the major [[bottleneck]]s.
And since [[MLNs]] subsume [[first-order Bayesian network]]s, [[the article]]'s claim that [[MLNs]] have [[problem]]s with [[variable numbers of object]]s and [[irrelevant object]]s that [[Bayes-net approach]]es avoid is [[incorrect]] .
and the [[column vector]]s correspond to [[document]]s ([[web page]]s, for example).
And the so-called [[real world]] will not discourage you from operating on your [[default setting]]s, because the so-called [[real world]] of [[men]] and [[money]] and power hums merrily along in a pool of [[fear]] and [[anger]] and [[frustration]] and [[craving]] and [[worship of self]] .
An early [[SPARQL-specific performance benchmark]] is the [[DBpedia Benchmark]] ([[Becker, 2008]]).
An [[e-commerce catalog]] typically comprises of [[specification]]s for millions of [[product]]s.
An [[efficient algorithm]] is derived to alternatively [[optimize]] the [[model parameter]] and [[class probability distribution]] of the [[unlabeled data]], such that the [[induction of classifier]] and the [[transduction of labels]] are [[adaptively unified]] into [[one framework]] .
An [[efficient algorithm]] named [[TKU (Top-K Utility itemsets mining)]] is proposed for [[mining]] such [[itemset]]s without setting [[min_util]] .
An [[efficient algorithm]], [[USpan]], is presented to [[mine]] for [[high utility sequential pattern]]s.
An [[efficient]] [[optimization algorithm]] based on [[CCCP]] and the [[bundle method]] is proposed to solve the [[optimization problem]] of the large [[margin formulation]] of [[SL]] .
An [[efficient]] [[subgradient algorithm]] is developed to [[train]] the [[model]] by [[convert]]ing the original [[energy-based objective function]] into its [[dual form]] .
An [[empirical evaluation]] shows that the [[LME]] is substantially more [[accurate]] than [[adaptation]]s of [[smoothed n-gram model]]s commonly used in [[natural language processing]] .
An [[empirical study]] using [[real data]] of over two billion [[query]] [[sessions]] demonstrates the usefulness and [[feasibility]] of our [[design]] .
An [[ensemble]] is a [[set]] of [[learned models]] that make [[decisions]] [[collectively]] .
An [[evaluation]] of [[the system]] using a [[Turing-like test]] shows that the [[automatic annotation]]s are [[hardly distinguishable]] from [[manual annotation]]s.
An [[evaluation]] study on [[dataset]]s comprising up to four million [[graph]]s [[explore]]s the strength and [[limitation]]s of [[the proposed method]]s.
An evident trend is that [[researcher]]s in both "camps" view their [[computer learning environment]]s as "[[cognitive tool]]s" that can enhance [[learning]], [[performance]], and [[understanding]] .
A new form of [[document coupling]] called [[co-citation]] is defined as the [[frequency]] with which two [[document]]s are [[cited]] [[together]] .
An [[example]] could be a [[low-income person]] being [[friends]] with many [[rich people]] even though his [[income]] is not anomalously [[low]] when considered over the [[entire population]] .
An example of [[database design]] and [[description]] using [[the model]] and the [[diagrammatic technique]] is given.
An example that has drawn significant [[research interest]] is the [[cyber-security domain]], where [[internet communication trace]]s are collected and [[real-time discovery]] of [[event]]s, [[behaviors]], [[patterns]], and [[anomalies]] is desired.
An extensive [[empirical study]] is conducted on [[real patient data]] collected between [[2001]] and [[2008]] from a variety of [[ICU]]s.
An extensive [[empirical study]] on both [[real data]] and [[synthetic data]] verifies the [[effectiveness]] of [[our method]]s.
An extensive [[evaluation]] on [[real]] and [[synthetic database]]s attests high [[efficiency]] and [[accuracy]] of [[our proposal]] .
An extensive [[experimental evaluation]] on [[synthetic]] and [[real world data]] demonstrates the benefits of our [[technique]]s.
An extensive [[set]] of [[experimental result]]s on two [[synthetic dataset]]s and two [[dataset]]s from [[real world application]]s demonstrate the advantages of [[the proposed method]] over several other [[baseline algorithm]]s.
An extensive [[set of]] [[experiment]]s on various [[dataset]]s demonstrates that [[this approach]] significantly improve [[accuracy]] over [[prior work]] .
An [[extreme case]] is that the [[source domain]] is in [[English]] and the [[target domain]] is in [[Chinese]] .
An ideal [[outcome]] of [[pattern mining]] is a [[small set]] of [[informative pattern]]s, containing no [[redundancy]] or [[noise]], that identifies the key [[structure]] of the [[data]] at hand.
An important characteristic of [[our approach]] is to [[infer]] the [[network]] by [[embedding]] it into a [[low-dimensional visualization space]] .
An important [[component]] of such [[analysis]] is to [[characterize]] the [[sentiment]] expressed in [[blog]]s about specific [[brand]]s and [[products]] .
An important [[computational challenge]] is to analyze these [[data]], [[extract pattern]]s, and understand [[activity trend]]s.
An important [[problem]] in [[large-scale sensor mining]] is that of [[selecting]] relevant [[sensor]]s for [[prediction]] purposes.
An important [[problem]] in the [[non-contractual marketing domain]] is [[discovering]] the [[customer lifetime]] and assessing the impact of [[customer's characteristic variable]]s on the [[lifetime]] .
An important [[task of analyzing]] an [[online community]] is to [[observe]] and [[track]] the popular [[event]]s, or [[topics that evolve over time]] in the [[community]] .
• An [[information filtering system]] is an [[information system]] designed for [[unstructured]] or [[semistructured data]] .
An [[inhomogeneous Poisson process]] is used for [[modeling]] a [[sequence]], in which [[adoption]] by a [[user trigger]]s the subsequent [[adoption]] of the same [[item]] by other [[user]]s.
An interesting alternative to the ordinary [[autoencoder]] is the [[Denoising Auto-encoder]] ([[Vincent et al., 2008]]) or [[DAE]], in which the input [[vector]] x is [[stochastically corrupted]] into a [[vector]] ~x, and the model is [[trained to denoise]], i.e., to [[minimize]] a [[denoising reconstruction error loss]] (x; [[r (~x))]] .
An [[interpretation]] specifies which [[object]]s, [[function]]s and [[relation]]s in the domain are [[represented by]] which [[symbol]]s.
An [[itemset]] [[satisfi]]es [[minimum support]] if the [[occurrence frequency]] of the [[itemset]] is [[greater than or equal to]] the [[minimal support threshold value]] defined by the [[user]] .
An [[itemset]] that contains <math>k</math> [[item]]s is a [[<math>k</math>-itemset]] .
An [[iterative enhancement method]] is developed that leads to effective [[ranking]]-based [[clustering]] in such [[heterogeneous network]]s.
An m × n [[matrix]] can [[encode information]] about m [[object]]s (each described by n [[feature]]s), or the [[behavior]] of a [[discretized differential operator]] on a [[finite element mesh]]; an n × n [[positive-definite matrix]] can encode the [[correlation]]s between all [[pairs]] of n [[object]]s, or the [[edge-connectivity]] between all [[pairs of node]]s in a [[social network]]; and so on.
[[Annotation types]] cover [[gene names]], [[gene IDs]], [[species]], and [[protein-protein interactions]] .
[[Annualized cost/saving]]: The full [[twelve-month cost]] or [[saving]] of an [[item]] .
A [[node]] in the [[intermediate layer]] represents a [[region]] formed by [[voxel]]s in the [[subtree]] [[root]]ed at that [[node]] .
[[Anomali]]es in [[credit card transaction data]] could indicate [[credit card]] or [[identity theft]] ([[Aleskerov et al. 1997]]) or [[anomalous reading]]s from a [[space craft sensor]] could signify a [[fault]] in some component of the [[space craft]] ([[Fujimaki et al. 2005]]).
[[Anomalous windows]] are the [[contiguous groupings]] of [[data points]] .
[[Anomaly Detection]]; [[Classification with incomplete data]]; [[Cluster analysis and applications]]; [[Clustering and visualization]]; [[Machine learning applications]]; [[Machine learning models]]; [[Machine learning trends]]; [[Multi-objective optimization]]; [[Artificial Neural Networks]]; [[Principal graphs and manifolds]]
[[Anomaly detection]] finds extensive use in a wide variety of applications such as [[fraud detection]] for [[credit card]]s, [[insurance]] or [[health care]], [[intrusion detection]] for [[cyber-security]], [[fault detection]] in [[safety critical system]]s, and [[military surveillance]] for [[enemy activiti]]es.
[[Anomaly detection]] for [[discrete sequence]]s is a [[challenging task]], since it involves exploiting the [[sequential nature of data]] to [[detect]] [[anomali]]es.
An [[ontology]] can be [[defined]] as a [[formally specified model]] of [[bodies of knowledge]] [[defining]] the [[concepts]] used to describe a [[domain]] and the [[relations]] that hold between them [4].
An [[ontology]] is a [[formal description]] of [[concepts]] and [[relationships]] that can exist for a [[community of human]] and/or [[machine agents]] .
Another [[approach to function estimation]] is to [[represent the function]] [[of interest]] as a [[linear combination]] of [[basis function]]s, that is, to represent the [[function]] in a [[series expansion]] .
Another challenge for [[LSA]] is a [[lack of incrementality]]: an [[inability]] to [[update [[semantic representation]]s incrementally]] in response to a [[continual accumulation]] of [[language input]] .
Another characteristic of our [[method]] is its [[capability]] for considering [[multiple classes]] and their [[interactions]] simultaneously.
Another class of [[approximate inference method]] widely used in [[fitting]] [[topic models]] is [[variational EM]] .
Another important [[advantage]] of [[our method]] is the ability to handle diverse [[types]] of [[similarities]] according to actual [[task requirements]], including both [[feature similarities]] and [[semantic similarities]] like [[label consistency]] .
Another is [[predicting]] the [[risk of heart failure]] related [[hospitalization]] for [[patient]]s with [[COPD pre-condition]] .
Another well-known [[classifier]] that is naturally [[represented as]] a [[graphical model]] is [[logistic regression]] (sometimes known as the [[maximum entropy classifier]] in the [[NLP community]]).
An [[outlier]] is a [[data point]] which is [[significantly different]] from the remaining [[data]] .
“An [[outlier]] is an [[observation]] which [[deviates]] so much from the other [[observation]]s as to [[arouse suspicion]]s that it was [[generated by]] a different [[mechanism]].”
An [[outlier]] (or a [[deviation]] or an [[anomaly]]) is an [[observation]] or a [[pattern]] in the [[data]] that appears to deviate significantly from other [[observation]]s or [[pattern]]s in the same [[data]] [ 8 ] and [ 9 ].
An overview of [[HSMM]]s is presented in [[this paper]], including modelling, inference, [[estimation]], implementation and [[application]]s.
[[Anticipatory government]] stresses [[prevention]] rather than [[cure]] .
A [[number]] of [[machine learning]] and [[computer vision problem]]s, such as [[matrix completion]] and [[subspace clustering]], require a [[matrix]] to be of [[low-rank]] .
A number of [[question]]s arise in the context of [[this task]]: how do we [[formalize]] and [[quantify]] the [[competitiveness relationship]] between two [[item]]s?
A number of [[research paper]]s on [[managing]] and [[mining]] [[uncertain graph data]] have been published in the [[database]] and [[data mining conference]]s such as [[VLDB]], [[ICDE]], [[KDD]], [[CIKM]] and [[EDBT]] .
An [[up-to-date]] chart of [[account]]s helps the [[organization]] view itself and [[it resources accurately]], [[making service]] and [[resource allocation decisions easier]], as well as [[communication]] with [[citizen]]s and [[official]]s.
A [[Park Test]] is a [[statistical hypothesis test]] for [[heteroscedasticity]] .
A particularly [[promising area]] is to [[extract]] [[energy-efficient]] [[transportation pattern]]s ([[green knowledge]]), which can be used as [[guidance]] for [[reducing inefficiencies]] in [[energy consumption]] of [[transportation sectors]] .
A pervasive [[problem]] in [[large]] [[relational databases]] is [[identity uncertainty]] which occurs when multiple [[entries in a database]] [[refer]] to the same underlying [[entity in the world]] .
A [[plausible model]] for [[user behavior analytic]]s in [[signed network]]s can be based upon the [[assumption]] that more [[extreme positive]] and [[negative relationship]]s are [[explored]] and [[exploited]] before less [[extreme one]]s.
A popular [[analysis]] is to [[cut the graph]] so that the [[disjoint subgraph]]s can [[represent communiti]]es (for [[social network]]) or [[background]] and [[foreground cognitive activity]] (for [[medical imaging]]).
A [[popular definition]] for an [[ontology]] is “a [[formal specification]] of a [[shared conceptualisation]]” ([[Gruber 1993]]).
A [[possible world]] is a [[formal object]] (think "[[data structure]]") with respect to which the [[truth]] of any [[assertion]] can be [[evaluate]]d.
[[Appendix]]es provide [[mathematical background]] and a discussion of [[Gaussian Markov process]]es.
[[Application chapter]]s: These chapters study important [[application]]s such as [[stream mining]], [[Web mining]], [[ranking]], [[recommendation]]s, [[social network]]s, and [[privacy preservation]] .
Applications include [[document annotation]], [[database organization]], [[query answering]], and [[automated summarization of text collection]]s.
[[Application]]s to [[two]] [[real-world]] [[multilabel learning problem]]s, i.e., [[functional genomics]] and [[text categorization]], show that the performance of [[BP-MLL]] is superior to that of some [[well-established]] [[multilabel learning algorithm]]s.
Applied to a [[surgical risk]] [[stratification task]], [[our method]], which used [[data]] from two [[institution]]s, performed comparably to the [[risk model published]] by the [[Society for Thoracic Surgeon]]s, which was developed and [[tested]] on over one hundred thousand [[surgeri]]es from [[hundred]]s of [[institution]]s.
Applied to the [[CiteSeer dataset]], a [[collection of documents in academia]], [[we]] show the trends of [[research topic]]s, how [[research topic]]s are [[related]] and which are [[stable]] .
Applying the [[concept]] of [[organizational structure]] to [[social network analysis]] may well [[represent]] the [[power]] of [[members]] and the scope of their [[power]] in a [[social network]] .
Applying to a [[Yeast gene expression data]], [[we]] evaluate, by using [[Pearson's correlation coefficient]] and [[P-value]], the [[biological]] [[relevance]] of [[collaborating]] [[negative correlations]] as an [[example]] among many [[real-life domains]] .
[[Approximately]] six hundred [[term]]s currently survive this [[filtering step]] .
[[Approximating]] the [[semantic similarity]] between [[entiti]]es in the [[learned Hamming space]] is the [[key]] for [[supervised hashing technique]]s.
[[Approximation Algorithm]]s, [[Community Identification]], [[Dynamic Social Network]]s.
[[Approximation]] of [[non-linear kernel]]s using [[random feature mapping]] has been successfully employed in [[large-scale]] [[data analysis application]]s, accelerating the [[training]] of [[kernel machine]]s.
A [[price index]] measures the [[price]] of a [[basket of goods and service]]s [[over]] [[time]] .
A [[probabilistic approach]] is proposed to solve the problem, and an [[effective]] and [[efficient]] [[iterative algorithm]], <i>[[PathSelClu]]s </i>, is proposed to [[learn]] the [[model]], where the [[clustering]] [[quality]] and the [[meta-path weight]]s are mutually enhancing each other.
A [[probability vector]] must be [[maintained]] for [[latent topic]]s for every [[token]] in a [[corpus]] .
A [[production structure]] calibrated to match these trends suggests modest [[aggregate]] [[welfare gains]] from [[automation]], even within a [[representative agent]] setup in which [[redistribution]] is [[frictionless]] .
A properly designed [[KBS]] is an [[interactive software-based system]] designed to help [[decision maker]]s compile [[useful information]] from [[raw data]], [[document]]s, [[personal knowledge]], and/or [[business model]]s for [[problem solving]] and [[decision-making]] .
A [[Proteolysis]] is a breakdown of [[protein]]s into smaller [[polypeptide]]s or [[amino acid]]s.
A [[prototype]] has been implemented for [[Java]], which currently supports [[requirement]]s expressed using [[past time]] and [[future time]] [[linear temporal logic]]s, as well as extended [[regular expression]]s.
A [[prototype]] of [[this algorithm]] is [[implemented]] and we present its [[performance]] w.r.t. [[memory-efficiency]] on [[real-life data]] and in [[controlled experiments]] with [[synthetic data]] .
A [[random experiment]] is an [[experiment]] in which the [[outcome]] varies in a [[unpredictable]] fashion when the [[experiment]] is repeated under the same [[condition]] .
[[Archived data]] often describe [[entities]] that participate in [[multiple]] [[roles]] .
A [[real life example]] of building an [[insurance-related]] [[query]] [[taxonomy]] illustrates the usefulness of [[our approach]] for specific [[domain]]s.
A recent [[introductory document]] ([[Hsu et al., 2003]]) supporting the [[LIBSVM package]] does encourage [[grid search]] for [[C]] .
A recent [[SPARQL benchmark]] is [[SP2Bench]] ([[Schmidt, et al., 2008a]] and [[2008b]]).
A recent [[study]] by two prominent [[finance]] [[researcher]]s, [[Fama]] and [[French]], introduces a new [[framework]] for studying [[risk]] vs. [[return]] : the [[migration of stock]]s across [[size-value portfolio space]] .
A [[recent]] [[study]] proposed a new [[algorithm]], [[RankClus]], for [[clustering]] on [[bi-typed]] [[heterogeneous network]]s.
A [[recommender system]] makes [[personalized product suggestion]]s by [[extracting knowledge]] from the previous [[users interaction]]s.
::* A [[regression diagnostic test]] to determine [[goodness of fit]] the [[regression model]] and the [[statistical significance]] of the [[estimated parameter]]s
A [[Regression-Kriging Task]] is a [[Prediction Task]] that combines a [[regression analysis task]] with the [[kriging]] of the [[regression residual]]s
A [[relational database]] can be viewed as a [[hypergraph]] with [[constants]] as [[nodes]] and [[relation]]s as [[hyperedge]]s.
A [[research front]] is defined as an [[emergent]] and [[transient]] [[grouping of concepts]] and underlying [[research issue]]s.
A [[reserve price]] is the [[minimum]] that the [[auctioneer]] would accept from [[bidder]]s in [[auction]]s, and in a second [[price auction]] it could potentially uplift the [[auctioneer's revenue]] by charging winners the [[reserve price]] instead of the second highest [[bid]]s.
Arguably, [[neighbor queries]], which search for all [[neighbors]] of a [[query vertex]], are the most essential [[operations]] on [[social networks]] .
Arguably, the most [[effective technique]] to ensure wide adoption of a [[concept]] (or [[product]]) is by [[repeatedly exposing individual]]s to messages that [[reinforce]] the [[concept]] (or [[promote the product]]).
... Arguably the most important [[parallel]] between [[mass incarceration]] and [[Jim Crow]] is that both have served to define the [[meaning]] and [[significance]] of [[race]] in [[America]] .
[[Armadillo]], [[Information Extraction]], [[Information Integration]], [[Information Retrieval]], [[Web Intelligence]]
[[ARMA Models]], [[Acceptance Region]], [[Accuracy]], [[Algorithm]], [[Alternative Hypothesis]], [[Analysis of Binary Data]], [[Analysis of Categorical Data]], [[Analysis of Residuals]], [[Analysis of Variance]], [[Anderson, Oskar]], [[Anderson, Theodore W.]], [[Anderson–Darling Test]], [[Arithmetic Mean]], [[Arithmetic Triangle]], [[Arrangement]], [[Attributable Risk]], [[Autocorrelation]], [[Avoidable Risk]], [[Bar Chart]], [[Barnard, George A.]], [[Bayes' Theorem]], [[Bayes, Thomas]], [[Bayesian Statistics]], [[Bernoulli Distribution]], [[Bernoulli Family]], [[Bernoulli Trial]], [[Bernoulli's Theorem]], [[Bernoulli, Jakob]], [[Beta Distribution]], [[Bias]], [[Bienaymé, Irénée-Jules]], [[Binary Data]], [[Binomial]], [[Binomial Distribution]], [[Binomial Table]], [[Binomial Test]], [[Biostatistics]], [[Block]], [[Bonferroni, Carlo E.]], [[Bootstrap]], [[Boscovich, Roger J.]], [[Box Plot]], [[Box, E.P.
[[Artificial intelligence]] is already well on its way to making "[[good job]]s" obsolete: many [[paralegal]]s, [[journalist]]s, [[office worker]]s, and even [[computer programmer]]s are poised to be [[replaced by]] [[robot]]s and [[smart software]] .
[[Art]]s and [[entertainment]] has been the [[fastest-growing sector]] of [[employment]] for [[young people in New York City]], but the [[real wages of young worker]]s in the sector fell nearly 26 percent between [[2000]] and [[2014]] .
As a [[case study]], [[we]] consider two [[application]]s using [[Twitter data]], [[civil unrest event detection]] and [[rare disease outbreak detection]], and present [[empirical evaluation]]s illustrating the [[effectiveness]] and [[efficiency]] of [[our proposed approach]] .
As a [[case study]], [[we]] develop a [[mobile recommender system]] which has the ability in recommending a [[sequence]] of [[pick-up points]] for [[taxi drivers]] or a [[sequence]] of potential [[parking positions]] .
As a [[case study]], [[we]] exploit [[multi-focal learning]] for [[profiling problem]]s in [[customer service center]]s.
As a complement to [[this result]], [[we]] also extend the [[Johnson-Lindenstrauss Lemma]] to [[this spherical setting]], by showing that [[projecting]] to a [[random]] [[<math>O((1/\varepsilon^2) \log n)</math>-dimensional sphere]] causes only an [[<math>\varepsilon</math>-distortion]] in the [[geodesic distance]]s.
As a [[conclusion]], [[online resource]]s and [[open research problem]]s on [[multi-label learning]] are outlined for [[reference]] purposes
As a [[concrete example]] of [[this framework]] [[we]] use [[exponential models]] .
As a [[corollary]], [[we]] elucidate the impact of the [[angle]] between the original [[vector]]s on the [[relative distortion]] of the [[dot product]] under [[random projection]], and [[we]] show that the [[obtuse]] vs. [[acute angles]] behave [[symmetrically]] in the same way.
As [[active participant]]s in [[game culture]], [[the authors]] have written [[Rules of Play]] as a [[catalyst for innovation]], filled with new [[concept]]s, [[strategi]]es, and [[methodologies for creating]] and [[understanding game]]s.
A [[sample dataset]] of [[this study]] can be [[downloaded]] from http://www.largenetwork.org/ebsn.
As a new [[computation platform]], [[Internet computing]] requires new [[theori]]es and [[method]]s.
As an example of such [[application]], we extracted [[news event]]s for a variety of [[topic]]s and then [[correlate]]d [[this data]] with the corresponding [[sentiment time series]], revealing the [[connection]] between [[sentiment shift]]s and [[event dynamics]] .
As an inherently [[coordinative]] [[social activity]], [[dialog]] does not fit [[classical assumption]]s of [[individual linguistic processing]]: In [[dialog]], it is often impossible to [[unambiguously]] assign [[roles of speaker]] and [[listener]] — who [[produce]]s and who receives — as they tend to [[overlap and mix (Clark & Schaefer, 1989]]).
As an [[instantiation]] of [[this framework]], [[we]] propose an [[efficient]] [[optimization method]], which is [[guaranteed]] to converge to Îµ [[precision]] in O (1/Îµ) steps.
As a [[non-convex optimization method]], [[DMCBoost]] shows [[competitive]] or better [[result]]s than [[state-of-the-art]] [[convex relaxation boosting method]]s, and it performs especially well on the [[noisy case]]s.
As a [[proof of concept]], [[we]] use [[Spine]] on [[real-world dataset]]s, revealing the backbone of their [[influence-propagation network]]s.
As a reaction to high and [[persistent unemployment]] in [[Germany]], the largest [[labour market reform]]s in [[post-war history]] were implemented between [[2003]] and [[2005]] .
As a result, [[capability model]]s are useful in applications in which traditional [[model]]s are [[difficult to obtain]], or [[model]]s must be [[learned from incomplete plan trace]]s, e.g., [[robots learning human model]]s from [[observation]]s and [[interaction]]s.
As a result, [[DTM]] achieves higher [[classification performance]] in a [[semi-supervised setting]] by effectively exposing the [[manifold structure]] of [[data]] .
As a result, [[our service]] [[accurately]] [[estimate]]s the [[travel time]] of a [[route]] for a [[user]]; hence finding the [[fastest route]] [[customized]] for the [[user]] .
As a result, [[researcher]]s have re-adopted the once-popular [[knowledge-rich approach]], investigating a variety of [[semantic knowledge source]]s for [[common noun resolution]], such as the [[semantic relation]]s between two [[NP]]s (e.g., [[Ji et al. (2005)]]), their [[semantic similarity]] as computed using [[WordNet]] (e.g., [[Poesio et al. (2004)]]) or [[Wikipedia]] ([[Ponzetto and Strube, 2006]]), and the contextual role played by an [[NP]] (see [[Bean and Riloff (2004)]]).
As a result, [[we]] [[construct]] two [[graphs]], i.e. [[data graph]] and [[feature graph]], to explore the [[geometric structure]] of [[data manifold]] and [[feature manifold]] .
As a result, we further propose a [[mixture model]], which combines [[linear regression]] on [[bid]]s with [[observable winning price]]s and [[censored regression]] on [[bid]]s with the [[censored winning price]]s, [[weighted]] by the [[winning rate]] of the [[DSP]] .
As a result, [[WordNet]] is not updated frequently and omits many [[lemma]]s and [[sense]]s, such as those from [[domain specific lexicon]]s (e.g., [[DNA replication]], [[regular expression]], and long shot), [[creative slang usage]]s (e.g., [[homewrecker]]), or those for [[technology]] or [[entiti]]es that came into recent existence (e.g., [[selfie]], [[mp3]]).
As a [[sample application]], [[we]] present a [[topic dynamics model]] for the [[large]] [[PubMed]]/[[MEDLINE database]] of [[biomedical publications]], using the [[MeSH (Medical Subject Heading) topic hierarchy]] .
As a [[search index]], the [[proposed method]] utilizes a [[degree-reduced k-nearest neighbor (k-DR) graph]] constructed from the [[object set]] with the [[dissimilarity]], and explores the [[k-DR graph]] along its [[edge]]s using a [[greedy search (GS) algorithm]] starting from multiple initial [[vertice]]s with [[parallel processing]] .
As a [[showcase]], [[we]] develop a new [[method]] to obtain a discriminative and compact [[feature representation]] for [[clustering problem]]s.
As a [[side-effect]] [[our model]] yields [[human-understandable result]]s which can be used in an [[intuitive fashion]] by [[advertiser]]s.
As a [[side effect]], running an [[ensemble]] of several [[outlier detector]]s on [[subsample]]s of the [[dataset]] is more [[efficient]] than [[ensemble]]s based on other means of [[introducing diversity]] and, depending on the [[sample rate]] and the size of the [[ensemble]], can be even more [[efficient]] than just the single [[outlier detector]] on the complete [[data]] .
As a single [[vulnerable friend]] in a [[user's social network]] might place all [[friend]]s at risk, [[we]] resort to [[experiment]]s and [[observe]] how much [[security]] an individual [[user]] can improve by [[unfriending]] a [[vulnerable friend]] .
As before, [[circle]]s denote [[continuous node]]s, [[square]]s denote [[discrete node]]s, [[clear]] means [[hidden]], [[shaded]] means [[observed]] .
As [[biography]] [[paragraphs]] on [[the Web]] are composed of the most important [[facts]], our [[joint summarization model]] can improve the [[accuracy]] of both [[fact extraction]] and [[biography]] [[ranking]] compared to [[decoupled methods]] in the [[literature]] .
As [[biological science]]s generate [[big]] and [[complex data]], the development of [[bio-ontologi]]es has been critical in handling these [[data]] and [[enabling interoperability]] between [[database]]s and between [[application]]s ([[Robinson and Bauer 2011]]).
As both [[benchmarks predate]] the [[SPARQL query language]], they do not support benchmarking specific [[SPARQL feature]]s such as [[OPTIONAL filter]]s or [[DESCRIBE]] and [[UNION operator]]s.
As [[botnet]]s continue to [[proliferate]] and grow in [[sophistication]], so does the need for more [[advanced]] [[security solution]]s to effectively [[detect]] and [[defend]] against such [[attack]]s.
A [[scalable]] [[conjoint analysis technique]], known as [[tensor segmentation]], is developed by utilizing [[logistic tensor regression]] in standard [[partworth framework]] for [[solutions]] .
As can be seen in Figure 1, the [[degree of formalisation]] constantly increases from the least to the most [[formalised knowledge resource]]: [[unstructured text]] – just a [[string of text]] with no [[additional structure]]; [[terminology]] – a [[set of terms]] expressing [[concept]]s for the [[domain of interest]] (e.g. [[hotel]], [[room]], [[tourist]], etc.); [[glossary]] – a [[terminology]] with [[textual definition]]s for each [[term]] (e.g. “an [[establishment]] that provides [[short-term lodging”]] as [[definition]] of [[hotel]]); [[thesauru]]s – which provides information about [[relationship]]s between [[word]]s, like [[synonym]]s (e.g. [[motel]] is a [[synonym of]] [[motor hotel]]) and [[antonym]]s (e.g. [[ugly]] is an [[antonym of]] [[beautiful]]); [[taxonomy]] – a [[hierarchical classification]] of [[concept]]s (e.g. a [[motel is-a hotel]]); [[ontology]] – a [[fully-structured]] [[knowledge model]], including [[concept]]s, [[relation]]s of various kinds and, possibly, [[rule]]s and [[axiom]]s.
As [[compani]]es substitute [[machine]]s and [[computer]]s for [[human activity]], [[worker]]s need to [[own part of the capital stock]] that [[substitutes for them]] to benefit from these new [["robot" technologi]]es.
As compared with [[individually randomized]] and [[cluster-randomized studi]]es, the [[cluster-crossover design]] allows several [[factor]]s ([[acceptance]], [[tolerance]] and [[cost]]) to be assessed more easily at the [[cluster level]] .
As [[data]] cannot fit in [[memory]], many [[design considerations]] are very different from those for [[traditional]] [[algorithms]] .
As [[data evolves]], [[regression model]]s must be [[recomputed]], and indeed much [[work]] has focused on [[quick]], [[efficient]] and [[accurate computation]] of [[linear regression model]]s.
As [[dataset]]s and their [[user community]] [[continuously grow]], the problem can only be [[exacerbated]] .
As [[dataset]]s become larger, more [[complex]], and more available to diverse [[group]]s of [[analyst]]s, it would be quite useful to be able to [[automatic]]ally and [[generic]]ally assess the [[quality of estimate]]s, much as we are able to [[automatic]]ally [[train]] and [[evaluate]] [[predictive model]]s such as [[classifier]]s.
As demonstrated in [[the paper]], accepting the different [[computational assumption]] called the [[Exponential Time Hypothesis]] (that involves [[PÃ¢ÂÂ NP]]) would justify the separation between a [[microscopic quantum system]] and a [[macroscopic apparatus]] (usually called the [[Heisenberg cut]]) since this [[hypothesis]], if [[true]], would imply that [[deterministic quantum]] and [[classical]] [[description]]s are impossible to [[overlap]] in order to obtain a [[rigorous derivation]] of complete [[properties of macroscopic objects]] from their [[microstate]]s.
As described [[herein]], [[we]] formalize [[this problem]] as a [[website optimization problem]] and provide a basis to apply [[existing search algorithm]]s to [[this problem]] .
As discussed, there are a multitude of [[application]]s where [[novelty detection]] is extremely important including [[signal processing]], [[computer vision]], [[pattern recognition]], [[data mining]], and [[robotics]] .
A second evaluation in an [[educational environment]] shows that [[enriching]] [[educational material]]s with [[such annotation]]s can improve the [[learning process]] by allowing [[faster access]] to [[background knowledge]] .
A [[second generation]] of [[universal robot]] with a [[mouselike]] 100,000 [[MIPS]] will adapt as the [[first generation]] does not and will even be [[trainable]] .
A [[Seegrid robot]] would [[see]], [[explore]] and [[map]] its [[premise]]s and would [[run unattended]], with a [[cleaning]] [[schedule]] minimizing [[owner]] [[disturbance]]s.
As [[electronic communication]], [[media]] and [[commerce]] increasingly permeate every aspect of modern life, [[real-time]] [[personalization of consumer experience]] through [[data-mining]] becomes practical.
A [[semantic association (SA)]] is a [[set of relationship]]s between two [[entitie]]s in [[knowledge base]] [[represented as]] [[graph path]]s consisting of a [[sequence]] of [[link]]s.
A “[[semantic wiki]]” extends a [[wiki]] by “semantic technologies” like [[RDF]], [[OWL]], [[Topic Map]], or [[Conceptual Graphs]] .
A [[semi-definite program (SDP)]] is an [[optimization problem]] with a [[linear objective]], and [[linear matrix inequality]] and [[affine equality constraint]]s.
A series of [[user]] [[studi]]es using [[Mechanical Turk]] shows that all three [[quality criteria]] are crucial to produce [[quality timeline]]s and that [[our algorithm]] significantly [[outperform]]s various [[baseline]] and [[state-of-the-art method]]s.
a [[set]] of [[outcome]]s of an [[experiment]] with an assigned a [[probability]]
As examples, [[factory worker]]s who operate [[welding]], [[fitting]], and [[metal press machine]]s fall into [[this category]], as do [[forklift operator]]s and [[home appliance repairer]]s.
As [[experimental results]] in [[real applications]] to the [[yeast]] [[protein interaction network]], [[we]] demonstrate that our [[approach]] outperforms [[previous]] [[graph clustering methods]] with respect to [[accuracy]] .
As [[feature]]s flow in one by one, [[we]] online evaluate each coming feature to determine whether it is useful for [[mining]] [[predictive]] [[emerging patterns (EPs)]] by exploiting the [[relationship]] between [[feature relevance]] and [[EP discriminability]] (the [[predictive ability of an EP]]).
As general [[exponential models]] are [[infeasible in practice]], [[we]] use [[decomposable models]]; a [[large]] [[sub-class]] for which the [[measure]] is [[solvable]] .
A [[shared lexicon]] is the [[prerequisite]] for [[knowledge-sharing]] through [[language]], and a [[shared ontology]] is the [[prerequisite]] for [[knowledge-sharing]] through [[information technology]] .
Aside from the [[theoretical insight]]s, [[offline experiment]]s on a [[real dataset]] and [[online experiment]]s on a [[production RTB system]] verify the [[effectiveness]] of [[our proposed]] [[optimal bidding strategi]]es and the [[functional optimisation framework]] .
A [[significant disadvantage]] of the [[OEC]] obtained in [[this way]] is that the <i>[[average value]]</i> of the [[key metric]] does not necessarily [[change]], even if its <i> [[distribution]]</i> [[changes significantly]] .
A [[Sign Test]] is a [[non-parametric]] [[paired difference test]] similar to the [[parametric]] [[t-test]] .
A simple and [[unbiased]] [[topic propagation]] across such a [[heterogeneous]] [[network]] does not make much [[sense]] .
As [[input]], our [[classifiers]] operate on [[high dimensional feature vectors]] that we extract from the [[text fields]], [[time stamps]], [[cross reference]]s, and other [[entries]] in existing [[vulnerability disclosure reports]] .
A [[slot]] has associated a set of [[facet]]s that put some [[restriction]]s on [[slot value]]s.
A [[small set]] of [[economist]]s have over the years proposed [[formal, general model]]s of [[time-inconsistent preference]]s.
As more [[capital]] is [[raised]], the [[marginal cost of capital]] rises.
As noted previously, the [[CR design]] is so called because [[subject]]s are [[assigned randomly]] to [[experimental condition]]s - or, alternatively, [[experimental treatment]]s are [[randomly applied]] to [[subject]]s - so that each [[subject]] initially has an [[identical probability]] of of being [[exposed to]] any one of the [[treatment]]s.
A [[social tagging system]] contains [[heterogeneous information]] like [[users' tagging behavior]]s, [[social network]]s, [[tag semantics]] and [[item profile]]s.
A [[solution]] can benefit many [[real life application]]s because i) [[keyword]]s give [[users]] the [[flexibility]] and ease to [[characterize]] a [[specific domain]]; and ii) in many [[application]]s, such as [[online advertisement]]s, the [[domain of interest]] is already [[represented by]] a [[set of keyword]]s.
As one of its “guiding [[principle]]s, ” the [[Commission expressed]] an interest in “encouraging [[accountable]], [[responsive]], and [[understandable local government]] and [[cost effective]], [[financially sound service delivery]] systems.” In its review of [[local government service delivery]] and [[financing]], the [[Commission]] noted how difficult it is “in the absence of a standard chart of accounts” to compare [[revenue]]s and [[expenditure]]s from one [[jurisdiction]] to another and for [[state]] and [[local official]]s and [[citizen]]s to make meaningful [[judgments about local government operation]]s and [[finance]]s.
As one of the fundamental [[tasks in text analysis]], [[phrase mining]] aims at [[extracting quality phrases]]s from a [[text corpus]] and has various [[downstream application]]s including [[information extraction]] / [[retrieval]], [[taxonomy construction]], and [[topic modeling]] .
As [[our experiment]]s show, [[integrating]] [[ranking]] with [[classification]] not only generates more [[accurate]] [[class]]es than the [[state-of-art]] [[classification method]]s on [[networked data]], but also provides meaningful [[ranking of object]]s within each [[class]], serving as a more [[informative view]] of the [[data]] than traditional [[classification]] .
As our principal [[domain]] of [[study]], [[we]] show how such a [[meme-tracking]] [[approach]] can provide a [[coherent]] [[representation]] of the [[news]] [[cycle]] --- the [[daily]] [[rhythm]]s in the [[news media]] that have long been the subject of [[qualitative interpretation]] but have never been [[captured]] [[accurately]] enough to permit actual [[quantitative analysis]] .
A [[sparse tensor]] formalization guarantees [[efficiency]] and [[parallelizability]] .
As part of [[our studi]]es [[we]] also explore the use of the [[on-task behavior]] of particular [[user cohort]]s, such as [[people who are expert]] in the [[topic]] currently [[being searched]], rather than all other [[user]]s.
As part of [[this shift]], [[specific expertise]] and [[deep knowledge]] of the subject at hand have become increasingly important, and many [[Q&A site]]s employ [[voting]] and [[reputation mechanism]]s as [[centerpiece]]s of their [[design]] to help [[user]]s identify the [[trustworthiness]] and [[accuracy]] of the [[content]] .
A special hierarchy of [[processor]]s and fast [[memory unit]]s allow very powerful and [[efficient]] [[parallelization]] but also demands novel [[parallel algorithm]]s.
A specialty is [[conceptualized]] and [[visualized]] as a [[time-variant duality]] between two fundamental concepts in [[information science]]: [[research front]]s and [[intellectual base]]s.
[[Aspect variables]] serve as a [[joint representation]] for [[observed]] and [[unobserved]] [[labels]] .
As [[pop culture]], [[game]]s are as important as [[film]] or [[television]] - but [[game design]] has yet to develop a [[theoretical framework]] or [[critical vocabulary]] .
As practicing [[AI]] and [[HCI researcher]]s, we have found the [[conversation today]] has many of the same [[feature]]s, so the [[historical narrative]] can be [[instructive]] .
As [[predicted]], those [[wafer]]s were significantly faster than [[average]] .
As [[probabilistic computation]]s play an increasing role in solving various [[problem]]s, [[researcher]]s have designed [[probabilistic language]]s that treat [[probability distribution]]s as [[primitive datatype]]s.
As [[product specification]]s often cover more [[product attribute]]s than [[free-text description]]s, being able to extract [[attribute-value pair]]s from these [[specification]]s is a critical [[prerequisite]] for achieving good results in [[task]]s such as [[product matching]], [[product categorisation]], faceted [[product search]], and [[product recommendation]] .
[[ASREML]] can be used in [[Windows]], [[Linux]], and as an add-on to [[S-PLUS]] and [[R]] .
As [[robot]]s become more [[ubiquitous]] and [[capable]] of performing [[complex task]]s, the importance of enabling [[untrained user]]s to [[interact with them]] has increased.
As [[scientist]]s become more [[technologically savvy]] and use these [[technique]]s, the traditional [[approach]]es to [[data analysis]] fail given the [[huge amount]]s of [[data]] produced by these [[method]]s.
As [[search cost]]s fall from [[very high]] to [[moderate]], new [[market]]s [[emerge]], and both [[seller]]s and [[buyer]]s [[benefit]] .
As [[search strategy]], [[we]] propose two [[approach]]es: the first [[algorithm]] selects a good [[pattern set]] from a large [[candidate set]], while the second is a [[parameter-free any-time algorithm]] that [[mines pattern set]]s directly from the [[data]] .
Assessing the [[relatedness of texts]] in this [[space]] amounts to [[comparing]] the corresponding [[vectors]] using [[conventional metrics]] (e.g., [[cosine]]).
[[Association rule mining]] often discovers a [[combinatorially]] large number of [[association rule]]s, [[eroding]] the [[interpretability]] of the [[rule set]] .
[[Associative map]]s are very useful because they help keep [[program]]s [[legible]] and [[concise]] .
As such, the [[dynamic semi-supervised clustering problem]] is [[simplified]] to the [[problem]] of [[updating]] a [[probability simplex vector subject]] to the newly received [[pairwise constraint]]s.
Assume that it is possible to have a single [[machine]] (`[[Godzilla]]'), which can [[store]] the [[massive dataset]] and support an [[ever-growing community submitting]] [[MV imputation request]]s.
Assuming that the [[multivariate data]] are a [[random sample]], the [[statistical measures of dependence]] with [[estimated probability density]] or [[mass function]]s can be studied [[asymptotically]] .
A [[stack of denoising auto-encoder]]s is first [[trained]] in a [[layer-wise]], [[unsupervised manner]] .
A [[standard approach]] distributes the [[graph]] over a [[cluster of node]]s, but performing [[computation]]s on a [[distributed graph]] is expensive if [[large amount]] of [[data]] have to be [[moved]] .
A [[statistical model]] is a [[probability distribution]] constructed to enable [[inference]] to be drawn or [[decisions]] made from [[data]] .
As [[tensor]]s provide a [[natural representation]] for the [[higher-order relation]]s, [[tensor factorization technique]]s such as [[Tucker decomposition]] and [[CANDECOMP]] / [[PARAFAC decomposition]] have been applied to many [[field]]s.
As the [[cost]] of [[computer]]s [[fell]], the [[effect]] of [[ever-cheaper]] [[computer]]s [[slowed]] the [[growth]] of [[output price]]s more than [[it]] [[slowed]] the [[growth]] of the [[CPI]] .
As the credibility of traditional [[explanation]]s - [[colonialism]], [[dependency]], [[racism]] - declines, many now believe that the [[principal reason]] why some [[countri]]es and [[ethnic group]]s are [[better off]] than others lies in the [[cultural value]]s that powerfully shape [[nation]]s and [[people]]'s [[political]], [[economic]], and [[social performance]] .
As the [[exact computation]] of [[this importance measure]] is [[expensive]], two [[approximations]] are proposed.
As the [[first paper]] to address [[this problem]]: we (1) propose a [[fine-grained model]] of [[information diffusion]] for the [[group-based problem]], (2) show that the [[process]] is [[submodular]] and present an [[algorithm]] to determine the [[influential group]]s under this [[model]] (with a [[precise approximation bound]]), (3) propose a [[coarse-grained model]] that [[inspect]]s the [[network]] at [[group level]] (not [[individual]]s) significantly [[speeding up]] [[calculation]]s for [[large network]]s, (4) show that the [[diffusion function]] [[we]] design here is [[submodular]] in general case, and propose an [[approximation algorithm]] for this [[coarse-grained model]], and finally by conducting [[experiment]]s on [[real dataset]]s, (5) demonstrate that [[seeding member]]s of [[selected group]]s to be the first adopters can broaden [[diffusion]] (when compared to the [[influential individual]]s case).
As the first step towards this [[objective]], [[we]] introduce an [[umbrella framework]] for [[defining]] and [[characterizing]] an [[ensemble]] of [[dynamic process]]es on a [[network]] .
As the most fundamental and widespread form of [[language use]], [[dialog]] has a [[pervasive]] and [[all-encompassing impact]] on [[everyday life]] .
As the most [[prominent example]], [[Wikipedia]] attracts a [[large number of user]]s, that are willing to [[create]] and [[maintain informal]] “[[world knowledge]]” through the [[wiki system]] .
As the [[Netflix Prize competition]] has demonstrated, [[matrix factorization model]]s are superior to classic [[nearest-neighbor technique]]s for [[producing product recommendation]]s, allowing the incorporation of additional [[information]] such as [[implicit feedback]], [[temporal effect]]s, and [[confidence level]]s.
As the number and size of [[large]] [[timestamped collections]] (e.g. [[sequence]]s of [[digitized]] [[newspapers]], [[periodicals]], [[blogs]]) increase, the [[problem]] of [[efficiently]] [[indexing]] and [[searching]] such [[data]] becomes more important.
As the [[public transport infrastructure]] of [[large cities]] expands, [[transport operator]]s are diversifying the [[range]] and [[price]]s of [[ticket]]s that can be [[purchased]] for [[travel]] .
As the [[size]] of [[data set]]s used to build [[classifier]]s steadily increases, [[training]] a [[linear model]] [[efficiently]] with [[limited memory]] becomes essential.
As the [[size of graphs]] reaches several [[giga-]], [[tera-]] or [[peta-bytes]], the necessity for [[such a library]] grows too.
As the [[world shrinks]], so [[our capacity]] for [[effective moral action]] [[grows]] .
As [[time evolves]], [[similar objects]] naturally [[synchronize]] together and form distinct [[clusters]] .
As to achieve optimal [[test results]], [[we]] present appropriate [[surrogate objective functions]] for [[efficient training]] on [[MNAR data]] .
As to [[test]] [[recommender system]]s, [[we]] present two [[performance measures]] that can be [[estimated]], under [[mild assumptions]], without [[bias from data]] even when [[ratings]] are [[missing not at random (MNAR)]] .
A [[strategy]] whose [[average regret per round]] tends to [[zero]] with [[probability 1]] for any [[bandit problem]] when the [[horizon]] tends to [[infinity]] is a [[zero-regret strategy]] .
A [[student]] who wants to [[learn]] some [[concept]] should be able to [[interact]] with the available [[information]] in a [[coherent]] and [[personalized way]] .
A [[student]] who wishes to [[learn]] about some particular [[topic]] does not have many [[option]]s.
A [[supervised classifier]] can be [[trained]] with [[annotated text sequence]]s and used to [[classify]] each [[word]] in a large set of [[unannotated]] [[sentence]]s.
A [[survey]] of [[online]] [[controlled experiments]] and [[lessons learned]] were [[previously documented]] in [[Controlled Experiments on the Web: Survey and Practical Guide]] ([[Kohavi, et al., 2009)]] .
As [[water]] is evaporated from the [[airway surface liquid]], it becomes [[hyperosmolar]] and provides an [[osmotic stimulu]]s for water to move from any [[cell nearby]], resulting in [[cell volume]] [[loss]] .
As well as giving a [[principled interpretation]] of '[[discriminative training]]', [[this approach]] opens door to very general ways of interpolating between [[generative]] and [[discriminative extreme]]s through [[alternative choice]]s of [[prior]] .
As we only include [[itemset]]s that are surprising with regard to the [[current model]], the [[summary]] is [[guarantee]]d to be both [[descriptive]] and [[non-redundant]] .
As [[we]] shall show with extensive [[empirical evaluations]] in diverse [[domains]], [[algorithm]]s based on the [[time series]] [[shapelet primitives]] can be [[interpretable]], more [[accurate]] and significantly faster than [[state-of-the-art]] [[classifiers]]
A [[SympGraph]] has [[symptom]]s as [[node]]s and [[co-occurrence relation]]s between [[symptom]]s as [[edge]]s, and can be constructed automatically through extracting [[symptom]]s over [[sequence]]s of [[clinical note]]s for a [[large number]] of [[patient]]s.
[[Async-EM]] enables [[our algorithm]] not only to [[accelerate convergence]] but also to reduce the [[overhead]] induced by [[memory bandwidth limitation]]s and [[synchronization requirement]]s.
A [[synset]] is a [[set of synonyms]] that are [[interchangeable]] in some [[context]] .
At a [[high level]], [[existing work]] identifies important [[entiti]]es either by [[ranking]] or by [[selection]] .
At a [[high level]], these [[region]]s may function like [[topographically defined]] [[data register]]s, [[encoding]] the [[fluctuating value]]s of [[abstract semantic variable]]s.
A [[task]] is [[posted]] and [[subsequently routed]] in the [[network]] from an [[expert]] to another until being [[resolved]] .
At [[baseline]], [[ADNI]]'s 780 [[participant]]s (172 [[AD]], 397 [[MCI]], 211 [[Normal]]), have at least one of four data types: [[magnetic resonance imaging (MRI)]], [[FDG-PET]], [[CSF]] and [[proteomic]]s.
At [[classifier]] level, a [[weight value]] is assigned to each [[classifier]] of the [[ensemble]] to ensure that [[learning]] can quickly adapt to [[users' interests]] .
At each [[node]], [[ShoppingAdvisor]] suggests a [[ranking of product]]s matching the [[preference]]s of the [[user]], and that gets progressively refined along the [[path]] from the [[tree's root]] to one of its [[leaf]]s.
At each step in the [[data mining process]], the [[randomization]] produces [[random samples]] from the set of [[data]] [[matrice]]s satisfying the already [[discovered]] [[patterns]] or [[models]] .
At each step, [[SBM]] [[updates the model]] using [[data]] consisting of two parts: (1) new [[data loaded]] from [[disk]] and (2) a [[set]] of [[informative sample]]s already in [[memory]] from previous [[step]]s.
At each [[successive level]], more [[sophisticated metrics]] are [[computed]] and the [[graph]] is viewed at finer [[temporal resolutions]] .
A [[term]] corresponds to an [[author]]’s [[textual representation]] of a particular [[concept]], and the goal of [[term identification]] is to recognize the [[term]] and capture its underlying [[meaning]] .
A [[term-frequency-based taxonomy]] is useful for [[application domains]] where the [[frequency]] with which [[term]]s [[occur]] on their own and in combination with other [[term]]s imposes a [[natural term hierarchy]] .
At [[Etsy]], an [[online marketplace]] for [[handmade]] and [[vintage good]]s with over 30 million diverse [[listing]]s, the [[problem of capturing taste]] is particularly [[important -]] - [[user]]s come to the site specifically to find [[item]]s that match their [[eclectic style]]s.
A [[text mining pipeline]], implemented based on the [[GATE framework]], [[automatically extracts rhetorical entiti]]es of type [[Claim]]s and [[Contribution]]s from [[full-text scientific literature]] .
At [[GetJar]], our goal is to make appealing [[recommendation]]s of [[mobile application]]s ([[app]]s).
At [[Google]], [[experimentation]] is practically a [[mantra]]; [[we]] [[evaluate]] almost every [[change]] that potentially affects what our [[users experience]] .
A [[theoretical analysis]] proves that the [[class]] of [[pseudo-likelihood]] [[distribution]]s representable by [[hybrid random field]]s strictly includes the [[class]] of [[joint probability distribution]]s representable by [[Bayesian network]]s.
A [[theoretical analysis]] shows that under certain [[assumption]], the [[classification model]] obtained by [[proposed approach]] [[converges]] to the [[optimal model]] when it is [[learned]] from the [[labeled examples]] for the [[target class]] .
A [[theoretical framework]] that unifies different [[data mining task]]s, on different [[type]]s of [[data]] can help to [[formalize]] the [[knowledge]] about the [[domain]] and provide a base for future [[research]], [[unification]] and [[standardization]] .
:(a) The term "[[direct loan]]" means a [[disbursement]] of [[fund]]s by the [[Federal government]] to a [[non-Federal borrower]] under a [[contract]] that requires the [[repayment]] of such [[fund]]s with or without [[interest]] .
A thorough [[financial-condition assessment]] that involves a [[large number]] of [[factor]]s and related [[indicator]]s can be very [[time consuming]] for a [[municipality]] .
At [[HP Labs]], [[we]] are working towards building a [[universal marketplace site]], i.e., a [[marketplace site]] that covers thousands of [[sectors]] and hundreds of [[providers]] [[per]] [[sector]] .
At [[instance]] level, both [[local]] and [[global filterings]] are considered for [[instance]] [[weight]] [[adjustment]] .
At [[Microsoft's Bing]], it is not uncommon to see [[experiment]]s that impact [[annual revenue]] by millions of [[dollar]]s, thus getting [[trustworthy result]]s is critical and investing in understanding [[anomali]]es has tremendous [[payoff]]: [[reversing]] a single [[incorrect decision]] based on the results of an [[experiment]] can fund a whole [[team of analyst]]s.
A [[tokenizer]]/[[morphological analyzer]] converts a [[stream of characters]] into a [[stream of words]], and the [[parser proper]] converts a [[stream of words]] into a [[parsed sentence]], or a [[stream of]] [[parsed sentence]]s.
[[Atomic activiti]]es are [[modeled]] as [[distribution]]s over [[low-level visual feature]]s, and [[multi-agent interaction]]s are modeled as [[interaction]]s over [[atomic activiti]]es.
At one time they were [[abhorred]] as [[abnormal]], [[unstable]], [[needy]], [[unbalanced]], but today [[their behavior]] is more accepted and [[appreciated]] .
A [[topical link]] is [[recommended]] between a [[user interested]] in a [[topic]] and a [[user authoritative]] in that [[topic]]: the [[explanation]] in [[this case]] is a [[set of binary feature]]s describing the [[topic responsible]] of the [[link creation]] .
A [[topic propagating]] in a [[social network]] reaches its [[tipping point]] if the [[number]] of [[user]]s discussing it in the [[network]] exceeds a [[critical threshold]] such that a [[wide cascade]] on the [[topic]] is likely to [[occur]] .
A [[training set]] consisting of [[data points]] belonging to <math>N</math> different [[classes]] is given, and the [[goal]] is to [[construct a function]] which, given a new [[data point]], will [[correctly predict]] the [[class]] to which the new [[point]] belongs.
:(a) [[Transaction]]s are properly [[record]]ed and [[account]]ed for, in order to:<br>
At that time, [[MUC]] was focusing on [[Information Extraction (IE) tasks]] where [[structured information]] of [[company]] [[activities]] and [[defense]] related [[activities]] is [[extracted]] from [[unstructured text]], such as [[newspaper articles]] .
At the core, [[Marky]] is a [[Web application]] based on the [[open source]] [[CakePHP framework]] .
At the [[document-level]], an [[F-measure]] of 72% (with 81% [[precision]]) for [[Task]] [[mention]]s, 60% (with 81% [[precision]]) for [[Method]] [[mention]]s, 74% (with 78% [[precision]]) for the [[Resource/Feature]] and 79% (with 81% [[precision]]) for the [[Implementation]] [[categori]]es have been achieved.
At the [[input layer]], N [[previous word]]s are [[encoded]] using [[1-of-V coding]], where V is [[size of the vocabulary]] .
At the same time, [[the algorithm]] is [[computationally efficient]] in both [[time]] and [[space]] .
At the same [[time]], [[this book advocate]]s for the [[development of ``goo]] </i> [[measure]]s and good [[measurement practice]]s that will advance the [[study]] of [[user engagement]] and improve our [[understanding]] of [[this construct]], which has become so vital in [[our wired world]] .
At these [[scale]]s, anything short of [[linear scale]] and [[incremental]] is costly to [[deploy]] .
At the task of producing generic [[DUC-style summari]]es, [[HierSum]] yields [[state-of-the-art]] [[ROUGE]] [[performance]] and in [[pairwise user evaluation]] strongly outperforms [[Toutanova et al. (2007)]]'s [[state-of-the-art discriminative system]] .
[[Attitude]]s, [[value]]s, and [[belief]]s that are sometimes collectively referred to as "[[culture]]" play an unquestioned role in [[human behavior]] and [[progress]] .
[[Attribution of climate change]] to [[causal factors]] has been based predominantly on [[simulations]] using [[physical]] [[climate models]], which have inherent [[limitations]] in describing such a [[complex]] and [[chaotic system]] .
A [[tuple]] is [[label]]ed as discriminated if we can observe a significant [[difference]] of [[treatment]] among its [[neighbor]]s belonging to a [[protected-by-law group]] and its [[neighbor]]s not belonging to [[it]] .
At worst, they degenerate into [[sequential]] bouts of [[inflation]], [[recession]], [[retaliation]], and sometimes [[actual violence]] .
A [[unified COA]] is configured with a [[hierarchical set]] of [[linked code]]s based on [[parent-child relationship]]s, with [[lower level code]]s being used by individual [[accounting unit]]s and [[higher level code]]s used for [[consolidation]] of [[accounting/financial information]] (see [[the diagram in Annex]] for an example of [[linked segment]]s and codes that will provide the [[required financial report]]s while effectively [[controlling budget execution]]).
A [[unified model]] that combines [[voting behavior modeling]] and [[topic modeling]] is presented, and an [[iterative learning algorithm]] is proposed to learn the [[topic]]s of [[bill]]s as well as the [[topic-factorized ideal point]]s of [[legislator]]s and [[bill]]s.
A [[universal]], [[Internet-based]], [[bibliographic]] and [[citation database]] would link every [[scholarly work]] ever [[written]] --- no matter how [[published]] --- to every [[work]] that it cites and every work that cites [[it]] .
A [[user]] can visit only a [[limited number]] of [[spatial item]]s, leading to a very [[sparse]] [[user-item matrix]] .
A ''[[user-centric]]'' [[entity detection system]] is one in which the primary [[consumer]] of the [[detected entities]] is a [[person]] who can perform [[actions]] on the [[detected entities]] (e.g. [[perform a search]], [[view a map]], [[shop]], etc.). [[We]] contrast this with [[machine-centric]] [[detection system]]s where the primary [[consumer]] of the detected entities is a [[machine]] .
Author Keywords: [[Automation]], [[Directed Technological Change]], [[Economic Growth]], [[Endogenous Growth]], [[Factor Share]]s, [[Productivity]], [[Task]]s, [[Technological Change]] .
[[AutoExtend]] is designed to take word [[vector]]s as input and unravel the word [[vector]]s to the [[vector]]s of their [[lexeme]]s.
[[Automated analyse]]s, or [[scorecard]]s, are generated on [[cluster]]s consisting of [[tens of thousands]] of [[machine]]s [1] to help guide [[product release]]s, to shorten [[product development cycle]]s, [[measure progress]], and gain valuable [[customer feedback]] .
[[Automated source code analysis]], implemented in [[integrated development environment]]s like [[Eclipse]], has improved [[software maintenance]] significantly.
[[Automated technique]]s can process [[enormous amounts of data]] to find [[new relationship]]s, but generally these are [[represented by]] fairly simple [[model]]s.
Automatically [[classifying web objects]] into manageable [[semantic categories]] has long been a fundamental [[preprocess]] for [[indexing]], [[browsing]], [[searching]], and [[mining]] these objects.
[[Automatic construction]] of [[user-desired]] [[topical hierarchi]]es over [[large volumes of text data]] is a [[highly desirable]] but challenging [[task]] .
[[Automatic information extraction]] and [[information retrieval]] concerning particular [[person]], [[location]], [[organization]], [[title of movie or book]], juxtaposes to the [[Named Entity Recognition (NER) task]] .
[[Automatic]] [[news extraction]] from [[news pages]] is important in many [[Web applications]] such as [[news aggregation]] .
[[Automatic recognition of named entities]] such as [[people]], [[place]]s, [[organization]]s, [[book]]s, and [[movie]]s across the [[entire web]] presents a number of challenges, both of scale and scope.
[[Automatic Task Identification]], [[User Task Modeling]], [[Clustering]], [[User Activity Monitoring]]
[[Automating]] this [[process]] enables the large-scale processing of the [[biomedical literature]] by [[identifying term]]s across [[authors]] and [[scientific documents]] .
[[Autonomous weapons system]]s [[select]] and [[engage target]]s without [[human intervention]]; they become [[lethal]] when those [[target]]s include [[human]]s.
[[Auxiliary variable method]]s; [[Density estimation]]; [[Latent class model]]s; [[Monte Carlo]]; [[Metropolis-Hasting algorithm]] .
Availability of such [[ranking]] can substantially reduce [[enterprise]] [[IP]] ([[Intellectual Property]]) [[management]] [[costs]] .
A [[Van Der Waerden Test]] is a [[non-parametric]] [[statistical hypothesis test]] used for [[analyzing]] the [[homogeneity]] of [[population]] [[distribution function]]s.
A variety of previously introduced annotation [[tool]]s and [[approach]]es also served to guide our design decisions, including the fast [[annotation mode]] of [[Knowtator]] ([[Ogren, 2006]]), the [[search capabiliti]]es of the [[XConc tool]] ([[Kim et al., 2008]]), and the design of [[web-based system]]s such as [[MyMiner]] ([[Salgado et al., 2010]]), and [[GATE Teamware]] ([[Cunningham et al., 2011]]).
A [[Video Game Dual Wield]] is an [[ability]] of the [[player character]] to use two [[weapons]] or hold two [[items]] simultaneously.
A [[Video Game ESRB]] is an [[organization]] that regulates [[video game]]s.
A [[Video Game Spawn]] is the [[generation]] of [[character]], [[item]], or [[mob]] .
A [[video]] of [[Sawyer]], a [[robot]] developed by [[Brooks]]'s company [[Rethink Robotics]], received more than 60,000 [[view]]s in one month (see go.nature.com/jqwfmz).
A [[Vinyl Chloride]] is an [[organochloride]] that is a [[monomer]] used to produce [[polyvinyl chloride]] .
A wealth of [[quantitative]] and [[case-study evidence]] documents a striking [[correlation]] between the [[adoption]] of [[computer-based technologi]]es and the increased use of [[college-educated labor]] within detailed [[industri]]es, within [[firm]]s, and across [[plant]]s within [[industri]]es.<ref>1.
A [[web-based interface]] provides [[sales rep]]s access to the company [[information]] and [[sales lead]]s in a single [[location]] .
(a) Which resulted from a [[violation]] or possible [[violation]] of a [[statute]], [[regulation]], or the [[terms and conditions]] of a [[Federal award]], including for [[fund]]s used to match [[Federal fund]]s;<br>
A widely used [[assumption]] in [[previous work]] is that the [[diffusion network]] is [[homogenous]] and [[diffusion process]]es of [[cascade]]s follow the same [[pattern]] .
A widely used [[benchmark]] for comparing the [[performance]], [[completeness]] and [[soundness]] of [[OWL reasoning engine]]s is the [[Lehigh University Benchmark (LUBM)]] ([[Guo et al., 2005]]).
A [[Winograd schema]] is a [[pair]] of [[sentence]]s that differ only in [[one]] or [[two]] [[word]]s and that contain a [[referential ambiguity]] that is [[resolved in opposite direction]]s in the two [[sentence]]s.
[[AZ]] has been shown to have been [[reliably annotated]] by [[independent]] [[human coders]] and useful for various [[information access tasks]] .
[[Babbage]], [[Turing]], and [[Shannon]] devised [[algorithm]]s and [[hardware]] to play [[the game of chess]] .
[[Backpropagating]] ([[Rumelhart et al., 1986]]) this [[loss]] to the [[word vector]]s trains them to be [[predictive of their contexts]], achieving the desired effect ([[words in similar contexts]] have [[similar vector]]s).
[[Bacterial colonies]] consisting of the [[progeny]] of a single [[parent cell]] [[scatter light]] at 635 nm to [[produce]] unique forward-[[scatter signature]]s.
:(b) [[Addition]]s, [[improvement]]s, [[modification]]s, [[replacement]]s, [[rearrangement]]s, [[reinstallation]]s, [[renovation]]s or [[alteration]]s to [[capital asset]]s that materially increase their [[value]] or [[useful life]] (not ordinary [[repairs and maintenance]]).
[[Bagging predictors]] is a [[method]] for [[generating]] multiple [[versions]] of a [[predictor]] and using these to get an [[aggregated predictor]] .
[[bag]]s, [[cloth]]es, [[luxurious]] </i>) that are competitively shared by multiple [[brand]]s (<i> e.g.
Bai et al. , 2009]]) ⇒ [[author::Ming-Hong Bai]], [[author::Jia-Ming You]], [[author::Keh-Jiann Chen]], [[author::Jason S. Chang]] .
[[Balanced edge partition]] has emerged as a new [[approach]] to [[partition]] an [[input graph data]] for the purpose of [[scaling out]] [[parallel computation]]s, which is of interest for several [[modern data analytics computation platform]]s, including [[platform]]s for [[iterative computation]]s, [[machine learning problem]]s, and [[graph database]]s.
[[Balance]]: See [[Balance Sheet]], [[Statutory Balance]], [[Structural Balance]] .
[[Balance Sheet]]: A [[document]] produced by [[Administration]] and [[Finance]], which [[summarizes revenue]] and [[spending]] by category and [[fund]], and displays the resulting [[condition]] .
[[Balance sheet]]s are produced for [[prior year]]s based on [[actual receipt]]s and [[expenditure]]s, and for [[current]] and future years based on [[projection]]s.
[[Banditron]] [8], a [[multi-class online learning algorithm]] for [[bandit setting]], [[maximize]]s the [[run-time gain]] by balancing between exploration and exploitation with a [[fixed tradeoff parameter]] .
:(b) A [[public international organization]], which is an [[organization]] entitled to enjoy [[privilege]]s, [[exemption]]s, and [[immunities]] as an [[international organization]] under the [[International Organizations Immunities Act]] (22 U.S.C. 288-288f);<br>
Based on a "[[random surfer model]]", [[PPR]] iteratively computes the [[relevance]]s of all [[node]]s in a [[graph]] until [[convergence]] for a given [[user preference distribution]] .
Based on a [[unified formulation]] for both [[directed]] and [[undirected network]]s, the [[optimization problem]] underlying [[BNMTF]] can use either the [[squared loss]] or the [[generalized KL-divergence]] as its [[loss function]] .
Based on [[CRFs]], [[we]] further present a novel [[approach]] for [[constraint]] [[co-reference information extraction]]; i.e., improving [[extraction performance]] given that we know some [[citations]] refer to the same [[publication]] .
Based on [[OLLGC]], we present a [[selective sampling algorithm]], namely [[Selective Sampling with Local and Global Consistency (SSLGC)]], which queries the [[label]] of each [[node]] based on the [[confidence]] of the [[linear function]] on [[graph]]s.
Based on [[ONM]], [[we]] develop a [[probabilistic algorithm to generate ticket routing recommendations]] for new [[tickets]] in a [[network]] of [[expert groups]] .
Based on our [[finding]]s, [[we]] develop a [[model]] of [[human mobility]] that combines [[periodic short range movement]]s with [[travel]] due to the [[social network structure]] .
Based on our findings, [[we]] developed [[heuristics]] for [[mapping]] [[weakly matching]] [[gene names]] to their [[official]] [[gene names]] .
Based on [[our findings]], [[we]] propose [[Rest-Sleep-and-Comment (RSC)]], a [[generative model]] that is able to match all four [[discovered pattern]]s.
Based on our own [[implementation]], [[we]] compare three recent [[proposals]] for [[implementing]] this [[regularization strategy]] .
Based on [[our [raining model]], [[population mobility]] in various [[citi]]es impacted by the [[disaster]]s throughout the [[country]] is able to be [[automatically simulated]] or [[predicted]] .
Based on our results, [[we]] believe that [[fine-tuned]] [[heuristic]]s may provide truly [[scalable solution]]s to the [[influence maximization]] [[problem]] with satisfying [[influence spread]] and blazingly [[fast]] [[running time]] .
Based on that [[our model]] [[suggests]] and [[ranks]] probable [[advisors]] for every [[author]] .
Based on that, [[we]] propose an [[effective]] and [[scalable algorithm]] to solve [[the problem]] .
Based on the [[analysis]], we develop an [[efficient tool]] to [[automatically find]] a [[suitable parameter]] for [[user]]s with no related [[background knowledge]] .
Based on the [[approximation]], [[we]] propose an [[Approximate P-RFP Mining (APM) algorithm]], which effectively and [[efficiently]] [[compress]]es the [[set of probabilistic frequent pattern]]s.
Based on the [[check-in behavior]] of [[user]]s, [[we]] extract [[features of place]]s from i) [[explicit patterns (EP)]] of individual [[place]]s and ii) [[implicit relatedness (IR)]] among [[similar place]]s.
Based on the gained [[insights]], a [[heuristic metric]] that considers both [[accuracy]] and [[diversity]] is proposed to [[explicitly evaluate]] each [[individual classifier]]'s [[contribution]] to the whole [[ensemble]] .
Based on the [[graph]], [[we]] propose a [[feature-aware factorization model]] to [[re-rank the tweet]]s, which unifies the [[linear discriminative model]] and the [[low-rank factorization model]] seamlessly.
Based on [[the model]], [[we]] develop [[local]] and [[global location prediction method]]s.
Based on the [[observation]]s and key [[function]]s, we [[design]], [[implement]] and [[deploy]] a [[web-based application]] of [[recruiting system]], named [[iHR]], for [[Xiamen Talent Service Center]] .
Based on the [[product adoption model]], [[we]] formalize the <i>[[k most marketable products(or k-MMP)]]</i> [[selection problem]] and [[formally prove]] that the problem is <i>[[NP-hard]] </i>.
Based on the [[properties of an item]], the [[max-frequency]] of an [[item]] is counted over a [[sliding window]] of which the [[length]] changes [[dynamically]] .
Based on the [[results of the presented analysis]] and focusing on the [[long-term business value]] of [[marketing strategi]]es in [[social media]], we find that [[promotional event]]s leveraging [[implicit]] or [[explicit advocacy]] in [[social media platform]]s result in [[significant abnormal]] [[return]]s for the participating [[brand]], in [[term]]s of [[expanding]] the [[social media]] [[fan base]] of the [[firm]] .
Based on these [[aggregations]], [[we]] compute multiple [[clustering results]] using [[K-Means]] .
Based on the [[STG model framework]], [[we]] propose a novel [[recommendation algorithm]] [[Injected Preference Fusion (IPF)]] and extend the [[personalized Random Walk]] for [[temporal recommendation]] .
Based on this [[assumption]], [[we]] propose a [[two-stage algorithm]], [[Periodica]], to solve [[the problem]] .
Based on this intuition, [[we]] [[formalize]] the [[social action]] [[tracking problem]] using the [[NTT-FGM model]]; then present an [[efficient algorithm]] to [[learn the model]], by [[combining]] the ideas from both [[continuous linear system]] and [[Markov random field]] .
Based on this [[model]], [[we]] develop an [[efficient]] [[crawler]] which is 260% [[faster]] than some [[state-of-the-art methods]] in terms of [[fetching]] new generated [[content]]; and meanwhile our [[crawler]] also ensure a high [[coverage]] [[ratio]] .
Based on [[this model]], [[we]] introduce the [[best-first search algorithm]] [[MiMAG]] .
Based on [[this model]], [[we]] propose the [[NOBLE algorithm]] using an [[efficient Gibbs sampler]] .
Based on this observation, [[we]] propose a [[never-ending learning framework]] for [[time series]] in which an [[agent]] examines an [[unbounded stream of data]] and occasionally asks a [[teacher]] (which may be a [[human]] or an [[algorithm]]) for a [[label]] .
Based on this [[PTD function]], [[we]] develop [[two]] [[algorithms]], [[LCP]] and [[SkyRoute]], for finding the [[recommended routes]] .
Basically, if we feed the [[sampled data]] from [[0-bit CWS]] to a highly [[efficient]] [[linear classifier]] (e.g., [[linear SVM]]), we effectively (and approximately) [[train]] a [[nonlinear classifier]] based on the [[min-max kernel]] .
Basic [[SLF]] [[models]] how likely a [[word]] could be assigned to [[target class]] [[types]] .
[[Basilisk]] begins with an [[unannotated]] [[corpus]] and [[seed word]]s for each [[semantic category]], which are then [[bootstrapped]] to [[learn new word]]s for each [[category]] .
[[Bayesian Network]]s, [[Hybrid Random Field]]s, [[Markov Random Field]]s, [[Modularity]], [[Scalability]]
[[Bayesian]]s believe [[learning]] is a form of [[probabilistic inference]] and have their roots in [[statistics]] .
:(b) Current [[fair market value]] of the [[property]], [[improvement]]s, or both, to the extent the [[cost]]s of [[acquiring]] or [[improving]] the [[property]] were included as [[project cost]]s.
Because a great number of [[machine learning]] and [[data mining applications]] need [[proximity measures]] over [[data]], a [[simple]] and [[universal]] [[distance metric]] is desirable, and [[metric learning methods]] have been explored to produce sensible [[distance measures]] consistent with [[data relationship]] .
Because an [[ontology]] is a [[formal]], [[explicit specification]] of a [[shared conceptualization]] for a [[domain]] of [[interest]], the use of [[ontologies]] is a natural way to solve traditional [[information retrieval problem]]s such as [[synonym]]/[[hypernym]]/[[hyponym problems]] .
Because a [[pairwise comparison]] of all [[record]]s [[scales quadratically]] with the [[number of records]] in the [[data set]]s to be [[matched]], it is common to use [[blocking]] or [[indexing technique]]s to reduce the number of [[comparison]]s required.
Because [[authors]] can use any one of the known [[names]] for a [[gene]] or [[protein]], [[information retrieval]] and [[extraction]] would benefit from identifying the [[gene]] and [[protein term]]s that are [[synonym]]s of the [[same]] [[substance]] .
Because both [[false positive]]s and [[false negative]]s have [[high cost]], [[our deployed system]] uses a [[tiered strategy]] combining [[automated]] and [[semi-automated method]]s to ensure [[reliable classification]] .
Because most previous [[work]]s on [[semantic similarity]] only support a unique [[input ontology]], [[we]] propose a [[method]] to enable [[similarity estimation]] across multiple [[ontologi]]es.
Because of this, even in [[experiment]]s involving millions of [[subject]]s, the [[difference]] in [[mean outcome]]s between [[control]] and [[treatment condition]]s can have substantial [[variance]] .
Because [[real hourly compensation]] and [[labor productivity]], which is [[output per hour]], both include [[hours worked]] in their [[calculations]], changes in [[hours worked]] have no impact on [[the gap]] .
Because they are [[centralized]] and [[hierarchical]], [[state]]s tend to produce [[higher degree]]s of [[social inequality]] than earlier [[kinship-based forms of organization]] .
Because [[user ID matching]] across [[multiple source]]s on [[the Internet]] is very [[difficult]], and most [[user activity logs]] have to be [[anonymized]] before they are [[processed]], no [[prior studies]] could collect comparable [[multifaceted]] [[activity data of individuals]] .
become necessary that [[digital contents]] should be expressed not only in [[natural language]], but also in a [[form]] that can be [[understood]], [[interpreted]] and used by [[software agents]], thus permitting them to [[find]], [[share]] and [[integrate information]] more easily.
Before we can solve the [[active sample selection problem]], [[we]] need to find a [[set]] of [[optimal]] [[subgraph feature]]s.
[[Behavioral pattern discovery]] is increasingly being [[studied]] to understand [[human behavior]] and the [[discovered pattern]]s can be used in many [[real world application]]s such as [[web search]], [[recommender system]] and [[advertisement targeting]] .
[[Behavioral targeting (BT)]] leverages [[historical user behavior]] to [[select]] the [[ad]]s most relevant to [[user]]s to display.
[[Behavior]]s are [[time-varying]], [[reactive value]]s, while [[event]]s are [[set]]s of [[arbitrarily complex]] [[condition]]s, carrying [[possibly rich information]] .
Being a [[single pass streaming algorithm]], our [[procedure]] also maintains a [[real-time estimate]] of the [[transitivity]] / [[number of triangles of a graph]], by storing a miniscule [[fraction]] of [[edge]]s.
[[Belief Revision]] ([[Alchourron, Gardenfors, and Makinson 1985]]) enables [[modeling]] and [[reasoning]] about [[changes of beliefs]], including as a result of [[knowledge producing actions]] in [[Situation Calculus]] ([[Shapiro et al. 2000]]).
[[Bering]] traces all of these [[belief]]s and [[desire]]s to a single trait of [[human psychology]], known as the "[[theory of mind]]," which enables us to guess at the [[intention]]s and [[thought]]s of [[other]]s.
[[Beside]]s, an [[efficient]] [[coordinate ascent algorithm]] is adopted to [[learn]] the [[model]] .
Besides [[application program]]s, such [[robot]]s would host a suite of [[software “conditioning modules”]] that would generate [[positive]] and [[negative reinforcement]] signals in [[pre­defined circumstance]]s.
Besides, [[AutoPhrase]] can be [[extend to model]] [[single-word quality phrase]]s.
Besides being one of the first [[papers]] to our [[knowledge]] on [[data mining]] for [[on-line]] [[brand]] [[advertising]], [[this paper]] makes several important [[contributions]] .
Besides being [[parameterless]], this way of [[measuring]] the [[support of item]]s was shown to have the advantage of a faster [[detection]] of [[bursts]] in a [[stream]], especially if the [[set of item]]s is [[heterogeneous]] .
Besides, both of [[TD]] and [[CP]] [[decompose]] a [[tensor]] into several [[factor matrice]]s.
Besides [[discovering latent semantics]], [[supervised topic models (STMs)]] can make [[prediction]]s on [[unseen test data]] .
Besides [[language modeling]], [[word embeddings induced by neural language models]] have been useful in [[chunking]], [[NER]] ([[Turian et al., 2010]]), [[parsing]] ([[Socher et al., 2011b]]), [[sentiment analyssi]] ([[Socher et al., 2011c]]) and [[paraphrase detection]] ([[Socher et al., 2011a]]).
Besides, many [[compani]]es (e.g., [[Amazon]] and [[Netflix]]) have been using [[RS]] extensively to target their [[customer]]s by [[recommending products or service]]s.
Besides, multiple [[model]]s usually carry complementary [[predictive information]], [[model combination]] can potentially provide more [[robust]] and [[accurate prediction]]s by [[correcting]] [[independent error]]s from [[individual model]]s.
Besides, [[our proposed system]] is capable of supporting automatically [[configuring]] and [[scheduling analysis task]]s, and [[balancing]] [[heterogeneous computing resource]]s.
Besides overcoming the [[limitation]]s of [[existing model]]s, [[we]] show that [[our work]] achieves better perplexity on [[unseen data]] and [[identifi]]es more [[coherent]] [[topic]]s.
Besides, [[researcher]]s have recently discovered a [[spatial clustering phenomenon]] in [[human mobility behavior]] on the [[LBSN]]s, i.e., [[individual]] [[visiting locations]] tend to [[cluster together]], and also demonstrated its effectiveness in [[POI recommendation]], thus we incorporate it into the [[factorization model]] .
Besides, [[researcher]]s have recently discovered a [[spatial clustering phenomenon]] in [[human mobility behavior]] on the [[LBSN]]s, i.e., [[individual]] visiting [[locations tend to cluster together]], and also demonstrated its [[effectiveness]] in [[POI recommendation]], thus we incorporate it into the [[factorization model]] .
Besides, since the [[transformed feature]]s naturally [[expect]] [[non-negative weight]]s, we only require [[smooth optimization]] even with the [[L1 regularization]] .
Besides the [[cluster]]s, the [[summary graph]] also contains the essential [[relationship]]s between both [[type]]s of [[cluster]]s and thus reveals the [[hidden structure]] of the [[data]] .
Besides the [[theoretical]] [[analysis]], [[we]] [[empirically]] show that our [[method]] outperforms other [[state-of-the-art]] [[tag recommendation method]]s like [[FolkRank]], [[PageRank]] and [[HOSVD]] both in [[quality]] and [[prediction runtime]] .
Besides, [[we]] also provide an [[efficient]] and [[exact]] [[FT-support]] [[counting procedure]] .
Better understanding of the [[temporal dynamics]] of this [[phenomenon]] can [[inform solution]]s to the [[similarity-diversity dilemma]] of [[recommender system]]s.
Between [[January]] [[1998]] and [[June]] [[2009]], about 1.45 million [[people]] were [[long-term unemployed]] on average in each month.
Betweenness [[centrality measure]]s the importance of a [[vertex]] by quantifying the [[number of time]]s it acts as a [[midpoint]] of the [[shortest path]]s between other [[vertice]]s.
Beyond accurate [[recommendation]] [[performance]], [[CTL]] is also insensitive to [[parameter tuning]] as confirmed in the [[sensitivity analysis]] .
Beyond our [[novel definition]]s / [[representation]]s, which allow for meaningful and [[extendable specification]]s of [[rule]]s, we further show [[novel algorithm]]s that allow us to quickly discover [[high quality rule]]s in [[very large dataset]]s that accurately [[predict]] the [[occurrence of future event]]s.
Beyond the [[theoretical support]], [[we]] show empirically that [[the proposed learning method]] is highly effective in dealing with [[biase]]s, that it is robust to [[noise]] and [[propensity model misspecification]], and that it [[scale]]s [[efficiently]] .
:(b) [[Federal agencies]] strengthening [[partnership]]s and [[working cooperatively]] with [[non-Federal entities]] and their [[auditor]]s; and [[non-Federal entities]] and their [[auditor]]s [[working cooperatively]] with [[Federal agencies]];<br>
[[Bibliometrics]], [[Context]], [[Gleason's Theorem]], [[Recommender Systems]] .
[[Big Data Pipeline]]s decompose [[complex analyse]]s of [[large data set]]s into a [[series]] of [[simpler task]]s, with independently [[tuned component]]s for each [[task]] .
[[Big deep neural network (DNN) models]] [[train]]ed on [[large amount]]s of [[data]] have recently achieved the best [[accuracy]] on [[hard task]]s, such as [[image]] and [[speech recognition]] .
Big [[information cascade]]s of [[post resharing]] can form as [[user]]s of these [[site]]s [[reshare]] others' [[post]]s with their [[friend]]s and [[follower]]s.
:(b) [[Improper payment]] includes any [[payment]] to an [[ineligible party]], any [[payment]] for an [[ineligible]] [[good]] or [[service]], any [[duplicate payment]], any [[payment]] for a [[good]] or [[service]] not received (except for such [[payment]]s where authorized by [[law]]), any [[payment]] that does not account for [[credit]] for applicable [[discount]]s, and any [[payment]] where insufficient or lack of [[documentation]] prevents a [[reviewer]] from discerning whether a [[payment]] was proper.
[[Biological system]]s, ranging from the [[molecular]] to the [[cellular]] to the [[organism level]], are [[distributed]] and in most cases [[operate without central control]] .
[[Biomedical named entity recognition]]; [[Conditional random fields]]; [[Literature mining]]; [[Linguistic feature]]s.
[[Blanket contract]]: [[Contract between the state]] and a [[vendor]], guaranteeing a [[fixed price]] for a [[product]] .
[[Blockmodelling]] is an important [[technique]] for [[decomposing graph]]s into [[set]]s of [[role]]s.
[[Blogger]]s are driven to [[document their lives]], [[provide commentary and opinions]], [[express deeply felt emotions]], [[articulate ideas through writing]], and [[form and maintain community forums]] .
[[Boldi and Vigna (WWW 2004)]], showed that [[Web]] [[graph]] can be [[compressed]] down to three [[bit]]s of [[storage]] [[per]] [[edge]]; we study the [[compressibility]] of [[social network]]s where again [[adjacency queries]] are a fundamental [[primitive]] .
[[Boolean matrix factorization (BMF)]] - where [[data]], [[factor]]s, and [[matrix product]] are [[Boolean]] - has received increased attention from the [[data mining community]] in recent years.
[[boosting algorithm]]s, [[multiclass classification]], [[output coding]], [[decision trees]]
[[bootstrap]] - [[clustering and classification]] - [[nonparametric probability density estimation]] - [[numerical method]]s - [[random number generation]]
Boredom is a [[listless casting-about]] for [[purpose]]; the drifting [[existential anguish]] of one's [[life]] experienced as [[meaningless]], even if only [[temporarily]] so.
Both [[algorithm]]s are very [[efficient]] and [[scale]] well to [[large graph]]s.
Both [[behavior]]s are not [[learned]] but [[genetically coded]] and [[emerge spontaneously]] as [[individuals interact]] .
Both [[benchmark]]s do not employ [[benchmarking technique]]s such as [[system warm-up]], [[simulating concurrent client]]s, and executing mixes of [[parameterized queri]]es in order to [[test]] the [[caching strategy]] of a [[SUT]] .
Both [[classification]] and [[ranking of the nodes]] (or [[data object]]s) in such [[network]]s are essential for [[network analysis]] .
Both [[document clustering]] and [[word clustering]] are well studied [[problems]] .
Both [[feature extraction]] and [[feature selection]] have the advantage of improving [[learning performance]], increasing [[computational efficiency]], [[decreasing]] [[memory storage requirement]]s, and building better [[generalization model]]s.
Both [[field]]s are broadly concerned with the [[connection]] between [[machine]]s and [[intelligent human agent]]s.
Both [[problem]]s have important [[practical application]]s, such as [[predicting interactions between pairs of proteins]] and [[recommending [[friend]]s in social networks]] respectively.
Both [[scoring function]]s are proven to be only [[dependent]] on the [[structure]] of a [[cascading]] [[graph]] and can be calculated in [[polynomial time]] .
Both sections follow the same [[general format]], including [[fund structure]], [[balance sheet account]]s, [[revenues]] and other [[financing source]]s, and [[expenditures]] and other [[financing use]]s, including [[function]], [[activity]], and [[object classifications]] .
[[Both surveys]] used [[computer-assisted self-interview]]s with [[randomized response question]]s.
Both [[system]]s [[extract facts]] on a [[large scale]] from [[Web corpora]] in an [[unsupervised manner]] .
Both the [[conditionals]] and the [[marginals]] of a [[joint Gaussian]] are again [[Gaussian]] .
Both the [[offline experiment]]s on a [[real-world]] [[large-scale]] [[dataset]] and [[online]] [[A/B test]]s on a [[commercial platform]] demonstrate the [[effectiveness]] of [[our proposed solution]] in exploiting [[arbitrage]] in various [[model setting]]s and [[market environment]]s.
Both the [[time and space complexiti]]es of [[our algorithm]]s are [[linear]] in the [[size of the network]], which allows us to [[experiment]] with millions of [[node]]s and [[edge]]s.
[[Bourigault]] claims that the [[grammar]] can [[parse]] "around 95% of the maximal length noun phrases" in a [[test]] [[corpus]] into possible [[terminological phrases]], which then require [[manual]] [[validation]] .
(b) Owing to [[synchronization-based constrained clustering]] and the [[P-Tree]], it supports an [[efficient]] and effective [[data representation]] and [[maintenance]] .
[[Brain Network]], [[Alzheimer’s Disease]], [[Neuroimaging]], [[FDGPET]], [[Sparse Inverse Covariance Estimation]]
[[BRAT]], along with [[conversion tool]]s and extensive [[documentation]], is [[freely available]] under the [[open-source MIT]] license from its homepage at
[[BRAT]] is [[browser-based]] and built entirely using [[standard web technologi]]es.
[[Breadth-first search (BFS)]] is a [[core primitive]] for [[graph traversal]] and a basis for many [[higher-level]] [[graph analysis algorithm]]s.
[[Breast cancer]] is a [[heterogeneous disease]] with respect to [[molecular alteration]]s, [[cellular composition]], and [[clinical outcome]] .
[[Brown cluster]]s have been used successfully in a variety of [[NLP application]]s: [[NER]] ([[Miller et al., 2004]]; [[Liang, 2005]]; [[Ratinov & Roth, 2009]]), [[PCFG parsing]] ([[Candito & Crabb´e, 2009]]), [[dependency parsing]] ([[Koo et al., 2008]]; [[Suzuki et al., 2009]]), and [[semantic dependency parsing]] ([[Zhao et al., 2009]]).
[[Bruno Frey]] barely recalls writing the piece, but I can still recite its first sentence: “The [[agent]] of [[economic theory]] is [[rational]], [[selfish]], and his [[taste]]s do not [[change]].”
[[BSC]] integrates [[financial measure]]s with other [[key performance indicator]]s to create a perspective that incorporates both [[financial]] and [[non-financial aspect]]s.
(b) The [[social context]] is determined by the available [[system]] [[features]] and is unique in each [[social media]] [[website]] .
[[Budgetary account]]s: [[Direct appropriation]] and [[retained revenue]] [[account]]s.
[[Budgeted fund]]s: The funds that contribute to the [[commonwealth]]'s [[statutory balance]], currently the [[General Fund]], [[Highway Fund]] and [[Health]] and [[Wellness Fund]] .
[[Bugs]] have caused billions of [[dollars]] [[loss]], in addition to [[privacy]] and [[security threats]] .
Building an [[aesthetic]]s of [[interactive system]]s, [[Salen and Zimmerman]] define core [[concept]]s like "[[play]]," "[[design]], " and "[[interactivity]] .
Building on [[Fiske]]'s [[relational theory (1992)]], [[we]] propose that there are two types of markets that determine relationships between [[effort]] and [[payment]]: [[monetary]] and [[social]] .
Building on recent developments in [[regularization theory]] for [[graphs]] and corresponding [[Laplacian-based methods for classification]], [[we]] develop an [[algorithmic framework]] for [[learning]] [[ranking functions on graph data]] .
Building on these [[finding]]s, [[we]] describe a [[supervised learning framework]] which exploits these [[prediction feature]]s to [[predict]] new [[link]]s among [[friends-of-friend]]s and [[place-friend]]s.
[[Building richer models]] of [[user]]s' [[current]] and [[historic search task]]s can help improve the [[likelihood]] of [[finding relevant content]] and enhance the [[relevance]] and [[coverage]] of [[personalization method]]s.
Building upon the [[distance dynamics]], [[Attractor]] has several remarkable [[advantage]]s: (a) [[It]] provides an [[intuitive way]] to [[analyze]] the [[community structure]] of a [[network]], and more importantly, [[faithfully capture]]s the [[natural communiti]]es (with [[high quality]]).
Built on our [[experience]] with [[Shark]], [[Spark SQL]] lets [[Spark programmer]]s leverage the benefits of [[relational processing]] (e.g. [[declarative queri]]es and [[optimized storage]]), and lets [[SQL user]]s call [[complex analytics librari]]es in [[Spark]] (e.g. [[machine learning]]).
[[Burberry]], [[Prada]], and [[Chanel]] </i>), and [[tracking temporal evolution]] of the [[brand]]s' [[stake]]s over the [[shared topic]]s.
[[Business partners in the US]], [[Europe]], and [[South America]] provided questions on new [[vehicle feature]]s, [[sales volume]]s, [[take rate]]s, [[pricing]], and [[macroeconomic trend]]s.
But [[autonomous robot]]s and [[human]]s differ greatly in their [[abiliti]]es.
But [[code]], the [[sequences of symbol]]s painstakingly created by [[programmer]]s, is not quite the same as [[software]], the [[sequences of instruction]]s that [[computers execute]] .
But [[experiential reward]]s are perhaps just as important: these refer to the [[temporary]] [[improvement]] in [[positive mood]] [[people]] experience when they are acting in [[goal-directed]], [[purposeful way]]s.
But for [[bilingual terminology mining]], and for many [[language pair]]s, [[large]] [[comparable corpora]] are not available.
But has the [[conception of]] [[scarcity]] been [[politicized]], [[naturalized]], and [[universalized]] in [[academic]] and [[policy debate]]s?
But [[inequality]] is increasing within the [[99%]], too, as a consequence of a growing premium on [[college]] and [[postgraduate education]]: The fates of the [[tech-savvy worker]] at [[Google]] and the [[blue-collar employee]] at [[General Motors]] have been decoupled (see [[Autor, p.843]]).
But rather than allowing a massive reduction of [[working hours]] to free the [[world’s population]] to pursue their own [[project]]s, [[pleasure]]s, [[vision]]s, and [[idea]]s, we have seen the ballooning not even so much of the [[“service” sector]] as of the [[administrative sector]], up to and including the creation of whole new [[industri]]es like [[financial service]]s or [[telemarketing]], or the unprecedented expansion of sectors like [[corporate law]], [[academic]] and [[health administration]], [[human resource]]s, and [[public relations]] .
But [[Simon]] treated [[labor]] as [[homogeneous]], and ignored the [[distribution]] of [[ownership of robots and related machines]] that is central to [[analyzing]] the [[impact of robots/mechanization]] on [[society]] .
But what if we need to [[predict complex objects]] like [[trees]], [[orderings]], or [[alignments]]?
But while such a "[[gold-standard]]" [[sample]] can be easily computed over the [[aggregated data]] (the set of [[key-frequency pair]]s), [[exact aggregation]] itself is [[costly]] and slow.
But within its [[domain]], [[capitalism]] works effectively through a [[system of ethics]] that provides the [[vision]] and the [[trust]] needed for successful use of the [[market mechanism]] and related [[institution]]s.
By [[2008-2012]], the [[suburb]]s accounted for [[40 percent]] of [[resident]]s living in such areas in [[the nation]]'s [[100]] largest [[metro area]]s.
By [[2014]] the [[unemployment rate]] for [[young Black]]s and [[Hispanics]] in [[New York City]] reached their [[lowest level]]s in a [[decade]], falling to [[11 percent]] and [[9 percent]] .
By [[2050]] [[robot "brains"]] based on [[computer]]s that execute [[100]] [[trillion instructions per second]] will start [[rivaling]] [[human intelligence]]
By a [[careful design]] to [[adjust]] the effort for each [[iteration]], [[our method]] is [[efficient]] regardless of loosely or strictly solving the [[optimization problem]] .
By allowing one to use [[domain]] [[knowledge]] as well as [[link]] [[knowledge]], the [[method]] was very successful for [[pinpoint]]ing misstated [[account]]s in our [[sample]] of general ledger [[data]], with a significant improvement over the default [[heuristic]] in [[true positive rates]], and a lift [[factor]] of up to 6.5 (more than twice that of the default [[heuristic]]).
By also examining the [[context]] of the [[tuple]], i.e. the other [[tuples]] to which it is [[linked]], [[we]] can come up with a more accurate [[linkage]] [[decision]] .
By analyzing [[real data collected]] from [[Meetup]], [[we]] investigated [[EBSN properti]]es and [[discovered]] many unique and interesting [[characteristic]]s, such as [[heavy-tailed degree distribution]]s and [[strong locality]] of [[social interaction]]s.
By applying a warm [[start strategy]], [[we]] investigate issues such as using [[primal]] or [[dual formulation]], choosing [[optimization method]]s, and creating practical [[implementation]]s.
By applying [[data integrity constraint]]s, potentially [[large section]]s of [[data]] could be [[flag]]ged as being [[noncompliant]] .
By assuming that a [[user's label]]s correspond to [[topic]]s in the [[article]]s he [[share]]s, we can [[learn]] a [[labeled dictionary]] from a [[training]] [[corpus]] of [[article]]s shared on [[Twitter]] .
By [[averaging]] [[predicted labels]] over all [[cases]] in the [[unlabeled corpus]], [[SLF training]] builds [[class label]] [[distribution]] [[patterns]] for each [[word]] (or [[word attribute]]) in the [[dictionary]] and [[re-trains]] the current [[model]] [[iteratively]] adding these [[distributions]] as extra [[word features]] .
By carrying out the majority of [[migration]] while [[OS]]es continue to [[run]], [[we]] achieve impressive performance with minimal [[service downtime]]s; [[we]] demonstrate the [[migration]] of entire [[OS instance]]s on a [[commodity cluster]], recording [[service downtime]]s as low as 60<i>ms</i>.
By characterizing their [[inherent]] and [[structural behaviors]] in a [[topological]] perspective, these [[studies]] have attempted to [[discover hidden knowledge]] in the [[systems]] .
By combining a [[set]] of [[sampling technique]]s we are able to reduce the <i>O</i> (<i> K</i> <sup>3</sup> + <i>[[DK]]</i> <sup>2</sup> + <i>[[DNK]]</i> [[complexity]] in [27] to <i>O</i> (<i> [[DK]]</i> + <i>[[DN]] </i>) when there are <i>K</i> [[topic]]s and <i>D</i> [[document]]s with [[average]] [[length]] <i>N </i>.
By combining [[organized algorithm]]s and [[device]]s, [[we]] propose a [[rapid testing method]] that [[detect]]s [[high-performing variation]]s with few [[user]]s.
By combining simple but effective [[indexing]] and [[disk block accessing technique]]s, [[we]] have developed a [[sequential algorithm]] [[iOrca]] that is up to an [[order-of-magnitude]] [[fast]]er than the [[state-of-the-art]] .
By combining the [[traditional]], [[declarative]], [[statistical semantics]] of [[factor graph]]s with [[imperative definition]]s of their [[construction]] and [[operation]], [[we]] allow the user to mix [[declarative]] and [[procedural domain knowledge]], and also gain significant [[efficienci]]es.
By combining [[topology]] in [[mathematics]] with the [[field theory]] in [[physic]]s, [[we]] propose the [[topological potential approach]], which set up a [[virtual field]] by the [[topological space]] to reflect [[individual activiti]]es, [[local effect]]s and [[preferential attachment]] .
By combining two [[complementary]] [[sources of information]] – [[corpus statistic]]s and [[network structure]] – [[we]] derive useful [[vector]]s also for [[concepts that occur rarely]] .
By comparing with [[state-of-the-art method]]s, [[the results]] demonstrate that the improvement in [[our proposed approach]] is [[consistent]] and [[promising]] .
By compiling [[sources of mortality]] from a [[comprehensive sample of mammal]]s, [[we]] assessed the [[percentage of deathd]] due to [[conspecific]]s and, using [[phylogenetic comparative tool]]s, [[predicted this value]] for [[human]]s.
By considering how [[firm]]s can deter imitation by [[innovation]], [[we]] develop a more [[dynamic]] view of how [[firm]]s create new [[knowledge]] .
By contrast, [[market segment]]s [[encode]] [[complex (future) behavior]] of the [[individual]]s which cannot be [[represented by]] a [[single variable]] .
By designing a [[concise route assignment mechanism]], [[SCRAM]] achieves better [[recommendation fairness]] for [[competing taxi]]s.
By doing so [[our approach]] outperforms [[state-of-the-art]] [[distant supervision]] .
By efficient, [[we]] refer to the [[low correlation]] (and thus [[low redundancy]]) among [[generated]] [[codes]] .
::: by [[estimating]] the best-fitting <math>\beta</math> [[parameter]]s that [[optimize]]s a [[objective function]] of the form:
::: by [[estimating]] the best-fitting <math>\beta</math> [[parameter]]s that [[optimize]]s the following [[objective function]]:
By examining the [[relation]] between [[urban mobility]] and [[fare purchasing habit]]s in [[large dataset]]s from [[London]], [[England's public transport network]], [[we]] estimate that [[traveller]]s in the [[city]] [[cumulatively]] spend, [[per year]], up to [[approximately GBP]] 200 million more than they need to, as a result of purchasing the [[incorrect fare]]s.
By exploiting all these different [[kinds of information]], the [[detection]] of more interesting [[cluster]]s in the [[graph]] can be [[supported]] .
By exploiting the [[connection]] between [[graph clustering objective function]]s and a [[kernel k-means objective]], our new [[low-rank solver]] can also compute [[overlapping communiti]]es of [[social network]]s with [[state-of-the-art]] [[accuracy]] .
By exploiting the [[local symmetri]]es of [[linear reconstructions]], [[LLE]] is able to learn the [[global structure]] of [[nonlinear manifold]]s, such as those [[generated by]] [[images of face]]s or [[documents of text]] .
By identifying [[concept]]s and [[semantic relation]]s between [[concept]]s, we can detect [[research paper]]s which deal with the same [[problem]], or [[track]] the [[evolution of result]]s on a certain [[task]] .
By identifying the [[high-risk patient]]s our [[prediction system]] can be used by [[healthcare provider]]s to prepare both [[financially]] and [[logistically]] for the [[patient need]]s.
By incorporating [[aspect-level user interaction]]s and various [[diffusion pattern]]s, a new model for [[inferring]] [[Multi-aspect transmission Rate]]s between [[user]]s using [[Multi-pattern cascades (MMRate)]] is proposed.
By incorporating [[ensemble members]] in [[decreasing order]] of their [[contributions]], [[subensembles]] are formed such that [[users]] can select the top <math>p</math> [[percent]] of [[ensemble members]], depending on their [[resource availability]] and tolerable [[waiting time]], for [[predictions]] .
By incorporating these three [[measure]]s, [[we]] propose a [[probabilistic graphical model]], which [[simultaneously infer]]s truth as well as [[source quality]] without any a [[priori training]] involving [[ground truth answer]]s.
By integrating the two [[model]]s, [[we]] effectively model the [[temporal dynamic]]s of multiple [[correlated]] [[text stream]]s in a [[unified framework]] .
By [[interpolating linearly]] between the two [[objective function]]s, [[we]] introduced the [[GDT estimator]] .
By introducing an [[auxiliary distribution]] based on the [[co-clustering assumption]], such [[conditional probability]] can be [[convert]]ed into an [[objective function]] .
By [[making explicit]] the [[conditions]] under which an [[agent]] can drop her [[goal]]s, i.e., by specifying how the [[agent]] is committed to her [[goal]]s, the [[formalism]] provides [[analyse]]s for [[Bratman]]'s three characteristic [[functional role]]s played by intentions ([[Bratman, 1986]]), and shows how [[agent]]s can avoid intending all the [[foreseen side-effect]]s of what they actually [[intend]] .
By marrying with [[advanced learning technique]]s, the [[predictive strength]]s of [[STM]]s have been dramatically enhanced, such as [[max-margin supervised topic model]]s, [[state-of-the-art method]]s that integrate [[max-margin learning]] with [[topic model]]s.
By [[mathematical transformation]], [[we]] show that some [[validation measures]] are equivalent.
By [[minimizing]] the [[objective function]], subject to a number of [[constraint]]s derived from [[supervision information]], [[we]] simultaneously [[learn]] the [[optimal]] [[parameters of the model]] and the [[optimal]] [[ranking score]]s of the [[node]]s.
By [[mining]] the [[linkage structure]] of [[heterogeneous information network]]s, multiple [[types of relationship]]s among different [[class label]]s and [[data samples]] can be [[extracted]] .
By mining this enormous [[set]] of [[Auto-GPS mobile sensor data]], the [[short-term]] and [[long-term]] [[evacuation behavior]]s for [[individual]]s throughout [[Japan]] during [[this disaster]] are able to be [[automatically discovered]] .
By modeling [[implicit species exchange]]s as a [[network]] that we refer to as the [[Species Flow Network (SFN)]], [[large-scale]] [[species flow dynamics]] are studied via a [[graph clustering approach]] that decomposes the [[SFN]] into [[clusters of port]]s and [[inter-cluster connection]]s.
By [[night]], unseen by [[villager]]s, the [[werewolve]]s choose and kill a [[victim]] .
By [[optimizing]] an [[objective function]] based on a [[fine-grained notion]] of [[influence]] between [[document]]s, [[our approach]] efficiently selects a [[set]] of highly relevant [[article]]s.
By “[[precarious work]],” I mean [[employment]] that is [[uncertain]], [[unpredictable]], and [[risky]] from the point of view of the [[worker]] .
By reinterpreting [[correlation measure]]s in [[ROC space]] and formulating [[correlated itemset mining]] as a [[constraint programming problem]], [[we]] obtain new [[theoretical insight]]s with [[practical benefit]]s.
By [[relativizing]] one [[agent]]'s [[intention]]s in terms of [[belief]]s about another [[agent]]'s [[intention]]s (or [[belief]]s), [[we]] derive a preliminary account of [[interpersonal commitment]]s.
By "[[repeated measures data]]" we mean [[data]] [[generated by]] [[observing]] a number of individuals repeatedly under [[differing]] [[experimental conditions]] where the [[individuals]] are assumed to constitute a [[random sample]] from a [[population]] of [[interest]] .
By separating [[artificial factor]]s from [[user behavior]], [[we]] show that there are actually underlying [[rule]]s in common for [[on-line conversation]]s in different [[social media]] [[website]]s.
By separating [[model representation]] from [[actual implementation]], [[Caffe]] allows [[experimentation]] and [[seamless switching]] among platforms for [[ease of development]] and [[deployment]] from [[prototyping machines]] to [[cloud environment]]s.
By [[summarizing]] the [[longitudinal data]], the [[temporal graph]]s are also robust and [[resistant to noisy]] and [[irregular observation]]s.
By translating the [[ontologi]]es of the [[OntoFarm dataset]] into eight different [[languages-Chinese]], Czech, Dutch, French, [[German]], [[Portuguese]], [[Russian]], and [[Spanish-we]] created a comprehensive set of [[realistic test case]]s.
By treating [[capital]] as [[elastic]] and [[labor]] as [[inelastic]], [[Simon]] essentially put [[Malthus]] upside down.
By using [[normalized affinity matrice]]s which are both [[symmetric]] and [[stochastic]], [[we]] also obtain both a [[probabilistic interpretation]] of [[our method]] and certain [[guarantees of performance]] .
By using two [[loss functions measuring]] the [[degree of divergence]] between the [[training]] and [[predicted rating]]s, [[we]] formulate the [[problem of learning binary code]]s as a [[discrete optimization problem]] .
By viewing the [[evolution of topics]] as a [[Markov chain]], [[we]] [[estimate a Markov transition matrix]] of [[topic]]s by leveraging [[social interaction]]s and [[topic semantics]] .
:(c) A [[charitable organization]] located in a [[country]] other than the [[United States]] that is [[nonprofit]] and [[tax exempt]] under the [[law]]s of its [[country]] of [[domicile]] and [[operation]], and is not a [[university]], [[college]], [[accredited degree-granting institution of education]], [[private foundation]], [[hospital]], [[organization]] engaged exclusively in [[research]] or [[scientific activities]], [[church]], [[synagogue]], [[mosque]] or other similar [[entities]] organized primarily for [[religious]] purposes; or<br>
[[Caching method]]s for efficient [[runtime performance]], [[methods for efficient training]] with [[large output vocabulari]]es and [[attention model]]s are also not discussed.
[[CAD]] is particularly suitable for [[detecting unusual outcome]]s, [[unusual behavior]]s, and [[unusual attribute pairing]]s.
[[Caffe]] fits [[industry]] and [[internet-scale media]] needs by [[CUDA]] [[GPU computation]], [[processing]] over 40 million [[images]] [[a day]] on a single [[K40]] or [[Titan GPU]] (approx 2 ms [[per image]]).
[[Caffe]] provides [[multimedia scientist]]s and [[practitioner]]s with a [[clean]] and [[modifiable framework]] for [[state-of-the-art]] [[deep learning algorithm]]s and a [[collection]] of [[reference model]]s.
Called the [[10-Point Test]], [[the exercise]] suggested in [[this article]] allows [[finance officer]]s to compare 10 key [[financial ratio]]s for their [[city]] to similar [[ratio]]s calculated for 750 [[smaller citi]]es across the [[nation]] .
Calling for [[research]] on [[automatic oversight]] for [[artificial intelligence system]]s.
Can a higher [[unconditional basic income]] be [[sustainably achieved]] under [[capitalism]] than under [[socialism]]?
[[CANaLI]] [[removes the ambiguiti]]es that are often present in [[NL communication]] by requiring the use of a [[Controlled NL]] and providing [[on-line]] [[knowledge-driven question-completion]] that shows [[alternate correct interpretation]]s.
[[Candidate]] [[definition]]s are then filtered using [[statistical indicator]]s and [[machine-learned regular pattern]]s.
Can we [[compress]] [[social networks]] effectively in a [[neighbor query]] friendly manner, that is, [[neighbor queries]] still can be answered in [[sublinear time]] using the [[compression]]?
Can we [[predict]] what [[proportion]] of the [[network]] will actually get "[[infected]] " (e.g., [[spread the idea]] or buy the [[competing product]]), when the [[node]]s of the [[network]] appear to have different [[sensitivity]] based on their [[profile]]?
[[Capability]], numbers sold, [[engineering]] and [[manufacturing quality]], and [[cost-effectiveness]] will increase in a [[mutually reinforcing spiral]] .
[[Capital account]]: [[Entity]] in [[MMARS]] which records the status of a spending [[authorization]] to be funded from the [[sale of bond]]s.
[[Capitalism]] is premised on the [[accumulation of capital]], but under [[perfect competition]] all [[profit]]s get [[competed away]] .
[[Career themes]] include: [[data science]], [[organizational leadership]], [[strategic planning]], and [[business process reengineering]].
Careful research has shown that innovations like [[mobile telephone]]s are improving [[people]]’s [[income]]s, [[health]], and other [[measures of well-being]] .
[[Carmen Reinhart]] and [[Kenneth Rogoff]], leading [[economist]]s whose work has been influential in the [[policy debate]] concerning the [[current financial crisis]], provocatively argue that [[financial combustion]]s are universal [[rites of passage]] for [[emerging]] and [[established market nation]]s.
[[Cascade]]s are ubiquitous in various [[network environment]]s such as [[epidemic network]]s, [[traffic network]]s, [[water distribution network]]s and [[social network]]s.
[[Case studi]]es also demonstrate our model's capability of [[extracting]] [[tag]]s from the [[crowdsourced]] [[short and noisy comment]]s.
[[Case studies]] show a good ability to rapidly [[pinpoint]] [[emerging subjects]] buried deep in [[large volumes of data]] and then highlight those that are [[rising]] or [[falling]] in [[significance]] as they relate to the [[firm]]s [[interests]] .
:# [[Cash]] [[disbursement]]s for [[direct charge]]s for [[property]] and [[service]]s;<br>
[[Casual observation]], [[introspection]], and [[psychological research]] all suggest that the [[assumption of]] [[time consistency]] is importantly wrong.
[[Category:Concept]] [[Category:Logic]] [[Category:Logical Inference]] [[Category:Statistics]] [[Category:Statistical Inference]]</text>
[[Category:Concept]] [[Category: Machine Learning]] [[Category: Deep Learning]]</text>
[[Category:Concept]] [[Category:Statistical Inference]] [[Category: Machine Learning]]</text>
[[Category:Concept]], [[Category: Statistical Inference]], [[Category:Machine Learning]]</text>
[[Category]] [[detection]] is an [[emerging area]] of [[machine learning]] that can help [[address]] this issue using a "[[human-in-the-loop]]" [[approach]] .
[[Category:Publication]], [[Category:Publication 1983]], [[Category:Information Retrieval Textbook]]
[[Category:Publication]], [[Category:Publication 1999]], [[Category:Information Retrieval Textbook]]
[[Category:Publication]], [[Category:Publication 2008]], [[Category:Information Retrieval Textbook]]
[[Category:Publication]], [[Category:Publication 2008]] [[Category:System Demo Paper]]
(c) [[Attractor]] is capable of [[discovering]] [[communiti]]es of [[arbitrary size]], and thus [[small-size communiti]]es or [[anomali]]es, usually existing in [[real-world network]]s, can be well [[pinpointed]] .
[[Causal graphical model]]s are developed to [[detect]] the [[dependence relationship]]s between [[random variable]]s and provide intuitive [[explanation]]s for the [[relationship]]s in [[complex system]]s.
[[CBPF]] takes recently proposed [[Bayesian Poisson factorization]] as its basic [[unit]] to [[model]] [[user response]] to [[event]]s, [[social relation]], and [[content text]] separately.
[[CCA]] ([[Hotelling, 1935]]) is the analog to [[Principal Component Analysis (PCA)]] for [[pairs of matric]]es.
:(c) Demonstrates that [[audit finding]]s are either [[invalid]] or do not [[warrant]] [[auditee]] action.
[[CEO]]s' [[family foundation]]s hold [[donated]] [[stock]] for long periods rather than [[diversifying]], permitting [[CEO]]s to continue [[voting the shares]] but violating standard [[prudent investor principle]]s of [[risk reduction]] through [[diversification]] .
[[CF-based method]]s [ 23, 27 ] use [[the past activiti]]es or [[preference]]s, such as [[user rating]]s on [[item]]s, without using [[user]] or [[product content information]] .
:::(c) [[Federal award]] does not include other [[contract]]s that a [[Federal agency]] uses to buy [[goods]] or [[service]]s from a [[contractor]] or a [[contract]] to operate [[Federal government owned, contractor operated facilities (GOCOs)]].<br>
:(c) [[Federal award]] does not include other [[contract]]s that a [[Federal agency]] uses to buy [[goods]] or [[service]]s from a [[contractor]] or a [[contract]] to operate [[Federal government owned, contractor operated facilities (GOCOs)]].<br>
:(c) For [[report]]s prepared on an [[accrual]] basis, [[expenditure]]s are the [[sum]] of:<br>
:(c) [[Fund]]s, [[property]], and other [[asset]]s are [[safeguarded against loss]] from [[unauthorized use]] or [[disposition]] .
(c) [[Gradual]] and [[abrupt concept drift]] can be effectively [[detected]] .
[[Chandola et al]] [16] provided a [[comparative evaluation]] of eight [[anomaly detection technique]]s on [[sequence data collected]] from different [[domain]]s, but only focussed on the [[problem of detecting]] [[anomalous sequence]]s from a [[database of sequence]]s.
Change between [[skewed]] [[baseline]] and [[post-treatment data]] tended towards a [[normal distribution]] .
Changes in [[budgeting]] often are [[political response]]s to [[political problem]]s.
Changes in [[price difference]]s, [[labor share]], and the [[compensation-productivity gap]], [[nonfarm business sector]], selected periods, 1947–2009
Chapters will typically cover one of three areas: [[method]]s and [[technique]]s commonly used in [[outlier analysis]], such as [[linear method]]s, [[proximity-based method]]s, [[subspace method]]s, and [[supervised method]]s; [[data domain]]s, such as, [[text]], [[categorical]], [[mixed-attribute]], [[time-series]], [[streaming]], [[discrete sequence]], [[spatial]] and [[network data]]; and key applications of these [[method]]s as applied to diverse [[domain]]s such as [[credit card fraud detection]], [[intrusion detection]], [[medical diagnosis]], [[earth science]], [[web log analytics]], and [[social network analysis]] are covered.
[[chart of accounts]], [[budget management]], [[accounting]], [[reporting]], [[financial management]]
[[Chavalarias and Cointet]] [3], [[Herrera et al.]] [8] or [[Skupin]] [17] work on [[analyzing]] and [[visualizing]] the [[evolution of topics over time]] .
; [[Check norm]]: “The [[inspecting authority]] should perform [[random check]]s of the [[compliance]] to the [[previous norm]] every two months...”.
[[Cherry sheet]]: A [[list]], [[prepared annually]] by the [[Department of Revenue]], of certain [[local aid]] [[distribution]]s.
[[Chess]] subsequently became the [[drosophila—or common fruitfly]], the most studied [[organism]] in [[genetics]] — of [[artificial intelligence research]] .
Chief among these [[demands]], [[administrators]] must [[monitor]] the continual [[ongoing disclosure]] of [[software vulnerabilities]] that have the potential to [[compromise]] their [[system]]s in some way.
[[CHIRP]] is not a [[hybrid]] or [[modification]] of existing [[classifier]]s; it employs a new [[covering algorithm]] .
[[Choosing the right ads]] for the [[query]] and the order in which they are displayed greatly affects the [[probability]] that a [[user]] will [[see]] and [[click]] on each [[ad]] .
[[Chronic disease]]s, such as [[Alzheimer's Disease]], [[Diabete]]s, and [[Chronic Obstructive Pulmonary Disease]], usually progress slowly over a long period of [[time]], causing increasing burden to the [[patient]]s, their [[famili]]es, and the [[healthcare system]] .
[[CitationPageFormattingStyleGuidelines]] for a [[Description]] of the [[Formatting Style]] [[Guideline]]s.
[[Citation]]s alone, however, are not enough to fully understand the [[evolution]] of a [[research field]]: [[researcher]]s need to [[analyze]] the [[contribution]] of [[individual paper]]s.
[[Citation]]s cannot be the only [[measure of success]] for [[grant]]s and [[academic progression]]; we must also value [[share]]s, [[view]]s, [[comment]]s or [[like]]s.
[[Classical]] [[time-window]] or [[instance-decay approaches]] cannot work, as they lose too much [[signal]] when discarding [[data instance]]s.
[[Classification]] [[als]], [[mcmc]], [[sgd]] [[Probit(Map)]], [[Probit]], [[Sigmoid]]
[[Classification]], [[Ensemble]], [[Discriminative classifier]], [[Logistic Regression]]
[[Classification error]]s occur when the [[class]]es [[overlap]] in the [[feature space]] .
[[Classification]] of [[sequence]]s drawn from a [[finite alphabet]] using a family of [[string kernel]]s with [[inexact matching]] (e.g., [[spectrum]] or [[mismatch]]) has shown great success in [[machine learning]] .
[[Classification]] of [[time series data]] is an important [[problem]] with [[application]]s in virtually every [[scientific endeavor]] .
[[Classifying]] [[nodes]] in [[network]]s is a [[task]] with a wide range of [[applications]] .
[[Clearly]], since there are considerable [[functional dependenci]]es between different [[sensor]]s, there are huge <i>[[redundanci]]es</i> in the [[data]] collected by [[sensor]]s.
[[Clerks]]: The [[Clerk]]s of the [[House]] and [[Senate]] are the [[official recorder]]s of all proceedings in the respective [[chamber]]s.
[[Click prediction]] of new [[ad]]s in [[the system]] is a challenging [[task]] due to the lack of such [[historical data]] .
[[ClickRank]], [[Aggregate User Behavior]], [[Intentional Surfer Model]], [[Learning to Rank]], [[Web Search]]
[[click-through rate]], [[sponsored search]], [[paid search]], [[Web advertising]], [[CTR]], [[CPC]], [[ranking]] .
[[Clipping Web page]]s, namely [[extracting]] the [[informative clip]]s ([[area]]s) from [[Web page]]s, has many [[application]]s, such as [[Web printing]] and [[e-reading]] on [[small handheld device]]s.
Closely working with many [[user]]s and [[organization]]s, [[we]] have identified several shortcomings of Hive in its [[file format]]s, [[query planning]], and [[query execution]], which are key factors determining the [[performance]] of [[Hive]] .
[[Cloud hosted marketplace]]s for [[crowdsourcing]] [[intelligent API]]s have been [[launch]]ed.
[[ClusRanking]] is able to exploit [[geographic individual]], [[peer]], and [[zone dependenci]]es in a [[probabilistic ranking model]] .
[[cluster analysis]], [[clustering]], [[partitioning]], [[unsupervised learning]], [[multi-learner system]]s, [[ensemble]], [[mutual information]], [[consensus function]]s, [[knowledge reuse]]
[[Cluster analysis]] groups data so that [[point]]s within a [[single group or cluster]] are [[similar]] to one [[another]] and distinct from points in other [[cluster]]s.
[[Clustering categorical data]] poses some unique [[challenge]]s: Due to [[missing order]] and [[spacing]] among the [[categori]]es, selecting a suitable [[similarity measure]] is a [[difficult task]] .
[[Clustering]], in [[data mining]], is useful for [[discovering groups]] and [[identifying interesting distribution]]s in the underlying [[data]] .
[[Clustering]] is a common and powerful [[technique]] for [[statistical data analysis]], [[document categorization]] and [[topic discovery]] .
[[Clustering]] is the [[unsupervised]] [[classification]] of [[patterns]] ([[observation]]s, [[data item]]s, or [[feature vector]]s) into [[groups (clusters)]] .
[[clustering]], [[mixture model]]s, [[nonparametric Bayesian statistics]], [[hierarchical model]]s, [[Markov chain Monte Carlo]]
[[Clustering]] on such a [[network]] may lead to better understanding of both [[hidden structures]] of the [[network]] and the individual [[role]] played by every [[object]] in each [[cluster]] .
[[Clustering]], [[relational clustering]], [[contingency table]]s, [[multi-criteria optimization]] .
[[clustering]]s as [[input]] and [[output]]s <math>m</math> [[clusterings]] of comparable [[quality]], which are in higher agreement with each other.
[[Clustering]]; [[Statistical Machine Learning]]; [[Statistical Language Learning]]; [[Inductive Logic Programming]]; [[Learning and Logic]]; [[Meta-Learning]]; [[ROC analysis]]; [[Information Theory]]; [[Instance-based Learning Time Series]]; [[Policy Search and Active Selection]]; [[Reinforcement Learning]]; [[Artificial Neural Network]]; [[Text Mining]]; [[Machine Learning in Bioinformatics]]; [[Rule Learning]]; [[Evolutionary Computation]]; [[Behavioral Cloning]]; [[Search]]; [[Computational Learning Theory]]; [[Online Learning]]; [[Learning Paradigms]]; [[Model-based Reinforcement Learning]]; [[Active Learning]]; [[Explanation-based Learning]]; [[Data Mining]]; [[Graph Mining]] .
[[Clustering time series]] is a useful [[operation]] in its own right, and an important [[subroutine]] in many [[higher-level]] [[data mining analyse]]s, including [[data editing]] for [[classifier]]s, [[summarization]], and [[outlier detection]] .
[[Clustering]] [[validation]] is a long standing [[challenge]] in the [[clustering]] [[literature]] .
[[Co-clustering]], [[Data Manifold]], [[Feature Manifold]], [[Graph Regularization]], [[Semi-nonnegative matrix tri-factorization]]
[[Co-clustering]] is based on the [[duality]] between [[data points]] (e.g. [[documents]]) and [[features]] (e.g. [[words]]), i.e. [[data points]] can be [[grouped]] based on their [[distribution]] on [[features]], while [[features]] can be [[grouped]] based on their [[distribution]] on the [[data points]] .
:: <code>linear_model.LogisticRegressionCV([Cs, ...])</code>, [[Logistic Regression]] [[CV]] (aka [[logit]], [[MaxEnt]]) [[classifier]] .
:: <code>linear_model.MultiTaskElasticNetCV([...])</code>, [[Multi-task]] L1/L2 [[ElasticNet]] with built-in [[cross-validation]] .
:: <code>linear_model.MultiTaskLassoCV([eps, ...])</code>, [[Multi-task]] L1/L2 [[Lasso]] with built-in [[cross-validation]] .
[[Coding, or programming]], is a way of [[writing]] [[instruction]]s for [[computer]]s that bridges the gap between how [[human]]s like to [[express themselve]]s and how [[computers actually work]] .
[[co-EM]] is [[iterative]], like [[EM]], but uses the [[feature split]] present in the [[data]], like [[co-training]] .
[[Cognitive]] and [[motivational consideration]]s led to [[prediction]]s that [[degree of desirability]], [[perceived probability]], [[personal experience]], [[perceived controllability]], and [[stereotype salience]] would influence the amount of [[optimistic bias]] evoked by different [[event]]s.
[[Cognitive scientist]]s have found [[value]]s to be [[inextricable component]]s of [[STEM research]] .
[[Collaboration]] is supported by many standard [[features of wiki]]s, for instance [[distributed editing]], [[versioning]], [[rights management]], and [[discussion page]]s.
[[Collaboration]] on [[Web search]] is common in many [[domain]]s, such as [[education]] and [[knowledge work]]; recently, [[HCI researcher]]s have begun to introduce prototype [[collaborative search tool]]s to support such scenarios.
[[Collaborative filtering]] and [[recommender system]]s also use [[vector]]s ([[Resnick, Iacovou, Suchak, Bergstrom, & Riedl, 1994]]; [[Breese, Heckerman, & Kadie, 1998]]; [[Linden, Smith, & York, 2003]]).
[[Collaborative filtering]]; [[matrix factorization]]; [[item embedding]]; [[implicit feedback]] .
[[Collaborative filtering]] or [[recommender system]]s use a database about [[user preference]]s to predict additional [[topics]] or [[products]] a new [[user]] might like.
[[Collaborative network]]s are composed of [[expert]]s who [[cooperate]] with each other to complete specific [[task]s, such as [[resolving problem]]s reported by [[customer]]s.
[[Collaborative prediction]] is a powerful [[technique]], useful in [[domain]]s from [[recommender system]]s to guiding the [[scientific discovery process]] .
[[Collaborator]]s are [[Ben Letham]], [[Berk Ustun]], [[Stefano Traca]], [[Siong Thye Goh]], [[Tyler McCormick]], and [[David Madigan]] .
[[Collective behavior]]s are [[action]]s of a [[large amount]] of various [[people]], which are neither [[conforming]] nor deviant.
[[Collocation]]s are [[recurrent combination]]s of [[word]]s that [[co-occur]] more often than [[expected]] by [[chance]] .
Columns are currently supported only by [[Gecko]]-based browsers (such as [[Mozilla Firefox]]) and [[WebKit]]-based bowsers (such as [[Google Chrome]] and [[Safari]]).
[[COMA++]] implements significant [[improvement]]s and offers a comprehensive [[infrastructure]] to [[solve]] [[large]] [[real-world match problem]]s.
Combined, our method of [[moment]]s and fast [[sampling scheme]] provide the first [[scalable framework]] for effectively [[modeling]] [[large network]]s with [[MFNG]] .
Combined with [[extreme value]] [[modeling]], [[we]] also show that there has been a significant increase in the [[intensity]] of [[extreme]] [[temperatures]], and that such [[changes]] in [[extreme]] [[temperature]] are also attributable to [[greenhouse gases]] .
Combined with [[geospatial]], [[temporal]] and [[historical contexts learned]] from [[trajectori]]es and [[map data]], we fill in the [[tensor]]'s [[missing value]]s through a [[context-aware tensor decomposition approach]] .
Combining [[correlated information]] from multiple [[context]]s can significantly improve [[predictive accuracy]] in [[recommender problem]]s.
Combining [[cutting-edge research]] with [[practical finding]]s, [[Focus]] reveals what distinguishes [[expert]]s from [[amateur]]s and [[star]]s from [[average performer]]s.
Combining [[multiple classifiers]] (a.k.a. [[Ensemble Learning]]) is a [[well studied]] and has been known to improve [[effectiveness]] of a [[classifier]] .
Combining the two [[observation]]s, [[we]] manage to prove a [[probably approximately correct (PAC)]] style [[learning bound]] for [[semi-supervised multi-view learning]] .
[[Commercial building]]s are significant [[consumer]]s of [[electricity]] .
[[Commercial success]] will provoke competition and ac­celerate [[investment]] in [[manufacturing]], [[engineering]] and [[research]] .
Common [[approaches]] to [[this problem]] include [[pattern matching]] ([[Brin, 1998]]; [[Agichtein and Gravano, 2000]]), [[kernel methods]] ([[Zelenko et al., 2003]]; [[Culotta and Sorensen, 2004]]; [[Bunescu & Mooney, 2005]]), [[logistic regression]] ([[Kambhatla, 2004]]), and augmented [[parsing]] ([[Miller et al., 2000]]).
Common conditions that are treated within ICUs include [[ARDS]], [[trauma]], [[multiple organ failure]] and [[sepsis]] .
Commonly addressed [[link mining tasks]] include [[object ranking]], [[group detection]], [[collective classification]], [[link prediction]] and [[subgraph discovery]] .
Common ways to [[evaluate user engagement]] include using [[self-report measure]]s, e.g., [[questionnaire]]s; [[observational method]]s, [[e.g.
[[Communication]] is one of the key issues in building [[multi-agent system]]s, where the [[agent]]s need to [[communicate]] in order to [[resolve]] [[differences of opinion]] or [[conflicts of interest]], to work coordinately, to [[resolve dilemmas]], and to [[reach agreement]]s.
[[Community detection]] is an important [[task]] for [[social network]]s, which helps us understand the [[functional module]]s on the whole [[network]] .
[[Community discovery]] in [[complex network]]s is an [[interesting problem]] with a number of [[application]]s, especially in the [[knowledge extraction task]] in [[social]] and [[information network]]s.
[[Community-owned government]] [[pushes control out]] of [[bureaucracy]] and into the [[community]] .
[[Community Question Answering (CQA) site]]s have become [[valuable platform]]s to [[create]], [[share]], and seek a massive volume of [[human knowledge]] .
[[Community]] [[structure]] [[represents]] the [[latent]] [[social context]] of [[user action]]s.
[[Compani]]es providing [[cloud-scale service]]s have an increasing need to [[store]] and [[analyze]] [[massive data set]]s such as [[search log]]s and [[click stream]]s.
[[Comparative experiment]]s clearly show the [[surprising advantage]] of [[corrupting the input]] of [[autoencoder]]s on a [[pattern classification benchmark]] [[suite]] .
Compared to current [[industry-standard heuristics]] based on [[expert knowledge]] and [[static formulas]], our [[classifiers]] [[predict]] much more accurately whether and how [[soon individual vulnerabilities]] are likely to be [[exploited]] .
Compared to [[DEXTER]], the current [[state-of-the-art approach]] for [[extracting attribute-value pair]]s from [[product specification]]s, [[we]] introduce several new features for [[specification detection]] and support the [[extraction of attribute-value pair]]s from [[specification]]s having more than two [[column]]s.
Compared to existing [[recommender system]]s, [[our work]] has the following [[contribution]]s and advantages: (i) to the best of [[our knowledge]], this is the first work that makes [[demand-aware recommendation]] by considering [[inter-purchase duration]]s for [[durable]] and [[nondurable good]]s; (ii) [[the proposed algorithm]] is able to [[simultaneously infer]] [[items’ inter-purchase duration]]s and users’ [[real-time purchase intention]]s, which can help [[e-retailer]]s make more [[informed decision]]s on [[inventory planning]] and [[marketing strategy]]; (iii) by effectively exploiting [[sparsity]], [[the proposed algorithm]] is [[extremely efficient]] and able to handle [[large-scale]] [[recommendation problem]]s.
Compared to [[open]] or [[closed fragment]] [[mining]], a large part of the [[search space]] can be [[prune]]d due to an improved [[statistical constraint]] ([[dynamic]] [[upper bound]] [[adjustment]]), which is also confirmed in the [[experiment]]s in lower [[running time]]s compared to ordinary ([[static]]) [[upper bound]] [[pruning]] .
Compared to other [[nonlinear model]]s such as [[kernel logistic regression (KLR)]] and [[SVM]], [[our approach]] is far more [[efficient]] since it solves an [[optimization problem]] with a much smaller [[size]] .
Compared to [[previous system]]s, [[Spark SQL]] makes two [[main addition]]s.
Compared to recent [[supervised approach]]es based on [[convolutional neural network]]s, [[predictive text embedding]] is comparable or more [[effective]], much more [[efficient]], and has [[fewer parameter]]s to [[tune]] .
Compared to their [[counterpart]]s in the [[previous generation]], [[employed millennial]]s in [[New York City]] in [[2014]] earned about [[20 percent]] less in [[real term]]s.
Compared to the [[state-of-the-art method]]s, [[AutoPhrase]] has shown significant improvements in both [[effectiveness]] and [[efficiency]] on five [[real-world dataset]]s across different [[domain]]s and languages.
Compared with [[binary unit]]s, these [[unit]]s [[learn feature]]s that are better for object recognition on the [[NORB dataset]] and [[face verification]] on the [[Labeled Face]]s in the [[Wild dataset]] .
Compared with [[conventional methods]], ours can effectively take [[advantage]] of [[semi-supervision]] and automatically [[discover]] the [[sparse metric structure]] underlying [[input data patterns]] .
Compared with existing [[method]]s, [[our method]] can [[effective]]ly filter out [[no-truth question]]s, which results in more [[accurate]] [[source quality estimation]] .
Compared with other [[existing method]]s, [[FFD]] has unmatched advantages, as it attains the [[efficiency]] and [[interpretability]] of [[linear model]]s as well as the [[accuracy]] of [[nonlinear model]]s.
[[Compared]] with several [[existing approaches]], our [[method]] is not only [[fully automatic]], but also has the best [[accuracy]] overall.
Compared with the standard [[regularization scheme]] in [[DML]], [[dropout]] is advantageous in [[simulating]] the [[structured regularizer]]s which have shown consistently better [[performance]] than [[non structured regularizer]]s.
Comparing prior works on [[unsupervised deep network]]s and [[supervised learning]], [[SUGAR]] better balances [[numerical tractability]] and the [[flexible utilization]] of [[supervision information]] .
[[Comparing]] your [[organization]] to [[peer]]s - also known as [[benchmarking]] - lets you understand [[how you're doing]], [[identify]] [[performance gap]]s and [[opportunities to improve]], and highlight [[peer achievement]]s that you could [[emulate]], or your own [[achievement]]s to be [[celebrated]] .
[[Comparison shopping portal]]s integrate [[product offer]]s from [[large number]]s of [[e-shop]]s in order to support [[consumer]]s in their [[buying decision]]s.
[[Comparison]]s with [[linear programming (LP)]] and [[orthogonal matching pursuit (OMP)]] demonstrate that [[our algorithm]] can be significantly faster in [[decoding speed]] and more [[accurate]] in [[recovery quality]], for the [[task of exact spare recovery]] .
[[Compensation]] [[data]] are [[adjusted]] by using a [[consumer price index]], and [[output]] is [[adjusted]] by using an [[implicit price deflator]] .
[[competition]], [[behavioral ethics]], [[behavioral economics]], [[decision making]], [[corruption]] .
[[Complex model]]s for [[regression]] and [[classification]] have [[high accuracy]], but are unfortunately no longer interpretable by [[user]]s.
[[Complex network]]s are [[ubiquitous]] in our daily [[life]], with the [[World Wide Web]], [[social network]]s, and [[academic citation network]]s being some of the [[common example]]s.
[[Complex networks]] describe a wide range of [[systems]] in [[nature]] and [[society]] .
Comprehensive [[evaluation]] on 1 million [[longitudinal clinical note]]s over 13K [[patient]]s shows that [[static symptom expansion]] can successfully expand a [[set]] of known [[symptom]]s to a [[disease]] with [[high agreement rate]] with [[physician input]] ([[average precision]] 0.46), a 31% improvement over [[baseline]] [[co-occurrence based method]]s.
Comprehensive [[experimentation]] with [[real]] and [[synthetic dataset]]s showcase our [[efficiency]], [[scalability]], and [[accuracy]] [[claim]]s.
Comprehensive [[experiment]]s on both [[synthetic]] and [[real dataset]]s demonstrate the effectiveness of our [[method]] .
Comprehensive experiments on [[real-life text dataset]]s show that [[the proposed method]] [[outperform]]s the existing [[baseline]]s on [[objective evaluation]] metrics for [[visualization quality]] and [[topic interpretability]] .
Comprehensive [[experiment]]s on the [[SEMG data set]] demonstrate that the [[proposed method]] improves the [[classification]] [[accuracy]] by 20% to 30% over the [[case]]s without any [[domain adaptation method]] and by 13% to 30% over the existing [[state-of-the-art]] [[domain adaptation method]]s.
[[Comprehensive experiment]]s show that [[our proposed method]]s yield [[stable]] and [[promising result]]s.
[[Compressing]] [[social networks]] can substantially facilitate [[mining and advanced analysis]] of [[large social networks]] .
[[Compression]], [[Social Network]]s, [[Linear Arrangement]], [[Reciprocity]]
Computational approaches to [[protein function prediction]] infer [[protein function]] by finding [[protein]]s with similar [[sequence]], [[structure]], [[surface cleft]]s, [[chemical properties]], [[amino acid motif]]s, [[interaction partner]]s or [[phylogenetic profile]]s.
[[Computational evolution experiment]]s with selection pressures to maximize [[network performance]] and [[minimize connection cost]]s yield [[network]]s that are significantly more [[modular]] and more [[evolvable]] than [[control experiment]]s that only select for [[performance]] .
[[Computational Linguistics]]; [[Information Technology]]; [[Natural Language Processing Systems]]; [[Theorem Proving]]; [[Heuristic Method]]s
[[Computationally]], [[we]] develop [[efficient algorithm]]s to compute a [[global solution]] as well as an entire [[regularization solution path]] .
[[Computational method]]s used by [[data miner]]s, such as [[cluster analysis]], [[association rule mining]], and [[graphical model]]s, were able to [[scale]] to the [[data]] and produce [[knowledge]] and [[insight]] that was previously unknown.
[[Computational models of lexical semantics]], such as [[latent semantic analysis]], can [[automatically generate semantic similarity measures]] between [[word]]s from [[statistical redundanci]]es in [[text]] .
[[Computational models of word segmentation]], [[semantics]], and [[syntax]] “often assume a [[computational complexity]] and [[linguistic knowledge]] likely to be beyond the abilities of developing [[young children]]” ([[Onnis & Christiansen, 2008]]).
[[Computational model]]s that perform [[probabilistic inference]] over [[hierarchi]]es of flexibly [[structured representation]]s can address some of the deepest questions about the [[nature]] and origins of [[human thought]]: How does [[abstract knowledge guide learning]] and [[reasoning]] from [[sparse data]]?
[[Computational phenotyping]] is the [[process]] of converting [[heterogeneou]]s [[electronic health records (EHRs)]] into meaningful [[clinical concept]]s.
[[Computational result]]s on both [[synthetic]] and [[real world problem]]s demonstrate the superior [[performance]] of [[the proposed approach]] over existing [[technique]]s.
[[Computer and information ethics]], as well as other [[fields of applied ethics]], need [[ethical theori]]es which [[coherently unify]] [[deontological]] and [[consequentialist]] aspects of [[ethical analysis]] .
[[Computer]]s and [[automation]] have captured [[man's]] [[imagination]] .
[[Computer scientist]]s, specifically, approach [[this field]] based on their [[practical experience]]s in managing [[large amounts of data]], and with far [[fewer assumption]]s the [[data]] can be of any type, [[structured]] or [[unstructured]], and may be [[extremely large]] .
[[Computer]]s [[understand]] very little of the [[meaning]] of [[human language]] .
[[Concept hierarchi]]es have been useful [[tool]]s for [[presenting]] and [[organizing knowledge]] .
[[Concept mapping]] consists in [[finding mappings]] with external [[ontologi]]es or [[vocabulari]]es.
[[Concept mention identification]] is performed with a [[trained]] [[CRF]] [[sequential model]] .
[[Concept Mention]]; [[Relation Mention]]; [[Ontology]]; [[Semantic Annotation]]; [[Reference Resolution]]; [[Supervised Classification]] .
[[Concept Mention]]; [[Relation Mention]]; [[Reference Resolution]]; [[Ontology]]; [[Supervised Classification]];
[[ConceptNet]] is a [[freely available]] [[commonsense knowledge base]] and [[natural-language-processing tool-kit]] which supports many practical [[textual-reasoning task]]s over [[real-world document]]s including [[topic-gisting]], [[analogy-making]], and other [[context oriented inference]]s.
[[ConceptNet]] is a [[freely available]] [[commonsense knowledge base]] and [[natural-language-processing tool-kit]] which supports many practical [[textual-reasoning task]]s over [[real-world document]]s including [[topic-gisting]], [[analogy-making]], and other context oriented [[inference]]s.
[[ConceptPageFormattingStyleGuidelines]] for a [[Description]] of the [[Formatting Style]] [[Guideline]]s.
[[Concept]]s are [[units of thought]], the [[constituents of]] [[belief]]s and [[theori]]es, and those that interest me here are roughly the grain of single [[lexical item]]s.
[[Concept]]s can be [[represented by]] [[distributed patterns of activity]] in [[networks of neuron-like units]] .
Concepts describe [[set]]s of individuals, [[role]]s describe [[relation]]s between individuals.
Concepts of [[graph theory]], including [[graph]], [[path]], [[path distance]], and [[shortest path]] are presented, followed by two fundamental [[distance]] or [[dissimilarity measures]] for [[paths]]: [[Perimeter Based]] ([[PB]]), and [[Spatial-Region-Based]] ([[SRB]]).
[[Conceptual database design]], [[database integration]], [[database schema integration]], [[information systems design]], [[models]], [[view integration]] .
[[Conceptual graphs]] form a [[knowledge representation language]] based on [[linguistics]], [[psychology]], and [[philosophy]] .
Concerning the [[top-k hit rate]] on [[test data]], [[our experiment]]s indicate dramatic [[improvements]] over even [[sophisticated]] [[methods]] that are optimized on [[observed ratings]] only.
[[Concise representations]] of [[frequent itemsets]] [[sacrifice]] [[readability]] and [[direct interpretability]] by a [[data analyst]] of the concise [[patterns extracted]] .
Concurrently, [[nonlabor cost]]s — which include [[intermediate inputs into production]] and [[returns to investments, or profits]] — represent a [[greater]] [[share]] of [[output]] .
[[Conditional Random Field]], [[Higher-Order Feature]], [[Sequence Labeling Task]] .
[[Conditional Random Field]]s, [[Constrained Optimization]], [[Cross Validation]], [[Extended Saddle Point]]s
[[Conditional random fields(CRFs)]] are a [[class]] of [[undirected graphical models]] which have been widely used for [[classifying]] and [[labeling]] [[sequence data]] .
[[Conditional random fields (CRFs)]] combine the [[modeling]] flexibility of [[graphical models]] with the ability to use [[rich]], [[nonindependent features]] of the [[input]] .
[[Conditional random fields]] for [[sequence labeling]] offer advantages over both [[generative models]] like [[HMMs]] and [[classifiers]] applied at each [[sequence position]] .
[[Conditional random fields]] ([[Lafferty, McCallum, & Pereira, 2001]]) ([[CRF]]) are [[probabilistic frameworks]] for [[labeling]] and [[segmenting sequential data]] .
[[Conditional random fields]] offer several advantages over [[hidden Markov models]] and [[stochastic grammars]] for such [[tasks]], including the ability to relax strong [[independence]] [[assumptions]] made in those [[models]] .
[[Confluence]] can [[distinguish]] and [[quantify the effect]]s of the different types of [[conformiti]]es.
[[Confounding]] can be [[controlled]] in several ways: [[restriction]], [[matching]], [[stratification]], and more sophisticated [[multivariate technique]]s.
[[Conjoint analysis]] is one of the most popular [[market research]] [[methodologies]] for assessing how [[customers]] with [[heterogeneous]] [[preference]]s appraise various [[objective characteristics]] in [[products]] or [[service]]s, which provides critical [[inputs]] for many [[marketing decisions]], e.g. optimal [[design]] of new [[products]] and [[target market]] [[selection]] .
[[Connecting]] [[mathematical logic]] and [[computation]], it ensures that some aspects of programming are [[absolute]] .
[[Connecting online]] has a [[small cost]] compared to the [[physical world]], leading to a [[proliferation of connection]]s, many of which carry little [[value]] or [[importance]] .
[[connectionism]] and [[neural nets]] [[problem solving]], [[control method]]s, and [[search theory]]
[[Connection strength]]s in the [[network]] and [[diffusion path]]s of [[infection]]s over the [[network]] are examples of such [[hidden variable]]s.
Consequently a good [[feature representation]] can [[encode]] this [[concept space]] and [[minimize]] the [[distribution]] gap.
[[Consequently]], [[our method]] provides more [[accurate]] and [[complete answer]]s to both [[has-truth]] and [[no-truth question]]s.
Consequently, [[this study]] applied a [[prototype]] [[KBS]] which linked the [[database management]], [[model base]], [[knowledge acquisition]], and [[dialogue subsystem]]s to construct a [[BSC knowledge-based system for strategic planning (BSCKBS)]] .
Consequently, [[we investigate]] and develop two [[biased propagation framework]]s, the [[biased random walk framework]] and the [[biased regularization framework]], for the [[TMBP algorithm]] from different [[perspective]]s, which can discover [[latent topic]]s and [[identify]] [[clusters of multi-typed object]]s simultaneously.
Consequently, [[we]] start by creating an [[overall]] [[ontology for the smart city]], [[defining]] the [[building block]]s of [[this ontology]] with respect to the most [[cited definition]]s of [[smart citi]]es, and structuring [[this ontology]] with the [[Protégé 5.0]] [[editor]], defining [[entiti]]es, [[class hierarchy]], [[object properti]]es, and [[data type properti]]es.
Consider [[data consisting of pairwise measurements]], such as [[presence]] or [[absence]] of [[link]]s between [[pairs of object]]s.
Considering a [[significance level]] = 0.05, [[Levene's test]] fails to reject the [[null hypothesis]] ([[p-value]] is greater than [[significance level]]).
Considering nowadays [[compani]]es providing similar [[product]]s or [[service]]s [[compete]] with each other for [[resource]]s and [[customer]]s, [[this work]] proposes a [[learning-based framework]] to tackle the [[multi-round]] [[competitive influence maximization problem]] on a [[social network]] .
Considering that the [[degeneration process]] may lose [[information]], [[we]] propose the [[D-MimlSvm algorithm]] which tackles [[MIML problem]]s directly in a [[regularization framework]] .
Considering the available [[information]] about [[offender]]s, [[we]] introduce [[social]], [[geographic]], [[geo-social]] and [[similarity feature set]]s which are used for [[classifying potential negative]] and [[positive pair]]s of [[offender]]s.
Consider the [[problem of building classifiers]] to help [[brands]] [[control]] the [[content]] [[adjacent]] to their [[on-line advertisements]] .
([[consistency of constraint]]s) "[[Completeness]]: Given a [[set]] of [[consistent]] [[relative constraint]]s, how can one derive a complete [[clustering]] without running into [[dead-end]]s?
Consistent with their [[exemption]] from [[insider trading law]], I find that [[CEO]]s' [[stock gift]]s occur just prior to significant [[drop]]s in their [[firm]]s' [[stock price]]s, a [[pattern]] that enables the [[donor]]s to obtain increased [[personal income tax benefit]]s.
[[Constrained clustering]] has been [[well-studied]] for [[algorithm]]s like [[K-means]] and [[hierarchical agglomerative clustering]] .
[[Constraint]]s in this form, [[termed]] [[Relative Constraint]]s, provide a [[unified knowledge representation]] for both [[partitional]] and [[hierarchical clustering]]s.
[[Constructing feature]]s using [[target-dependent aggregation]]s can transform [[relational prediction task]]s so that [[well-understood]] [[feature-vector-based modeling algorithm]]s can be applied successfully.
[[Consuming]] is defined as [[behavior]] whereby [[entropy]] is increased in exchange for [[existential]] or [[experiential reward]]s.
[[Contemporary dataset]]s often have [[p]] comparable with or even [[much larger than]] [[n]] .
[[Content-based method]]s [ 17 ] make use of [[user profile]]s or [[product description]]s for [[recommendation]] .
[[Contention]]s are perhaps the most important [[feature]] of [[forum]]s that discuss [[social]], [[political]] and [[religious issue]]s.
[[Content personalization]] is a key [[tool]] in creating attractive [[website]]s.
[[Content signal]]s relate mostly to the [[text]] and [[categories of question]]s and associated [[answer]]s, while [[social signal]]s capture the various [[user interaction]]s with [[question]]s, such as [[asking]], [[answering]], [[voting]], etc. [[We]] fuse and generalize known [[recommendation approach]]es within a single [[symmetric framework]], which incorporates and properly balances multiple [[types of signal]]s according to [[channel]]s.
[[Contextual bandit algorithm]]s have become popular for [[online recommendation system]]s such as [[Digg]], [[Yahoo! Buzz]], and [[news recommendation]] in general.
[[Continuous physical process]]es like [[wave]]s, [[flow]]s, and [[oscillation]]s are characterized in terms of [[change]]s, and [[rates of change]], of the [[world]] over [[time]] .
[[Contract]]s for [[natural gas storage]] essentially represent [[real option]]s on [[natural gas price]]s.
Contrary to [[antipathy model]]s, 2 [[dimensions mattered]], and many [[stereotype]]s were mixed, either [[pitying]] ([[low competence]], [[high warmth]] subordinates) or [[envying]] ([[high competence]], [[low warmth competitor]]s).
[[Contrary]] to [[national trend]]s, [[more New York City millennials]] are living [[independently]] and a smaller [[share]] is living with their [[parent]]s.
[[Controlled Experiment]]s, [[A/B Testing]], [[E-commerce]], [[Simpson’s Paradox]], [[Robot Detection]]
[[Controlled experiment]]s; [[A/B testing]]; [[e-commerce]]; [[Website optimization]]; [[MultiVariable Testing]]; [[MVT]]
[[Controlled experiments]]; [[A/B testing]]; [[predictive modeling]]; [[overall evaluation criterion]]
[[Controlled experiment]]s, [[A/B testing]], [[Website Testing]], [[MultiVariable Testing]]
[[Controlled experiment]]s, also called [[randomized experiment]]s and [[A/B test]]s, have had a profound [[influence]] on multiple [[fields]], including [[medicine]], [[agriculture]], [[manufacturing]], and [[advertising]] .
[[Controlled experiment]]s embody the best [[scientific design]] for [[establishing]] a [[causal relationship]] between changes and their [[influence]] on [[user-observable behavior]] .
[[Controlling for confounds]] in previous work, [[we]] demonstrate that [[this metric]] benefits from [[training on extremely large amounts of data]] and [[correlates]] more closely with [[human semantic similarity rating]]s than do [[publicly available]] [[implementation]]s of several more [[complex model]]s.
Conventional [[CF-based method]]s use the [[rating]]s given to [[item]]s by [[user]]s as the sole [[source of information]] for [[learning]] to make [[recommendation]] .
Conventional [[data analysis tools]] are usually not customized for handling the [[massive quantity]], [[complex]], [[dynamic]], and [[distributed nature]] of [[location trace]]s.
[[Conventional wisdom]] attributes [[small generalization error]] either to [[properties of]] the [[model family]], or to the [[regularization technique]]s used during [[training]] .
Conversely, all [[production]] must be distributed as [[income]] in one form or another, to either [[labor]] or [[capital]]: whether as [[wage]]s, [[salari]]es, [[honoraria]], [[bonus]]es, and so on (that is, as [[payments to workers]] and others who contributed [[labor]] to the [[process of production]]) or else as [[profit]]s, [[dividend]]s, [[interest]], [[rent]]s, [[royalti]]es, and so on (that is, as [[payment]]s to the [[owners of capital]] used in the [[process of production]]).
[[Conversely]], every [[kernel matrix]] is symmetric and [[positive semidefinite]] .
[[Converting spreadsheet data]] to the [[relational model]] would allow [[data analyst]]s to use [[relational integration tool]]s.
[[Convolutional]] and [[pooling architecture]] show promising results on many [[task]]s, including [[document classification]] ([[Johnson & Zhang, 2015]]), [[short-text categorization]] ([[Wang, Xu, Xu, Liu, Zhang, Wang, & Hao, 2015a]]), [[sentiment classification]] ([[Kalchbrenner, Grefenstette, & Blunsom, 2014]]; [[Kim, 2014]]), [[relation type classification between entities]] ([[Zeng, Liu, Lai, Zhou, & Zhao, 2014]]; [[dos Santos, Xiang, & Zhou, 2015]]), [[event detection]] ([[Chen, Xu, Liu, Zeng, & Zhao, 2015]]; [[Nguyen & Grishman, 2015]]), [[paraphrase identification]] ([[Yin & Schutze, 2015]]) [[semantic role labeling]] ([[Collobert, Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011]]), [[question answering]] ([[Dong, Wei, Zhou, & Xu, 2015]]), [[predicting]] [[box-office revenues of movies]] based on [[critic review]]s ([[Bitvai & Cohn, 2015]]) [[modeling text interestingness]] ([[Gao, Pantel, Gamon, He, & Deng, 2014]]), and [[modeling the relation]] between [[character-sequence]]s and [[part-of-speech tag]]s ([[Santos & Zadrozny, 2014]]).
[[Convolution kernels]] and [[recursive neural networks (RNN)]] are both suitable approaches for [[supervised learning]] when the [[input portion]] of an [[instance]] is a [[discrete structure]] like a [[tree]] or a [[graph]] .
[[Coordination]] is a [[syntactic construction]] with a varying [[semantics]] .
[[COPE]] [[model]]s [[crowdsource]]d work as a [[trading market]], where the "[[worker]]s " behave like "[[trader]]s " to maximize their profit by presenting their [[opinion]] .
[[Coreference resolution]] is the [[problem of clustering]] [[mentions (or records)]] into [[set]]s [[referring]] to the same [[underlying entity]] (e.g., [[person]], [[place]]s, [[organization]]s).
[[Coreference]]s are the occurrences of a given [[word]] or [[word sequence]] referring to a given [[entity]] within a [[document]] .
[[CoreSCs]] have been annotated by 16 [[chemistry experts]] over a total of 265 [[full papers]] in [[physical chemistry]] and [[biochemistry]] .
[[Corporation]]s and [[government agenci]]es are pouring billions into achieving [[AI's Holy Grail]]: [[human-level intelligence]] .
[[Correlated]] or [[discriminative pattern]] [[mining]] is concerned with finding the [[highest scoring]] [[pattern]]s w.r.t. a [[correlation measure]] (such as [[information gain]]).
[[Correlation clustering]] is a basic [[primitive]] in [[data miner]]'s [[toolkit]] with [[application]]s ranging from [[entity matching]] to [[social network analysis]] .
[[Correlations]] between [[news titles]] and [[news bodies]] are [[exploited]] .
[[Cost Allocation]] and [[Indirect Cost]]s — Since [[salari]]es and benefits are a major [[cost]] of [[local government]], consistent allocation of these [[cost]]s is important for achieving comparable [[cost data]] for [[localiti]]es in [[this state]] .
Counting the number of distinct [[element]]s in a [[large dataset]] is a [[common task]] in [[web application]]s and [[database]]s.
[[Coupled]] with [[automated ticket creation]], it ensures that a [[degradation]] of the [[vital sign]]s, defined by acceptable [[threshold]]s or [[monitoring condition]]s, is flagged as a [[problem candidate]] and sent to [[supporting personnel]] as an [[incident ticket]] .
[[Coupling]] this formal [[representation]] with an [[optimization approach]], [[we]] show that [[GrowCode]] is able to [[discover]] models for [[protein interaction network]]s, [[autonomous systems network]]s, and [[scientific collaboration network]]s that [[closely match properti]]es such as the [[degree distribution]], the [[clustering coefficient]], and [[assortativity]] that are [[observed]] in [[real network]]s of these [[class]]es.
Covering sixty-six [[countri]]es across five [[continent]]s, [[This Time Is Different]] presents a comprehensive look at the varieties of [[financial crises]], and guides us through eight astonishing centuries of [[government default]]s, [[banking panic]]s, and [[inflationary spike]]s -- from [[medieval]] [[currency debasement]]s to today's [[subprime catastrophe]] .
Creating insecurity for many people, it has pervasive [[consequence]]s not only for the [[nature of work]], [[workplace]]s, and [[people’s work experience]]s, but also for many [[non-work individual]] (e.g., [[stress]], [[education]]), [[social]] (e.g., [[family]], [[community]]), and [[political]] (e.g., [[stability, [[democratization]]) [[outcome]]s.
[[Creating]], [[maintaining]], and [[expanding]] such resources by [[manual means]] is a [[tedious]] and [[expensive task]] .
[[CRF]] is a [[well-known model]] for solving other [[sequence tagging problem]]s.
[[CRFs]] are [[undirected graphical models]] which define a [[conditional distribution]] over [[labellings]] given an [[observation]] ([[Lafferty et al., 2001]]).
[[CRFs]] have achieved empirical success recently in [[POS tagging]] ([[Lafferty et al., 2001]]), [[noun phrase segmentation]] ([[Sha & Pereira, 2003]]) and [[table extraction from government reports]] ([[Pinto et al., 2003]]).
[[CRFs]] provide a principled way for incorporating various [[local feature]]s, [[external [[lexicon]] feature]]s and [[global layout feature]]s.
[[CRFs]] provide a principled way for incorporating various [[local features]], [[external]] [[lexicon features]] and [[global]] [[layout features]] .
[[Crime rate]]s, [[historically highest]] among [[young people]], have continued to [[fall]] and so has the [[incarceration rate]] among the [[city’s youth]], from an estimated 2.1 percent in [[2008]] to an estimated 1.3 percent in [[2015]] .
[[Crime reduction]] and [[prevention strategi]]es are essential to increase [[public safety]] and [[reduce]] the [[crime cost]]s to [[society]] .
[[CRM]] incorporates all the information of [[node]]s and [[edge]]s that form a [[social network]] .
Cross different [[task]]s, additional [[regularization function]]s are utilized to ensure the [[function]]s that we [[learn]] in each [[view]] are [[similar]] .
[[Cross-domain collaboration]]s exhibit very different [[pattern]]s compared to traditional [[collaboration]]s in the same domain: 1) <b> [[sparse connection]] </b>: [[cross-domain collaboration]]s are rare; 2) <b> [[complementary expertise]] </b>: [[cross-domain collaborator]]s often have different [[expertise]] and [[interest]]; 3) <b> [[topic skewness]] </b>: [[cross-domain collaboration topic]]s are focused on a [[subset of topic]]s.
[[Crowdsourcing]] has recently become popular among [[machine learning researcher]]s and [[social scientist]]s as an effective way to collect [[large-scale]] [[experimental data]] from [[distributed worker]]s.
[[Crowdsourcing]] is an [[effective method]] for [[collect]]ing [[labeled data]] for various [[data mining task]]s.
[[Crowdsourcing]] is the [[de-facto standard]] for [[gathering]] [[annotated data]] .
Crucially, these [[approach]]es are [[trained]] based on the assumption that each [[sentence]] which [[mentions]] the two related [[entities]] is an [[expression]] of the given [[relation]] .
[[CSTM]]'s chief [[novelty]] lies in its [[learned model of individual librari]]es, or [[sets of document]]s, associated with each [[user]] .
[[CTC]] relaxes the [[normalization constraint]]s as in [[probabilistic model]]s and [[learn]]s [[non-negative document code]]s and [[word code]]s.
:(c) The term "[[loan guarantee]]" means any [[Federal government]] [[guarantee]], [[insurance]], or other [[pledge]] with respect to the [[payment]] of all or a part of the [[principal]] or [[interest]] on any [[debt obligation]] of a [[non-Federal borrower]] to a [[non-Federal lender]], but does not include the [[insurance]] of [[deposit]]s, [[share]]s, or other [[withdrawable account]]s in [[financial institution]]s.<br>
[[CTL]] [[outperform]]s baselines significantly on multiple [[recommendation metric]]s.
[[Culotta]] and [[McCallum]] ([[Culotta and McCallum, 2004]]) provide a [[model]] for [[assessing the confidence of extracted information]] using [[conditional random fields (CRFs)]] .
[[Cultural aspect]]s include a [[thing’s name]], [[value]], [[proper location]] and [[purpose]] .
[[CURE]] achieves this by [[representing]] each [[cluster]] by a certain [[fixed]] [[number of points]] that are [[generated by]] [[selecting]] [[well scattered point]]s from the [[cluster]] and then [[shrinking]] them toward the [[center of the cluster]] by a [[specified fraction]] .
[[Current end application]]s include [[information extraction]], [[machine translation]], [[summarization]], [[search]] and [[human-computer interface]]s.
[[Current example]]s of [[Semantic Wiki implementation]]s are, for instance, [[IkeWiki]] [47], [[KnowWE]] [ 7], [[MoKi]] [20], [[Semantic MediaWiki]] [30], and [[SweetWiki]] [14].
[[Currently]], an [[open repository]] of [[biomedical ontologi]]es [[BioPortal]]7 contains over 350 [[ontologi]]es.
Currently however no [[benchmark dataset]] exists to [[evaluate]] the performance of [[text mining system]]s that can accurately [[identify]] and [[disambiguate]] [[product entiti]]es within a large [[product catalog]] .
[[Currently]], [[iHR]] has been deployed at http://i.xmrc.com.cn / [[XMRCIntel]] .
Currently, most [[approaches]] to [[aviation safety]] are [[reactive]], meaning that they are designed to react to an [[aviation]] [[safety incident]] or [[accident]] .
Currently, the most significant line of defense against [[malware]] is [[anti-virus products]] which focus on [[authenticating]] [[valid]] [[software]] from a [[white list]], [[blocking]] [[invalid]] [[software]] from a [[black list]], and running any unknown [[software]] (i.e., the [[gray list]]) in a [[controlled manner]] .
Current [[NER approach]]es include: [[dictionary based]], [[rule based]], or [[machine learning based]] .
Current [[recommender system]]s often show the same [[most-highly recommended item]]s again and again ignoring the [[feedback]] that [[user]]s neither [[rate]] nor [[click]] on those [[item]]s.
Current [[research]] in [[text mining]] favors the [[quantity of text]]s over their [[representativeness]] .
[[Current]] [[research]] on [[graph classification]] assumes the existence of [[large amounts]] of [[labeled]] [[training graphs]] .
Current [[research]] on [[graph classification]] focuses on [[mining]] [[discriminative]] [[subgraph feature]]s under [[supervised setting]]s.
[[Current services budget]]: [[Synonymou]]s with "[[maintenance budget]] ".
Current [[technique]]s (including our previous [[work]]) are [[unfocused]]/[[unrefined]] in that they broadly attempt to find an [[alternative clustering]] but do not specify which [[properties]] of the [[original]] [[clustering]] should or should not be retained.
[[Current work]] in [[lexical distributed representation]]s maps each [[word]] to a [[point vector]] in [[low-dimensional space]] .
Current [[work]] in [[lexical distributed representations]] maps each [[word]] to a [[point vector]] in [[low-dimensional space]] .
[[Cycling]] across [[session]]s and [[serpentining]] increase [[user activiti]]es without significantly affecting [[opt-out rate]]s.
[[Cycling]] within the same [[session]] might be a "[[love it]] or [[hate it]]" [[recommender property]] because [[user]]s in it have a higher [[opt-out rate]] but engage in more [[activiti]]es.
[[D3E]] is an [[experimental system]] supporting the [[publication of Web based documents]] with integrated [[discourse faciliti]]es and embedded [[interactive component]]s.
[[DARPA]] has been investing in [[data science]] and [[building]] [[open source]] [[tool]]s for [[applications ranging]] from [[counter threat finance]], through [[radar operation]]s and [[cancer research]], to [[anti-human trafficking]] .
[[DARPA's Deep Exploration and Filtering of Text (DEFT) program]] aims to improve [[state-of-the-art capabilities]] in [[automated deep natural language processing]], with a particular focus on [[technologi]]es dealing with [[inference]], [[causal relationship]]s, and [[anomaly detection]] ([[DARPA, 2012]]).
[[Data analytics tools]] and [[framework]]s abound, yet rapid [[deployment]] of [[analytics solutions]] that deliver [[actionable insights]] from [[business data]] remains a challenge.
[[data asymmetry]], [[quality]], [[volume]], [[variety]] and [[velocity]]) and [[deployment-related]] (e.g.
[[database applications]], [[experimentation]], [[information search and retrieval]], [[large-scale network]], [[social influence analysis]], [[social networks]], [[topical analysis propagation]]
[[Database Management Systems (DBMSs)]] are [[complex]], [[mission-critical software system]]s.
[[Data describing network]]s ([[communication network]]s, [[transaction network]]s, [[disease transmission network]]s, [[collaboration network]]s, etc.) is becoming increasingly [[ubiquitous]] .
[[Data element]]s have <i>keys</i> (cookies, [[user]]s, [[queri]]es) and elements with different keys [[interleave]] .
[[Data embedding]] is used in many [[machine learning application]]s to create [[low-dimensional feature representation]]s, which preserves the [[structure]] of [[data point]]s in their original [[space]] .
[[Data]] from the [[U.S. Census Bureau]] and [[Department of Labor]] were used to establish trends in [[employment]], [[wage]]s and other [[economic factor]]s for [[young people]] before, during and after the [[recession]] in [[New York City]] and [[nationwide]] .
[[Data]] [[generated by]] [[observing]] the [[action]]s of [[web browser]]s across the [[internet]] is being used at an ever [[increasing rate]] for both [[building model]]s and [[making decision]]s.
[[Data glitch]]es are unusual [[observation]]s that do not conform to [[data quality expectation]]s, be they [[logical]], [[semantic]] or [[statistical]] .
[[Data Integration]], [[Coreference]], [[Schema Matching]], [[Canonicalization]], [[Conditional Random Field]], [[Weighted Logic]]
[[Data]] is [[modeled]] as [[sets of rows]] composed of [[typed column]]s.
[[Data mining algorithm]]s for [[computing solution]]s to [[online resource allocation (ORA) problem]]s have focused on [[budgeting resource]]s [[currently in possession]], e.g., [[investing in the stock market]] with [[cash on hand]] or [[assigning]] current [[employee]]s to [[project]]s.
[[Data mining]] allows [[user model]]s to be constructed based on vast [[available data]] [[automatically]] .
[[Data mining]] efforts associated with [[the Web]], called [[Web mining]], can be broadly divided into three [[class]]es, i.e. [[content mining]], [[usage mining]], and [[structure mining]] .
[[Data Mining]]; [[Knowledge discovery]]; [[Rule induction]]; [[Hierarchical decision rule]]s
[[Data mining research]] has [[developed]] many [[algorithm]]s for various [[analysis task]]s on [[large]] and [[complex]] [[dataset]]s.
[[Data mining technique]]s use '[[score function]]s' to [[quantify]] how well a [[model]] [[fit]]s a given [[data set]] .
[[Data mining]], the [[discovery of knowledge from data]], bridges several disciplines such as [[database management]], [[artificial intelligence]], [[statistics]], [[visualization]] and the domain of the [[data]], e.g., biology or [[engineering]] .
[[Data]] received by [[the system]] are either [[SMS response]]s to a poll conducted by [[UNICEF]], or [[unsolicited report]]s of a [[problem occurring within the community]] .
[[Datasets]] are taken from [[generative processes]] involving [[heterogeneous]] and [[inconsistent]] [[dynamics]] .
[[Data streams classification]] [31] studies the [[problem]] where [[real-world object]]s are generated [[online and processed]] in a [[real-time manner]] .
[[Data Stream]]s, [[Ensemble Method]]s, [[Concept Drift]], [[Decision Tree]]s
[[Data uncertainty]] is inherent in [[applications]] such as [[sensor monitoring systems]], [[location-based service]]s, and [[biological databases]] .
[[Data warehousing]] is a collection of [[decision support technologi]]es, aimed at enabling the [[knowledge worker]] ([[executive]], [[manager]], [[analyst]]) to make better and faster decisions.
[[DBPredictor]] is shown to be as accurate as [[IB1]] and [[C4.5]] against general [[dataset]]s.
[[Decentralization]] means [[empowering employees]], [[pushing decisions down]] from one [[level of government]] to another.
[[Decision tree]]s, [[boosted decision tree]]s, [[support vector machines (SVM)]], and [[case based reasoning]] are [[general machine learning method]]s.
[[Deep artificial neural networks (DNNs)]] are typically [[trained]] via [[gradient-based learning algorithm]]s, namely [[backpropagation]] .
[[DeepDive]] has been successfully applied to domains such as [[pharmacogenomic]]s, [[paleobiology]], and [[antihuman trafficking enforcement]], achieving [[human-caliber quality]] at [[machine-caliber scale]] .
[[DeepDive]] includes all of these [[contribution]]s, and we evaluate [[DeepDive]] on five [[KBC system]]s, showing that it can speed up [[KBC inference task]]s by up to two [[orders of magnitude]] with [[negligible impact]] on [[quality]] .
[[DeepDive]] is a [[data management system]] that we built to study [[this problem]]; its ultimate goal is to allow [[scientist]]s to build a [[KBC system]] by declaratively specifying [[domain knowledge]] without worrying about any [[algorithmic]], [[performance]], or [[scalability issue]]s.
[[DeepDive]] leverages the [[effectiveness]] and [[efficiency]] of [[statistical inference]] and [[machine learning]] for [[difficult extraction task]]s, whereas not requiring users to directly write any [[probabilistic inference algorithm]]s.
[[Deep neural network]]s have achieved impressive [[supervised classification]] [[performance]] in many [[task]]s including [[image recognition]], [[speech recognition]], and [[sequence to sequence learning]] .
[[DeepWalk]] generalizes recent advancements in [[language modeling]] and [[unsupervised feature learning]] (or <i>[[deep learning]] </i>) from [[sequence]]]s of [[words to graph]]s.
[[DeepWalk's representation]]s can provide [[F1 score]]s up to 10% [[higher]] than [[competing method]]s when [[labeled data]] is [[sparse]] .
[[DeepWalk]] uses [[local information]] obtained from [[truncated random walk]]s to <i>[[learn]]</i> [[latent representation]]s by treating walks as the [[equivalent of sentence]]s.
[[Defined]] in this report as [[people]] born between [[1985]] and [[1996]], [[New York City]] [[millennials]] experienced many of the same challenges as their [[national]] [[counterpart]]s – escalating [[educational requirement]]s for [[entry-level job]]s, soaring [[higher education]] [[cost]]s, and a proliferation of [[low-wage job]]s.
[[Defining]] and [[extracting]] a [[relevant]] [[viewpoint neighborhood]] [[efficiently]], while also [[quantifying]] the key [[relationships]] among [[nodes]] involved are the key [[challenges]] [[we]] address.
[[Definition acquisition]] is a necessary step in building an [[artificial cognitive assistant]] that helps [[military personnel]] to [[gain]] [[fast]] and [[precise understanding]] of the various [[term]]s and [[procedure]]s defined in applicable [[legal document]]s.
Definition: a [[social-security system]] in which all [[citizen]]s or [[resident]]s regularly receive an [[unconditional]] [[sum of money]] in addition to any [[income received from elsewhere]] .
[[Definition extraction]] is the [[task]] of [[automatically]] [[identifying]] [[definitional sentence]]s within [[texts]] .
Demand for [[dynamic scaling]] and benefits from [[economies of scale]] are driving the creation of [[mega data centers]] to host a broad range of services such as [[Web search]], [[e-commerce]], [[storage backup]], [[video streaming]], [[high-performance computing]], and [[data analytics]] .
[[Demonizer]]s of [[scientism]] often confuse [[intelligibility]] with a sin called [[reductionism]] .
:# Demonstrate [[compliance]] with [[Federal statute]]s, [[regulation]]s, and the [[terms and conditions]] of the [[Federal award]];<br>
[[Denote]] the [[sets]] of [[true]] and [[false positive]]s of Model i by [[TPi]] and FPi, then we can construct Model 4 under the condition that TP1 � [[TP3]] � [[TP2]] and [[FP1]] � [[FP3]] � [[FP2]] .
[[Department code]]: [[Three-letter identification field]] in [[MMARS]], unique to each [[agency]] .
[[Dependency parsing]] is a [[procedure]] that [[extract]]s [[syntactic dependenci]]es among the [[term]]s [[contained in]] a [[sentence]] .
::Depending on the [[sampling method]], a [[sample]] can have fewer [[observation]]s than the [[population]], the same number of [[observation]]s, or more [[observation]]s.
Depending upon whether this was a "[[tree]]" (only one [[node]] without a [[master]]) or a "[[forest]]" (multiple [[node]]s at the [[top]] without [[master]]s)]], applicable [[constraint]]s would also need to be [[added]] .
Deriving [[highly heritable quantitative trait]]s of a [[complex disease]] can improve the [[identification]] of [[genetic risk]] of the [[disease]] .
Despite enormous [[progress]] made in [[reconciling]] and [[simplifying]] [[semantic standard]]s, [[industry-strength model]]s are still few.
Despite [[it]]s simplicity, this assumption proves to be powerful since extensive [[experiment]]s show that [[TransE]] significantly outperforms [[state-of-the-art method]]s in [[link prediction]] on two [[knowledge base]]s.
Despite promising [[progress]], existing [[semi-supervised clustering approach]]es overlook the [[condition]] of [[side information]] being [[generated sequentially]], which is a [[natural setting]] arising in numerous [[real-world application]]s such as [[social network]] and [[e-commerce system analysis]] .
Despite the [[built-in redundancy]] in [[data center network]]s, [[performance issue]]s and [[device]] or [[link failure]]s in the [[network]] can lead to [[user-perceived service interruption]]s.
Despite the buzz surrounding [[these models]], the [[literature]] is still lacking a [[systematic comparison]] of the [[predictive model]]s with [[classic]], [[count-vector-based distributional semantic approach]]es.
Despite their [[massive size]], successful [[deep artificial neural network]]s can exhibit a remarkably [[small difference]] between [[training]] and [[test performance]] .
Despite the [[large number]] and variety of [[tool]]s and [[services available]] today for exploring [[scholarly data]], [[current support]] is still very [[limited in the context]] of [[sensemaking task]]s, which go beyond [[standard search]] and [[ranking of authors and publication]]s, and focus instead on i) understanding the [[dynamics of research area]]s, ii) [[relating author]]s ' [[semantically']] (e.g., in terms of [[common interest]]s or [[shared academic trajectori]]es), or iii) performing [[fine-grained academic expert search]] along multiple [[dimension]]s.
Despite the prevalence of [[community detection algorithm]]s, relatively [[less work]] has been done on [[understanding]] whether a [[network]] is indeed [[modular]] and how [[resilient]] the [[community structure]] is under [[perturbation]]s.
[[Detected event]]s are [[automatically broadcasted]] by [[our system]] via a [[dedicated Twitter account]] and by [[email notification]]s.
[[Detecting change]]s in [[multidimensional data stream]]s brings difficulties to the [[density estimation]] and [[comparison]]s.
[[Detecting]] [[community]] [[structures]] from [[large]] [[network data]] is a classic and challenging [[task]] .
[[Detecting memory leaks]] is challenging because [[real-world applications]] are built on multiple [[layers]] of [[software frameworks]], making it difficult for a [[developer]] to know whether [[observed]] [[references to objects]] are legitimate or the [[cause]] of a [[leak]] .
[[Detecting outlier]]s or [[anomalies in data]] has been studied in the [[statistics community]] as early as the [[19th century]] ([[Edgeworth 1887]]).
[[Determinantal Point Processes for Machine Learning]] provides a [[comprehensible introduction]] to [[DPP]]s, focusing on the [[intuition]]s, [[algorithm]]s, and [[extension]]s that are most relevant to the [[machine learning community]], and shows how [[DPP]]s can be applied to [[real-world application]]s like [[finding diverse set]]s of [[high-quality search result]]s, building [[informative summari]]es by selecting diverse [[sentences from document]]s, [[modeling non-overlapping human pose]]s in [[images or video]], and [[automatically building timeline]]s of important [[news stori]]es.
Determining fraud in [[financial transaction]]s, [[trading activity]], or [[insurance]] claims typically requires the determination of [[unusual pattern]]s in the [[data generated]] by the [[action]]s of the [[criminal entity]] .
[[Developing]] the [[simulator]]s will be a huge undertaking involving thousands of [[programmer]]s and [[experience-gathering robot]]s.
"[[Development]]" is [[the systematic use of]] [[knowledge]] and [[understanding gained]] from [[research]] directed toward the [[production]] of useful [[material]]s, [[device]]s, [[system]]s, or [[method]]s, including [[design]] and [[development]] of [[prototype]]s and [[process]]es.
[[DHC]]'s [[algorithmic layering capability]] is [[trained]] and [[tested]] over two [[real world dataset]]s and is currently integrated into the [[clinical decision support tool]]s at [[MultiCare Health System (MHS)]], a major provider of [[healthcare service]]s in the [[northwestern US]] .
[[Dialog systems (DS)]] allow [[intuitive interaction]] through [[natural language]] .
[[Dialogue topic tracking]] aims at [[analyzing]] and [[maintaining]] [[topic transition]]s in [[on-going dialogue]]s.
Different [[author]]s tried to improve the [[feature selection]] [[stability]] using [[ensemble method]]s which aggregate different [[feature set]]s into a single [[model]] .
Different [[belief]]s about the [[fairness]] of [[social competition]] and what determines [[income inequality]] influence the [[redistributive policy]] chosen in a [[society]] .
Different [[commentator]]s have emphasized distinct motivational [[feature]]s in [[Japan]] .
Different from [[existing]] [[feature selection methods in vector space]]s which assume the [[feature set]] is given, [[we]] perform [[semi-supervised feature selection]] for [[graph data]] in a [[progressive way]] together with the [[subgraph feature mining process]] .
Different from existing [[workflow mining algorithms]], [[our approach]] can construct [[concurrent]] [[workflows]] from [[trace]]s of [[interleaved events]] .
Different from previous [[approaches]] that employed [[rule-based]] or [[statistical technique]]s, [[we]] propose a novel [[machine learning approach]] built under the [[framework]] of [[lexicalized HMMs]] .
Different from [[simulator-based approach]]es, [[our method]] is [[completely data-driven]] and [[very easy to adapt]] to different [[application]]s.
Different from the existing [[works]], this [[scheme]] provides a [[complete]] [[view]] of these [[itemsets]] by covering the entire [[collection]] of them.
Different from the [[previous studi]]es that only [[apply dropout]] to [[training data]], we [[apply dropout]] to both the [[learned metric]]s and the [[training data]] .
[[Differentially private algorithm]]s require a [[degree of uncertainty]] in their [[output]] to preserve [[privacy]] .
[[Differential privacy]] offers [[individual]]s a [[rigorous]] and appealing [[guarantee]] of [[privacy]] .
[[Differential privacy]] requires that [[computations]] be insensitive to [[changes]] in any [[particular individual's record]], thereby [[restricting data leaks]] through the [[results]] .
Differently from [[decision tree]]s, [[ShoppingAdvisor]] [[partition]]s the [[user space]] rather than the [[product space]] .
Different [[message passing scheme]]s on [[this graphical model]] provide various [[inference algorithm]]s to [[trade-off]] [[end-to-end performance]] and [[computational cost]] .
Different [[types of edge]]s and [[node]]s have different set of [[feature]]s.
Different [[weighting scheme]]s for [[quantifying]] the [[relative importance]] of the [[discovered pattern]]s are explored: [[Frequency]], [[Support Vector Machine (SVM)]] and [[Frequency + SVM]] .
[[Digital data explosion]] mandates the [[development]] of [[scalable tool]]s to organize the [[data]] in a [[meaningful]] and [[easily accessible form]] .
[[Digital storage]] of [[personal music]] [[collection]]s and [[cloud-based music service]]s (e.g.
[[Digitization]] creates [[winner-take-all market]]s because, as noted above, with [[digital good]]s [[capacity constraint]]s become increasingly irrelevant.
[[Dimensionality reduction]] facilitates the [[classification]], [[visualization]], [[communication]], and [[storage]] of [[high-dimensional data]] .
[[Dimensionality reduction]], [[generalized eigenvalue problem]], [[least square]], [[regularization]], [[scalability]]
[[Dimensionality reduction]] plays an important [[role]] in many [[data mining applications]] involving [[high-dimensional data]] .
[[Dimensionality Reduction]], [[Random Projection]], [[Compressed Learning]], [[Linear Discriminant Analysis]], [[Classification]]
[[Dimensionality reduction]] within [[this space]] is useful, as it decreases [[scoring time]] and [[model storage requirement]]s.
[[Dipole]]s represent [[long distance connection]]s between the [[pressure anomali]]es of two [[distant region]]s that are [[negatively correlated]] with each other.
[[Direct appropriation account]]: [[Entity]] in [[MMARS]] which records the status of [[appropriations]] which are financed by budgeted fund [[unrestricted revenue]]s.
[[Directed link]]s -- representing [[asymmetric social tie]]s or [[interaction]]s (e.g.," [[follower-followee ")]] -- arise naturally in many [[social network]]s and other [[complex network]]s, giving rise to [[directed graphs (or digraphs)]] as basic [[topological model]]s for these [[network]]s.
[[Directly reading documents]] and being able to [[answer questions]] from [[them]] is an [[unsolved challenge]] .
[[Disambiguating entity reference]]s by [[annotating]] them with unique [[id]]s from a [[catalog]] is a critical step in the [[enrichment]] of [[unstructured content]] .
[[Discourse processing]] requires recognizing how the [[utterances of the discourse aggregate into segment]]s, [[recognizing the intentions expressed]] in the [[discourse]] and the [[relationships among intention]]s, and [[tracking]] the [[discourse]] through the [[operation of the mechanism]]s associated with [[attentional state]] .
[[Discovering frequent graph pattern]]s in a [[graph database]] offers valuable [[information]] in a variety of [[application]]s.
[[Discovery]] of [[alternative clusterings]] is an important [[method]] for exploring [[complex datasets]] .
[[Discovery]] of those [[relationships]] can benefit many interesting [[applications]] such as [[expert finding]] and [[research community analysis]] .
[[Discrete]] and [[continuous sequence]]s (or [[time series]]) are two most important [[form]] of [[sequences encountered]] in real [[life]] .
[[Discrimination discovery]] boils down to [[extract]]ing a [[classification model]] from the [[labeled tuple]]s.
[[Discrimination prevention]] is tackled by changing the [[decision value]] for [[tuples labeled]] as discriminated before [[training]] a [[classifier]] .
[[Discriminative classifier]]s, which directly model the [[posterior distribution of class label]] given [[feature]]s, i.e. [[SVM]] ([[Isozaki and Kazawa 2002]]) and [[Maximum Entropy model]] for [[NER]] ([[Chieu and Ng 2003]]), have been shown to outperform [[generative model based classifier]]s.
[[Discriminative Model]], [[EM Algorithm]], [[Link analysis]], [[Two-Stage Optimization]]
[[Display ad]]s on the [[Internet]] are often sold in bundles of [[thousand]]s or [[million]]s of [[impression]]s over a particular [[time period]], typically [[week]]s or [[month]]s.
[[Display advertising]] has been a significant [[source of revenue]] for [[publisher]]s and [[ad network]]s in [[online advertising ecosystem]] .
[[Distance metric learning (DML)]] aims to [[learn]] a [[distance metric]] better than [[Euclidean distance]] .
[[Distance metric]]s and distances have now become an essential tool in many areas of [[Mathematics]] and its applications including [[Geometry]], [[Probability]], [[Statistics]], [[Coding]] / [[Graph Theory]], [[Clustering]], [[Data Analysis]], [[Pattern Recognition]], [[Networks]], [[Engineering]], [[Computer Graphics]] / [[Vision]], [[Astronomy]], [[Cosmology]], [[Molecular Biology]], and many other [[areas of science]] .
[[Distance queri]]es are particularly useful when [[data set]]s such as [[measurement]]s, [[snapshots of a system]], [[content]], [[traffic matrice]]s, and [[activity log]]s are collected repeatedly.
[[Distributed word representation]]s have a long history, with early proposals including ([[Hinton, 1986]]; [[Pollack, 1990]]; [[Elman, 1991]]; [[Deerwester et al., 1990]]).
[[Distributional semantics]] is an [[empirical field of research]] and development that attempts to [[discover]] and [[model]] the [[meanings of word]]s by analyzing and comparing their [[distribution]]s in large [[text corpora]] .
[[Diversified ranking]] on [[graph]]s is a fundamental [[mining task]] and has a variety of [[high-impact]] [[application]]s.
[[DivRank]] [[outperform]]s existing [[network-based ranking methods]] in terms of enhancing [[diversity]] in [[prestige]] .
[[DL reasoners]] can [[infer]] and [[detect]] [[logical contradictions]] in the [[ontologies]] specified in a certain [[web ontology language]], such as [[OWL]] .
[[Document clustering]] has been used for better [[document retrieval]], [[document browsing]], and [[text mining]] in [[digital library]] .
[[Document clustering]] is an [[enabling technique]] for many other [[machine learning application]]s, such as [[information classification]], [[filtering]], [[routing]], [[topic tracking]], and [[new event detection]] [2].
[[Document data]] is often available in raw and [[unstructured form]], and the [[data]] may contain [[rich linguistic relation]]s between different [[entiti]]es.
[[Document Ranking]], [[Information Retrieval]], [[Patent Processing]], [[Patent Visualization]]
Documents are [[represented by]] ca [[100]] [[item vector]]s of factor weights.
[[Doing nothing]] is a [[vote in favour]] of continued [[development]] and [[deployment]] .
[[Domain]] and [[style independence]] is obtained thanks to the [[annotation]] of a [[sample]] of [[the Wikipedia corpus]] and to a novel [[pattern generalization algorithm]] based on [[word-class lattices (WCL)]] .
[[Domain chapter]]s: These chapters discuss the specific [[method]]s used for different [[domains of data]] such as [[text data]], [[time-series data]], [[sequence data]], [[graph data]], and [[spatial data]] .
Do the [[incidents]] [[communication]], or [[weight]]s on the [[edges]] of a [[clique]] follow any [[pattern]]?
[[Dot product]] is a key [[building block]] in a number of [[data mining algorithm]]s from [[classification]], [[regression]], [[correlation clustering]], to [[information retrieval]] and many others.
[[Dremel]] is a [[scalable]], [[interactive ad-hoc query system]] for [[analysis]] of [[read-only nested data]] .
:(d) The term "[[loan guarantee commitment]]" means a [[binding agreement]] by a [[Federal awarding agency]] to make a [[loan guarantee]] when specified [[condition]]s are fulfilled by the [[borrower]], the [[lender]], or any other [[party]] to the [[guarantee agreement]] .
d) to [[revive interest]] in [[statistics]] among [[student]]s, since they will see its [[usefulness]] and [[relevance]] in almost all [[branches of Science]] .
Due to its [[complexity]] and its connection to [[location]] based [[social networks (LBSNs)]], the [[decision process]] of a [[user]] choose a [[POI]] is [[complex]] and can be [[influence]]d by various [[factor]]s, such as [[user preference]]s, [[geographical influence]]s, and [[user mobility behavior]]s.
Due to the [[avalanche]] of [[biological literature]] in recent years, and increasing [[popularity]] of various [[bio-imaging technique]]s, [[automatic]] [[retrieval]] and [[summarization]] of [[biological information]] from [[literature]] [[figure]]s has emerged as a major [[unsolved challenge]] in [[computational knowledge extraction]] and [[management]] in the [[life science]] .
Due to the [[difficulty]] and [[complexity]] of [[software systems]], [[bugs]] and [[anomalies]] are prevalent.
Due to the [[financial gain]]s associated with [[positive review]]s, however, [[opinion spam]] has become a [[widespread problem]], with often [[paid spam reviewer]]s writing [[fake review]]s to unjustly [[promote]] or [[demote]] certain [[product]]s or [[business]]es.
Due to the [[hardness of the problem]], [[previous]] [[accounts]] of such [[problem]]s employ [[heuristics]] and the resulting [[approximation]] may be far away from the [[optimal one]] .
Due to their damages to [[Internet security]], [[malware]] (such as [[virus]], [[worm]]s, [[trojan]]s, [[spyware]], [[backdoor]]s, and [[rootkits]]) [[detection]] has caught the attention not only of [[anti-malware industry]] but also of [[researcher]]s for [[decade]]s.
Due to their [[simplicity]], [[standard wiki system]]s show [[limitation]]s when actually using the included [[knowledge]] .
Due to the [[KKT condition]]s, the [[M<sup>3</sup>N]] enjoys [[dual sparsity]] .
Due to the [[large volume]] of [[event]]s in [[EBSN]]s, [[event recommendation]] is essential.
Due to the [[power of copula]] in [[modeling semi-parametric distribution]]s, [[SpaGraphR]] can model a [[rich class]] of [[dynamic non-Gaussian correlation]]s.
Due to the [[reinforcement]] between [[explicit feature]]s and [[implicit pattern]]s, [[our approach]] can provide better [[group recommendation]]s.
Due to the [[rich information]] in [[graph data]], the [[technique]] for [[privacy protection]] in [[published social network]]s is still in its [[infancy]], as compared to the [[protection]] in [[relational database]]s.
Due to the [[sequential nature]] of our [[RE task]], [[H-CRF]] employs a [[CRF]] as the [[meta-learner]], as opposed to a [[decision tree]] or [[regression-based classifier]] .
Due to this [[imbalance]], [[RBM]] tends to [[learn]] multiple [[redundant]] [[hidden unit]]s to best represent [[dominant topic]]s and ignore those in the [[long-tail region]], which renders the [[learned representation]]s to be [[redundant]] and [[non-informative]] .
During [[2008]]-[[2009]], the then [[president]] of the [[Association for the Advancement of Artificial Intelligence (AAAI)]], [[Eric Horvitz]], [[assembled a group of AI expert]]s from multiple [[institution]]s and [[areas of the field]], along with [[scholar]]s of [[cognitive science]], [[philosophy]], and [[law]] .
During 2121 [[patient-year]]s of [[follow-up]], 94 [[patient]]s were [[readmitted to the hospital]] for [[ischemic stroke]] ([[stroke rate]], 4.4 per [[100]] patient-years).
During its development, the [[Busy Child]], as the [[scientist]]s have named the [[AI]], had been connected to the [[Internet]], and accumulated exabytes of [[data]] (one exabyte is one billion billion characters) representing [[mankind’s knowledge]] in [[world affairs]], [[mathematics]], [[the arts]], and [[science]]s.
[[During manufacture]], [[downstream corrective processing]] restored 25 [[nominally unacceptable wafer]]s to [[normal operation]] .
During phase I, fifteen [[Chemistry experts]] were split into five [[groups]] of three, each of which [[annotated]] eight different [[papers]]; A 16th [[expert]] annotated across [[groups]] as a [[consistency check]] .
During the same [[period]], [[children]] of the [[baby boomer generation]] – a [[large population cohort]] known as the [[millennials]] – began their [[working live]]s.
During this time, [[employment]] in [[service]]s (both [[low-]] and [[high-skill]]) grew rapidly, while [[employment]] in [[manufacturing]], [[mining]], and [[agriculture sector]]s fell (even in countries that are strong exporters of manufactured goods).
[[During training]], the goal is to [[minimize]] the [[reconstruction error]]s of [[this input]] pair.
[[Dynamic Bayesian Networks (DBNs)]] are [[directed graphical model]]s of [[stochastic process]]es.
[[Dynamic Network Analysis]], [[Graph Mining]], [[Biological Network]], [[Graph Rewriting Rule]] .
[[Dynamic pricing]] is the study of determining optimal [[selling price]]s of [[products or services]], in a [[setting]] where [[price]]s can easily and frequently be [[adjusted]] .
[[Dynamic probabilistic network]]s are a compact representation of [[complex]] [[stochastic process]]es.
Each [[abstract]] has its [[concept mentions]] [[identified]] and, where possible, [[linked]] to the appropriate [[concept in the ontology]] .
Each [[abstract]] was [[internally annotated]] to identify the [[concepts mentioned]] within the [[text]] .
Each case study illustrates a unique [[intersection]] of [[data mining]] and [[healthcare]] with a [[common objective]] of improving the [[cost-care ratio]] by [[mining]] for opportunities to improve [[healthcare operation]]s and reducing what seems to fall under [[fraud]], [[waste]], and [[abuse]] .
Each chapter includes [[detailed example]]s along with further [[reading]] and [[problem]]s.
Each [[classification group]] is often called a [[segment]] and identifies a [[discrete information]] requirement for [[management]], [[reporting]] and [[control]] purposes.
Each [[cluster]] that a [[word]] [[belongs to]] [[represents]] one of its [[senses]] .
Each [[column name]] of a [[relation]] represents a [[role name]] in the [[relation]] and its [[domain]] is a [[set of values]] which may possibly be inserted into the [[column]] .
Each common [[prefix]] of [[T-pattern]]s becomes common [[path]] on the [[tree]] .
Each [[document]] from an [[unlabeled corpus]] <math>D</math> is to be [[categorized]] into one or more [[class]]es from a [[class set]] <math>C</math>.
Each [[document]] is [[represented as]] a [[list]] of [[mixing proportions]] for these [[mixture components]] and thereby reduced to a [[probability distribution]] on a [[fixed set]] of [[topics]] .
Each [[grade]] contains several [[salary rate]]s, increasing in [[increment]]s from the bottom to the [[top]] .
Each [[mention]] is associated with a [[set]] of candidate [[ontology concepts]], and [[binary]] [[training feature vectors]] are generated for these pairings.
Each mention is categorised in four [[semantic categori]]es: [[Task]], [[Method]], [[Resource/Feature]] and [[Implementation]] .
Each [[mixture]] describes a specific [[view]] on the [[data]] by using a [[mixture of Beta distribution]]s in <i>[[subspace projection]]s </i>.
Each step of the [[iterative procedure]] involves a [[convex]], but [[non-smooth]] and [[non-separable problem]] .
Each [[sub-topic map]]s to a single [[distribution]] over the [[vocabulary]] .
Each [[term]] is then [[weighted]] with a [[log–entropy transform]] applied to the [[value of each cell]] in order to [[reduce the influence]] of [[very frequent word]]s.
Each time a [[vehicle]] enters an [[intersection]], a [[highway segment]], or any other type of [[entity (a trial)]] on a given [[transportation network]], it will either [[crash]] or [[not]] [[crash]] .
Each [[topic]] is, in turn, [[modeled]] as an [[infinite mixture]] over an [[underlying]] [[set]] of [[topic]] [[probabilities]] .
[[Earlier model]]s assume that there is [[perfect competition]]: if a [[user]] buys [[product]] 'A' (or gets [[infected with virus]] ' X'), she will never buy [[product]] 'B' (or [[virus]] 'Y').
[[Early AI researcher]]s such as [[McCarthy]], [[Minsky]], and [[Shannon]] were mathematicians by [[training]], so [[theorem-proving]] and [[formal model]]s were attractive [[research direction]]s.
Early attempts by [[Leibniz]], [[Bernoulli]], [[De Morgan]], [[Boole]], [[Peirce]], [[Keynes]], and [[Carnap]] (surveyed by [[Hailperin]]12 and [[Howson]]14) involved attaching [[probabiliti]]es to [[logical sentence]]s.
Early [[classification]] of [[time series]] is prevalent in many [[time-sensitive application]]s such as, but not limited to, [[early warning]] of [[disease outcome]] and [[early warning]] of [[crisis in stock market]] .
Early "[[hardening]] " of [[extraction decision]]s at a [[table level]] leads to poor [[accuracy]] .
Early [[NER system]]s ([[Fisher et al., 1997]]), ([[Black et al., 1998]]) etc., participating in [[Message Understanding Conferences (MUC)]], used [[linguistic tool]]s and [[gazetteer list]]s.
[[Early pattern]]s of [[Digg digg]]s and [[YouTube view]]s reflect [[long-term user interest]] .
Early successes in [[chess]] and other [[games]] shaped the emerging [[field of AI]]: many [[planning algorithm]]s first used in [[game]]s became pillars of subsequent [[research]]; [[reinforcement learning]] was first developed for a [[checkers playing program]]; and the performance of [[game-playing program]]s has frequently been used to [[measure progress]] in [[AI]] .
[[E-commerce recommender system]]s aim to present [[item]]s with [[high utility]] to the [[consumer]]s ([[Lee & Hosanagar, 2014]]).
[[Economic culture]] is defined as the [[belief]]s, [[attitude]]s, and [[value]]s that bear on the [[economic activiti]]es of [[individual]]s, [[organization]]s, and other [[institution]]s.
[[Economic planner]]s and [[social engineer]]s are rather like [[architect]]s and [[real engineer]]s in that they strive to [[maximize something]] .
[[Economists]] almost always capture [[impatience]] by assuming that [[people]] [[discount]] [[streams of utility]] over time [[exponentially]] .
[[Edge]]s provide a [[sparse]] yet informative [[representation]] of an [[image]] .
Effective [[broad matching]] leads to [[improvements]] in both [[relevance]] and [[monetization]], while increasing [[advertisers']] [[reach]] and making [[campaign management]] easier.
Effective [[classification]], [[prediction]] and [[change modeling]] of [[consumer]] [[interests]], [[behavior]]s and [[purchasing habits]] using [[machine learning]] and [[statistical method]]s drives efficiency, insights and [[consumer relevance]] that were never before possible.
[[Efficient algorithm]]s based on [[Iterative Group Hard Thresholding]] are developed to achieve efficient and effective [[model]] [[training]] and [[prediction]] .
[[Efficient algorithm]]s that are capable of [[estimating]] [[network]]s from [[high-dimensional data]] are highly desired.
[[Efficient management]] of [[data centers]], including [[power management]], [[networking]], and [[cooling infrastructure]], is hence crucial to sustainability.
[[Efficient]] [[thermal management]] is important in modern [[data center]]s as [[cooling]] consumes up to 50% of the total [[energy]] .
[[Efficient]] [[update]]s and [[scalable learning]] is still possible due to the [[Sherman-Woodbury-Morrison formula]] .
[[Eigenapp]], which exploits [[neighborhood information]] in [[low dimensional space]]s, did well both on [[precision]] and variety, [[underscoring]] the importance of [[dimensionality reduction]] to form [[quality neighborhood]]s in [[high kurtosis distribution]]s.
[[Electronic commerce]], [[product description]], [[relational schema]], [[schema integration]]
[[EL]] is key for [[Information Extraction (IE)]] and many other [[application]]s.
[[Elizabeth Kneebone]] explains why this [[increased]] [[concentration]] of [[poverty]] in [[suburban communiti]]es can pose greater [[challenge]]s.
[[Email message]]s are an example of [[semistructured data]] in that they have [[well-defined]] [[header field]]s and an [[unstructured text body]] .
[[Embedding]]s are mostly derived for [[word form]]s although a number of recent papers have extended this to other [[linguistic unit]]s like [[morpheme]]s ([[Luong et al., 2013]]), [[phrase]]s and [[word sequence]]s ([[Socher et al., 2010]]; [[Mikolov et al., 2013]]).
[[EMD]] allows us to represent [[inherent relationship]]s in [[this space]], and enables us to successfully [[cluster]] even [[sparse signature]]s.
[[EMeralD]] substantially improves upon the [[state-of-the-art]] [[filtering mechanism]]s by achieving much lower [[computational cost]] and [[higher accuracy]] .
[[Emergency management]] is a [[candidate field]] of [[application]] for [[social sensing]] .
[[Emergency preamble]]: [[Language inserted]] at the [[beginning]] of some [[Act]]s, which declares that the [[Act]] is an [[emergency law]] .
[[Empathy]] is an ideal candidate [[mechanism]] to underlie so-called [[directed altruism]], i.e., [[altruism]] in response to anothers's [[pain]], [[need]], or [[distress]] .
[[Empirical comparisons]] with several [[state-of-the-art algorithm]]s demonstrate the [[efficiency]] of the proposed [[Lassplore algorithm]] for [[large-scale]] [[problems]] .
[[Empirical evaluation]] and [[online user studi]]es demonstrate the [[efficacy]] and effectiveness of [[our proposed system]] .
[[Empirical evaluation]] demonstrates the [[efficiency]] of [[the proposed method]]s and their ability to fully utilize [[computational resources]] and [[scale]] to [[out-of-memory dataset]]s.
[[Empirical evaluation]] shows that [[STROD]] reduces the [[runtime of construction]] by several [[orders of magnitude]], while generating [[consistent]] and [[quality]] [[hierarchi]]es.
[[Empirical evidence]] suggests that [[provenance-similarity account]]s for a significant portion of [[variation]] in existing [[binari]]es, particularly in [[malware]] .
[[Empirical experiments]] on a [[hotel]] [[review data set]] show that [[the proposed latent rating regression model]] can effectively solve the [[problem]] of [[LARA]], and that the detailed [[analysis of opinions]] at the level of [[topical aspects]] enabled by the [[proposed model]] can support a [[wide range]] of [[application tasks]], such as [[aspect opinion summarization]], [[entity ranking]] based on [[aspect ratings]], and [[analysis]] of [[reviewers]] [[rating]] [[behavior]] .
[[Empirical experiments]] with two different [[communities]] and [[datasets]] (i.e., [[Twitter]] and [[DBLP]]) show that [[our approach]] is [[effective]] and [[outperform]]s [[existing approaches]] .
[[Empirically]], [[Tensor Sketching]] achieves higher [[accuracy]] and often runs [[orders of magnitude]] faster than the [[state-of-the-art]] [[approach]] for [[large-scale]] [[real-world dataset]]s.
[[Empirically]], this new approach effectively simulates [[properti]]es of several [[social]] and [[information network]]s.
Empirically [[we]] demonstrate [[state-of-the-art]] [[accuracy]] on [[publicly available benchmark]]s.
[[Empirically]], [[we]] develop and [[experiment]] with two [[real-time]] [[bid adjustment approach]]es to adapting to the [[non-stationary nature]] of the [[marketplace]]: one [[adjusts bid]]s against [[real-time]] [[constraint satisfaction level]]s using [[control-theoretic method]]s, and the other [[adjusts bid]]s also based on the [[statistically modeled]] [[historical bidding landscape]] .
[[Empirical results]] benchmarking on three [[real-world data]] [[sets]] demonstrate a [[performance]] improvement of the proposed [[method]] over other existing [[collaborative filtering]] [[model]]s.
[[Empirical result]]s demonstrate that <i>both the [[clustering]] and [[cleaning accuraci]]es</i> can be improved by [[our approach]] of [[repairing]] and utilizing the [[dirty data]] in [[clustering]] .
[[Empirical results]] on diverse [[domains]] show that [[our approach]] performs better than using [[background knowledge]] or [[training data]] in isolation, as well as alternative [[approaches]] to using [[lexical knowledge]] with [[text classification]] .
[[Empirical results]] on [[real-world]] [[search engine]] [[logs]] show significant gains over a [[strong baseline]] that uses single-[[keyword]] [[reformulations]]: a gain of 14% and 23% in terms of [[human-judged accuracy]] and [[click-through data]] respectively, and around 20% in terms of [[consistency]] among [[aspect]]s [[predicted]] for "[[similar]]" [[queries]] .
[[Empirical results]] on several [[large]] [[publicly available]] [[graphs]] like [[DBLP]], [[Citeseer]] and [[Live-Journal]] (~90 [[M]] [[edges]]) demonstrate that turning [[high degree nodes]] into [[sinks]] not only improves [[running time]] of [[RWDISK]] by a [[factor]] of <i>3</i> but also boosts [[link prediction]] [[accuracy]] by a [[factor]] of <i>4</i> on [[average]] .
[[Empirical result]]s shows that our [[method]] achieves good [[predictive performance]] compared to [[state-of-the-art]] [[algorithm]]s and that it requires much [[less time]] than another [[instance-based stream mining algorithm]] .
[[Empirical results]] show that using [[SVM classifier]] [[our algorithm uHARMONY]] [[outperform]]s the [[state-of-the-art]] [[uncertain data classification algorithm]]s significantly with 4% to 10% [[improvements]] on [[average]] in [[accuracy]] on 30 [[categorical datasets]] under varying [[uncertain degree]] and [[uncertain attribute number]] .
[[Empirical result]]s validate [[CRPS]]’s [[efficiency]] with no degradation in [[accuracy]] .
[[Empirical risk minimization (ERM)]] provides a useful [[guideline]] for many [[machine learning]] and [[data mining algorithm]]s.
[[Empirical studi]]es justify the effectiveness of [[CSGD]] by comparing it with [[SGD]] and other [[state-of-the-art]] [[approach]]es.
[[Empirical studies]] on a [[large]] [[real-world]] [[mobile social network]] show that [[our algorithm]] is more than an [[order of magnitudes]] faster than the [[state-of-the-art]] [[Greedy algorithm]] for [[finding top-K influential nodes]] and the [[error]] of [[our approximate algorithm]] is [[small]] .
[[Empirical studies]] on both [[synthetic]] and [[real data sets]] demonstrate the [[effectiveness]] of [[our method]] .
[[Empirical studi]]es on both [[synthetic]] and [[real-world data]] demonstrate that our proposed [[rMTFL]] is capable of simultaneously [[capturing]] [[shared feature]]s among [[task]]s and [[identifying]] [[outlier task]]s.
[[Empirical studi]]es on [[large-scale]] [[dynamic composite social network]]s demonstrate that the proposed [[approach]] improves the [[performance]] of [[link prediction]] over several [[state-of-the-art]] [[baseline]]s and unfolds the [[network evolution]] accurately.
[[Empirical studi]]es on nine [[real-world task]]s demonstrate that [[the proposed method]] can obtain better [[accuracy]] on [[graph data]] than [[alternative approach]]es.
[[Empirical studi]]es on [[real-world task]]s demonstrate that the [[performance]] of [[multi-label classification]] can be effectively [[boosted]] using [[heterogeneous information network]]s.
[[Empirical studies]] on several [[real-world tasks]] demonstrate that [[our semi-supervised feature selection approach]] can effectively [[boost]] [[graph classification performance]]s with [[semi-supervised feature selection]] and is very efficient by [[pruning]] the [[subgraph search space]] using both [[labeled]] and [[unlabeled graphs]] .
[[Employer]]s’ ability to [[raise]] [[wage]]s and [[other compensation]] is [[tied]] to [[increase]]s in [[labor productivity]] .
[[Employer]]s who [[use IT]] often make [[complementary]] [[innovation]]s in their [[organization]]s and in the [[service]]s they offer.
[[Employing Twitter]] as a [[source]] for [[our experimental data]], and working within a [[semi-supervised framework]], [[we]] propose [[model]]s that are induced either from the [[Twitter follower]] / [[followee network]] or from the [[network]] in [[Twitter]] formed by [[user]]s referring to each other using [["@" mention]]s.
[[Encumbrance]]: [[The setting aside of money]] in [[MMARS]] by an [[agency]] to meet known [[obligation]]s.
\end{pmatrix}+b</math> are [[neuron adder function]]s for a [[Neural Network Layer]] with n [[input]]s and p [[output]]s.
Enough [[businesses]] have successfully [[reengineer]]ed their [[process]]es to provide some [[rules of thumb]] for others.
[[Ensemble clustering]], also known as [[consensus clustering]], is emerging as a [[promising solution]] for [[multi-source]] and/or [[heterogeneous data clustering]] .
[[Ensemble pruning]] tackles [[this problem]] by selecting a [[subset]] of [[ensemble members]] to form [[subensembles]] that are subject to [[less]] [[resource consumption]] and [[response time]] with [[accuracy]] that is [[similar to or better]] than the original [[ensemble]] .
[[Entities]] are typically [[noun phrases]] and comprise of [[one]] to a few [[tokens]] in the [[unstructured text]] .
[[Entity Annotation]], [[Entity Disambiguation]], [[Wikipedia]], [[Collective Inference]] .
[[Entity annotation]] is the [[task of recognizing instance]]s of [[domain concept]]s in the [[text]] .
[[entity]]) [[dependent sentiment classification approach]] to [[identifying the opinion]] towards a given [[target]] (i.e.
[[Entity discovery]] is based on [[pattern discovery]] and [[entity assignment]] is based on [[mining]] of [[comparative sentence]]s.
[[Entity Linking (EL)]] is the [[task]] of [[linking]] [[name mention]]s in [[Web text]] with their [[referent entiti]]es in a [[knowledge base]] .
[[Entity Linking]] is the [[task of assigning]] [[entities from a Knowledge Base]] to [[textual <i>mentions</i>]] of such [[entiti]]es in a [[document]] .
[[Entity linking]] mainly involves [[measuring]] the [[compatibility]] and [[semantic relatedness]] between [[mention]]s and [[entiti]]es, for which the [[semantic representation]] plays a critical role.
[[Entity linking system]]s link [[noun-phrase mention]]s in [[text]] to their corresponding [[Wikipedia article]]s.
[[Entity matching]] (also referred to as [[duplicate identification]], [[record linkage]], [[entity resolution]] or [[reference reconciliation]]) is a crucial task for [[data integration]] and [[data cleaning]] [19,33,47].
[[Entity Record Deduplication Algorithm]], [[Citation Record Deduplication Task]], [[Record Deduplication Function]], [[Active Learning Algorithm]] .
[[Entity resolution]] [[Entity matching]] [[Matcher combination]] [[Match optimization]] [[Training selection]]
[[Entity resolution (ER)]] is a common [[data cleaning task]] that involves [[determining]] which [[record]]s from one or more [[data set]]s refer to the same [[real-world entiti]]es.
[[Entity synonym]]s are critical for many [[application]]s like [[information retrieval]] and [[named entity recognition]] in [[document]]s.
[[Entrepreneurial government]]s are [[catalytic]], [[competitive]], [[mission driven]], [[results oriented]], [[customer driven]], and [[enterprising]] .
Entries are provided for [[standard]] and [[specialized]] [[statistical software]] .
[[Episode]]s can impose [[restriction]]s to the order of the [[event]]s, which makes them a [[versatile technique]] for describing [[complex pattern]]s in the [[sequence]] .
[[Error]]s are introduced as the result of [[transcription error]]s, [[incomplete information]], [[lack of]] [[standard format]]s or any combination of these factors.
[[Error-tolerant graph matching]]; [[Distance measure]]; [[Maximal common subgraph]]; [[Graph edit distance]]; [[Metric]]
[[ESN]]s can provide [[employee]]s with various [[professional service]]s to help them deal with [[daily work issue]]s.
Especially during [[software maintenance]] or [[reverse engineering]], [[semantic information]] conveyed in these [[documents]] can provide important [[knowledge]] for the [[software engineer]] .
Especially since new [[consumer]]s and [[item]]s emerge every [[day]], which are promptly [[rated]] or [[reviewed]], [[updating]] [[lists of items]] and [[lists of rankings]] is crucial.
[[Essentially]], [[Hawkes process]]es utilize their [[self-exciting properti]]es to [[identify search task]]s if [[influence exist]]s among a [[sequence]] of queries for individual [[user]]s, while the [[topic model]] exploits [[query co-occurrence]] across different users to discover the [[latent information]] needed for [[labeling search task]]s.
[[Estimated Receipt Amount]]: The field in [[MMARS]] that records the amount of [[money]] that [[Administration]] and [[Finance believe]]s will become available during the [[fiscal year]] in [[retained revenue]], [[Intragovernmental Service Fund]], and [[federal grant]] [[account]]s.
[[Estimating similarity]] between [[vertice]]s is a [[fundamental issue]] in [[network analysis]] across various [[domain]]s, such as [[social network]]s and [[biological network]]s.
[[Ethics]] is an important component of [[STEM education]] as illustrated by the fact that [[ABET accreditation]] requires [[proof]] of [[training in ethics]] for [[engineering field]]s.
: [[Ethics]] may be defined as [[philosophical inquiry]] into the [[nature]] and [[grounds of morality]] .
[[Ethics]] may be defined as [[philosophical inquiry]] into the [[nature]] and [[grounds of morality]] .
[[Ethnicity Detection]], [[Name Classification]], [[News Analysis]], [[Social Science Research]]
[[Etiologic theory]] examines the role of [[risk]] and [[protective factor]]s in prevention, and an [[RFT]] formally tests whether changes in these [[hypothesized factor]]s lead to the [[prevention]] of [[targeted outcome]]s.
[[Evaluating]] [[semantic relatedness]] using [[network representation]]s is a problem with a long history in [[artificial intelligence]] and [[psychology]], dating back to the [[spreading activation approach]] of [[Quillian (1968)]] and [[Collins and Loftus (1975)]] .
[[Evaluation]] is made on a [[large]] [[movie]] [[rating]] [[dataset]] by [[Netflix]] .
[[Evaluation]] is made on a [[large]] [[movie-rating dataset]] underlying the [[Netflix Prize contest]] .
[[Evaluation]] on a [[Business-to-Business (B2B)]] [[marketing application]] demonstrates that [[our]] [[approach]] can effectively discover critical [[buying path]]s from [[noisy]] [[customer event data]] .
[[Evaluation result]]s using [[real-life discussion]] / [[debate post]]s from several [[domain]]s demonstrate the effectiveness of the proposed [[model]]s.
[[Evaluation]]s are conducted on three [[public-domain data set]]s and [[the results]] suggest that [[our proposed method]] [[outperform]]s several [[baseline alternative]]s.
[[Evaluations of the system]] show that the [[automatic]] [[annotations]] are reliable and hardly distinguishable from [[manual]] [[annotations]] .
[[Evaluations]] show that [[IPLoM]] [[outperform]]s the other [[algorithm]]s [[statistically significantly]], and it is also able to [[achieve]] an [[average]] [[F-Measure performance]] [[78%]] when the closest other [[algorithm]] [[achieves]] an [[F-Measure performance]] of [[10%]] .
[[Evaluation]] using [[crossvalidation run]]s shows that their [[classification]] [[accuracy]] is similar to the complete [[set]] of [[trees]] but significantly better than that of open [[tree]]s.
Even for [[imperfect]] and [[noisy feedback]], [[we]] show that the [[algorithm]]s admit [[theoretical guarantee]]s for [[maximizing]] any [[submodular utility measure]] under [[approximately]] [[rational user behavior]] .
[[Event-based social networks (EBSNs)]], in which [[organizers publish event]]s to attract other [[user]]s in [[local city]] to [[attend offline]], [[emerge]] in recent years and grow rapidly.
Even though [[human movement]] and [[mobility pattern]]s have a high [[degree of freedom]] and [[variation]], they also exhibit [[structural pattern]]s due to [[geographic]] and [[social constraint]]s.
Even though [[the data]] is relatively [[small]], we show that [[automatic prediction]] of a [[belief class]] is a [[feasible task]] .
Even though the [[information-theoretic]] [[regularization]] [[term]]s make the [[optimization]] [[non-convex]], [[we]] propose simple [[sequential]] [[gradient descent optimization algorithms]], and obtain impressively [[improved]] [[results]] on [[synthetic]], [[benchmark]] and [[real world tasks]] over [[supervised]] [[boosting algorithm]]s which use the [[labeled data]] alone and a [[state-of-the-art]] [[semi-supervised]] [[boosting algorithm]] .
Even when the [[data]] satisfies the [[low-rank assumption]], [[classical]] [[matrix-completion method]]s may [[output completion]]s with [[significant error]] -- in that the [[reconstructed matrix]] differs significantly from the [[true underlying matrix]] .
Even when there are no sufficient [[measurement]]s, [[our algorithm]] can still reliably [[recover]] a [[significant portion]] of the [[nonzero coordinate]]s.
Every day, [[huge volumes]] of [[sensory]], [[transactional]], and [[web data]] are continuously generated as [[stream]]s, which need to be [[analyzed online as they arrive]] .
Every [[economics textbook]] says the same thing: [[Money]] was [[invented]] to replace onerous and complicated [[barter system]]s — to relieve [[ancient people]] from having to haul their [[goods]] to [[market]] .
Everyone knows that [[high IQ]] is no [[guarantee]] of [[success]], [[happiness]], or [[virtue]], but until [[Emotional Intelligence]], [[we]] could only guess why.
Everything about [[behavior]] on the [[internet]] can be [[quantified]] and [[responses]] to [[behavior]] can occur in [[real time]] .
[[Evidence]] related to reporting delays and [[seasonal pattern]]s suggests that some [[CEO]]s [[backdate]] [[stock gift]]s to their own [[family foundation]]s in order to increase [[personal tax benefit]]s.
[[Evidence]] shows that [[our models]] can work for both [[English]] and [[Chinese]] .
[[Evolutionari]]es [[simulate]] [[evolution]] on the [[computer]] and [[draw on genetic]]s and [[evolutionary biology]] .
[[Evolutionary theory]] postulates that [[altruistic behavior]] evolved for the [[return-benefit]]s it bears the [[performer]] .
[[Evolution]], [[Social Network]], [[Affiliation Network]], [[Graph Generator]], [[Group]]s
[[Evolution strategies (ES)]] can rival [[backprop-based algorithm]]s such as [[Q-learning]] and [[policy gradients]] on challenging [[deep reinforcement learning (RL) problem]]s.
Examining human need by category — [[water]], [[food]], [[energy]], [[healthcare]], [[education]], [[freedom]] - [[Diamandis and Kotler]] introduce dozens of innovators making great strides in each area: [[Larry Page]], [[Steven Hawking]], [[Dean Kamen]], [[Daniel Kahneman]], [[Elon Musk]], [[Bill Joy]], [[Stewart Brand]], [[Jeff Skoll]], [[Ray Kurzweil]], [[Ratan Tata]], [[Craig Venter]], among many, many others.
[[Example result]]s and [[interpretation]]s are presented for two [[real dataset]]s, demonstrating promising [[performance]], with comparison to other [[state-of-the-art method]]s.
Examples are [[movie review]]s ([[Pang and Lee, 2005]]), [[opinion]]s ([[Wiebe et al., 2005]]), [[customer review]]s ([[Ding et al., 2008]]) or multiple aspects of [[restaurant]]s ([[Snyder and Barzilay, 2007]]).
Examples for [[word embedding]]s are [[SENNA]] ([[Collobert and Weston, 2008]]), the [[hierarchical log-bilinear model]] ([[Mnih and Hinton, 2009]]), [[word2vec]] ([[Mikolov et al., 2013c]]) and [[GloVe]] ([[Pennington et al., 2014]]).
Examples include [[Descartes's coordinate]]s, which [[link]]s [[geometry]] to [[algebra]], [[Planck]]'s [[Quantum Theory]], which [[link]]s [[particle]]s to [[wave]]s, and [[Shannon]]'s [[Information Theory]], which [[link]]s [[thermodynamic]]s to [[communication]] .
Examples include [[insider threat detection across multiple organization]]s, [[web image classification]] in different [[domain]]s, etc. [[Existing method]]s for addressing such problems typically assume that [[multiple task]]s are equally related and [[multiple view]]s are equally [[consistent]], which limits their [[application]] in [[complex setting]]s with varying [[task relatedness]] and [[view consistency]] .
Examples include [[sensor network]]s, [[web log]]s, and [[computer network traffic]] .
Examples include [[social networks]] (2–4) such as [[acquaintance network]]s (5) and [[collaboration network]]s (6), [[technological network]]s such as [[the Internet]] (7), [[the Worldwide Web]] (8, 9), and [[power grid]]s (4, 5), and [[biological network]]s such as [[neural network]]s (4), [[food web]]s (10), and [[metabolic network]]s (11, 12).
Examples include the [[predicate]]s ([[subject]], [[verb]], object) in [[knowledge base]]s, [[hyperlink]]s and [[anchor text]]s in the [[Web graph]]s, [[sensor stream]]s ([[time]], [[location]], and [[type]]), [[social network]]s [[over time]], and [[DBLP conference-author-keyword relation]]s.
[[Examples]] include [[web data]] or [[hypertext documents connected via hyperlinks]], [[social networks]] or [[user profiles]] connected via [[friend links]], [[co-authorship]] and [[citation information]], [[blog data]], [[movie reviews]] and so on.
Examples of [[business transaction]]s are [[buying or selling goods]], [[renting a business]], [[paying employees]], and [[buying insurance]] .
Examples of [[feature extraction method]]s include [[Principle Component Analysis (PCA) (Jolliffe, 2002]]), [[Linear Discriminant Analysis (LDA)]] ([[Scholkopft and Mullert, 1999]]), [[Canonical Correlation Analysis (CCA)]] ([[Hardoon et al., 2004]]), [[Singular Value Decomposition]] ([[Golub and Van Loan, 2012]]), [[ISOMAP]] ([[Tenenbaum et al., 2000]]) and [[Locally Linear Embedding (LLE)]] ([[Roweis and Saul, 2000]]).
Examples of [[heterogeneous networks]] include those in [[medical domains]] describing [[patients]], [[diseases]], [[treatments]] and [[contacts]], or in [[bibliographic domains]] describing [[publications]], [[authors]], and [[venues]] .
Examples of [[homogeneous networks]] include [[single mode social networks]], such as [[people]] connected by [[friendship links]], or [[the WWW]], a collection of [[linked]] [[web pages]] .
Examples of [[sequence labeling problem]]s include [[labeling]] [[words]] in [[sentence]]s with its [[type]] in [[named-entity recognition problem]]s [16], [[handwriting recognition problem]]s [15], and deciding whether each [[DNA base]] in a [[DNA sequence]] is part of a [[gene]] in [[gene prediction problem]]s [2].
Examples of such applications include [[demand forecasting]], [[assortment optimization]], [[product recommendation]]s, and [[assortment comparison]] across [[retailer]]s and [[manufacturer]]s.
Examples of such collections include [[scientific publication]]s, [[enterprise log]]s, [[news article]]s, [[social media]], and [[general web page]]s.
Examples of such [[communiti]]es include [[people]] who like the same [[object]]s on [[Facebook]], follow common [[subject]]s on [[Twitter]], or [[join similar group]]s on [[LinkedIn]] .
Examples of such [[segment level features]] for [[extraction tasks]] are: the whole [[entity]] has an exact match in a [[database of entities]], the [[length]] of the [[entity]] is between 4 and 8 [[words]], the third or fourth [[token]] of the [[entity]] is a “-”, and the last three [[tokens]] in the [[entity]] are [[digits]] .
"[[Executive]]" means [[officer]]s, [[managing partner]]s, or any other [[employee]]s in [[management position]]s.<br>
"[[Executive]]" means [[officer]]s, [[managing partner]]s, or any other [[employee]]s in [[management]] positions.<br>
[[Exemplar outcomes]] of [[this shift]] are a wide range of [[graph data]] such as [[information]], [[social]], and [[knowledge graph]]s.
[[Existing]] [[AI]] and [[robotics component]]s can provide [[physical platform]]s, [[perception]], [[motor control]], [[navigation]], [[mapping]], [[tactical decision-making]] and [[long-term planning]] .
[[Existing algorithm]]s for [[cost-sensitive classification]] are successful in [[term]]s of [[minimizing the cost]], but can result in a [[high error rate]] as the [[trade-off]] .
[[Existing algorithm]]s have high [[computational complexity]] and [[do not scale]] to [[large-size problems]] .
[[Existing approache]]s [[learn]] the [[diffusion model]]s from [[event]]s in [[social network]]s.
[[Existing approach]]es to [[opinion spam]] have successfully but separately utilized [[linguistic clues of deception]], [[behavioral footprint]]s, or [[relational ti]]es between [[agent]]s in a [[review system]] .
Existing [[approaches]] to [[outlier detection]] suffer from one or more of the following drawbacks : The results of many [[methods]] strongly depend on suitable [[parameter]] [[settings]] being very difficult to [[estimate]] without [[background knowledge]] on the [[data]], e.g. the [[minimum]] [[cluster size]] or the number of desired [[outliers]] .
[[Existing approach]]es typically [[recommend article]]s to [[optimize]] for a [[single objective]], i.e., number of [[click]]s.
[[Existing approaches]] usually focus on either the burstiness of [[topic]]s or the [[evolution of networks]], but ignoring the [[interplay]] between [[textual topics]] and [[network structures]] .
Existing commercial [[tools]] usually employ lots of [[hand-craft]], [[domain-specific]] [[rules]] and [[reference data dictionary]] of [[cities]], [[states]] etc.
[[Existing contextualized topic model]]s rely on [[arbitrary manipulation]] of the [[model structure]], by incorporating various [[context variable]]s into the [[generative process]] of [[classical topic model]]s in an [[ad hoc]] manner.
Existing [[graph partitioning heuristic]]s incur high [[computation]] and [[communication cost]] on [[large graph]]s, sometimes as high as the future [[computation]] itself.
[[Existing]] group [[anomaly detection approach]]es rely on the [[assumption]] that the [[group]]s are known, which can hardly be [[true]] in [[real world]] [[social media application]]s.
[[Existing]] [[method]]s are mainly variants of [[lifted variable elimination]] and [[belief propagation]], neither of which take [[logical structure]] into account.
[[Existing method]]s employ a [[pull model]] or [[user-initiated model]], where a [[user issue]]s a [[query]] to a [[server]] which replies with [[location-aware answer]]s.
[[Existing method]]s for [[clustering]] [[uncertain data]] compute a [[single clustering]] without any [[indication]] of its [[quality]] and [[reliability]]; thus, [[decision]]s based on their results are [[questionable]] .
[[Existing methods]], however, are only able to [[train]] [[ranking models]] by minimizing [[loss functions]] loosely related to the [[performance measures]] .
[[Existing method]]s simultaneously [[factorize]] these [[matrice]]s by sharing a single [[set of factor]]s for [[entiti]]es across all [[context]]s.
[[Existing phenotyping approach]]es typically require [[labor]] intensive [[supervision]] from [[medical expert]]s.
Existing [[programming paradigm]]s for expressing [[large-scale]] [[parallelism]] such as [[MapReduce (MR)]] and the [[Message Passing Interface (MPI)]] have been the [[de facto]] choices for implementing these [[ML-DM algorithm]]s.
Existing [[relatedness measures]] perform better using Wikipedia than a [[baseline]] given by [[Google counts]], and [[we]] show that [[Wikipedia]] [[outperform]]s [[WordNet]] when applied to the largest available [[dataset]] designed for that purpose.
Existing [[research effort]]s on [[disaggregating consumption]] focus on [[analyzing]] [[consumption feature]]s with [[high sample rate]]s (mainly between 1 [[Hz]] ~ 1[[MHz]]).
Existing research on [[this problem]] either perturbs the [[data]], [[publish]]es them in [[disjoint grou]]ps [[disassociated]] from their [[sensitive label]]s, or generalizes their [[value]]s by assuming the [[availability]] of a [[generalization hierarchy]] .
Existing [[sensor selection]] and [[prediction model]]s either select a [[set of sensor]]s <i>[[a priori]] </i>, or they use [[adaptive algorithm]]s to determine the most relevant [[sensor]]s for [[prediction]] .
[[Existing solution]]s for [[learning to rank]] and [[domain adaptation]] either leave the [[heavy burden]] of [[adapting keyword contribution]] to [[feature designer]]s, or are difficult to be [[generalized]] .
[[Existing technique]]s for [[privacy-preserving GWAS]] focus on [[answering specific question]]s, such as [[correlation]]s between a given [[pair of SNP]]s ([[DNA sequence variation]]s).
[[Existing work]] in [[outlier detection]] regards being an [[outlier]] as a [[binary property]] .
[[Existing work]] show [[prospect]]s of [[modeling content]]s and [[social link]]s, aiming at [[discovering social communiti]]es, whose [[definition]] varies by [[application]] .
Existing work solves [[this problem]] by [[simultaneously estimating]] [[sources' reliability]] and [[inferring]] [[question]]s' [[true answer]]s (i.e., the [[truth]]s).
[[Expenditure Classification Handbook]]: A [[manual published]] by the [[Comptroller]] which sets forth the [[official]] [[object class]] and [[object code]]s used for [[budgeting fund]]s and [[recording expense]]s within [[account]]s, along with [[definition]]s.
[[Experiment 2]] then [[identifies distinct]], but [[neighboring]], [[subregion]]s of [[lmSTC]] whose [[activity pattern]]s [[carry information]] about the [[identity]] of the current “[[agent]]” (“Who did it?” ) and the current “[[patient]]” (“To whom was it done?” ).
[[Experimental data]] show that [[pSkip]] can [[measure]] aspects of the [[search]] [[quality]] that these existing [[metric]]s are not designed or fail to address, such as [[identifying]] the [[real search intent]]s expressed in the [[ambiguous queries]] .
[[experimental evaluation]] on [[real datasets]] demonstrates the [[efficiency]] of [[the proposed algorithm]]s and the [[quality of the solutions]] [[we]] obtain.
[[Experimental evidence]] show that [[our algorithm]]s [[outperform]] a [[baseline algorithm]] both in the task of [[reconstructing]] a [[ground-truth clustering]] and in terms of [[objective function value]] .
[[Experimentally]], [[we]] evaluate [[our system]] on the [[Amazon product catalog]], a [[large dataset]] consisting of 9 million [[product]]s, 237 million [[link]]s, and 144 million [[review]]s.
[[experimental results]] and [[detailed analysis]] reveal several important [[findings]] of [[EE behaviors]] in [[online advertising]] and demonstrate that [[our algorithm]]s perform superiorly in terms of [[ad reach]] and [[click-through-rate (CTR)]] .
[[experimental results]] and initial [[user feedback]] suggest that [[TIARA]] is [[effective]] in aiding [[users]] in their [[exploratory text analytic tasks]] .
[[Experimental result]]s clearly demonstrate that [[our proposed method]] outperforms the [[state-of-the-art]] [[method]]s.
[[Experimental results]] demonstrate that our [[methods]] can perform well even if the [[number]] of [[samples]] is [[small]] .
[[Experimental result]]s demonstrate that the improvement in [[our proposed approach]] is [[consistent]] and [[promising]] .
[[Experimental result]]s demonstrate that the [[LAA algorithm]] significantly [[outperform]]s [[existing algorithm]]s.
[[Experimental results]] demonstrate that the [[proposed method]] outperforms both [[traditional]] [[inductive classifiers]] and the [[state-of-the-art]] [[boosting-based]] [[transfer algorithm]]s on most [[domains]], including [[text categorization]] and [[web page]] [[ratings]] .
[[Experimental result]]s demonstrate that they achieve [[satisfactory performance]] and always [[outperform]] the [[baseline method]]s.
[[Experimental result]]s have also shown that [[our algorithm]] is [[computationally efficient]], and effective for [[identification]] of interesting and explainable [[co-ranking result]]s.
[[Experimental result]]s indicate [[that the presented algorithm]] outperforms [[more naive approaches to ordinal regression]] such as [[support vector classification]] and [[support vector regression]] in the [[case of more]] than two [[rank]]s.
[[Experimental result]]s indicate that [[the presented algorithm]] outperforms more [[naive approach]]es to [[ordinal regression]] such as [[support vector classification]] and [[support vector regression]] in the case of more than two [[rank]]s.
[[Experimental results]] obtained from (1) [[historical logs]] and (2) [[live trials]] on a [[large-scale]] [[advertising platform]] demonstrate the [[effectiveness]] of the proposed [[algorithm]] and the overall [[success]] of [[our approach]] in [[identifying]] [[high-quality]] [[broad match]] [[mappings]] .
[[Experimental results]] on 26 [[UCI data sets]] show that [[subensembles]] formed by the proposed [[EPIC (Ensemble Pruning via Individual Contribution ordering) algorithm]] [[outperform]] the [[original]] [[ensemble]] and a [[state-of-the-art]] [[ensemble pruning method]], [[Orientation Ordering (OO)]] .
[[Experimental result]]s on [[benchmark dataset]]s show that [[the proposed method]] achieves a [[correlation coefficient]] of 0.66 with [[human judgment]]s.
[[Experimental result]]s on both [[synthetic]] and [[microblogging dataset]]s demonstrate the superior [[performance]] of [[our approach]] over the [[state-of-the-art]] [[method]]s in [[inferring]] [[multi-aspect diffusion network]]s.
[[Experimental result]]s on both [[synthetic]] and [[real dataset]]s show that [[the proposed approach]] is highly [[effective]] in [[discovering interesting]] [[evolutionary community outlier]]s.
[[Experimental result]]s -- on both [[synthetic]] and [[real life data]] -- show that using more [[information]] than [[just timing]] leads to [[greater accuracy]] in the [[inferred network]]s.
[[Experimental result]]s on both [[synthetic data]] and [[real data]] from [[cargo price bidding]] demonstrate the [[effectiveness]] and [[efficiency]] of [[the proposed algorithm]] .
[[Experimental results]] on [[budding]] [[yeast]] [[cell cycle]] [[models]] demonstrate [[compelling results]] comparable to [[human interpretations]] of the [[cell cycle]] .
[[Experimental result]]s on [[data]] from a major [[Asian stock market]] show that [[the proposed framework]] outperforms the [[CHMM-based analysis]] in terms of [[detecting abnormal]] [[collaborative market manipulation]]s.
[[Experimental result]]s on [[document classification]] and a novel [[application]] - [[Insider Threat Detection (ITD)]], clearly demonstrate the superior [[performance]] of [[the proposed method]] over [[state-of-the-art]] [[MIL method]]s.
[[Experimental results]] on four [[benchmark datasets]] show that [[AdaRank]] significantly outperforms the [[baseline methods]] of [[BM25]], [[Ranking SVM]], and [[RankBoost]] .
[[Experimental results]] on [[gene]] and [[stock-price]] [[prediction tasks]] show that the [[constrained formulation]] is able to significantly improve the generalization ability of [[CRF]] [[training]] .
[[Experimental results]] on [[large-scale]] [[English]] and [[Chinese Language]] [[corpus]] show that the proposed [[method]] significantly enhances the [[performance]] of [[standardization]] with less [[efforts]] and [[training data]] .
[[Experimental result]]s on many [[benchmark data set]]s demonstrate that [[the proposed method]] [[outperform]]s many [[state of the art]] [[unsupervised feature selection method]]s.
[[Experimental result]]s on multiple [[real data set]]s demonstrate the [[effectiveness]] of [[the proposed algorithm]] .
[[Experimental result]]s on [[real]] and [[synthetic dataset]]s show that [[UP-Span]] has [[excellent performance]] and serves as an [[effective solution]] to the new [[problem of mining high utility episode]]s from [[complex event sequence]]s.
[[Experimental results]] on some commonly used [[transfer learning applications]] demonstrate the [[effectiveness]] of [[our method]] .
[[Experimental results]] on [[synthetic]] and [[real-world]] [[data stream]]s demonstrate that the proposed [[VOCL framework]] significantly [[outperform]]s other [[methods]] for [[vaguely labeled]] [[one-class]] [[data streams]] .
[[Experimental result]]s on [[text]] and [[image data]] support our [[analysis]], which also suggest the [[potential value]] of [[twice normalized graph Laplacian]]s in practice.
[[Experimental results]] on three [[clinically relevant]] [[problem]]s show the effectiveness of [[our proposed approach]] in achieving the [[desired tradeoff]] between [[accuracy]] and [[feature acquisition]] [[cost]] .
[[Experimental result]]s on two [[online course dataset]]s demonstrate that [[SPARFA-Trace]] is capable of tracing each [[learner]]'s [[concept knowledge evolution]] [[over time]], [[analyz]]ing the [[quality]] and [[content organization]] of [[learning resource]]s, and [[estimating the question]] -- [[concept association]]s and the [[question difficulti]]es.
[[Experimental result]]s on two [[real-world dataset]]s show that [[FaitCrowd]] can [[significantly]] reduce the [[error rate]] of [[aggregation]] compared with the [[state-of-the-art]] [[multi-source aggregation approach]]es due to its ability of [[learning topical expertise]] from [[question content]] and [[collected answer]]s.
"[[Experimental result]]s on two [[real-world dataset]]s show that [[the proposed model]] is effective in [[discovering users' spatial-temporal topic]]s, and [[outperform]]s [[state-of-the-art]] [[baseline]]s significantly for the [[task]] of [[location prediction]] for [[tweet]]s.
[[Experimental result]]s on two [[text benchmark dataset]]s ([[20newsgroup]]s and [[RCV1]]) show that incorporating [[world knowledge]] as [[indirect supervision]] can significantly [[outperform]] the [[state-of-the-art]] [[clustering algorithm]]s as well as [[clustering algorithm]]s enhanced with [[world knowledge feature]]s.
[[Experimental result]]s on various [[real-world data set]]s demonstrate that [[SEC]] is an [[effective]] and [[efficient competitor]] to some [[state-of-the-art]] [[ensemble clustering method]]s and is also suitable for [[big data clustering]] .
[[Experimental result]]s reveal that [[investor composition]] can help us evaluate the [[profit potential]] of an [[investment]] and the [[decision model]] based on [[investor composition]] can help [[investor]]s make better [[investment decision]]s.
[[Experimental result]]s showed nice [[properties of MSCMF]] on [[selecting similarities useful]] in improving the [[predictive performance]] and the [[performance advantage]] of [[MSCMF]] over six [[state-of-the-art method]]s for [[predicting drug-target interaction]]s.
[[Experimental result]]s showed that [[feature representation]]s computed by [[deep model]]s based on [[transfer]] and [[multi-task learning]] significantly [[outperformed]] other [[method]]s for [[annotating]] [[gene expression pattern]]s at different [[stage range]]s.
[[Experimental result]]s showed that [[the proposed algorithm]] produces [[classification accuracy]] comparable to its [[batch counterpart]], while [[consuming]] significantly less [[computational resource]]s.
[[Experimental results]] show effective [[improvements]] over the [[supervised baselines]] on all [[tasks]] .
[[Experimental results]] show promising [[performance]] of [[Coverage]], [[Bandwidth]] [[utilization]], and [[Timeliness]] of our [[crawler]] on 18 various [[forums]] .
[[Experimental result]]s show that: (1) [[OptRank]] outperforms the [[existing graph]] based [[method]]s when only ([[user]], [[tag]], item) relation is available.
[[Experimental result]]s show that [[CF-NADE]] with a single [[hidden layer]] beats all previous [[state-of-the-art method]]s on [[MovieLens 1M]], [[MovieLens 10M]], and [[Netflix dataset]]s, and adding more [[hidden layer]]s can further improve the [[performance]] .
[[Experimental result]]s show that: (<i> i </i>) With 360K [[page]]s from 6 major [[travel website]]s, we obtain 84K [[matching]]s (of 179K [[page]]s) that refer to the same [[entiti]]es, with an [[average precision]] of 0.826; (<i> ii </i>) The [[quality of matching]] obtained from a [[classifier]] [[train]]ed on the resulted [[seed data]] is promising: the [[performance match]]es that of [[editorial data]] at [[small size]] and improves with [[size]] .
[[Experimental result]]s show that [[MLBE]] compares favorably with two [[state-of-the-art]] [[model]]s.
[[Experimental result]]s show that [[our proposed method]] outperforms the [[state-of-the-art]] [[approach]]es significantly: [[boosting]] the [[disambiguation accuracy]] from 43% ([[baseline]]) to 86% on [[short queri]]es derived from [[tweet]]s.
[[Experimental result]]s show that [[SCRAM]] achieves better [[recommendation fairness]] and higher [[driving efficiency]] than [[three compared approach]]es.
[[Experimental result]]s show that some [[VOC]]s and some [[label]]s can be predicted with [[relatively low error]], and that [[hint]] for [[causality]] with [[low p-value]]s can be detected in the [[data]] .
[[Experimental result]]s show that [[the proposed algorithm]] gives substantially better [[result]]s than [[prior approach]]es, especially for [[high levels of privacy]] .
[[Experimental result]]s show that [[the proposed algorithm]] is able to [[output]] [[frequent pattern]]s with good [[precision]] .
[[Experimental results]] show that the [[proposed approach]] infer [[advisor-advisee relationships]] [[efficiently]] and achieves a [[state-of-the-art]] [[accuracy]] (80-90%).
[[Experimental result]]s show that [[the proposed method]] is effective in [[detecting]] [[singleton review attack]]s.
[[Experimental results]] show that [[the proposed model]] has better [[generalization performance]] or [[tag prediction]] [[ability]] than other two [[models]] proposed in [[previous research]] .
[[Experimental results]] show that the [[representation]] based on [[sparse learning]] outperforms the [[bag-of-words representation]] significantly.
[[experimental results]] show that [[UP-Growth]] not only [[reduce]]s the [[number of candidates]] [[effectively]] but also [[outperform]]s other [[algorithm]]s substantially in terms of [[execution time]], especially when the [[database]] contains lots of [[long transactions]] .
[[Experimental results]] show that using 0.3% of [[vehicles]] as the [[sample]]s, [[mobility-based clustering]] can accurately identify [[hot spots]] which can hardly be obtained by the latest [[representative algorithm]] [[UMicro]] .
[[Experimental result]]s using 28 [[topic]]s show that [[the proposed technique]]s are highly [[effective]] .
[[Experimental results]] using a combination of [[synthetic]] and [[real]] [[tropical cyclone event]] [[data sequence]]s are presented to demonstrate the feasibility of [[our parameter learning approach]] and its [[robustness]] to [[variability]] in the [[instance constraints]] .
[[Experimental result]]s using a [[Texas inpatient dataset]] show that [[individual-level data]] can be [[reasonably reconstructed]] from [[county]] -, [[hospital]] -, and zip [[code-level aggregate data]] .
[[Experimental result]]s using [[review document]]s from [[100]] [[product domain]]s show that [[the proposed approach]] makes dramatic [[improvement]]s over [[state-of-the-art]] [[baseline]]s.
[[Experimental result]]s verify the [[usefulness]] of [[online active learning]], especially in the [[non-stationary situation]] with [[concept drift]] .
[[Experimental studi]]es on three [[real-world data set]]s demonstrate that our [[MVMT method]]s significantly [[outperform]] the existing [[state-of-the-art method]]s.
[[Experimental validation]] of [[MLBE]] has been conducted using both [[synthetic data]] and two [[realistic data set]]s.
[[Experimentation]] on [[our method]] demonstrates the [[measure]] [[works in practice]] and [[results]] in [[interpretable]] and [[insightful itemsets]] for both [[synthetic]] and [[real-world data]] .
[[Experimentation]] on [[synthetic]] and [[real data]] demonstrates we [[efficiently]] [[discover]] [[small set]]s of [[informative pattern]]s.
[[Experiment result]]s demonstrate that [[Comet]] outperforms the [[state-of-the-art technique]]s and is able to [[build complete]] and [[accurate profile]]s for [[real world entiti]]es.
[[Experiment result]]s, including comparisons to other [[state of the art technique]]s on different [[data set]]s and under different [[parameter setting]]s, confirm that [[LWI-SVD]] and [[LWI2-SVD]] are both [[efficient]] and [[accurate]] in [[maintaining decomposition]]s.
Experiment results on both [[synthetic dataset]]s and two [[large-scale]] [[text collection]]s show that [[our algorithm]] can achieve considerable [[speedup]] as well as better [[inference accuracy]] for [[HDP]] compared with existing [[parallel sampling algorithm]]s.
[[Experiment result]]s on several [[social communication dataset]]s, including [[email]]s and [[Twitter message]]s, demonstrate that [[the model]] can [[discover]] [[users' communiti]]es effectively, and provide concrete [[semantics]] .
[[Experiment]] results on two different [[review data set]]s demonstrate that [[the proposed model]] can effectively perform the <i>[[Latent Aspect Rating Analysis]]</i> [[task]] without the [[supervision]] of [[aspect keyword]]s.
[[Experiment results]] show that the [[exploration]] of [[social tags]] effectively boosts [[web object classification]] .
[[Experiment result]]s show that [[the proposed mixture model]] in general prominently [[outperform]]s [[linear regression]] in terms of the [[prediction accuracy]] .
[[Experiments]] and [[case studies]] on [[trace]]s of several [[benchmark software system]]s and a [[real-life]] [[concurrency bug]] from [[MySQL]] [[server]] show the [[utility]] of the [[technique]] in [[capturing]] [[failures]] and [[anomalies]] .
[[Experiments]] are conducted on [[20 Newsgroups data]], [[Cora research papers]], [[DBLP author-conference network]], and [[Yahoo! Movies datasets]], and the [[results]] show that the [[proposed method]] [[improves]] the [[classification accuracy]] and the [[clustering quality measure]] ([[NMI]]) over the best base [[model]] by up to [[10%]] .
[[Experiment]]s are performed on both [[artificial]] and [[real data set]]s.
[[Experiment]]s are shown using [[multinomial model]]s for [[text]], [[hidden Markov model]]s for [[biological data set]]s and [[linear dynamical system]]s for [[time series data]] .
[[Experiment]]s based on [[perplexity score]]s for [[test document]]s and [[precision-recall]] for [[document retrieval]] are used to illustrate systematic differences between the proposed [[author-topic model]] and a number of [[alternative]]s.
[[Experiment]]s based on such [[ground-truth data]] show that [[our proposed model]] significantly [[outperform]]s [[existing method]]s that [[estimate]] [[social influence]] using [[biased social data]] .
[[Experiments]] conducted on [[real data]] show our [[methods]] are effective at [[discovering]] the [[organizational structure]] and [[representing]] the [[evolution]] of [[organizational structure]] in a [[dynamic]] [[social network]] .
[[Experiment]]s demonstrate [[computational complexity]] that is [[orders of magnitude]] lower than [[state-of-the-art]], and [[competitive result]]s on [[benchmark data]] and [[real churn prediction data]] .
[[Experiment]]s demonstrate that [[predictive]] [[client-side personalization]] allows [[ad platform]]s to retain almost all of the [[revenue gain]]s from [[personalization]] even if they give [[user]]s the freedom to opt out of [[behavior tracking]] backed by [[server-side storage]] .
[[Experiment]]s demonstrate that the improved [[GLMNET]] is more [[efficient]] than a [[state-of-the-art]] [[coordinate descent method]] .
[[Experiment]]s demonstrate that these [[procedure]]s can improve the [[accuracy]] of [[pattern-based model]]s similar to [[frequent set]]s and often also lead to substantial [[gain]]s in terms of [[scalability]] .
[[experiments]] demonstrate that [[very large CRFs]] can be [[trained]] [[efficiently]] and that [[very large models]] are able to [[improve]] the [[accuracy]], while delivering [[compact]] [[parameter]] [[sets]] .
[[Experiment]]s demonstrate that with suitable [[implementation]]s of [[approximate optimization]], the resulting [[algorithm]] can [[outperform]] [[standard]] [[SGD]] in many [[scenario]]s.
[[Experiment]]s demonstrate the potential of [[the framework]] through [[qualitative]] and [[quantitatively evaluation]] of the [[generated sample]]s.
[[experiments]] have shown that [[proposed model]] is capable of [[predicting]] both [[trust]] and [[distrust]] in a unified way.
[[Experiment]]s in both [[sufficient]] and [[limited memory condition]]s show that [[the proposed approach]] [[learn]]s [[many times faster]] than other [[state-of-the-art]] [[solver]]s without sacrificing [[accuracy]] .
[[Experiment]]s indicate that [[our algorithm]] for [[minimizing]] the [[KL-divergence]] is faster than the [[Lee & Seung multiplicative rule]] by a [[factor]] of 10 on the [[CBCL image dataset]] .
Experiments in multiple domains show that [[the new model]] can substantially improve [[extraction]] [[performance]] over previous [[methods]] for using [[external dictionaries]] in [[NER]] .
[[Experiments]] of [[clustering]] on many [[benchmark data sets]] demonstrate that the propose [[method]] outperforms many [[state of the art]] [[clustering methods]] .
[[experiments]] on a [[biological dataset]] show that the [[ROPSM model]] better [[captures the characteristics]] of [[noise]] in [[gene expression data matrix]] compared to the [[AOPC model]] .
[[Experiments]] on a [[biomedical corpus]] show that this [[approach]] is able to successfully [[translate]] [[syntactic variations]] into a [[logical representation]] of their [[common meaning]] (e.g., USP learns to map [[active]] and [[passive voice]] to the same [[logical form]], etc.). This in turn allows it to correctly answer many more [[questions]] than [[system]]s based on [[TextRunner]] ([[Banko et al., 2007]]) and [[DIRT]] ([[Lin and Pantel, 2001]]).
[[Experiments]] on a [[dataset]] of two million [[tuples]] show that it [[outperform]]s three other [[relational clustering approaches]], and [[extracts]] [[meaningful]] [[semantic networks]] .
[[Experiment]]s on a [[larger database]] indicate that [[MetaCost]] [[scales well]] .
[[Experiments]] on a [[large scale]] [[click-through]] [[data]] containing over 1.5 billion [[query]]-[[URL]] [[pairs]] show that the [[proposed approach]] can conduct very [[accurate]] [[NEM]] and significantly [[outperform]]s the [[baseline]] .
[[Experiments]] on a [[publicly available dataset]] containing [[clinical]] [[free text]] and their associated [[medical codes]] showed that [[our proposed multi-label classifier]] [[outperform]]s [[related]] [[multi-label models]] in [[this problem]] .
[[Experiment]]s on a [[real-life Amazon review dataset]] demonstrate the effectiveness of [[the proposed model]]s which significantly [[outperform]] the [[state-of-the-art]] [[competitor]]s.
[[Experiment]]s on [[artificial]] and [[real-world dataset]]s show that the [[proposed method]]s achieve higher [[accuracy]] than other [[loss function]]s used in [[prior work]], e.g., [[Hamming loss]], and [[recent work]] in [[ambiguous]] [[label classification]] .
[[Experiment]]s on [[benchmark]] and [[real-world]] [[Yahoo!]] [[dataset]]s clearly illustrate the usefulness of [[our approach]] .
[[Experiment]]s on [[benchmark data set]]s and [[real-world application]]s demonstrate the superior [[performance]] of [[our proposed method]] in [[comparison]] with the [[state-of-the-art method]]s.
[[Experiment]]s on [[benchmark]] [[graph dataset]]s show that [[OLLGC]] outperforms the [[state-of-the-art]] [[first-order algorithm]] significantly, and [[SSLGC]] achieves comparable or even better results than [[OLLGC]] while [[query]]ing substantially fewer [[node]]s.
[[Experiment]]s on both [[DBLP]] and [[PubMed dataset]]s demonstrate the power of [[the proposed approach]], with 17.68% improvement in [[Recall@50]] and 9.57% growth in [[MRR]] over the [[best performing]] [[baseline]] .
[[Experiments]] on both synthetic and [[real-world data]] [[sets]] show that an [[algorithm]] developed under this [[framework]] is effective at alleviating the [[problem]] of small [[sample size]] and leads to more [[stable]] [[feature]] selection results and comparable or better generalization [[performance]] than state-of-the-art [[feature]] selection [[algorithms]] .
[[Experiment]]s on both [[synthetic]] and [[real-world dataset]]s show the [[effectiveness]] of the [[TAT model]] .
[[Experiment]]s on both [[synthetic]] and [[real-world data stream]]s demonstrate the [[performance]] of [[our approach]] .
[[Experiment]]s on [[commonly-used face data set]]s demonstrate the [[effectiveness]] of the approach for [[face recognition]] through comparing with the leading [[state-of-the-art]] in the [[literature]] .
[[Experiment]]s on [[data]] from [[real subject]]s [[attest]] to the [[effectiveness]] of [[our approach]] .
[[Experiment]]s on [[document retrieval]] and [[clustering]] demonstrate that with [[diversification]], the [[document modeling power]] of [[DRBM]] can be greatly improved.
[[Experiment]]s on [[DUC'04 task]] show that [[our approach]] is superior to the [[best-performing method]] from the [[DUC'04 evaluation]] on [[ROUGE-1 score]]s.
[[Experiment]]s on [[huge graph]]s demonstrate the [[scalability]] of [[our algorithm]] and its applicability to [[data mining problem]]s.
[[Experiments]] on [[large]] [[co-authorship networks]] suggest that [[information]] about future [[interactions]] can be [[extracted]] from [[network topology]] alone, and that fairly [[subtle measures]] for [[detecting]] [[node proximity]] can [[outperform]] more [[direct measures]] .
[[Experiment]]s on [[large]] [[real-world network]]s show that [[social sampling]] is a powerful [[paradigm]] in obtaining [[accurate estimate]]s with very few [[sample]]s.
[[Experiment]]s on [[LETOR dataset]]s and two [[large dataset]]s, [[Yahoo challenge data]] and [[Microsoft 30K web data]], show an [[improvement]] over [[state-of-the-art]] [[system]]s.
[[Experiment]]s on [[real]] and [[synthetic dataset]]s show that [[our approach]] [[reduces communication]] by up to two [[orders of magnitude]] while providing an [[accurate estimate]] of the [[current]] [[global model]] in all [[node]]s.
[[Experiment]]s on [[real data]] from a [[commercial search engine]] show that [[the proposed algorithm]] can outperform [[previous algorithm]]s on several [[task]]s.
[[Experiment]]s on [[real social network]]s with different [[valuation distribution]]s demonstrate the [[effectiveness]] of [[PRUB]] and [[PRUB]] + [[IF]] .
[[Experiment]]s on [[real Twitter data]] demonstrate the effectiveness of [[our model]] at [[capturing rich]] and [[interpretable]] [[user trait]]s that can be used to provide [[transparency]] for [[personalization]] .
[[Experiment]]s on [[real-world dataset]]s demonstrate [[CSTM]]'s [[performance]] .
[[Experiment]]s on [[real world news data]] demonstrate that [[our algorithm]] can better incorporate [[historical tree information]] and is more [[efficient]] and [[effective]] than the traditional [[evolutionary hierarchical clustering algorithm]] .
[[Experiment]]s on [[robotic control]] and [[stock price prediction]] justify its appealing performance in challenging [[MTL problem]]s.
[[Experiment]]s on several [[real-world data]] [[sets]] (all are publicly available) demonstrate that [[our approach]] can obtain promising [[classification]] result with much fewer [[labeled data]] than [[state-of-the-art]] [[method]]s.
[[Experiment]]s on several [[real-world dataset]]s demonstrate that our proposed [[classifier]] boosts [[classification performance]] over common [[benchmark]]s on [[networked multi-label data]] .
[[Experiment]]s on [[simulated data]] show that [[FHIM]] outperforms the [[state-of-the-art]] [[sparse learning technique]]s.
[[Experiment]]s on [[synthetic]] and [[benchmark data]] show that the [[discover]]ed [[summari]]es are [[succinct]], and correctly identify the key [[pattern]]s in the [[data]] .
[[Experiment]]s on [[synthetic]] as well as [[real data]] show that our [[low rank model]] substantially improves [[accuracy]] of [[sign inference]] as well as [[clustering]] .
[[Experiment]]s on the [[benchmark]] and the [[real-world data set]]s show that [[our proposed methodology]] indeed achieves lower [[test error rate]]s and similar (sometimes lower) [[test cost]]s than existing [[cost-sensitive classification algorithm]]s.
[[Experiments]] on the [[DBLP]] [[dataset]] show that our [[framework]] works well in practice and gives useful and [[intuitive result]]s.
[[Experiment]]s on the [[Letor 3.0 benchmark dataset]] for [[information retrieval]] validate [[our approach]] .
[[Experiment]]s on the new [[task]] and previous [[data set]]s show significant improvement of our [[model]] over [[baseline]]s and other traditional [[latent variable model]]s.
[[Experiment]]s on three different [[application]]s confirm the effectiveness of [[T-RBM]], which achieves significant improvement compared with four [[state-of-the-art]] [[baseline method]]s.
[[Experiment]]s on three [[real-world dataset]]s illustrate the notable [[advantage]] of [[our method]] over existing [[state-of-the-art]] [[truth discovery method]]s.
[[Experiment]]s on three very [[large]] [[real-world dataset]]s show that [[our model]] [[outperform]]s current [[state-of-the-art method]]s for [[response prediction]] .
[[Experiment]]s on twelve [[language]]s show that [[stacking transition-based]] and [[graph-based parser]]s improves performance over existing [[state-of-the-art dependency parser]]s.
[[Experiment]]s on two [[synthetic dataset]]s, one [[social network dataset]] and one [[climatology dataset]] demonstrate the [[superior performance]] of [[our proposed model]]s.
[[Experiments]] over [[synthetic]] and [[real-world]] [[multiple]] [[correlated]] [[time-varying data sets]] illustrate [[the]] [[effectiveness]] of [[EvoHDP]] on [[discovering cluster evolution patterns]] .
[[Experiment]]s reveal that [[FastXML]] can be [[trained on problem]]s with more than a million [[label]]s on a [[standard desktop]] in [[eight hour]]s using a single [[core]] and in an hour using multiple [[core]]s.
[[Experiment]]s showed that [[our algorithm]] achieves significantly [[higher accuracy]] than the existing [[metric learning approach]] based on [[relative comparison]]s.
[[Experiments]] show that [[COA]] produces meaningful [[ranking]] when [[comparing]] it with other [[indirect]] [[patent]] [[evaluation metrics]] --- [[citation]] [[count]], [[patent]] [[status]], and [[attorney's]] [[rating]] .
[[Experiment]]s show that [[F-AP]] is much faster than [[previous approach]]es with no [[loss]] in [[clustering performance]] .
[[Experiment]]s show that [[FFMs]] are very useful for certain [[classification problem]]s.
[[Experiment]]s show that in some [[task]]s they are able to achieve better [[performance]] than [[learning]] the [[single-instance]]s or [[single-label example]]s directly.
[[Experiments]] show that [[our method]] significantly [[outperform]]s [[existing solutions]] .
[[Experiment]]s show that [[our proposed local]] and [[global feature]]s achieve better [[performance]] than using only keyphrases to [[construct the concept hierarchi]]es.
[[Experiment]]s show that [[SL]] and [[RL]] are complementary: [[SL]] alone can derive a reasonable initial [[policy]] from a small number of [[training dialog]]s; and starting [[RL optimization]] with a [[policy trained with SL]] substantially accelerates the [[learning rate of RL]] .
[[Experiment]]s show that [[StreamSVM]] outperforms other [[linear SVM solver]]s, including the [[award winning work]] of [38], by [[orders of magnitude]] and produces more [[accurate]] [[solution]]s within a [[shorter amount]] of [[time]] .
[[Experiment]]s show that [[the proposed algorithm]] [[outperform]]s a wide [[spectrum]] of [[state-of-the-art]] [[model combination method]]s on 11 [[task]]s.
[[Experiment]]s show that the [[SBM algorithm]] dramatically reduces the [[number of blocks loaded]] from [[disk]] and [[consequently]] obtains an [[accurate]] and [[stable model]] quickly on both [[binary]] and [[multi-class classification]] .
[[Experiments]] show that [[this approach]] can reduce 60% more [[risks]] than the standard [[cost-sensitive SVM]] which assumes the [[expected cost]] is the [[true value]] .
[[Experiment]]s show that [[unigram language model]]s smoothed using a [[normalized extension]] of [[stupid backoff]] and a simple [[queue]] for [[history retention]] performs well on the [[task]] .
[[Experiments]] show that when there are only [[cost intervals]] available, [[CISVM]] is significantly superior to standard [[cost-sensitive SVMs]] using any of the [[minimal cost]], [[mean cost]] and [[maximal cost]] to [[learn]] .
[[Experiment]]s suggest that [[DSGD]] [[converge]]s significantly [[faster]] and has better [[scalability properti]]es than [[alternative algorithm]]s.
[[Experiment]]s using a [[state-of-the-art]] [[D2W system]] support [[this claim]] .
[[Experiments]] using [[data sets]] 20 times [[larger than]] the [[memory]] demonstrate the effectiveness of the [[proposed method]] .
[[Experiment]]s using several general [[crowdsourcing task]]s show that [[our method]] [[outperform]]s popular [[vote aggregation method]]s, which implies that [[our method]] can deliver [[high quality result]]s with [[lower cost]]s.
[[Experiments with dataset]]s from three different [[domain]]s illustrate our ability to [[generate]] [[hierarchi]]es of [[high quality]] [[topic]]s [[represented by]] [[meaningful phrase]]s.
[[Experiment]]s with over 25 [[million Web table]]s and 350 diverse [[queri]]es show [[robust]], [[large benefit]]s from our [[quantity catalog]], [[unit extractor]], and [[collective inference]] .
[[Experiment]]s with [[real-life dataset]]s demonstrate the [[effectiveness]] of our [[MLN-based approach]] compared to existing [[state-of-the-art]] [[extraction method]]s.
[[Experiment]]s with [[real-world dataset]]s prove that [[the proposed method]] is both [[effective]] and [[flexible]] to handle [[arbitrary types of context]]s.
[[Explicitly]], given a [[location sequence]] and a [[time span]], the [[RICK]] is able to construct the [[top-k route]]s which [[sequentially pass]] through the [[location]]s within the [[specified time span]], by aggregating such [[uncertain trajectori]]es in a [[mutual reinforcement way]] (i.e., [[uncertain + uncertain]] â certain).
[[Exploring]], [[classify]]ing and [[extracting information]] from [[scholarly resource]]s is a [[complex challenge]] and [[interesting challenge]] .
Extensive [[comparisons]] with other [[benchmark methods]] show significant [[improvements]] in [[prediction accuracy]] .
Extensive [[computational comparison]]s using [[data set]]s that are from [[real life application]]s as well as those [[artificially]] [[generated show]] that the [[proposed algorithm]] provides [[state-of-the-art performance]] in terms of [[computational speed]] .
Extensive [[empirical evaluation]] on both [[synthetic]] and [[real data set]]s demonstrates the [[efficiency]] and effectiveness of [[our proposed algorithm]] in finding the [[temporal dependenci]]es with [[lag interval]]s in [[sequential data]] .
Extensive [[empirical evidence]] shows that [[the framework]] yields [[state-of-the-art]] [[classification accuraci]]es on several [[classification data set]]s.
Extensive [[empirical result]]s on both [[synthetic data]] and [[real-world benchmark data set]]s show that our new [[clustering method]]s consistently [[outperform]]s the related [[clustering approach]]es.
Extensive [[evaluation]] on [[real dataset]]s demonstrates that [[AEClass]] [[outperform]]s existing [[representative method]]s in terms of both [[effectiveness]] and [[efficiency]] .
Extensive [[experimental evaluation]]s demonstrate the [[effectiveness]] and the [[efficiency]] of [[our method]]s.
Extensive [[experimental evaluation]]s on [[Twitter data]] from four different [[countries]] in [[Latin America]] demonstrated the [[effectiveness]] of [[our proposed approach]] .
Extensive [[experimental result]]s demonstrate that by considering all the [[task information collectively]], [[TCM]] can better interpret [[user click behavior]] and achieve significant [[improvement]]s in terms of [[ranking metric]]s of [[NDCG]] and [[perplexity]] .
Extensive [[experimental result]]s demonstrate the superior [[performance]] of [[the proposed algorithm]] in [[comparison]] with current [[state-of-the-art method]]s.
Extensive [[experimental result]]s on [[face clustering]] and [[motion segmentation data]] demonstrate the [[effectiveness]] of [[the proposed method]] .
Extensive [[experimental results]] on several [[real]] and [[synthetic datasets]] demonstrate the [[effectiveness]] of [[our approach]] when compared to [[state-of-the-art algorithms]] .
Extensive [[experimental result]]s on [[synthetic]] and [[real-life dataset]]s show the [[effectiveness]] of [[our method]] .
Extensive [[experimental result]]s validate that [[MAHR]] is able to achieve superior [[performance]] and [[discover]] reasonable [[label relationship]] .
Extensive [[experimental result]]s with [[large]] [[real dataset]]s show that [[TurboGraph]] consistently and significantly [[outperform]]s [[Graph-Chi]] by up to <i>four [[orders of magnitude]]</i> !
Extensive [[experimentation]] demonstrates [[statistically significant improvement]] in terms of [[win]]s and [[rank]]s against 13 [[baseline]]s over 28 [[time-series dataset]]s.
Extensive [[experiment]]s are performed that demonstrate that [[the proposed method]] is able to obtain [[discriminative feature representation]]s yielding [[superior clustering performance]], and correctly [[recover]] the [[intrinsic structure]]s of various [[real-world dataset]]s including [[curves]], [[hierarchies]] and a [[cancer progression path]] .
Extensive [[experiment]]s based on [[online bucket test]]s ([[A/B experiment]]s) and [[offline evaluation]] illustrate the [[effect]] of our [[personalization model]]s in [[LinkedIn feed]] .
Extensive [[experiment]]s based on two [[real LBSN]] [[dataset]]s have demonstrated the superior [[effectiveness]] of [[our proposal]]s than existing [[static model]]s of [[propagation probabiliti]]es to truly reflect the [[information propagation]] in [[LBSN]]s.
Extensive [[experiment]]s conducted on [[real world dataset]]s indicate that [[the proposed framework]] [[outperform]]s [[state-of-art]] [[algorithm]]s for [[item recommendation]], [[user recommendation]] and [[tag recommendation]] .
Extensive [[experiment]]s conducted on [[real-world]] [[online]] [[ESN]] [[dataset]] demonstrate that [[Create]] can [[perform]] very well in addressing the [[IOC problem]] .
Extensive [[experiment]]s conducted on [[real-world partially aligned heterogeneous network]]s, [[Foursquare]] and [[Twitter]], demonstrate that [[Mli]] can solve the [[multi-network link prediction problem]] very well.
Extensive [[experiment]]s demonstrate the [[effectiveness]] and [[efficiency]] of [[our approach]] .
Extensive [[experiments]] on a [[broad range]] of [[data sets]] validate the [[effectiveness]] of [[our approach]] against other [[well-established methods]] .
Extensive [[experiment]]s on [[big dataset]]s demonstrate the [[feasibility]] and [[applicability]] of [[the proposed method]]s.
Extensive [[experiment]]s on [[large real data set]]s have demonstrated the [[superb scalability]] and [[efficiency]] achieved by our [[method]]s, when compared to the [[state-of-the-art method]]s.
Extensive [[experiment]]s on [[real data]] demonstrate that [[FUNNELFIT]] does indeed discover important [[properties of epidemic]]s: (P1) [[disease seasonality]], e.g., [[influenza spikes]] in [[January]], [[Lyme disease spike]]s in [[July]] and the absence of [[year]]ly [[periodicity]] for [[gonorrhea]]; (P2) [[disease reduction effect]], e.g., the [[appearance of vaccine]]s; (P3) [[local/state-level sensitivity]], e.g., many [[measles]] cases in [[NY]]; (P4) [[external shock event]]s, e.g., [[historical flu pandemic]]s; (P5) [[detect]] [[incongruous value]]s, i.e., [[data reporting error]]s.
Extensive [[experiment]]s on [[real dataset]]s demonstrate excellent [[accuracy]] in terms of [[prediction quality]] .
Extensive [[experiment]]s on [[real dataset]]s demonstrate that [[our solution]] substantially outperforms the [[state-of-the-art]] [[competitor]]s.
Extensive [[experiment]]s on [[real dataset]]s demonstrate that [[TriMine]] discovers meaningful [[topic]]s and makes [[long-range forecast]]s, which are notoriously difficult to [[achieve]] .
Extensive [[experiment]]s on [[real-life STM dataset]]s confirm that [[our approach]] maintains [[high utility]] and is [[scalable]] to [[large dataset]]s.
Extensive [[experiments]] on [[real-world data]] collected from the [[Digg]] [[social media]] [[website]] suggest that our [[technique]] is [[scalable]] and is able to [[extract]] meaningful [[communities]] based on the [[social media]] [[contexts]] .
Extensive [[experiment]]s on [[real-world data]] suggest that [[the proposed framework]] is able to provide a [[co-ranking scheme]] for [[object]]s and [[relation]]s successfully.
Extensive [[experiment]]s on [[synthetic]] and [[real data]] demonstrate that <i>[[SCMiner]]</i> outperforms [[state-of-the-art technique]]s for [[clustering]] and [[link prediction]] .
Extensive [[experiment]]s on [[synthetic]] and [[real data sets]] verify that [[the proposed methods]] are [[efficient]] and [[effective]] .
Extensive [[experiment]]s on [[synthetic]] and [[real world data]] demonstrate the [[effectiveness]] and [[efficiency]] of [[our approach]] .
Extensive [[experiment]]s on three [[real-world dataset]]s from different [[domain]]s show that [[CDL]] can significantly [[advance the state of the art]] .
Extensive [[experiment]]s on three [[real-world dataset]]s from different [[domain]]s show that [[CDL]] can significantly advance the [[state of the art]] .
Extensive [[experiment]]s reveal the [[effectiveness]] of the [[method]] .
Extensive [[experiment]]s show that [[our algorithm]] allows the [[effective]] and [[efficient community detection]] and has good [[performance]] compared to [[state-of-the-art algorithm]]s.
Extensive [[experiment]]s show that [[our algorithm]]s [[outperform]] the [[existing solution]] in terms of [[data distortion]] and [[side-effect]]s and are more [[efficient]] .
Extensive [[experiment]]s show that our proposed [[GIGATENSOR]] solves [[100]] times [[bigger problem]]s than existing [[method]]s.
Extensive [[experiments]], using [[real]] and [[synthetic datasets]], were conducted to validate the [[performance]] of [[our methods]] .
Extensive [[experiments]] using [[real datasets]] validate that our [[solution]] is reasonably [[efficient]] for completely [[mining FTFIs]] .
Extensive [[experiments]] with [[benchmark data sets]] show that the [[proposed framework]] significantly outperforms the [[state-of-the-art approaches]] for combining [[link]] and [[content analysis]] for [[community detection]] .
Extensive [[experiments]] with the [[widely-used]] [[LETOR dataset]] show large [[ranking]] [[accuracy]] [[improvements]] beyond [[recent]] and [[competitive algorithms]] .
Extensive [[simulations]] demonstrated that the [[<i>SOD</i> method]] based on the [[<i>GLS</i> model]] significantly [[outperformed]] all [[existing approaches]] when the [[spatial data]] exhibits a [[linear]] or [[nonlinear trend]] .
: [[extract]] from the [[document]]s [[instance]]s of the [[class]]es and [[relation]]s that are [[described]] in the [[document]]s.
[[Extracting dense subgraph]]s from [[large graph]]s is a [[key primitive]] in a [[variety]] of [[graph mining application]]s, [[ranging]] from [[mining]] [[social network]]s and [[the Web graph]] to [[bioinformatics]] [41].
[[Extracting knowledge]] by performing [[computation]]s on [[graph]]s is becoming increasingly challenging as [[graph]]s grow in [[size]] .
[[Extracting]] useful [[pattern]]s from [[such data]] is particularly challenging because it is <i>[[longitudinal]]</i>, <i>[[sparse]]</i> and <i> [[heterogeneous]] </i>.
[[Extraction]] and [[classification]] of the [[segment]]s is formalised as a [[sequence tagging problem]] and four separate [[phrase-based]] [[Conditional Random Field]]s are used to accomplish the task.
[[Extreme value theory]] is a [[branch of statistics]] that concerns the [[distribution of data]] of unusually [[low]] or [[high value]], i.e. in the [[tail]]s of some [[distribution]] .
[[Face analysis]], [[computer vision]], [[text analysis]], [[speech recognition]], and more traditional [[analytics]] such as [[churn prediction]], [[recommendation]]s, [[anomaly detection]], [[forecasting]], and [[clustering]] are all available now as [[cloud API]]s, and far more are being created at a rapid pace.
facial expression analysi]]s, [[speech analysi]]s; [[neuro-physiological signal processing method]]s, e.g., [[respiratory]] and [[cardiovascular acceleration]]s and [[deceleration]]s, [[muscle spasm]]s; and [[web analytic]]s, e.g., [[number of site visit]]s, [[click depth]] .
Facing these possible and [[unexpected disaster]]s, [[accurately predicting]] [[human emergency behavior]] and their [[mobility]] will become the critical issue for planning effective humanitarian [[relief]], [[disaster management]], and [[long-term societal reconstruction]] .
[[factorization machine]]; [[sparse data]]; [[tensor factorization]]; [[support vector machine]]
[[FaitCrowd]] jointly models the [[process of generating]] [[question content]] and [[source]]s' provided [[answer]]s in a [[probabilistic model]] to [[estimate]] both [[topical expertise]] and [[true answer]]s [[simultaneously]] .
[[Fake review]]s [[generated]] by [[this mechanism]] are extremely hard to [[detect]]: Both the [[state-of-the-art]] [[computational approach]]es and [[human reader]]s acquire an [[error rate]] of 35%-48%, [[just slightly]] better than a [[random guess]] .
Far more than having [[big data]] or a crack [[team]] of [[unicorn data scientist]]s, it requires establishing an [[effective]], [[deeply-ingrained]] [[data culture]] .
Farradane (1967) used nine types of relations: [[Concurrence Relation]], [[Equivalence Relation]], [[Distinctness Relation]], [[Self-activity Relation]], [[Dimensional Relation]], [[Reaction Relation]], [[Association Relation]], [[Appurtenance Relation]], [[Functional Dependence Relation]] .
[[Fault-tolerant frequent itemsets (FTFI)]] are [[variant]]s of [[frequent itemset]]s for [[representing]] and [[discovering]] generalized [[knowledge]] .
[[Feasible planning]], [[optimal planning]], [[search algorithm]]s, [[A*]], [[Dijkstra's algorithm]], [[forward search]], [[backward search]], [[bidirectional search]], [[value iteration]], [[logic-based planning]], [[STRIPS]], [[plan graph]], [[planning]] as [[satisfiability]] .
[[Feature selection]], as a [[data preprocessing strategy]], has been proven to be [[effective]] and [[efficient]] in [[preparing high-dimensional data]] for [[data mining]] and [[machine learning problem]]s.
[[Feature selection]], combined with additional [[structure information]] on the [[feature]]s has been considered to be promising in promoting [[regression]] / [[classification]] [[performance]] .
[[Feature selection]] is an important [[task]] in order to achieve better generalizability in [[high dimensional learning]], and [[structure learning]] of [[Markov random fields (MRFs)]] can automatically [[discover]] the [[inherent structures]] underlying [[complex data]] .
[[Feature Selection]], [[Stability]], [[Ensemble]], [[High-dimensional Data]], [[Small Sample]] .
[[Feature selection technique]]s are designed to find the relevant [[feature subset]] of the original [[features]] which can facilitate [[clustering]], [[classification]] and [[retrieval]] .
[[Federal Financial Participation]]: [[Reimbursement]] from the [[federal government]] for part of the [[cost]] of certain programs, such as [[Medicaid]] and [[TAFDC]] .
[[Federal grant account]]: [[Entity]] in [[MMARS]] which records the status of [[grants authorized]] by the [[Legislature]] to be received from the [[federal government]] and [[subsequently expended]] .
:# [[Federal statute]]s, [[regulation]]s, and the [[terms and conditions]] of the [[Federal award]] that could have a direct and [[material effect]] on a [[Federal program]]; and<br>
[[FEMA]] utilizes a [[flexible]] and [[dynamic factorization scheme]] for [[analyzing]] [[human behavioral data sequence]]s, which can incorporate various [[knowledge embedded]] in different [[object domain]]s to alleviate the [[sparsity problem]] .
; [[Fewer NYC millennials]] are disengaged from the [[workforce]] and [[school]], and [[incarceration rate]]s have fallen.
FIGURE 10.9. [[Rate of return]] versus [[growth rate]] at the [[world level], from [[Antiquity]] until [[2100]]
: Figure 1: [[Learned diagonal variance]]s for each [[word]], with the [[first letter]] of each word indicating the [[position of its mean]], and [[projected onto]] [[generalized eigenvector]]s between the [[mixture mean]]s and [[variance]] of [[query word]] Bach.
Figure 1 shows four example topics that were derived from the [[TASA corpus]], a [[collection]] of over 37,000 [[text passages]] from [[educational materials]] (e.g., [[language & arts]], [[social studies]], [[health]], [[science]]s) collected by [[Touchstone Applied Science Associates]] (see [[Landauer, Foltz, & Laham, 1998]]).
• [[Filtering]] is based on [[descriptions of individual]] or [[group information preference]]s, often called [[profile]]s.
Finally a [[Bayesian]] [[maximum likelihood]] [[classifier]] is implemented using [[samples]] from real as well as [[simulated class]]es.
Finally, an [[iterative learning strategy]] is presented to [[dynamically refine]] both [[vertex-centric clustering]] and [[edge-centric clustering]] by continuously [[learning]] the [[contribution]]s and [[adjusting]] the [[weight]]s of different [[path graph]]s.
Finally, [[annotator]]s can attach [[free-form text note]]s to any [[annotation]] .
Finally, [[application]]s, [[open problem]]s, and [[future direction]]s are discussed.
Finally, [[experimental result]]s on [[real-world LBSNs data]] show that [[the proposed recommendation method]] outperforms [[state-of-the-art]] [[latent factor model]]s with a significant [[margin]] .
Finally, [[experimental results]] show that [[proposed system]] can provide effective [[mobile sequential recommendation]] and the [[knowledge]] extracted from [[location trace]]s can be used for [[coaching drivers]] and leading to the [[efficient use]] of [[energy]] .
Finally, [[experiment]]s on [[real-world]] [[travel tour data]] show that the [[cost-aware recommendation model]]s outperform [[state-of-the-art]] [[latent factor model]]s with a significant [[margin]] .
Finally, it provides [[user]]s with the extent to which the [[cross-domain instance relationship]] violates the [[in-domain clustering structure]], and thus enables [[user]]s to [[re-evaluate]] the [[consistency]] of the [[relationship]] .
Finally, our [[empirical results]] show that, while in some [[context]] [[NoRegret KLRank]] might be considered [[conservative]], a [[greedy]] [[variant]] of this [[algorithm]] actually outperforms many popular [[ranking algorithms]] .
Finally, [[our SS-Nystrom method]] demonstrates significant [[improvement]]s over the [[standard]] and [[modified Nystrom method]]s on several [[real-world dataset]]s.
Finally, [[our theory]] and [[experiment]]s also show [[exponential saving]]s in the amount of [[required data]] compared with [[general unbiased estimator]]s.
Finally, [[random-intercepts-only LMEM]]s used on [[within-subjects]] and/or [[within-items data]] from [[population]]s where [[subjects and/or item]]s vary in their sensitivity to [[experimental manipulation]]s always [[generalize worse than]] separate [[F1]] and [[F2 test]]s, and in many cases, even worse than [[F1]] alone.
Finally, [[Rubik]] is [[scalable]] to [[large dataset]]s containing millions of [[EHR record]]s.
Finally, [[the analysis]] shows how [[intention]]s can be adopted relative to a [[background]] of relevant [[belief]]s and other [[intention]]s or [[goal]]s.
Finally, the [[BEA]] asked [[Metron]], a [[scientific consulting firm]] based in [[Reston, VA]], to [[generate a probability map]] for the [[airplane]]'s [[location]] using [[Bayesian inference]], a [[statistical approach]] to combining [[prior belief]]s and [[experience]]s with [[new evidence]] .
Finally, the [[embedding]] serves as a [[visualization]] enhancing the [[interpretability]] of the [[clustering result]] .
Finally, [[the event]] [[represented by]] each [[cluster]] is summarized with information such as [[type of event]], [[geographical location]]s, [[time]], and [[participant]]s.
Finally, the [[opinion summary]] is generated through [[integrating information]] from [[dimension]]s of [[topic]], [[opinion]] and [[insight]], as well as other [[factor]]s (e.g.
Finally, [[the resource]] also prepares for the [[transition]] into [[semantic navigation]] of [[computing science research publication]]s.
Finally, the [[Top-N]] [[thread-title, reply]]' [[pair]]s are selected as [[chatbot knowledge]] .
Finally, [[this survey]] focuses on studies where the seller learns about the [[demand function]], and not on [[studi]]es where buyers are [[learning]] ([[Bergemann and V¨alim¨aki, 1997]], [[Ottaviani, 1999]], [[Caminal]] and [[Vives, 1999]], [[Schlee, 2001]], [[Bose et al., 2006]], [[Bhalla]], [[2012]], [[Ifrach et al., 2012]]).
Finally, [[this work]] proposes two [[method]]s to update the [[hyperparameter]] <math>\alpha</math> of the [[online CRP]] .
Finally, to address the case where a [[fixed number]] of [[output cluster]]s is required, [[we]] devise a [[third algorithm]] that [[directly optimize]]s the [[objective function]] via a [[strategy]] based on the <i>[[alternating minimization]]</i> [[paradigm]] .
Finally, [[we]] achieve new [[state-of-the-art result]]s on several [[text classification]] and [[sentiment analysis task]]s.
Finally, [[we]] also outline a [[large-scale]] [[annotated data set]] of fashion [[image]]s <b> [[Fashion-136K]] </b>) that can be [[exploited]] for [[future research]] in [[data driven visual fashion]] .
Finally [[we]] also show how [[our method]] can help in diverse [[application]]s like [[influence maximization]] and [[detecting patterns of propagation]] at the [[level]] of [[automatically created group]]s on [[real cascade data]] .
Finally, [[we]] [[analyze]] the [[click]] [[data]], presenting [[models]] for why and how the [[clickthrough rate]] for new [[content]] declines as it [[ages]] .
Finally, [[we]] combat the extreme [[sparsity]] of [[response prediction data]] by incorporating [[hierarchical information]] about the [[page]]s and [[ad]]s into [[our factorization model]] .
Finally [[we]] compare [[our result]]s with [[official data]] provided by the [[National Institute of Geophysics and Volcanology (INGV)]], the [[authority responsible]] for [[monitoring]] [[seismic event]]s in [[Italy]] .
Finally, we compare these two [[ELLR algorithm]]s with [[top-performing]] [[baseline method]]s over four [[benchmark dataset]]s, among which [[the largest network]] has more than [[100]] thousand [[node]]s and seven million [[entries]] .
Finally, [[we]] conduct a comprehensive [[evaluation]] with [[real-world data]] .
Finally, [[we]] conduct a comprehensive [[evaluation]] with [[real-world]] [[estate related data]], and the [[experimental result]]s demonstrate the [[effectiveness]] of our [[method]] .
Finally, [[we]] conduct a comprehensive [[experimental study]] based on a [[real dataset]] [[collected]] from a [[location-based social network]], [[Whrrl]] .
Finally, [[we]] conduct comprehensive [[large scale experiment]]s to show the [[effectiveness]] and [[efficiency]] of [[our approach]] .
Finally, [[we]] conducted extensive [[experiment]]s on a [[real dataset]] [[crawled]] from [[Twitter]] .
Finally, [[we]] conduct extensive [[experiment]]s to verify the [[effectiveness]] and [[efficiency]] of the [[TCS model]] and the [[BPE algorithm]] .
Finally, [[we]] [[demonstrate]] the [[effectiveness]] and [[efficiency]] of [[TAP]] on [[real]] [[large data sets]] .
Finally [[we]] demonstrate the [[efficacy]] and [[efficiency]] of [[our solution]] in minimizing [[user effort]], compared to [[state-of-the-art]] [[ranking method]]s, by means of an extensive [[experimental evaluation]] and a comprehensive [[user study]] on [[Amazon Mechanical Turk]] .
Finally, [[we]] describe how these [[sampling biase]]s can be [[exploited]] in [[several]], [[real-world application]]s including [[disease outbreak detection]] and [[market research]] .
Finally, [[we]] describe some current [[areas of focused innovation]], which include making [[our recommender system]] [[global]] and [[language aware]] .
Finally, [[we]] describe the first [[burstiness-driven]] [[search framework]] and thoroughly [[evaluate our approach]] in the context of different [[scenarios]] .
Finally, [[we]] describe two [[user studies]] that test [[our model]]s of [[multi-document summarization]] .
Finally, [[we]] develop a [[two-stage optimization strategy]] and provide an efficient [[SSL approach]] that takes advantage of [[Laplacian spectral regularization]]: [[semi-supervised learning]] with <i>[[Enhanced Spectral Kernel(ESK)]]</i>.
Finally, [[we]] discuss novel [[applications]] of [[ClickRank]] in providing enriched [[user]] [[web search]] [[experience]], highlighting the usefulness of [[our approach]] for non-[[ranking task]]s.
Finally, [[we]] discuss [[open research issue]]s related to the [[use of ontologies]], [[evaluation metric]]s, and [[applications of IE]] in [[decision-making]] .
Finally, [[we]] discuss the [[implication]]s of these [[result]]s for the design of [[Q&A site]]s.
Finally, [[we]] discuss [[two]] [[applications]] of the [[Friend Suggest algorithm]] that have been released as [[Gmail Labs features]] .
Finally, we evaluate [[our algorithm]]s on [[real data set]]s and show that they [[outperform]] simple [[baseline]]s.
Finally, [[we]] [[evaluate our methodology]] on the [[Adult dataset]] from the [[UCI machine learning repository]] .
Finally, we [[evaluate]] [[our method]] on four diverse [[prediction scenario]]s using four [[dataset]]s: [[preference]] ([[Foursquare]]), [[repost]] ([[Twitter]]), [[response]] ([[Plurk]]), and [[citation]] ([[DBLP]]).
Finally, [[we]] [[evaluate]] [[the]] [[effectiveness]] of [[our method]] using [[two]] [[real datasets]] on [[citations]] and [[social bookmarking]], in which [[our proposed method IPF]] gives 15% - 34% [[improvement]] over the previous [[state-of-the-art]] .
Finally, [[we]] explore [[properti]]es of the discussed [[model]]s [[empirical]]ly in the context of [[email spam filtering]] .
Finally, we explore [[topological properties of communiti]]es and [[seed set]]s that [[correlate]] with [[algorithm performance]], and explain these [[empirical observation]]s with [[theoretical one]]s.
Finally, [[we]] give an improved [[lower rate result]] describing [[worst]] [[asymptotic behavior]] on individual [[probability measures]] rather than over [[classes]] of [[priors]] .
Finally, we have conducted extensive [[experiment]]s to validate [[effectiveness]] of the proposed [[algorithm]]s on [[real]] and [[synthetic data]] .
Finally, [[we]] illustrate the relevance of [[metric learning]] in [[real-world problem]]s through a series of successful applications to [[computer vision]], [[bioinformatics]] and [[information retrieval]] .
Finally, [[we]] introduce a [[constrained version]] of the [[PMF model]] that is based on the [[assumption]] that [[user]]s who have [[rated]] similar sets of [[movi]]es are likely to have similar [[preference]]s.
Finally, [[we]] look at current and [[future research direction]]s for [[novel application]]s of [[ensemble system]]s.
Finally [[we]] modify the [[greedy algorithm]] and [[we]] present two [[heuristic algorithm]]s that [[find communities]] of [[size]] no [[greater than]] a [[specified]] [[upper bound]] .
Finally, [[we]] [[parallelize]] [[our algorithm]] to further [[reduce]] the overall [[running time]] .
Finally [[we]] perform an extensive [[empirical evaluation]] on [[benchmark data set]]s where [[we]] show that [[classifying graph]]s using [[matchings]] of [[geometric embeddings]] [[outperform]]s the previous [[state-of-the-art method]]s.
Finally [[we]] perform a [[user study]] [[validating]] [[our algorithm]]s and [[diversity metric]]s.
Finally, [[we]] present a [[case-study]] applying a [[loopy CRF]] to a [[relational problem]] in [[natural language processing]] .
Finally, [[we]] present a [[case study]] of [[our model]] on [[predicting]] [[future]] [[social actions]] .
Finally [[we]] present a new [[data structure]], the [[cone tree]], for increasing the [[efficiency]] of the [[dual-tree algorithm]] .
Finally, [[we]] present [[empirical evidence]] supporting the [[effectiveness]] of our [[test]] for [[disparate impact]] and [[our approach]] for both [[masking bias]] and [[preserving relevant information]] in the [[data]] .
Finally, [[we]] propose an [[algorithm]] for [[approximate probabilistic theorem proving]], and show that it can greatly outperform [[lifted]] [[belief propagation]] .
Finally, [[we]] propose an “[[Iterative Feature Selection (IF)]]” [[method]] that addresses the [[unavailability of label problem]] by utilizing effective [[supervised feature selection methods]] to [[iteratively select features]] and perform [[clustering]] .
Finally, [[we]] propose a [[piecewise metric]] [[index structure]] to reschedule the [[joining order]] of local [[cluster]]s to further reduce the [[time]] [[cost]] .
Finally, [[we]] provide a [[validation]] of [[our technique]]s by comparing with [[existing software team]]s.
Finally, [[we]] provide [[evidence]] that the [[quasi-social network]] embeds a [[true]] [[social network]], which along with [[results]] from [[social theory]] offers one [[explanation]] for the increase in [[brand]] [[affinity]] of the selected [[audience]]s.
Finally, [[we]] report comprehensive [[experimental results]], using [[industrial proprietary codebase]] and [[datasets]] .
Finally, [[we]] reveal the [[interrelationships]] among these [[external measures]] .
Finally, [[we]] showcase [[our model]] as an [[exploratory analysis tool]] for [[political scientist]]s.
Finally, [[we]] show [[experimental result]]s with [[real-world]] [[ad delivery data]] that support our [[theoretical conclusion]]s.
Finally, [[we]] show how to implement [[query]] on any [[Church program]], [[exactly]] and [[approximately]], using [[Monte Carlo technique]]s.
Finally, [[we]] show that the [[reconstruction error]] of the [[decomposition]] provides a useful [[measure]] of the [[network strength]] and is useful at [[predicting]] key [[cognitive score]]s both by itself and with [[clinical information]] .
Finally, [[we]] show the effectiveness of [[the proposed algorithm]] on a [[human-labeled ground truth dataset]] consisting of 2000 [[web page]]s from [[100]] major [[Web site]]s, and demonstrate its [[efficiency]] on [[large synthetic dataset]]s.
Finally, we study [[extension]]s and [[variation]]s of our [[problem]] both [[theoretically]] and [[experimentally]] .
Finally, we test the [[performance]] of [[the proposed framework]] ([[gSH]]) on various [[types of graph]]s, showing that from a [[sample]] with -- 40K [[edge]]s, it produces [[estimate]]s with [[relative error]]s < [[1%]] .
Finally, we test the [[performance]] of [[the suggested criteria]] and [[algorithm]] on both [[simulated]] and [[real world data]] .
Finally, [[we]] use the [[classifier]] to find the best [[caption]]s and study how its [[prediction]]s could be used to significantly [[reduce]] the [[load]] on the [[cartoon contest's judge]]s.
Finally, [[we]] validate [[our result]]s with a [["<i>side-by-side</i>" comparison]] of [[scheme]]s through [[live experiment]]s conducted on a [[random sample]] of [[real user visit]]s to [[Yahoo !]]
Finally [[we]] [[validate Pacifier]] on two [[real world]] [[EMR]] [[cohort]]s for the [[tasks of early prediction]] of [[Congestive Heart Failure (CHF)]] and [[End Stage Renal Disease (ESRD)]] .
Finally [[we]] validate the [[efficiency]] and [[effectiveness]] of <i>[[MulSLR]]</i> on [[predicting]] the [[onset risk]] of [[patient]]s with [[Alzheimer's disease]] and [[heart failure]] .
[[Financial service]]s and [[healthcare]] [[compani]]es could be the biggest [[beneficiari]]es of [[big data]] .
[[Finding dense subgraph]]s is an important [[graph-mining task]] with many [[application]]s.
Finding [[periodic behaviors]] is essential to [[understanding]] [[object movements]] .
Finding such [[sketches]] efficiently is an important [[building block]] in [[modern algorithm]]s for [[approximating]], for example, the [[PCA]] of [[massive matrice]]s.
Finding the best ad [[impression]], i.e., the [[opportunity]] to show an [[ad]] to a [[user]], requires the ability to [[estimate the probability]] that the [[user]] who sees the [[ad]] on his or her [[browser]] will take an [[action]], i.e., the [[user]] will convert.
Finished only weeks before [[his]] [[death]] in [[1994]], it is the [[self-portrait]] of one of this century's most [[original]] and [[influential]] [[intellectual]]s.
First coined by [[Richard Dawkin]]s in [[The Selfish Gene]], a [[meme]] is any [[idea]], [[behavior]], or [[skill]] that can be transferred from one [[person]] to [[another]] by [[imitation]]: [[stori]]es, [[fashion]]s, [[invention]]s, [[recipe]]s, songs, ways of plowing a [[field]] or throwing a [[baseball]] or making a [[sculpture]] .
First, [[core knowledge]] has [[conceptual content]]; it cannot be characterized in terms [[perceptual]] or [[sensorimotor primitive]]s.
First, drawing upon recent advances in [[Machine Learning (ML)]] and [[Mobile Robotics (MR)]], [[we]] develop a novel methodology to [[categorise]] [[occupation]]s according to their [[susceptibility]] to [[computerisation]] .
First, how to [[infer real-time]] [[air quality]] of any [[arbitrary location]] given [[environmental data]] and [[historical air quality data]] from [[very sparse]] [[monitoring location]]s.
First, in all the four [[cases]], our [[system]] significantly improves the [[clustering]] [[stability measure]]d in [[term]]s of the [[macro-averaged Jaccard index]] .
First, it can guide [[experimenter]]s with [[rules of thumb]] that can help them [[optimize]] their [[site]]s.
First, it regularizes the [[network construction]] by utilizing [[similarity]] or [[dissimilarity constraint]]s between [[data pair]]s, rather than [[sample-specific annotation]]s.
First, [[I]] will describe [[the strategy]] behind our [[hiring process]] to attract [[millennial]] [[big data developer]]s and the results of [[this endeavor]] .
Firstly a [[hierarchical strategy]] is used to reduce the [[complexity]] to <I>O</I>(<I>N</I><SUP>1+<I>e</I></SUP>) the [[distortion loss]] incurred is [[analyzed]] in [[relation]] with the [[dimension]] of the [[data items]] .
First, [[manually created]] [[lexicons]] often contain [[rare]] [[senses]] .
First, many [[researcher]]s have never [[tweet]]ed, [[blogged]] or made a [[YouTube video]] .
First of all, there are the various students who over the years helped to craft the course, notably [[Nathaniel Love]], [[David Haley]], [[Eric Schkufza]], [[Evan Cox]], [[Alex Landau]], [[Peter Pham]], [[Mirela Spasova]], and [[Bertrand Decoster]] .
First, [[ONION]] features an [[innovative]] [[interactive anomaly exploration model]] that offers an "[[outlier-centric panorama " into big dataset]]s along with [[rich class]]es of [[exploration operation]]s.
First, the existing [[methods]] cannot [[correctly]] [[extract]] [[pages]] belonging to an [[unseen]] [[template]] .
First, [[the methodology]] may be used to [[quantify]] [[user learning effect]]s and to [[evaluate online experiments]] in contexts other than [[ad]]s.
First, the [[nearest neighbor algorithm]] requires [[storing]] and [[searching]] the entire [[dataset]], resulting in an [[time]] and [[space complexity]] that [[limit]]s its [[applicability]], especially on [[resource-limited]] [[sensors]] .
First, the [[total number]] of [[concepts]] required to [[describe]] the [[data]] may itself be [[reduced]] .
First, utilizing a novel [[star-tree data structure]], [[we]] forecast the [[bid]] for each [[sample]] using [[non-linear regression]] by [[gradient boosting decision tree]]s.
First, we build a [[classifier]] to recognize the specific type of a [[verbal question]] (e.g., [[analogy]], [[classification]], [[synonym]], or [[antonym]]).
First, [[we]] demonstrate the role played by the [[user interest]] and the [[past exposure]] in modeling [[user response]] by [[jointly estimating]] the [[parameter]]s of these [[factor]]s.
First, we develop an [[approximation algorithm]] that obtains a [[near-optimal alert set]] in [[quadratic time]], and [[propose pruning technique]]s to improve its [[runtime]] [[performance]] .
First, [[we]] document that [[gross]] and [[net labor share]]s generally declined together in most [[countri]]es around the [[world]] over the past four [[decade]]s.
First, [[we]] examine a [[general framework]] for using [[prior knowledge]] to [[regularize parameter]]s in the [[topmost layer]]s.
First, [[we]] explore the [[spatial]] and [[temporal characteristic]]s of [[uncertain trajectori]]es and construct a [[routable graph]] by [[collaborative learning]] among the [[uncertain trajectori]]es.
First, [[we]] introduce a novel [[social influence]] based [[vertex similarity metric]] in terms of both [[self-influence similarity]] and [[co-influence similarity]] .
First, [[we]] model excess [[heterogeneity]] by [[fitting]] [[local LMMH model]]s to relatively [[homogeneous subset]]s of the [[data]] .
First we [[observe]] several [[properti]]es of [[magnet communiti]]es, such as [[attention flow]], [[attention qualify]], and [[attention persistence]] .
First [[we]] place the [[BBS theory]] into the [[unifying framework]] of [[optimal transport theory]] .
First, [[we]] propose a [[conditional model]] for [[link analysis]] and in the [[model]], [[we]] introduce [[hidden variables]] to [[explicitly]] model the [[popularity]] of [[nodes]] .
First, [[we]] propose an [[edge-centric path graph]] [[model]] to capture the [[meta-path]] [[dependenci]]es between [[pairwise path]] [[edge]]s.
First, [[we]] propose [[efficient implementation]]s for [[training FFM]]s.
First we [[prove]] that in order to [[solve]] [[this problem]] exactly, [[we]] still need a [[prohibitive amount]] of [[memory]] (at least [[linear]] in the [[number of items]]).
First, [[we]] provide a [[discussion]] of a [[large-scale]] related [[search recommendation system]] .
First, [[we]] re-define the [[problem]] of [[sequential pattern hiding]] to capture the [[information loss]] incurred by [[sanitization]] in terms of both [[events' modification]] ([[distortion]]) and [[lost nonsensitive knowledge pattern]]s ([[side-effects]]).
First, [[we]] seek to [[characterize]] [[network failure pattern]]s in [[data centers]] and understand overall reliability of the [[network]] .
First, [[we]] show that the [[structure]]s of [[capability model]]s can be [[learned or easily specified]], and both [[model structure]] and [[parameter learning]] are robust to high degrees of [[incompleteness in plan trace]]s (e.g., with only [[start]] and [[end state]]s [[partially observed]]).
First, [[we]] survey various [[approximation method]]s, [[analyze]] their [[properti]]es and [[relation]]s and provide some new insights into their [[difference]]s.
[[Fisher's experiment]]s at the [[Rothamsted Agricultural Experimental Station]] in [[England]] in the [[1920s]], the [[deployment]] and [[mining]] of [[online controlled experiment]]s at [[scale]] -- thousands of [[experiment]]s now -- has taught us many lessons.
[[Fitting]] a [[topic model]] given a [[set]] of [[training documents]] requires [[approximate inference technique]]s that are [[computationally expensive]] .
Five [[year]]s later in [[2013]], [[Rocket Fuel]] had the best [[technology]] [[IPO]] of [[the year]] on [[NASDAQ]], reported $240 million in [[revenue]], and was [[rank]]ed by [[accounting firm]] [[Deloitte]] as the # 1 [[fastest-growing technology]] [[company]] in [[North America]] .
Focusing on listings from [[eBay]]'s [[clothing and shoes categories]], our [[bootstrapped NER system]] is able to identify new [[brand]]s corresponding to [[spelling variant]]s and [[typographical error]]s of the known [[brand]]s, as well as [[identifying]] novel [[brand]]s.
Focusing on [[sequence modeling]] for [[music playlist]]s, [[we]] show that the [[method]] substantially [[speeds up]] [[training]] while maintaining [[high model]] [[quality]] .
Following [[this finding]] -- and building on other [[recent work]] for finding simple [[network structure]]s -- [[we]] propose a new [[architecture]] that consists solely of [[convolutional layer]]s and yields [[competitive or state of the art performance]] on several [[object recognition datasets (CIFAR-10]], [[CIFAR-100]], [[ImageNet]]).
Following [[this hypothesis]], [[we]] derive a [[scalable method]] to [[detect]] both [[communiti]]es and [[innovative consumer]]s in each [[community]] from a [[web-scale data]] from a [[behavior log]] .
Following this idea, [[we]] can [[learn]], during the run of a [[clustering algorithm]], the [[optimal]] [[trade-off]] for [[attribute weight]]s and distinguish relevant [[attribute dependenci]]es from [[coincidental]] ones.
Following this line of research, [[we]] propose four [[fast]] and [[scalable]] [[spectral algorithm]]s for [[learning word embeddings]] - [[low dimensional real vector]]s (called [[Eigenword]]s) that [[capture the "meaning" of words from their context]] .
Following [[Wagstaff's proposal]] for [[practical exploitation]] of [[machine learning (and data mining) approach]]es, [[we]] describe how [[data]] have been [[collecte]]d and [[prepare]]d for the [[production]] of different [[dataset]]s, how suitable [[classification model]]s have been [[identifi]]ed and how the [[interpretation]] of the results suggests the [[emergence]] of an [[active role]] of [[classification technique]]s, based on [[standard chemical profiling]], for the [[assesment]] of the [[authenticity]] of the [[wines target]] of [[the study]] .
[[FoodSIS]] improves [[efficiency]] of such focused [[information gathering process]] with the use of [[machine learning technique]]s to [[identify]] and [[rank]] [[relevant content]] .
For a given new [[word sense]] s and a [[target]] [[synset]] [[S]] we define two possible [[operation]]s:
For a [[given]] [[set of seed entities]] [[we]] use [[co-occurrence statistics]] taken from a [[text collection]] to define a membership function that is used to [[rank]] [[candidate]] [[entities]] for inclusion in the [[set]] .
For all five, [[we]] show high agreement between [[Mechanical Turk]] [[non-expert]] [[annotations]] and existing [[gold standard labels]] provided by [[expert labelers]] .
For a long time, [[core]] [[NLP technique]]s were dominated by [[machine-learning approaches that used linear model]]s such as [[support vector machine]]s or [[logistic regression]], [[trained]] over very [[high dimensional]] yet very [[sparse feature vector]]s.
For a long time, [[people]] have made tremendous efforts in [[establishing]] [[credit record]]s for the [[borrower]]s.
For an [[outlook]], [[I]] will postulate potential [[evaluation approach]]es for a better [[user's satisfaction]] .
For [[applications]] involving [[binary classification]], [[Receiver Operating Characteristic (ROC) curves]], [[performance curves]] that explicitly [[trade off]] [[false alarm]]s and [[missed detections]], are often utilized to [[support decision making]] .
For [[appropriations]] [[bill]]s, the [[Governor]] may disapprove [[line item]]s, or in some [[instances portion]]s of [[line item]]s, and [[outside sections]] .
For [[app usage]], [[we]] observe a [[distribution]] that has higher [[kurtosis]] (heavier [[head]] and longer [[tail]]) than that for the aforementioned movie [[dataset]]s.
For a [[sentence]] with m [[word]]s, [[we]] apply the [[autoencoder recursively]] .
For a [[set]] of [[ads]] selected by a new [[prediction model]], the [[online user behavior]] is estimated from the [[historic user behavior]] in the [[search log]]s.
For a [[set]] of [[face image]]s of <i>N</i> [[subject]]s, the [[decomposition find]]s <i>N</i> common [[component]]s, one for each [[subject]], <i>K</i> [[low-rank component]]s, each [[capturing]] a different [[global condition]] of the [[set]] (e.g., different [[illumination condition]]s), and a [[sparse residual]] for each [[input image]] .
For a specific [[period]], [[periodic behaviors]] are [[statistically generalized]] from [[partial]] [[movement sequence]]s through [[hierarchical clustering]] .
For a [[training dataset]] with a [[nonexhaustive]] [[list]] of [[class]]es, i.e. some [[class]]es are not yet known and hence are not [[represented]], the resulting [[learning problem]] is [[ill-defined]] .
For [[auction optimization]], [[keyword]]s shall be deemed as [[interchangeable commoditi]]es with respect to their [[valuation]]s from [[advertiser]]s, [[represented as]] [[bid distribution]]s or [[landscape]]s.
For automated [[interpretation]] and [[read-ability]] on [[semi-structured text cluster]]s, [[we]] propose a [[method]] to [[visualize cluster]]s that preserves the [[structure]] and [[human-readability]] of the [[text data]] as compared to traditional [[word cloud]]s where the [[text structure]] is not preserved; for [[unstructured text cluster]]s, we find a simple way to define [[prototypes of cluster]]s for [[easy interpretation]] .
For both [[benchmarks]], the [[scoring scheme]] for [[measuring]] [[interaction set]] [[accuracy]] is in the form of a [[log odds ratio]] of [[gene]] [[pairs]] either sharing [[annotations]] or [[physically interacting]] .
For both [[personal]] and [[large-scale technologi]]es, the [[service quality]] drops or [[disappear]]s entirely outside of a handful of [[languages]] .
For both [[task]]s, [[our model]] [[outperform]]s several existing [[state-of-the-art method]]s, achieving higher [[accuracy]] with significantly [[less computation]], [[analyzing]] a [[data set]] with 1.3 million [[word]]s and 44 thousand [[link]]s in a few [[minute]]s.
For [[budget management]] purposes, the [[COA]] should meet the requirements of [[planning]], [[controlling]] and [[reporting]] of [[budgetary allocation]]s / appropriations as well as internal management needs of [[budget unit]]s and/or [[cost center]]s.
::[[Force]]s are inherently [[vector quantities]], requiring [[vector addition]] to combine them.
For comparison, [[random-projection]], [[hashing]] or [[sampling]] based [[algorithm]]s produce [[convergence bound]]s [[proportional]] to 1/â <i>l </i>.
For [[continuous]] or [[ordinal-categorical variable]]s, a certain [[transformation]] of [[relative entropy]] to the [[interval]] [0,1] leads to generalizations of the [[correlation]], [[multiple-correlation]], and [[partial-correlation coefficient]]s.
For [[cost]] and [[performance reason]]s, [[processing]] is typically done on [[large cluster]]s of [[shared-nothing commodity machine]]s.
For [[dataset]]s where even [[this level]] of [[speedup]] is inadequate, [[we]] show that we can use a simple [[heuristic]] to order the unavoidable [[calculation]]s in a [[<i>most-useful-first</i> ordering]], thus casting the [[clustering]] as an [[anytime algorithm]] .
For decades, [[computer Go]] has defied the [[classical method]]s in game [[tree search]] that worked so successfully for [[chess]] and [[checker]]s.
For decades, [[political scientist]]s have collected and [[analyzed record]]s of the [[form " country]] <i>i</i> [[took action]] <i>a</i> toward [[country]] <i>j</i> at [[time]] <i>t</i> " - known as [[dyadic event]]s - in order to [[form]] and [[test]] [[theories]] of [[international relation]]s.
For each [[category]], [[we]] provide a basic [[anomaly detection technique]], and show how the existing [[technique]]s are [[variant]]s of the [[basic technique]] .
For each [[example]], the closest [[example]] of the same [[class]] ([[nearest hit]]) and the closest [[example]] of a [[different]] [[class]] ([[nearest miss]]) are [[selected]] .
For each [[image]], <i>all</i> [[webpage]]s that contain [[it]] are considered.
For each [[node]] of the [[tree]], [[the model]] must [[predict]] a [[semantic role label]], which is interpreted as the [[labelling]] for the corresponding [[syntactic constituent]] .
For each [[pair]] of [[entiti]]es that appears in some [[Freebase relation]], [[we]] find all [[sentence]]s containing those [[entiti]]es in a [[large unlabeled corpus]] and extract [[textual feature]]s to train a relation classifier.
For each [[pair]], [[we]] compute the [[Euclidean distance]] between the original [[input]] and its [[reconstruction]]:
For each [[query]], [[ranking]] is applied to [[order]] a [[set]] of [[relevant item]]s from which the [[user]] [[select]]s his [[favorite]] .
For each [[user]], or more generally [[context]], [[they]] try to [[discriminate]] between a [[small set]] of selected [[item]]s and the [[large set]] of remaining [[(irrelevant) item]]s.
[[Forecast accuracy]]; [[Forecast evaluation]]; [[Forecast error measure]]s; [[M-competition]]; [[Mean absolute scaled error]]
For effective [[management]], the [[COA]] should cover all [[transaction]]s ([[flows)]] and [[balance]]s (stocks) of the [[reporting entity]] for [[budget management]] and general purpose financial reporting (see [[Box 2]] for the [[“reporting entity” concept]] and how it relates to the [[budgetary sector]]).
For efficient [[learning]], [[we]] propose [[the algorithm]] [[SMVC]] using [[variational Bayesian method]]s.
For efficiently [[estimating the parameters]] of [[the proposed model]], [[we]] develop an [[adaptive sampler]] to draw [[informative training instance]]s based on [[content information]] of [[user]]s and [[item]]s.
For [[efficiently learning]] the [[distribution]]s, [[we]] propose [[the algorithm]] [[MVGen]] that exploits the [[ICM principle]] and uses [[Bayesian model selection]] to [[trade-off]] the [[cluster model's complexity]] against its [[goodness of fit]] .
For empirical [[evaluation]], [[we]] apply [[ITMC]] to two [[temporal]] [[anomaly detection tasks]] .
For evaluation, [[we]] also compute [[Spearman correlation]] between a [[model’s computed similarity scores]] and [[human judgment]]s.
For example, a [[clothing retailer]] would know that a particular [[customer]] [[bought]] a [[shirt]] and would also know the [[SKU]], [[date]], [[time]], [[price]], and [[size]] of a particular [[shirt]] that was purchased.
For example, a [[community]] in [[blogsphere]] could be [[users]] mostly interested in [[cell phone]] [[reviews]] and [[news]] .
For example, a [[measurable characteristic]] of a [[population]], such as a [[mean]] or [[standard deviation]], is called a [[parameter]]; but a [[measurable characteristic]] of a [[sample]] is called a [[statistic]] .
For example, an [[anomalous]] [[traffic pattern]] in a [[computer network]] could mean that a [[hacked computer]] is sending out [[sensitive data]] to an [[unauthorized destination]] ([[Kumar 2005]]).
For example, a [[register transaction]] collected at a [[retail store]] may have been initiated by a [[person]] who is a [[woman]], a [[mother]], an [[avid reader]], and an [[action movie]] [[fan]] .
For example, [[author]]s describe their various [[approach]]es as [[outlier detection]], [[novelty detection]], [[anomaly detection]], [[noise detection]], [[deviation detection]] or [[exception mining]] .
For example, by combining the [[meaning]]s of “[[bite]],” “[[dog]],” and “[[man]],” we can think about a [[dog]] [[biting]] a [[man]], or a [[man]] [[biting]] a [[dog]] .
For example, [[convolutional neural networks]] are now able to [[directly classify]] [[raw pixels]] into [[high-level concept]]s such as [[object categori]]es ([[Krizhevsky et al., 2012]]) and [[messages on traffic sign]]s ([[Ciresan et al., 2011]]), without using [[hand-designed feature extraction algorithm]]s.
For example, even for a [[graph]] with 200 million [[edge]]s, [[our algorithm]] stores just 60,000 [[edge]]s to give [[accurate result]]s.
For example for an [[actor]], such a [[timeline]] should show the most important [[professional]] and [[personal milestone]]s and [[relationship]]s such as [[work]]s, [[award]]s, [[collaboration]]s, and [[family relationship]]s.
For example, for [[click prediction model]]s for [[search advertising]], [[error]]s in [[prediction]]s in the very [[low range]] of [[predicted click score]]s impact the [[online performance]] much more negatively than [[error]]s in other [[region]]s.
For example, given two [[large graph]]s from the same [[domain]], how can [[we]] use [[information]] in one to do [[classification]] in the other (i.e., perform [[across-network classification]] or [[transfer learning]] on [[graph]]s)?
For example, [[Google advertiser]]s have [[collectively input million]]s of [[keyword]]s into their [[advertising campaign]]s.
For example, if an [[agent]] receives a [[threat]], this [[agent]] may [[accept]] the offer even if it is not [[fully "acceptable"]] for it (because otherwise really important [[goal]]s would be [[threatened]]).
For example, in a [[first-order partof]] - [[speech tagger]], the [[feature]]s might be [[" current word]], [[previous word]], [[next word]], [[previous part of speech]]".
For example, in a [[research publication network]], the [[advisor-advisee relationships]] among [[researchers]] are [[hidden]] in the [[coauthor network]] .
For example, [[income]], [[capital]], the [[economic growth rate]], and the [[rate of return on capital]] are [[abstract concept]]s — [[theoretical construct]]s rather than [[mathematical certainti]]es.
For example, in [[social network]]s such as [[Facebook]], [[system]] may want to [[recommend]] potential [[friend]]s to a particular [[user]] based on [[connection]]s between [[user]]s.
For example, in [[social network]]s, [[we]] connect with [[people]] for various [[reason]]s, such as [[sharing]] [[common interest]]s, working in the same [[company]], being [[alumni]] and so on.
For example, in [[the study]] of [[Alzheimer's Disease (AD)]], different types of [[measurement]]s such as [[neuroimage]]s, [[gene/protein expression data]], [[genetic data]] etc. are often collected and [[analyzed]] together for improved [[predictive power]] .
For example, in the study of [[gene expression pattern]]s in <i>[[Drosophila melanogaster]] </i>, [[texture feature]]s based on [[wavelet]]s were particularly [[effective]] for determining the [[developmental stage]]s from [[in situ hybridization(ISH) image]]s.
For example, [[labels]] have been assigned for [[land cover classification]] of the [[Earth]] but it has been suspected that these [[labels]] are not [[ideal]] and some [[classes]] may be best [[split]] into [[subclasses]] whereas others should be [[merged]] .
For example, many [[Twitter]] [[user]]s describe themselves in a short [[profile]], [[labeling]] themselves with phrases such as "[[vegetarian]]" or "[[liberal]]".
For example, [[MLE]] is a prerequisite for the [[chi-square test]], the [[G-square test]], [[Bayesian method]]s, [[inference with missing data]], [[modeling of random effects]], and many [[model selection criteria]] such as the [[Akaike information criterion]] ([[Akaike, 1973]]) and the [[Bayesian information criteria]] ([[Schwarz, 1978]]).
For example, [[money budgeted]] in the [[AA]] can only be spent on [[salari]]es; [[money]] in [[KK]] can only be spent for [[equipment purchase]]s.
For example, most [[retailer]]s treat their [[product]]s as [[atomic entiti]]es with very few [[related attribute]]s (typically [[brand]], [[size]], or [[color]]).
For example, on the 2010 [[Twitter]] graph with 1.5 billion [[edge]]s, for [[target node]]s [[sampled]] by [[popularity]], [[FAST-PPR]] has a 20 [[factor speedup]] over the [[state of the art]] .
For example, [[our method]] can filter 500 [[tweet]]s in a second for 10 million registered [[subscription]]s on a [[commodity]] [[computer]] .
For example, [[position bias]] in [[search ranking]]s strongly influences how many [[click]]s a [[result]] [[receive]]s, so that directly using [[click data]] as a [[training signal]] in [[Learning-to-Rank (LTR) method]]s yields [[sub-optimal result]]s.
For example, [[prediction]] requires that the domain include all future [[unit]]s, [[subject]]s or [[time point]]s.
For example, [[product catalog]]s of [[online shop]]s or web directories categorize [[product]]s or [[website]]s to help [[user]]s finding relevant entries.
For example, [[protein-protein interaction (PPI) data]] can be [[represented as a graph]], where [[vertice]]s represent [[protein]]s, and edges represent [[PPI']]s.
For example, [[Ranking SVM]] and [[RankBoost]] [[train]] [[ranking models]] by [[minimizing]] [[classification errors]] on [[instance pairs]] .
For example, [[result]]s show that relative to [[worker]]s who remained in [[secure employment]], [[self‐reported morbidity]] was higher among [[worker]]s [[reporting insecurity]] in their [[job]]s.
For example, some [[merchant web site]]s (like [[Amazon.com]]) may need a [[set of classifier]]s to [[predict]] the [[sentiment polarity]] of [[product review]]s collected from various [[domain]]s (e.g., [[electronic]]s, [[book]]s, [[shoe]]s).
For example, some [[people]] may be more [[liberal]] on [[economic issue]]s while more [[conservative]] on [[cultural issue]]s.
For example, the [[colleagues]] have strong [[influence]] on one's [[work]], while the [[friends]] have strong [[influence]] on one's daily [[life]] .
For example, the [[Netflix dataset]] is very [[imbalanced]], with [[“infrequent” user]]s rating 5 movies, while [[“frequent” user]]s rating over 10,000 [[movi]]es.
For example, the [[Project Tycho]] provides open access to the [[count infection]]s for [[U.S. state]]s from 1888 to [[2013]], for 56 [[contagious disease]]s (e.g., [[measles]], [[influenza]]), which include [[missing value]]s, possible [[recording error]]s, [[sudden spikes]] (or [[dive]]s) of [[infection]]s, etc. So how can we find a [[combined model]], for all these [[disease]]s, [[location]]s, and [[time-tick]]s?
For example, to [[predict]] the [[research areas]] of some [[people]], the best [[results]] are usually achieved by [[combining]] and [[consolidating]] [[predictions]] obtained from the [[publication network]], [[co-authorship network]] and the [[textual content]] of their [[publications]] .
For example, [[user]]s in [[Twitter]] can be [[represented as]] [[advertiser]], [[content contributor]], [[information receiver]], etc; [[user]]s in [[Linkedin]] can be in different [[professional role]]s, such as [[engineer]], [[salesperson]] and [[recruiter]] .
For example, using [[clustering]] can give [[indication]] of a clear [[cluster]] [[structure]], and [[computing]] [[correlations]] between [[variables]] can show that there are many significant [[correlations]] in the [[data]] .
For example, [[we]] can [[observe graph]]s representing different [[types of relation]]s between the [[vertice]]s.
For example, [[we]] observe significant [[assortativity]] in the [[reputation]]s of [[co-answerer]]s, [[relationship]]s between [[reputation]] and [[answer speed]], and that the [[probability]] of an answer being chosen as the best one strongly depends on [[temporal characteristic]]s of [[answer arrival]]s.
For example, when applying [[MIL]] to [[image categorization]], the [[characteristic]]s of each [[image]] can be derived from both its [[RGB feature]]s and [[SIFT feature]]s.
For example, when presented a [[choice]] between doing [[seven]] [[hour]]s of an [[unpleasant activity]] on April 1 versus [[eight hour]]s on [[April 15]], if asked on [[February]] 1 virtually everyone would prefer the [[seven hour]]s on [[April 1]] .
For example, when the [[source]] and [[target domain]]s are [[document]]s and [[image]]s respectively, [[TTL]] could use some [[annotated image]]s as the [[intermediate domain]] to [[bridge]] them.
For example, while a [[user]] is [[browsing]] [[mobile phone]]s, it might make sense to recommend other [[phone]]s, but once they buy a [[phone]], we might instead want to recommend [[batteri]]es, [[case]]s, or [[charger]]s.
For example, workers in India are reported to be [[reading x-rays]] ([[Pollak, 2003]]), [[developing software]] ([[Thurm, 2004]]), [[preparing tax form]]s ([[Robertson et al., 2005]]), and even performing [[heart surgery]] on [[American]] [[patient]]s ([[Baker et al., 2006]]).
For [[feature representation]], [[we]] adapt the [[bag-of-words]] scheme commonly used in [[visual recognition problem]]s so that the [[image]] [[group]] [[information]] in the [[BDGP study]] is retained.
For finding [[prominent streak]]s, [[we]] make the [[observation]] that [[prominent streak]]s are [[skyline point]]s in [[two dimension]]s - [[streak interval length]] and [[minimum value]] in the [[interval]] .
For [[fully automatic clustering]], [[we]] propose to combine [[Sync]] with the [[Minimum Description Length principle]] .
For [[general distribution]]s, even if an [[analytic expression]] for the [[kernel]] is not feasible, [[we]] show a straightforward [[sampling method]] to [[evaluate]] [[it]] .
For handling [[complementary]] [[expertise]], [[CTL model]]s [[topic distribution]]s from [[source]] and [[target domain]]s separately, as well as the [[correlation across domain]]s.
For handling [[sparse connection]]s, [[CTL]] [[consolidate]]s the existing [[cross-domain collaboration]]s through [[topic layers instead]] of at [[author layer]]s, which alleviates the [[sparseness issue]] .
For [[high-status]], competitive [[out-group]]s (e.g., [[Asian]]s), the [[positive stereotype]] of their [[competence justifi]]es the overall [[system]] but acts [[jointly]] with the [[negative stereotype]] of [[low warmth]] to justify the [[in-group’s resentment]] of them.
For [[human]]s, [[understanding]] a [[natural language sentence or discourse]] is so effortless that [[we]] hardly ever think about [[it]] .
For [[information retrieval]], [[language model]]s exhibiting [[word burstiness]] are important.
For instance, a [[printing]] [[services provider]] would need from its [[customers]] [[specifications]] such as the [[size]] of [[paper]], [[type]] of [[ink]], [[proofing]] and [[perforation]] .
For instance, despite the [[competition]] for [[consumers' attention]], [[brand]]s and [[marketer]]s should [[broadcast marketing message]]s on [[social network]]s during the [[time]] of [[peak usage]] in order to [[maximize]] their [[return]]s.
For instance, in [[networks of website]]s, [[scientific paper]]s, and other [[document]]s, each [[node]] has content consisting of a [[collection of word]]s, as well as [[hyperlink]]s or [[citation]]s to other [[node]]s.
For instance, in order to [[recommend relevant content to a user]] or [[optimize for revenue]], many [[web compani]]es use [[logistic regression model]]s to [[predict the probability]] of the [[user's clicking]] on an [[item]] (e.g., [[ad]], [[news article]], job).
For instance, on [[IMDb user]]s leave [[review]]s, [[comment]]ing on different aspects of a [[movie]] (e.g.
For instance, [[small-scale]] [[user studi]]es are [[deep and rich]], but limited in terms of [[generalizability]], whereas [[large-scale]] [[web analytic studi]]es are powerful but [[negate users' motivation and context]] .
For instance, the [[rating]] that a [[customer]] gives on a [[movie]] might be one of [[do-not-bother]], [[only-if-you-must]], [[good]], [[very-good]], and [[run-to-see]] .
For instance, the [[word]] [[mouse]] can refer to a [[rodent]] or an [[electronic device]] .
For instance, [[they]] are unable to [[detect]] [[emerging]] but yet [[unlabelled]] [[research area]]s (e.g., [[Semantic Web]] before [[2000]]).
For instance, [[we]] can [[aggregate]] many [[personal hierarchies]] into a [[common]] [[taxonomy]], also known as a [[folksonomy]], that will aid [[users]] in [[visualizing]] and [[browsing]] [[social content]], and also to help them in [[organizing]] their own [[content]] .
For [[learning our model]], [[we]] propose an [[efficient algorithm]] combining [[principle]]s from [[variational inference]] and [[dynamic programming]] .
For many [[application]]s like [[Fashion]], [[image-based search]] offers a compelling [[interface]] as compared to [[text form]]s by better capturing the [[visual attribute]]s.
For many [[language technology applications]], such as [[question answering]], the overall [[system]] runs several independent [[processors]] over the [[data]] (such as a [[named entity recognizer]], a [[coreference system]], and a [[parser]]).
For minimizing [[human efforts]] and the size of [[labeled training data set]], [[we]] present a [[free-text]] [[address standardization method]] with [[latent semantic association]] ([[LaSA]]).
For [[modeling adoption of multiple item]]s, [[we]] employ [[multiple]] [[inhomogeneous Poisson process]]es, which [[share parameter]]s, such as [[influence]] for each [[user]] and [[relation]]s between [[user]]s.
For modern [[machine-learning-based targeting]], as conducted by [[Media6Degrees (M6D)]], this can mean [[scoring]] against thousands of [[model]]s in a [[large]], [[sparse feature space]] .
For more [[in-depth, general discussion]] of [[neural network]]s, the [[theory behind them]], [[advanced optimization method]]s and other [[advanced topic]]s, the [[reader]] is referred to other existing [[resource]]s.
For most [[large enterprise]]s today, [[data]] constitutes their [[core asset]], along with [[code]] and [[infrastructure]] .
For most [[online service]]s, [[incident diagnosis]] is mainly conducted by analyzing a [[large amount]] of [[telemetry data]] collected from the [[service]]s at [[runtime]] .
[[Formula]]s are constructed using four types of [[symbol]]s: [[constant]]s, [[variable]]s, [[function]]s, and [[predicate]]s.
For [[Nilsson]], [[logical formula]]s are [[indivisible constraint]]s; in contrast, [[MLNs]] are [[log-linear model]]s that use [[first-order formula]]s as [[feature template]]s, with one [[feature per grounding]] of the [[formula]] .
For now, through [[this case-study]], [[we]] have demonstrated the [[cost-effective feasibility]] of [[data-driven shallow semantic parsing]] of [[product offering title]]s (for better [[automatic hyperlink insertion]]).
For our [[analysis]] [[we]] use a [[set]] of diverse [[state-of-the-art]] [[collaborative filtering (CF) algorithms]], which include: [[SVD]], [[Neighborhood Based Approaches]], [[Restricted Boltzmann Machine]], [[Asymmetric Factor Model]] and [[Global Effects]] .
For our experiments [[we]] use [[MINIPAR]] ([[Lin, 1998]]) to parse the whole [[corpus]] due to its [[robustness]] and [[speed]] .
For [[predicting relevance]], the [[model]] that [[attributes]] all the [[CTR]] [[change]] to [[relevance]] again does [[better than]] the [[model]] that [[attributes]] the [[change]] to [[examination]] .
For [[predictive classification problem]]s, a wide variety of [[score function]]s exist, including [[measure]]s such as [[precision]] and [[recall]], the [[F measure]], [[misclassification rate]], the [[area under the ROC curve (the AUC)]], and others.
For [[quality of service]] we present a [[utility function]] to [[evaluate]] the [[cloud performance]] .
For [[small municipaliti]]es, [[the structure]] can be quite simple — for [[larger municipaliti]]es, more [[complex]] .
For some [[applications]] the [[cost]] of misclassifying a [[sample]] could be [[negligible]] .
For the actual [[computation]] of the [[score]] we [[sample models]] from the [[posterior distribution]] using an [[MCMC approach]] .
For the [[analysis]] of such [[data]], we often have to [[reveal]] [[common]] as well as [[discriminative characteristic]]s of [[document]]s with respect to their [[associated information]], e.g., [[mail-]] vs. [[female-authored document]]s, [[old]] vs. [[new document]]s, etc.
For the [[computer]], [[word form]]s in an [[online text]] are simply [[letter sequence]]s between [[blank]]s.
For the demand side, a fundamental [[technical challenge]] is to [[automate]] the [[bidding process]] based on the [[budget]], the [[campaign objective]] and various [[information gathered]] in [[runtime]] and in history.
For the experiments reported in [[this paper, we]] developed a web crawler that [[crawls]] [[retailer website]]s and [[extracts]] [[product description]]s.
For the [[harder problem]] of ego [[3-profile]]s, [[we]] introduce an [[algorithm]] that can [[estimate profile]]s of [[hundreds of thousands]] of [[vertice]]s in [[parallel]], in the [[timescale of minute]]s.
For the [[k2-degree anonymization problem]], [[we]] propose an [[Integer Programming formulation]] to find [[optimal solution]]s in [[small-scale]] [[network]]s.
For the [[kernel function]], [[we]] use the [[2-nd polynomial function]] .
For the [[local collective classification model]], [[we]] use [[Iterative Classification Algorithm]] ([[ICA]]) ([[Neville and Jensen 2000]]; [[Lu and Getoor 2003a]] ], and, for the [[global collective classification model]], [[we]] use a [[pairwise Markov Random Fields]] ([[MRF]]) based on the [[relational Markov network]] of [[Taskar et al. [2002]] ].
For the [[personality trait]] "[[Openness]]," [[prediction accuracy]] is close to the [[test-retest accuracy]] of a standard [[personality test]] .
For the [[prediction]] of [[human behavior]]s, the proposed [[FEMA]] significantly [[outperform]]s other [[state-of-the-art]] [[baseline method]]s by 17.4%.
For the [[rare case]]s in which we want indicate the [[power]] of a [[matrix]] or a [[vector]], a [[pair]] of [[bracket]]s is added around the item to be [[exponentiated]] <math>(\mathbf{W})^2, (\mathbf{W^3})^2</math>.
For the sake of [[comparison]], [[we]] simulated a naive [[IP-based filter]] that does not consider the [[size]]s of the [[IP]]s.
For the task of [[affect recognition]], [[we]] also show that using [[non-expert]] [[labels]] for [[training machine learning algorithm]]s can be as effective as using [[gold standard annotations]] from [[experts]] .
For the [[this category of collective models]], each [[object]] is described as a [[vector]] of its [[local attributes]] ''X''<sub>i</sub> and an [[aggregation]] of [[attributes]] and [[labels]] of its [[neighbors]] .
For thirty years a combination of the [[IMF]], the [[World Trade Organization (WTO)]], the [[financial institution]]s that came out of [[Bretton Woods]], the [[investment bank]]s, the [[multinational]]s, and the [[international NGO]]s has constituted an [[international bureaucracy]] of [[global scope]] .
For [[this kind of problem]], [[recommendation fairness]] and [[driving efficiency]] are two [[fundamental aspect]]s.
For this [[parallel training]], [[we]] use [[mini-batch asynchronous gradient descent]] with an [[adaptive learning rate procedure]] called [[Adagrad]] [ 7].
For [[this problem]], we not only identify a [[proper small group]] of [[individual]]s as [[seed]]s for [[promotion]] but also determine the [[pricing]] of the [[commodity]] .
For this problem [[we]] propose [[WTFW (" Who to Follow and Why ")]], a [[stochastic topic model]] for [[link prediction]] over [[directed]] and [[nodes-attributed graph]]s.
For this purpose, it is convenient to view [[chunking]] as a [[tagging problem]] by [[encoding]] the [[chunk]] [[structure]] in new [[tags]] attached to each [[word]] .
For this purpose, [[we]] propose to build the [[summary]] based upon a novel [[formulation]], <i>[[conditional profile]]</i> (or <i>[[c-profile]]</i>).
For this reason, [[classical]] [[AI]] adopted [[first-order logic]] — the [[mathematics]] of [[object]]s and [[relation]]s — as [[classical]] [[its]] foundation.
For this [[reason]], there is a [[strong demand]] to develop an [[objective model]] to [[quantify]] [[patent quality]] and characterize the [[attribute]]s that lead to [[higher-quality]] [[patent]]s.
For this [[reason]], we turn to [[statistical]], rather than [[combinatorial]], [[analysis]] .
For training, 32 [[fully]] and 500 [[partially annotated]] [[article]]s were prepared.
[[Fortunately]], in [[real-world application]]s, [[user]]s can answer questions from multiple [[historical task]]s.
For [[word–context VSMs]], [[we]] explore the [[Semantic Vectors package]], which builds on [[Lucene]] .
[[Fourth-generation universal robot]]s with a [[human-like]] [[100 million]] [[MIPS]] will be able to [[abstraction task\abstract]] and [[generalize]] .
[[Fran]] has been implemented in [[Hug]]s, yielding surprisingly good [[performance]] for an [[interpreter-based system]] .
Fredrickson & T Roberts, 1997]]) posits that [[American culture]] [[socialize]]s [[women]] to adopt [[observer]]s' [[perspective]]s on their [[physical selv]]es.
[[Frequent Itemset]]s, [[Cartesian Product]], [[Set Cover]], [[Concise Pattern Representation]]
Frequently [[cited examples]] include [[the cell]], a [[network]] of [[chemicals]] linked by [[chemical reactions]], and [[the Internet]], a [[network]] of [[routers]] and [[computers]] connected by [[physical links]] .
[[Frequent pattern mining]] often produces an [[enormous number]] of [[frequent pattern]]s, which imposes a great [[challenge]] on [[understanding]] and further [[analysis]] of the [[generated pattern]]s.
[[Frequent patterns]] are [[itemsets]], [[subsequence]]s, or [[substructures]] that appear in a [[data set]] with [[frequency]] noa [[user-specified threshold]] .
[[Frequent substructure]]s of these [[causality tree]]s reveal not only [[recurring interaction]]s among [[spatio-temporal outlier]]s, but potential [[flaw]]s in the design of existing [[traffic network]]s.
[[Fringe benefit]]s: [[Employee-related cost]]s other than [[salary]], e.g., [[insurance]] and [[retirement cost]]s.
From [[2007]] to [[2009]], the nation suffered the worst [[recession]] since the [[Great Depression]] .
From [[2009]] to [[2012]], [[average]] [[real income per family]] grew modestly by 6.0% (Table 1).
From a [[foundational perspective]], [[RandNLA]] has its roots in [[theoretical computer science (TCS)]], with [[deep connection]]s to [[mathematic]]s ([[convex analysis]], [[probability theory]], [[metric embedding theory]]) and [[applied mathematics]] ([[scientific computing]], [[signal processing]], [[numerical linear algebra]]).
From a [[theoretical perspective]], [[we]] give the first [[analysis]] that shows several [[clustering algorithm]]s are in [[MRC0]], a [[theoretical MapReduce class]] introduced by [[Karloff et al. (26)]] .
From [[drinking coffee]] to [[losing weight]], from [[buying a car]] to [[choosing a romantic partner]], [[we]] consistently [[overpay]], [[underestimate]], and [[procrastinate]] .
From each [[node]]'s viewpoint, the [[graph]] is just a [[union]] of its [[private graph]] and the [[public graph]] .
From [[lift]]s to cars to [[airliner]]s to [[smartphone]]s, [[modern civilisation]] is powered by [[software]], the [[digital instruction]]s that allow [[computer]]s, and the [[device]]s they [[control]], to perform [[calculation]]s and respond to [[their]] [[surrounding]]s.
From our extensive [[evaluation]]s, [[MLGF-MF]] significantly [[outperform]]s (or [[converges faster]] than) the [[state-of-the-art algorithm]]s in both [[shared-memory]] and [[block-storage environment]]s.
From [[publicly available databases]] that [[document]] past [[vulnerabilities]], [[we]] show how to [[train classifiers]] that [[predict]] whether and how soon a [[vulnerability]] is likely to be [[exploited]] .
From [[remote sensing perspective]], [[informal settlement]]s share unique [[spatial characteristic]]s that distinguish them from other [[urban structure]]s like [[industrial]], [[commercial]], and [[formal residential settlement]]s.
From [[social network]]s to [[P2P system]]s, [[network sampling]] arises in many [[setting]]s.
From the [[CTR]] of an [[ad]], and the [[discounting curve]] <math>p(seen \vert pos)</math>, [[we]] can then [[estimate the probability]] an [[ad]] would be [[clicked]] at any [[position]] .
From these [[candidate]]s we select the most [[confidently predicted]] [[novel term]]s for review by [[crowd-sourced annotator]]s.
From these insights and [[our algorithm]]s [[we]] construct new [[method]]s of [[generating]] and [[pruning constraint]]s and [[empirically]] demonstrate their usefulness.
From the [[text graph]] [[state-of-the-art features]] such as [[named-entity features]] and [[syntactic features]] are produced for each [[argument]] [[pairing]] .
From the viewpoint of the [[minimum description length (MDL) principle]], [[we]] propose an [[algorithm]] that [[tracks changes]] of [[clustering structure]]s so that the [[sum]] of the [[code-length]] for [[data]] and that for [[clustering change]]s is [[minimum]] .
From [[this method]] [[we]] derive an [[online-update algorithm]] for [[RKMF model]]s that allows to solve the [[new-user / new-item problem]] .
From this viewpoint, [[generative]] and [[discriminative model]]s correspond to specific choices for [[the prior]] over [[parameter]]s.
From [[Twitter]] to [[Facebook]] to [[Reddit]], [[user]]s have become accustomed to [[sharing]] the [[article]]s they read with [[friend]]s or [[follower]]s on their [[social network]]s.
[[Frugality]], for example, served [[Japan]] well until its recent [[prolonged recession]]; now it is an [[obstacle]] to [[recovery]] .
[[Function estimation]]/[[approximation]] is viewed from the perspective of [[numerical optimization]] in [[function space]], rather than [[parameter space]] .
[[Fund accounting]] is an overlay on [[accounting]], which is also done by [[account]], [[object]] [[class]] and [[object]] [[code]] .
[[Fund]]: An [[accounting entity]] in [[MMARS]] to which all [[receipt]]s are [[credited and against]] which all [[expenditure]]s and other liabilities are charged.
[[Fund Structure]] — The [[Advisory Committee]] was sensitive to the state’s interest in [[comparability]] and recognized the [[value]] of requiring that [[local government]]s consistently account for some [[activiti]]es in [[specified fund]]s.
Further [[analysis]] using [[large-scale]] [[datasets]] yields insight into important [[properties]] of the proposed [[descriptor]]s, such as the [[dataset]] [[coverage]] and the [[class]] size [[represented by]] each [[descriptor]] .
Further experiments on [[our experimentally generated data]] from [[patient]] [[blood sample]]s using a novel [[SOMAmer (Slow Off-rate Modified Aptamer) technology]] show that, [[FHIM]] performs [[blood-based cancer diagnosis]] and [[bio-marker discovery]] for [[Renal Cell Carcinoma]] much better than other [[competing method]]s, and it identifies interpretable [[block-wise]] [[high-order gene interactions predictive]] of [[cancer stage]]s of [[sample]]s.
Further, [[it]] describes a method for [[analysing]] and [[coding sentence]]s, which is applied to specimen [[text]]s.
Furthermore, based on the observation that [[SimRank]] is essentially a [[first-order Markov Chain]], [[we]] propose to utilize the [[iterative aggregation technique]]s for [[uncoupling Markov chains]] to [[compute]] [[SimRank scores]] in [[parallel]] for [[large graphs]] .
Furthermore, between the [[two]] [[proposed algorithm]]s, the [[greedy algorithm]] is more [[efficient]] than the [[DC programming]], while they achieve comparable [[accuracy]] .
Furthermore, by inheriting the [[objective function]] from [[spectral clustering]] and [[explicitly encoding]] the [[constraints]], much of the [[existing analysis]] of [[spectral clustering technique]]s is still valid.
Furthermore, comparisons between our [[offline replay]] and [[online bucket evaluation]] of several [[contextual bandit algorithm]]s show [[accuracy]] and [[effectiveness]] of our [[offline evaluation method]] .
Furthermore, [[experimental result]]s on several [[public dataset]]s demonstrate that [[CDAE]] consistently outperforms [[state-of-the-art]] [[top-N recommendation method]]s on a variety of common [[evaluation metric]]s.
Furthermore, in many [[applications]], the underlying [[network]] over which the [[diffusions]] and [[propagations]] spread is actually [[unobserved]] .
Furthermore, in order to [[model]] the [[uncertainty]] in the travel [[cost]], [[we]] further introduce a [[Gaussian prior]] into the [[cPMF model]] and develop the [[GcPMF model]], where the [[Gaussian prior]] is used to express the [[uncertainty]] of the [[travel cost]] .
Furthermore, in virtually every [[domain]], [[text]]s contain [[lexical variation]]s that are often missing from [[dictionari]]es, such as [[acronym]]s, [[abbreviation]]s, [[spelling variant]]s, [[informal shorthand term]]s (e.g., “abx” for “antibiotics”), and [[composite term]]s (e.g., “may-december” or “virus/worm”).
Furthermore, [[ITMC]] employs a [[regularization]] [[term]] derived from the preference for [[high]] [[compression rate]], which is critical to the [[precision]] of [[detection]] .
Furthermore, it [[runs in time]] proportional to the [[number]] of [[instance]]s, which is very [[efficient]] for [[large scale data sets]] .
Furthermore, on [[data set]]s that are sufficiently [[large]], [[our algorithm]]s are faster than the other [[parallel algorithm]]s that we [[tested]] .
Furthermore, [[our design]] facilitates [[multi-objective sample]]s, which provide [[tight estimate]]s for a specified [[set]] of [[statistics]] using a single smaller [[sample]] .
Furthermore, [[our method]] can learn [[true top-K shapelet]]s by [[capturing]] their [[interaction]] .
Furthermore, our [[method]] can suggest appropriate [[pre-processing]] / [[normalization of the data]] to [[improve]] the [[results]] of [[community identification]] .
Furthermore, [[our method]] has been deployed in a [[real-life setting]] where the [[partner]] [[electric company]] runs a [[targeted campaign]] for 292, 496 [[customer]]s.
Furthermore, [[our methods]] provide an [[understandable description]] of the [[discovered cluster]]s by their [[frequent term set]]s.
Furthermore, [[RainMon]] integrates the full [[analysis process]] from [[data storage]] to the [[user interface]] to provide accessible [[long-term diagnosis]] .
Furthermore, [[robust]] [[estimation]] and [[outlier detection methods]] are designed for the new [[<i>GLS</i> model]] .
Furthermore, the [[collaboration]] among a [[group of specialists]] is not supported sufficiently in many ([[industrial]]) [[system]]s concerning the [[distributed development]] of a [[knowledge base]] .
Furthermore, the [[economic model]] of many [[web service]]s is allowing [[free access]] to most [[content]], and [[generating]] [[revenue]] through [[advertising]] .
Furthermore, the [[prediction accuracy]] can be significantly improved by [[learning]] a [[supervised classifier]] based on combined [[mobility]] and [[network measure]]s.
Furthermore, the resulting [[phenotype]]s and [[baseline characteristic]]s from [[real]] [[EHR]] [[data]] are consistent with known [[characteristic]]s of the [[patient population]] .
Furthermore, they demonstrate that [[random sampling]] and partitioning enable [[CURE]] to not only [[outperform]] existing [[algorithm]]s but also to scale well for [[large database]]s without sacrificing [[clustering]] [[quality]] .
Furthermore, [[they]] usually [[quantify]] the [[popularity]] of a [[topic]] simply in terms of the number of [[related publication]]s or [[author]]s for each [[year]]; hence they can provide good [[forecast]]s only on [[trend]]s which have existed for at least 3-4 years.
Furthermore, [[this decomposition]] creates a [[large number]] of [[matrice]]s that need to be [[transferred]] and combining them in the [[target domain]] is [[non-trivial]] .
Furthermore, [[we]] analyze its [[error bound]], which improves over existing [[result]]s of [[related method]]s.
Furthermore, [[we]] define [[constraint]]s reflecting [[communities' magnet properti]]es to [[regularize the model]] .
Furthermore, [[we]] [[demonstrate empirically]] that [[our algorithm]] can successfully adapt to [[user preference]]s.
Furthermore, [[we]] demonstrate that [[SPG-GMKL]] can scale well beyond [[gradient descent]] to large [[problem]]s involving a million [[kernel]]s or half a million [[data point]]s.
Furthermore, we develop [[kernel method]]s for [[estimating]] the [[optimal group transformation]]s based on [[cross-covariance]] and [[conditional covariance operator]]s.
Furthermore, [[we]] employed [[multi-task learning method]] to [[fine-tune]] the [[pre-trained model]]s with [[label]]ed [[ISH image]]s, and also [[extract]]ed [[feature]]s from the [[fine-tune]]d [[model]]s.
Furthermore, [[we]] employ [[GIGATENSOR]] in order to analyze a [[very large real world]], [[knowledge base tensor]] and present our astounding findings which include [[discovery]] of potential [[synonym]]s among [[million]]s of [[noun-phrase]]s (e.g.
Furthermore, we exploit the [[structure]] of [[the model]] [[update]]s to achieve an [[order of magnitude]] (up to 40X) [[speedup]] in [[our experiment]]s without resorting to [[approximation]]s.
Furthermore, we extend [[our approach]] to scenarios where the [[dependenci]]es between the [[class-label]]s are [[encod]]ed in the form of a [[graph]] rather than a [[hierarchy]] .
Furthermore, [[we]] find that [[tension release]], [[social integrative]] and [[affective motivation]]s are positively associated with how many [[hours people]] watch [[stream]]s.
Furthermore, [[we]] introduce three different [[algorithmic paradigm]]s – [[contextual prefiltering]], [[post-filtering, and modeling]] – for incorporating [[contextual information]] into the [[recommendation process]], discuss the possibilities of combining several [[context]] - aware [[recommendation technique]]s into a [[single unifying approach]], and provide a [[case study]] of one such combined [[approach]] .
Furthermore, [[we]] plan to extend the [[representational framework]] for representing [[experiment]]s for [[mining structured data]] in the context of [[experiment database]]s.
Furthermore, [[we]] present a [[margin-based]] [[censored regression algorithm]] that combines the concept of [[margin-based classifiers]] with [[censored regression]] to achieve a better [[concordance index]] than the [[Cox model]] .
Furthermore [[we]] propose a [[general strategy]] for [[efficiently]] [[detecting]] the new [[outlier class]]es.
Furthermore, [[we]] propose a [[unified modeling approach]] to simultaneously [[model]] [[topical aspects of papers]], [[author]]s, and [[publication venue]]s.
Furthermore, [[we]] show how to use [[ensemble methods]] for [[blending predictors]] in order to [[outperform]] a single [[blending algorithm]] .
Furthermore, [[we]] show that [[FHIM]] has [[oracle]] properties when extended to [[generalized linear regression model]]s with [[pairwise interaction]]s.
Furthermore, [[we]] take the [[ordinal nature]] of the [[preference]]s into consideration and propose an [[ordinal cost]] to optimize [[CF-NADE]], which shows [[superior performance]] .
Furthermore, [[we]] use an [[event matrix based representation]] that can encode quantitatively all key [[temporal concept]]s including [[order]], [[concurrency]] and [[synchronicity]] .
Further, [[NetClus]] generates [[informative cluster]]s, presenting good [[ranking]] and [[cluster membership information]] for each [[attribute object]] in each [[net-cluster]] .
Further, since the audience is more restricted, [[informal documents]] often use [[group-]] and [[task-specific]] [[abbreviations]] and are not [[self-contained]] .
Further [[technical difference]]s in the [[search algorithm]], [[training procedure]] and [[network architecture]] are described in [[Methods]] .
Further, [[the article]] said [[MLNs]] [[perform inference]] by applying [[MCMC]] to a [[ground network]], but several [[lifted inference algorithm]]s for them exist.
Further, there are a [[large number]] of [[lexical resource]]s available representing a [[wealth]] of [[linguistic information]], but this [[data]] exists in various [[format]]s and is difficult to [[link to ontologi]]es and other [[resource]]s.
Further, to better manage resources such as [[lighting]] and [[HVAC]], [[we]] propose a [[semi-supervised approach]] combining hidden [[Markov models (HMM)]] and a standard [[classifier]] to model [[occupancy]] based on readily available [[port-level network statistic]]s.
Further, we build an [[interactive]] [[web-based system]] for [[answering queri]]es regarding the [[moving pattern]]s of the [[tourist]]s, which can be used by [[stakeholder]]s to gain insight into [[tourists' travelling behavior]]s in [[Singapore]] .
Further, [[we]] conduct a [[task-based evaluation]] -- two [[basic framework]]s for [[collaboration prediction]] are considered with the [[circle information]] (obtained from [[our model]]) included in the [[feature set]] .
Further work on [[coupled behavior analysis]], including [[coupled sequence]]/[[event analysis]], [[hidden group analysis]] and [[behavior dynamics]] are very critical.
[[Fused Lasso]], [[ℓ1 regularization]], [[Author Keyword::restart]], [[Author Keyword::subgradient]]
[[Game theory]] began with questions regarding [[optimal strategi]]es in [[card game]]s and [[chess]], later developed into a [[formal system]] by [[von Neumann]] .
[[Gaussian processes (GPs)]] provide a [[principled]], [[practical]], [[probabilistic approach]] to [[learning in kernel machines]] .
[[Gaussian process]], [[probabilistic regression]], [[sparse approximation]], [[Bayesian committee machine]] .
[[Gaussian process]], [[upper confidence bound]], [[batch]], [[active learning]], [[regret-bound]] .
[[Gaussians innately]] represent uncertainty, and provide a [[distance function]] [[per]] [[object]] .
[[GBASE]] provides a [[parallel indexing mechanism]] for [[graph mining operation]]s that both saves [[storage space]], as well as accelerates [[queri]]es.
[[GDP]] [[measure]]s the [[total]] of [[goods and service]]s [[produced]] in a given [[year]] within the [[borders of a given country]] .
[[Gene Expression Pattern]], [[Image Annotation]], [[Bag-of-words]], [[Sparse Learning]], [[Regularization]]
[[General game player]]s are [[computer system]] s able to [[play strategy game]]s based solely on [[formal game description]]s supplied at “[[runtime]]”.
[[General game playing expertise]] depends on [[intelligence]] on the part of the [[game player]] and not just [[intelligence]] of the [[programmer]] of the [[game player]] .
[[Generalized linear model]]s with [[nonlinear feature transformation]]s are widely used for [[large-scale regression]] and [[classification problem]]s with [[sparse]] [[input]]s.
[[General Law]]s: [[The codified collection of laws passed]] by the [[Legislature]], [[organized]] into [[Chapter]]s and Sections by subject.
[[Generative]], [[discriminative]], [[Bayesian inference]], [[semi-supervized]], [[unlabelled data]], [[machine learning]] .
[[Generative model]]s produce [[system]] responses that are [[autonomously generate]]d [[word-by-word]], opening up the possibility for [[realistic]], [[flexible interaction]]s.
: [[Genes]] and [[proteins]] are often associated with [[multiple names]] .
[[GENETAG Train]], [[Test]] and [[Round1 data]] and [[ancillary program]]s are freely available at ftp://ftp.ncbi.nlm.nih.gov/pub/tanabe/GENETAG.tar.gz.
[[GENIA corpus version 3.0]] consisting of 2000 [[MEDLINE abstract]]s has been released with more than 400 000 [[words]] and almost [[100]] 000 [[annotations]] for [[biological term]]s.
[[Genome-wide association studies (GWAS)]] have become a popular [[method]] for [[analyzing]] [[set]]s of [[DNA sequence]]s in order to discover the [[genetic basis]] of [[disease]] .
George]], [[C p Criterion]], [[Categorical Data]], [[Category]], [[Cauchy Distribution]], [[Causal Epidemiology]], [[Cause and Effect in Epidemiology]], [[Census]], [[Central Limit Theorem]], [[Chebyshev's Inequality]], [[Chebyshev, Pafnutii Lvovich]], [[Chi-Square Distance]], [[Chi-Square Table]], [[Chi-Square Test]], [[Chi-square Distribution]], [[Chi-square Goodness of Fit Test]], [[Chi-square Test of Independence]], [[Classification]], [[Cluster Analysis]], [[Cluster Sampling]], [[Coefficient of Determination]], [[Coefficient of Kurtosis]], [[Coefficient of Skewness]], [[Coefficient of Variation]], [[Collinearity]], [[Combination]], [[Combinatory Analysis]], [[Compatibility]], [[Complementary]], [[Complete Linkage Method]], [[Completely Randomized Design]], [[Composite Index Number]], [[Conditional Probability]], [[Confidence Interval]], [[Confidence Level]], [[Contingency Table]], [[Continuous Distribution Function]], [[Continuous Probability Distribution]], [[Contrast]], [[Convergence]], [[Convergence Theorem]], [[Correlation Coefficient]], [[Correspondence Analysis]], [[Covariance]], [[Covariance Analysis]], [[Covariation]], [[Cox, David R.]], [[Cox, Mary Gertrude]], [[Cramér, Harald]], [[Criterion Of Total Mean Squared Error]], [[Critical Value]], [[Cyclical Fluctuation]], [[Daniels, Henry E.]], [[Data]], [[Data Analysis]], [[Data Collection]], [[De Moivre, Abraham]], [[De Moivre–Laplace Theorem]], [[Decile]], [[Degree of Freedom]], [[Deming, W. Edwards]], [[Demography]], [[Dendrogram]], [[Density Function]], [[Dependence]], [[Dependent Variable]], [[Descriptive Epidemiology]], [[Descriptive Statistics]], [[Design of Experiments]], [[Determinant]], [[Deviation]], [[Dichotomous Variable]], [[Dichotomy]], [[Discrete Distribution Function]], [[Discrete Probability Distribution]], [[Discrete Uniform Distribution]], [[Dispersion]], [[Distance]], [[Distance Table]], [[Distribution Function]], [[Dot Plot]], [[Durbin–Watson Test]], [[Econometrics]], [[Edgeworth, Francis Y.]], [[Efron, Bradley]], [[Eigenvalue]], [[Eigenvector]], [[Epidemiology]], [[Error]], [[Estimation]], [[Estimator]], [[Event]], [[Expected Value]], [[Experiment]], [[Experimental Unit]], [[Exploratory Data Analysis]], [[Exponential Distribution]], [[Extrapolation]], [[Factor]], [[Factor Level]], [[Factorial Axis]], [[Factorial Experiment]], [[Fisher Distribution]], [[Fisher Index]], [[Fisher Table]], [[Fisher Test]], [[Fisher, Irving]], [[Fisher, Ronald Aylmer]], [[Forecasting]], [[Fractional Factorial Design]], [[Frequency]], [[Frequency Curve]], [[Frequency Distribution]], [[Frequency Polygon]], [[Frequency Table]], [[Galton, Francis]], [[Gamma Distribution]], [[Gauss, Carl Friedrich]], [[Gauss–Markov Theorem]], [[Generalized Inverse]], [[Generalized Linear Regression]], [[Generation of Random Numbers]], [[Genetic Statistics]], [[Geometric Distribution]], [[Geometric Mean]], [[Geometric Standard Deviation]], [[Geostatistics]], [[Gini Index]], [[Gini, Corrado]], [[Goodness of Fit Test]], [[Gosset, William Sealy]], [[Graeco-Latin Square Design]], [[Graeco-Latin Squares]], [[Graphical Representation]], [[Harmonic Mean]], [[Hat Matrix]], [[Histogram]], [[Homogeneity Test]], [[Hotelling, Harold]], [[Huber, Peter J.]], [[Hypergeometric Distribution]], [[Hypothesis]], [[Hypothesis Testing]], [[Hájek, Jaroslav]], [[Idempotent Matrix]], [[Identity Matrix]], [[Incidence]], [[Incidence Rate]], [[Incomplete Data]], [[Independence]], [[Independent Variable]], [[Index Number]], [[Indicator]], [[Inertia]], [[Inertia Matrix]], [[Inference]], [[Inferential Statistics]], [[Inner Product]], [[Interaction]], [[Interquartile Range]], [[Interval]], [[Inverse Matrix]], [[Inversion]], [[Irregular Variation]], [[Jackknife Method]], [[Jeffreys, Harold]], [[Joint Density Function]], [[Joint Distribution Function]], [[Joint Probability Distribution Function]], [[Kendall Rank Correlation Coefficient]], [[Kendall, Maurice George]], [[Kiefer, Jack Carl]], [[Kolmogorov, Andrei Nikolaevich]], [[Kolmogorov–Smirnov Test]], [[Kruskal-Wallis Table]], [[Kruskal-Wallis Test]], [[L 1 Estimation]], [[Lagrange Multiplier]], [[Laplace Distribution]], [[Laplace, Pierre Simon De]], [[Laspeyres Index]], [[Latin Square]], [[Latin Square Designs]], [[Law of Large Numbers]], [[Least Absolute Deviation Regression]], [[Least Significant Difference Test]], [[Least Squares]], [[Least-Squares Method]], [[Legendre, Adrien Marie]], [[Lehmann, Erich]], [[Level of Significance]], [[Leverage Point]], [[Likelihood Ratio Test]], [[Line Chart]], [[Linear Programming]], [[Logarithmic Transformation]], [[Logistic Regression]], [[Lognormal Distribution]], [[Longitudinal Data]], [[Mahalanobis Distance]], [[Mahalanobis, Prasanta Chandra]], [[Main Effect]], [[Mann–Whitney Test]], [[Marginal Density Function]], [[Marginal Distribution Function]], [[Markov, Andrei Andreevich]], [[Matrix]], [[Maximum Likelihood]], [[Mean]], [[Mean Absolute Deviation]], [[Mean Squared Error]], [[Measure of Central Tendency]], [[Measure of Dispersion]], [[Measure of Dissimilarity]], [[Measure of Kurtosis]], [[Measure of Location]], [[Measure of Shape]], [[Measure of Similarity]], [[Measure of Skewness]], [[Median]], [[Median Absolute Deviation]], [[Method of Moments]], [[Missing Data]], [[Missing Data Analysis]], [[Mode]], [[Model]], [[Moment]], [[Monte Carlo Method]], [[Moving Average]], [[Multicollinearity]], [[Multimodal Distribution]], [[Multinomial Distribution]], [[Multiple Correlation Coefficient]], [[Multiple Linear Regression]], [[Negative Binomial Distribution]], [[Nelder, John A.]], [[Newcomb, Simon]], [[Neyman, Jerzy]], [[Nonlinear Regression]], [[Nonparametric Statistics]], [[Nonparametric Test]], [[Norm of a Vector]], [[Normal Distribution]], [[Normal Equations]], [[Normal Probability Plot]], [[Normal Table]], [[Normalization]], [[Null Hypothesis]], [[Observation]], [[Odds and Odds Ratio]], [[Official Statistics]], [[Ogive]], [[Olkin, Ingram]], [[One-Sided Test]], [[One-Way Analysis of Variance]], [[Operations Research]], [[Optimal Design]], [[Optimization]], [[Outlier]], [[Paasche Index]], [[Pair of Random Variables]], [[Paired Student's T-Test]], [[Panel]], [[Parameter]], [[Parametric Test]], [[Partial Autocorrelation]], [[Partial Correlation]], [[Partial Least Absolute Deviation Regression]], [[Partial Least-Squares Regression]], [[Pearson, Egon Shape]], [[Pearson, Karl]], [[Percentage]], [[Percentile]], [[Permutation]], [[Pictogram]], [[Pie Chart]], [[Pitman, Edwin James George]], [[Point Estimation]], [[Poisson Distribution]], [[Poisson, Siméon-Denis]], [[Pooled Variance]], [[Population]], [[Prevalence]], [[Prevalence Rate]], [[Probability]], [[Probability Distribution]], [[Probability Function]], [[Q-Q Plot (Quantile to Quantile Plot)]], [[Qualitative Categorical Variable]], [[Quantile]], [[Quantitative Graph]], [[Quantitative Variable]], [[Quartile]], [[Quetelet, Adolphe]], [[Quota Sampling]], [[Random Experiment]], [[Random Number]], [[Random Number Generation]], [[Random Variable]], [[Randomization]], [[Randomized Block Design]], [[Range]], [[Rank of a Matrix]], [[Rao, Calyampudi Radhakrishna]], [[Regression Analysis]], [[Rejection Region]], [[Relation]], [[Relative Risk]], [[Resampling]], [[Residual]], [[Rice, Stuart Arthur]], [[Ridge Regression]], [[Risk]], [[Robust Estimation]], [[Sample]], [[Sample Size]], [[Sample Space]], [[Sampling]], [[Sampling Distribution]], [[Scatterplot]], [[Scheffé, Henry]], [[Seasonal Index]], [[Seasonal Variation]], [[Secular Trend]], [[Semilogarithmic Plot]], [[Serial Correlation]], [[Sign Test]], [[Significance Level]], [[Simple Index Number]], [[Simple Linear Regression]], [[Simple Random Sampling]], [[Simulation]], [[Snedecor, George Waddel]], [[Spatial Data]], [[Spatial Statistics]], [[Spearman Rank Correlation Coefficient]], [[Spearman Table]], [[Standard Deviation]], [[Standard Error]], [[Standardized Data]], [[Statistic]], [[Statistical Software]], [[Statistical Table]], [[Statistics]], [[Stem and Leaf]], [[Stem-And-Leaf Diagram]], [[Stochastic Process]], [[Stratified Sampling]], [[Student]], [[Student Distribution]], [[Student Table]], [[Student Test]], [[Sure Event]], [[Survey]], [[Systematic Sampling]], [[Target Population]], [[Test of Independence]], [[Time Series]], [[Trace]], [[Transformation]], [[Transpose]], [[Treatment]], [[Tukey, John Wilder]], [[Two-Sided Test]], [[Two-way Analysis of Variance]], [[Type I Error]], [[Type II Error]], [[Uniform Distribution]], [[Unimodal Distribution]], [[Value]], [[Variable]], [[Variance]], [[Variance Analyses]], [[Variance of a Random Variable]], [[Variance–Covariance Matrix]], [[Vector]], [[Von Mises, Richard]], [[Weighted Arithmetic Mean]], [[Weighted Least-Squares Method]], [[Wilcoxon Signed Table]], [[Wilcoxon Signed Test]], [[Wilcoxon Table]], [[Wilcoxon Test]], [[Wilcoxon, Frank]], [[Willcox, Walter Francis]], [[Yates' Algorithm]], [[Youden, William John]], [[Yule and Kendall Coefficient]], [[Yule, George Udny]], [[p-Value]] .
[[Geo-SAGE]] considers both [[user personal interest]]s and the [[preference]] of the [[crowd]] in the [[target region]], by exploiting both the [[co-occurrence pattern]] of [[spatial item]]s and the [[content]] of [[spatial item]]s.
([[Ghani 2006]]) used [[EM]] to [[add more training data from unlabeled data]], while in [[our approach]] [[bootstrapping]] is used to expand the [[seed list]] .
[[Gibbs sampling]] is a [[workhorse]] for [[Bayesian inference]] but has several [[limitation]]s when used for [[parameter estimation]], and is often much slower than [[non-sampling inference method]]s.
[[GIGATENSOR]] exploits the [[sparseness]] of the [[real world tensor]]s, and avoids the [[intermediate data explosion problem]] by carefully [[redesigning]] the [[tensor decomposition algorithm]] .
[[GIM-V]] is highly [[optimized]], achieving (a) good [[scale-up]] on the [[number]] of available [[machines]] (b) [[linear running time]] on the number of [[edges]], and (c) more than 5 times faster [[performance]] over the [[non-optimized version]] of [[GIM-V]] .
[[GiveDirectly]] is a [[non-governmental organization (NGO)]] at the vanguard of delivering this [[proven]] and [[effective approach]] to reducing [[poverty]] .
Given a [[book]], we extract [[important]] [[concept]]s in each [[book chapter]] using [[Wikipedia]] as a resource and from this construct a [[concept hierarchy]] for that [[book]] .
Given a [[budget]] on the [[cumulative cost]] of the selected [[item]]s, how can we pick a [[subset]] of [[maximal value]]?
Given a [[classifier]] and an [[instance]], there are four possible [[outcome]]s.
Given a [[collection]] of [[documents]], [[TIARA]] first uses [[topic analysis technique]]s to [[summarize]] the [[documents]] into a [[set]] of [[topics]], [[each of which]] is [[represented by]] a [[set]] of [[keywords]] .
Given a [[directed graph]] of [[million]]s of [[node]]s, how can we [[automatically spot]] [[anomalous]], [[suspicious node]]s, judging only from their [[connectivity pattern]]s?
Given a [[forum]], the [[high-quality]] '[[thread-title, reply]]' [[pair]]s are extracted using a [[cascaded framework]] .
Given a [[graph]], how can we [[extract]] [[good feature]]s for the [[node]]s?
Given a [[large collection]] of [[epidemiological data]] consisting of the count of d [[contagious disease]]s for l [[location]]s of [[duration]] n, how can we find [[pattern]]s, [[rule]]s and [[outlier]]s?
Given a [[large]], [[general-purpose knowledge base]] and a [[large corpus of web documents]], the goal is to [[consolidate information]] from both sources to [[jointly extract]] [[material to answer]] the given [[query]] with an [[automatically compiled]] [[query-specific Wikipedia]] .
Given a long [[video sequence]], [[moving pixels are clustered]] into different [[atomic activiti]]es and [[short video clip]]s are [[clustered]] into different [[interaction]]s.
Given an [[entity]] in a [[source domain]], [[finding]] its [[matched entiti]]es from another [[(target) domain]] is an important [[task]] in many [[application]]s.
Given an [[estimator that is unbiased]], [[we]] focus on [[minimizing]] the [[variance]] .
Given an [[input]] [[document]], the system identifies the [[important]] [[concepts in the text]] and [[automatically]] [[links]] these [[concepts]] to the corresponding [[Wikipedia pages]] .
Given an input [[sentence]] and a [[target word]] and [[frame]], [[the system]] labels [[constituent]]s with either [[abstract semantic role]]s, such as [[AGENT]] or [[PATIENT]], or more [[domain-specific semantic role]]s, such as [[SPEAKER]], [[MESSAGE]], and [[TOPIC]] .
Given an [[unknown set]] of [[objects embedded]] in the [[Euclidean plane]] and a [[nearest-neighbor oracle]], how to [[estimate]] the [[set size]] and other [[properties of the object]]s?
Given any [[generative classifier]] based on an [[inexact]] [[density model]], [[we]] can define a [[discriminative counterpart]] that reduces its [[asymptotic error rate]] .
Given a [[pre-trained]] [[set of word embeddings]] [[trained from]] [[contexts]], there is a simple way to construct variances using the [[empirical variance]] of a [[word type]]’s [[set of context vectors]] .
Given a [[quarter]] of [[petabyte]] [[click log data]], how can [[we]] [[estimate]] the [[relevance]] of each [[URL]] for a given [[query]]?
Given a [[query disease]], how can we prioritize its [[candidate]] [[gene]]s by incorporating the [[tissue-specific]] [[protein interaction network]]s of those similar [[disease]]s?
Given a [[query]] issued by the [[user]], [[the system]], apart from returning the result [[pattern]]s, also proposes a [[number of specialization]]s (i.e., [[supergraph]]s) of the [[original query]] to facilitate the [[exploration]] of [[the results]] .
Given a [[query]], our [[system]] predicts the [[expected reward]] for each [[vertical]], then organizes the [[search result page (SERP)]] in a way which [[maximize]]s the [[total reward]] .
Given a [[real]], and [[weight]]ed [[person-to-person]] [[network]] which [[changes over time]], what can we say about the [[clique]]s that it contains?
Given a relation, [[KnowItAll]] used a [[set]] of [[domain-independent]] [[patterns]] to automatically instantiate [[relation-specific]] [[extraction rules]] .
Given a [[sequence of value]]s, a [[prominent streak]] is a long consecutive [[subsequence]] consisting of only [[large]] ([[small]]) value]]s.
Given a [[set of entiti]]es, the [[all-pairs similarity search]] aims at [[identifying]] all [[pairs of entiti]]es that have [[similarity]] [[greater than]] (or [[distance]] [[smaller than]]) some [[user-defined threshold]] .
Given a [[set]] of [[seed URL]]s, the [[crawler]] [[download]]s and [[extract]]s the [[hyperlinks]] [[embedded]] in them and [[schedule]]s the [[crawling]] of the [[page]]s addressed by those [[hyperlink]]s for a [[subsequent iteration]] .
Given a set of [[unlabeled data point]]s [[generated by]] some [[unknown]], [[time-varying set of object]]s, the goal is to [[detect]] and track the underlying [[object]]s.
Given a [[small number]] of [[document]]s, the classic [[topic model LDA]] generates very poor [[topic]]s.
Given a [[social graph]] and a [[log]] of past [[propagation]]s, [[we]] build an [[instance]] of the [[independent-cascade model]] that describes the [[propagation]]s.
Given a [[social network]], can we quickly '[[zoom-out]]' of the [[graph]]?
Given a [[spatial data set]] placed on an [[n x n grid]], our goal is to find the [[rectangular]] [[regions]] within which [[subsets]] of the [[data set]] exhibit [[anomalous behavior]] .
Given a [[target number]] of [[node]]s to [[label]], the [[goal]] is to choose those [[node]]s that are most [[informative]] and then [[predict]] the unknown [[label]]s.
Given [[data]] from multiple modaliti]]es, [[we]] devise an [[efficient algorithm]] for the [[learning]] of [[binary latent factor]]s which corresponds to [[hash function learning]] .
Given emerged new [[constraint]]s, [[classical]] [[semi-supervised clustering algorithm]]s need to [[re-optimize]] their [[objective]]s over all [[data sample]]s and [[constraint]]s in [[availability]], which prevents them from [[efficiently]] [[updating]] the obtained [[data partition]]s.
Given [[huge collection]]s of [[time-evolving event]]s such as [[web-click log]]s, which consist of multiple [[attribute]]s (e.g., [[URL]], [[userID]], times - [[tamp]]), how do we find [[pattern]]s and [[trend]]s?
[[Given input]], the [[output]]s of [[MTC]] are allowed to follow [[arbitrary joint continuous distribution]] .
Given large [[collection]] of [[news document]]s [[our proposed algorithm]] generates a [[map of connection]]s that explicitly [[captures story development]] .
Given massive [[geo-sensory data]], we study the [[problem of mining]] [[spatial co-evolving patterns (SCPs)]], i.e., [[group]]s of [[sensor]]s that are [[spatially correlated]] and [[co-evolve frequently]] in their [[reading]]s.
Given multiple [[time sequence]]s with [[missing values]], [[we]] propose [[DynaMMo]] which [[summarizes]], [[compresses]], and finds [[latent variable]]s.
Given our massive [[user base]], it is enough to learn from a [[minority]] of the [[user]]s who [[label]] certain [[message]]s in order to [[label]] that kind of [[message]]s for the [[general population]] .
Given purchase [[triplet]]s ([[user]], [[item]], [[time]]) and [[item categori]]es, the [[objective]] is to make [[recommendation]]s based on [[user]]s’ overall [[predicted combination]] of [[form utility]] and [[time utility]] .
Given that the [[corpus]] is based on [[abstracts]], not [[copyrighted papers]], the [[data]] is [[publicly available]] from [http://www.gabormelli.com/RKB/PPLRE_Project koch.pathogenomics.ca/pplre/].
Given that the [[direct optimization]] of [[edge density]] is not meaningful, as even a single [[edge]] achieves [[maximum density]], [[research]] has focused on [[optimizing]] alternative [[density function]]s.
Given the [[commercial opportunity]] related to [[understanding]] [[product offering]]s on [[the Web]], there has been significant [[published research]] on the [[automated identification of product properties]] from [[textual information]] .
Given the [[feature]]s associated with a [[pair]] of [[origination]] and [[destination]], how can we [[simultaneously predict]] both the [[optimal price]] for the [[bid]] [[stage]] and the [[outcome]] of the [[transaction]] ([[win rate]]) in the [[decision stage]]?
Given the [[high volume]] and [[velocity]] of the [[data stream]]s, [[manual inspection]] of all [[message]]s is no longer [[sustainable]] .
Given the [[large size]] of our [[dataset]] a fast [[approximation]] is crucial for this [[application]] .
Given the [[reference sentence]] U which is [[written by human]], [[we]] [[rank]] the [[input sentence]]s according to the [[relative score]]s.
Given the times when [[nodes]] adopt [[pieces of information]] or become [[infected]], [[we]] identify the [[optimal network]] that best explains the [[observed]] [[infection times]] .
Given the [[travel speed]] and [[traffic volume]] of a [[road segment]], [[gas consumption]] and [[emission]]s can be calculated based on [[existing environmental theori]]es.
Given this [[hardness]], we explore several [[natural heuristic]]s on 3 [[real world dataset]]s - [[Movielen]]s, [[Yahoo! Music]] and [[Jester Joke]] and [[report our finding]]s.
Given this [[tree structure]], [[we]] can now compute the [[parent representation]]s.
Given two [[caption]]s and a [[cartoon]], [[our classifier]] picks the [[funnier]] one 69% of the [[time]] for [[caption]]s hinging on the same [[joke]], and 64% of the [[time]] for any [[pair]] of [[caption]]s.
Given two [[coupled network]]s, [[we]] first leverage [[atomic propagation rule]]s to [[automatically construct]] [[implicit link]]s in the [[target network]] for addressing the [[challenge of]] [[target]] [[network]] [[incompleteness]], and then propose a [[coupled factor graph model]] to incorporate the [[meta-path]]s [[extracted]] from the coupled part of the two [[network]]s for [[transferring]] [[heterogeneous knowledge]] .
Given two [[homogeneous rating matrice]]s with some [[overlapped]] [[user]]s / [[item]]s whose [[mapping]]s are unknown, [[this paper]] aims at answering two [[question]]s.
Give someone [[frequent flyer mile]]s, and he'll [[fly]] in [[absurd way]]s to [[optimize]] his [[miles]] .
[[GLAD]] takes both [[pair-wise]] and [[point-wise data]] as [[input]], [[automatically infer]]s the [[group]]s and [[detect]]s [[group anomali]]es simultaneously.
[[GLMNET]] proposed by [[Friedman et al.]] is an [[algorithm]] for [[generalized linear model]]s with [[elastic net]] .
[[Global-state network]]s provide a powerful [[mechanism]] to [[model]] the increasing [[heterogeneity]] in [[data generated]] by current [[system]]s.
[[GN]]: [[Gene Normalization]]; [[EM]]: [[Expectation Maximization]]; [[TAP]]: [[Threshold Average Precision]]; [[MOD]]: [[Model Organism Database]]; [[BMC]]: [[BioMed Central]]; [[PLoS]]: [[Public Library of Science]]; [[PMC]]: [[Central]]; [[NCBI]]: [[National Center for Biotechnology Information]]; [[NLM]]: [[National Library of Medicine]]; [[UAG]]: [[User Advisory Group]]; [[AP]]: [[Average Precision]]; [[SVM]]: [[Support Vector Machine]]; [[CRF]]: [[Conditional Random Fields]]; [[GNR]]: [[Gene Name Recognition]]; [[GOCat]]: [[Gene Ontology Categorizer]]; [[CLKB]]: [[Cell Line Knowledge Base]]; [[NER]]: [[Named Entity Recognition]]; [[KNoGM]]: [[Knowledge-based Normalization of Gene Mentions]]; [[WSD]]: [[Word Sense Disambiguation]]
[[Goleman]] boils down [[attention research]] into a three parts: [[inner]], [[other]], and [[outer focus]] .
[[Goleman]] shows why [[high-achiever]]s need all three kinds of focus, as demonstrated by rich [[case studi]]es from [[field]]s as diverse as [[competitive sports]], [[education]], [[the arts]], and [[business]] .
[[GOODS]] [[crawl]]s [[Google's infrastructure]] and [[builds a catalog of discovered dataset]]s, including [[structured files, database]]s, [[spreadsheet]]s, and even services that provide access to the [[data]] .
[[GOODS]] [[extracts metadata]] about dataset]]s in a [[post-hoc way]]: [[engineer]]s continue to [[generate]] and [[organize dataset]]s in the same way that they have before, and [[GOODS]] provides value without disrupting [[teams' practice]]s.
[[Good textbook]]s are organized in a [[systematically progressive fashion]] so that students acquire new knowledge and [[learn]] new [[concept]]s based on known [[item]]s of [[information]] .
[[Google]] and [[Bing]] have emerged as the [[duopoly]] that largely [[arbitrate]]s which [[English language]] [[document]]s are seen by [[web searcher]]s.
[[Government]]s could [[tax]] the]]wealthy capital owners]] and [[redistribute income to worker]]s, but that is not the [[direction]] [[societi]]es are moving in.
[[Government]]s, [[research institute]]s, [[business-development agenci]]es, and [[research]] and [[industry association]]s do welcome and fund outreach and [[science-communication effort]]s.
[[Governor's Council]]: A [[body of elected officials]] which, according to the [[state constitution]], must approve all [[expenditures except]] those for [[debt service]] and [[salari]]es for [[legislator]]s and the [[Supreme Court justice]]s.
[[GP-SELECT]] uses [[Gaussian process prediction]] to balance [[exploration]] (estimating the [[unknown value]] of [[item]]s) and exploitation (selecting [[item]]s of [[high value]]).
[[Gradient boosting of regression trees]] produces competitive, highly robust, [[interpretable]] [[procedures]] for both [[regression]] and [[classification]], especially appropriate for [[mining]] [[less than clean data]] .
[[Grafting-Light]] iteratively performs one-step of [[orthant-wise gradient descent]] over [[free parameters]] and selects new [[features]] .
[[Graph algorithms]], [[hypertext structure]], [[link analysis]], [[World Wide Web]]
[[Graph clustering]] and [[graph outlier detection]] have been studied extensively on [[plain graph]]s, with various [[application]]s.
[[Graph clustering]], often addressed as [[community detection]], is a [[prominent task]] in the [[domain]] of [[graph data mining]] with dozens of [[algorithm]]s proposed in recent years.
[[Grapheme-to-phoneme (G2P) model]]s are [[key component]]s in [[speech recognition]] and [[text-to-speech system]]s as they describe how [[word]]s are [[pronounced]] .
[[Graphical Granger modeling]] uses [[graphical modeling technique]]s on [[time series data]] and invokes the notion of "[[Granger causality]]" to make assertions on [[causality]] among a potentially [[large number]] of [[time series]] [[variables]] through [[inference]] on [[time-lagged effect]]s.
[[graphical models]], [[Bayesian networks]], [[constraint networks]], [[Markov networks]], [[induced-width]], [[treewidth]], [[cycle-cutset]], [[loop-cutset]], [[pseudo-tree]], [[bucket-elimination]], [[variable-elimination]], [[AND/OR search]], [[conditioning]], [[reasoning]], [[inference]], [[knowledge representation]]
[[Graphical model]]s come in two [[basic flavors]] — [[directed graphical model]]s and [[undirected graphical models]] .
[[Graphical models]] (e.g., [[Bayesian]] and [[constraint networks]], [[influence diagrams]], and [[Markov decision processes]]) have become a central paradigm for [[knowledge representation]] and [[reasoning]] in both [[artificial intelligence]] and [[computer science in general]] .
[[Graphical Models]], [[Social Network Learning]], [[Statistical Topic Model]]s.
[[Graph mining]], [[community detection]], [[social network]]s, [[graph algorithm]]s
[[Graph ranking]] plays an important role in many [[application]]s, such as [[page ranking]] on [[web graph]]s and [[entity ranking]] on [[social network]]s.
[[Graph]]s appear in numerous applications including [[cyber-security]], the [[Internet]], [[social network]]s, [[protein network]]s, [[recommendation system]]s, and many more.
[[Graph]]s are used to [[model]] many [[real object]]s such as [[social network]]s and [[web graph]]s.
[[Graphs]] or [[networks]] can be used to [[modePl]] [[complex systems]] .
[[Graph]]s with millions or even billions of [[node]]s and [[edge]]s are [[common-place]] .
[[Grocery Shopping Recommendation]], [[Basket Sensitive Random Walk]], [[Popularity-based Performance Evaluation]] .
[[Group-randomized trial]]s are [[comparative studi]]es in which (1) the [[units of assignment]] are [[identifiable group]]s and (2) the [[units of observation]] are [[member]]s of those [[group]]s.
[[Group recommendation]] is a challenging problem because different [[group member]]s have different [[preference]]s, and how to make a [[trade-off]] among their [[preference]]s for [[recommendation]] is still an [[open problem]] .
[[Group]]s play an essential role in many [[social website]]s which [[promote]] [[users' interaction]]s and accelerate the [[diffusion of information]] .
[[GrowCode]] introduces a [[set]] of [[basic command]]s that are general enough to [[encode]] several previously developed [[model]]s.
Growing up in the [[high desert of California]], [[Jim Doty]] was poor, with an [[alcoholic]] [[father]] and a [[mother]] [[chronically depressed]] and [[paralyzed]] by a [[stroke]] .
[[GSDMM]] can also cope with the [[sparse]] and [[high-dimensional problem]] of [[short text]]s, and can obtain the [[representative word]]s of each [[cluster]] .
[[Guidelines for the annotation]] were adapted from the [[Penn Discourse TreeBank (PDTB)]], which has [[discourse relation]]s [[annotated]] over [[open-domain]] [[news article]]s.
[[Guiding principle]]s for [[selecting]] the best [[crowdsourcing methodology]] for a given [[information gathering task]] remain insufficient.
[[Halpern13]] and Bacchus3 adopted and extended [[Gaifman's technical approach]], adding [[probability expression]]s to the [[logic]] .
[[Hashing]] has enjoyed a great success in [[large-scale]] [[similarity search]] .
Has overhasty recourse to [[scarcity]] evoked a standard [[set of]] [[market]], [[institutional]] and [[technological solution]]s which have blocked out [[political contestation]]s, overlooking access as a legitimate focus for [[academic debate]]s as well as [[polici]]es and [[intervention]]s?
Having been involved in running thousands of [[controlled experiment]]s at [[Amazon]], [[Booking.com]], [[LinkedIn]], and multiple [[Microsoft properti]]es, we share seven [[rules of thumb]] for [[experimenter]]s, which we have generalized from these [[experiment]]s and their [[result]]s.
having dinners with [[colleague]]s, and [[watching movi]]es with [[spouse]]s.
Having more than one [[representative point]] [[per]] [[cluster]] allows [[CURE]] to adjust well to the geometry of [[non-spherical shape]]s and the [[shrinking]] helps to dampen the effects of [[outlier]]s.
[[H-CRF]] uses the [[probability distribution over the set of possible labels]] according to each [[O-CRF]] and [[R1-CRF]] as [[feature]]s.
[[Health insurance claim]]s, [[Claim rework identification]], [[Predictive system]], [[Machine Learning]] .
He charts a course through [[machine learning]]'s five major [[schools of thought]], showing how they turn ideas from [[neuroscience]], [[evolution]], [[psychology]], [[physic]]s, and [[statistics]] into [[algorithms ready to serve you]] .
[[He]] is [[condemned]] [[eternally]] to a [[task]] that is [[boring]], [[difficult]], and [[futile]] .
Hence, among top earners, [[retiming]] [[income]] from [[2013]] to [[2012]] saves about 6.5 percentage points of [[marginal tax]] for [[labor income]] and about 10 percentage points for [[capital income]] .
Hence, a variety of [[technique]]s have tried to tackle the coverage limitation of [[WordNet]], often by [[drawing new word sense]]s from other [[domain-specific]] or [[collaboratively-constructed dictionari]]es and [[adding]] the new [[word sense]]s to the [[WordNet hierarchy]] ([[Poprat et al., 2008]]; [[Snow et al., 2006]]; [[Toral et al., 2008]]; [[Yamada et al., 2011]]; [[Jurgens and Pilehvar, 2015]]).
Hence, even when [[our model]] does not predict the [[exact annotation]] given by a [[human labeler]], it often predicts similar [[annotation]]s, a fact that [[we]] try to [[quantify]] by [[measuring the newly introduced " sibling]] " [[precision metric]], where [[our method]] also obtains excellent [[result]]s.
Hence, [[the system]] ends up providing [[method]]s for [[knowledge acquisition]] and [[wordsense disambiguation]] within the context of [[semantic parsing]] in a single [[elegant framework]] .
Hence, [[we]] propose [[TransE]], a [[method which models relationships]] by interpreting them as [[translations operating]] on the [[low-dimensional embedding]]s of the [[entiti]]es.
He offered this definition: “[[Courage]] is the [[disposition to voluntarily act]], perhaps [[fearfully]], in a [[dangerous circumstance]], where the [[relevant]] [[risk]]s are [[reasonably appraised]], in an [[effort to obtain or preserve]] some [[perceived good for one self or others]] [[recognizing]] that the [[desired]] [[perceived good]] may not be [[realized]]” (p.
[[Herbert Simon]]’s [[1965 analysis]] of [[technological change]] showed that, in a [[well-functioning market economy]], [[labor]] gains from [[labor-saving]] and [[capital-saving technologi]]es — as long as the [[labor supply curve]] is [[less elastic than]] the [[capital supply curve]] ([[6]]).
Here AI is [[human-level artificial intelligence]], [[AI+]] is [[greater-than-human-level artificial intelligence]], and [[AI++]] is [[far-greater-than-human-level artificial intelligence]] (as far beyond [[smartest humans]] as humans are beyond a [[mouse]]).
Here are independent links to recent publication years: [[2015]], [[2014]], [[2013]], [[2012]], [[2011]], [[2010]],
[[He]] recalls his promising [[talent]] as an [[operatic tenor]] (a [[lifelong passion]]), his encounters with everyone from [[Martin Buber]] to [[Bertolt Brecht]], innumerable [[love affair]]s, four [[marriage]]s, and a career so rich he once held [[tenured position]]s at four [[universiti]]es at the same time.
Here, for the first time, [[Feyerabend]] traces the [[trajectory]] that led him from an isolated, [[lower-middle-class]] [[childhood]] in [[Vienna]] to the height of [[international]] [[academic success]] .
[[Here]], [[player]]s are assigned the [[role]]s of either [[villager]]s or [[werewolve]]s.
[[He]] researches and writes extensively on [[economic development]], [[foreign aid]], [[global health]], [[demographic]]s, and [[poverty]] .
[[Here]], the [[recommendation]] considers not only the <i>[[pattern support]]</i> but also the <i>[[pattern occupancy]]</i> (proposed in [[this work]]).
Here, [[variance]] quantifies the [[uncertainty]] of the [[discriminative classifiers]]' [[parameters]], which in turn depends on the [[training samples]] .
[[Here]] we advocate for a comprehensive [[annotation approach]]: proceeding [[sentence]] by [[sentence]], our [[annotator]]s manually group tokens into [[MWE]]s according to guidelines that cover a broad range of [[multiword phenomena]] .
[[Here we]] argue, based on results from [[statistical physic]]s, [[random matrix theory]], [[neural network theory]], and [[empirical evidence]], that a [[deeper]] and more [[profound difficulty originate]]s from the proliferation of [[saddle point]]s, not [[local minima]], especially in [[high dimensional problem]]s of [[practical interest]] .
Here, [[we]] assume that each [[object]] has a [[geographic location]] and the [[nearest-neighbor oracle]] can be realized by [[application]]s such as [[map]]s, [[local]], or [[store-locator API]]s.
Here we compare [[abstracts]], [[sentence]]s, and [[phrases]] in [[MEDLINE]] using the standard [[information retrieval performance measures]] of [[recall]], [[precision]], and [[effectiveness]], for the [[task of mining]] [[interactions]] among [[biochemical term]]s based on [[term co-occurrence]] .
[[Here we]] demonstrate [[they]] can: we evolve the [[weights of a DNN]] with a simple, [[gradient-free]], [[population-based genetic algorithm (GA)]] and it performs well on [[hard]] [[deep RL problem]]s, including [[Atari]] and [[humanoid locomotion]] .
[[Here]] [[we]] describe a [[system]] built to [[automate]] the [[collection]] and [[aggregation of information]] on [[compani]]es, which is then [[mined]] to identify [[actionable sales lead]]s.
[[Here]], we describe how [[linguistic resource]]s can be linked with each other, and we illustrate possible use cases of [[information integration]] from various sources with example queries for the major types of [[linguistic resource]]s: Using [[DBpedia]] ([[Hellmann et al.]], this vol.) to represent [[lexical-semantic resource]], the [[German NEGRA corpu]]s in its [[POWLA representation]] ([[Chiarco]]s, this vol.) to [[represent linguistic corpora]], the [[OLiA ontologi]]es (Chiarcos, this vol.) to [[represent repositories of linguistic terminology]], and [[languoid definition]]s in [[Glottolog]] / [[Langdoc (Nordhoff]], [[this vol.]]) to represent [[linguistic knowledge base]]s and [[metadata repositori]]es.
[[Here]], we developed [[problem-independent feature extraction]] [[method]]s to [[generate]] [[hierarchical representation]]s for [[ISH image]]s.
[[Here]] [[we]] employ a [[Gaussian mixture model (GMM)]] as [[representation]] of [[clustering]], and compute the [[code-length]] for [[data sequence]]s using the [[normalized maximum likelihood (NML) coding]] .
[[Here]], [[we]] focus on [[concept extraction]] from [[textbook]]s based on the [[knowledge in Wikipedia]] .
[[Here we]] focus on the "surprise me" [[query]]: A [[user]] may be bored with his/her usual [[genre]] of [[item]]s ([[e.g.]], [[book]]s, [[movie]]s, [[hobbies]]), and may want a [[recommendation]] that is related, but off the beaten path, possibly leading to a new [[genre]] of [[book]]s/[[movie]]s/[[hobbies]] .
[[Here]] [[we]] introduce a class of [[variance allocation model]]s for [[pairwise measurement]]s: [[mixed membership stochastic blockmodels]] .
Here [[we]] introduce a generalization of [[sequential CRFs]] called [[semi-Markov conditional random fields]] (or [[semi-CRFs]]).
[[Here we]] introduce an [[algorithm]] based solely on [[reinforcement learning]], without [[human data]], [[guidance]] or [[domain knowledge]] beyond [[game rule]]s.
Here [[we]] introduce a new approach to [[computer Go]] that uses '[[value networks]]' to evaluate [[board position]]s and '[[policy networks]]' to [[select move]]s.
Here [[we]] introduce a radically new and [[natural formulation]] of [[diversity]] as [[finding]] [[center]]s in [[resistive graph]]s.
Here [[we]] introduce [[Knowledge Vault]], a [[Web-scale probabilistic knowledge]] base that combines [[extraction]]s from [[Web content]] (obtained via a[[nalysis of text]], [[tabular data]], [[page structure]], and [[human annotation]]s) with [[prior knowledge]] derived from existing [[knowledge repositori]]es.
Here, [[we]] introduce [[locally linear embedding (LLE)]], an [[unsupervised learning algorithm]] that computes [[low-dimensional]], [[neighborhood-preserving embedding]]s of [[high-dimensional inputs]] .
Here we make two [[contributions]] : firstly we find [[patterns]] in [[BGP]] [[updates]], like [[self-similarity]], [[power-law]] and [[lognormal marginals]]; secondly using these [[patterns]], [[we]] find [[anomalies]] .
Here we present an [[analysis]] of [[longitudinal micro-blogging data]], revealing a more [[nuanced view]] of the [[strategi]]es [[employed]] by [[user]]s when expanding their [[social circle]]s.
Here, we present [[Simple Greedy Matching (SiGMa)]], a simple [[algorithm]] for [[aligning]] [[knowledge base]]s with millions of [[entiti]]es and [[fact]]s.
[[Here]] we present some [[methods]] for [[defining]] [[series representations]] for [[discrete structures]] using a general type of [[kernel function]] we call a <i>[[convolution kernel]]</i>.
Here [[we]] propose a [[conceptual approach]] towards understanding these roots based on the [[assumption]] that [[aggression in mammals]], including [[humans]], has a significant [[phylogenetic component]] .
Here, [[we]] propose a [[model]] of [[individual]] [[e-mail communication]] that is sufficiently rich to capture [[meaningful]] [[variability]] across [[individuals]], while remaining [[simple]] enough to be [[interpretable]] .
[[Here]], [[we]] propose and study [[subsampling]] as a [[technique]] to [[induce diversity]] among individual [[outlier detector]]s.
Here, [[we]] provide four [[procedure]]s to help make them more robust: [[recognizing]] [[implicit regularization]] in the [[diffusion]], using a [[scalable]] [[push method]] to [[evaluate]] the [[diffusion]], using [[rank-based rounding]], and [[densifying the graph]] through a [[matrix polynomial]] .
Here [[we]] report on the [[development]] of an [[ethnicity classifier]] where all [[training data]] is [[extracted]] from [[public]], [[non-confidential]] (and hence somewhat unreliable) [[source]]s.
Here we review developments in this field, including such concepts as the [[small-world effect]], [[degree distribution]]s, [[clustering]], [[network correlation]]s, [[random graph model]]s, [[models of network growth]] and [[preferential attachment]], and [[dynamical process]]es taking place on [[networks]] .
Here, [[we]] use it to add [[non-local dependenci]]es to [[sequence model]]s for [[information extraction]] .
[[Heritability]] is [[estimate]]d to assess the [[validity]] of the derived [[quantitative trait]]s.
he role of [[big data]] in addressing the needs of the present [[healthcare system]] in [[US]] and [[rest of the world]] has been echoed by [[government]], [[private]], and [[academic sector]]s.
[[He]] sheds light on such [[topic]]s as [[our search]] for a [[predestined life purpose]], our [[desire]] to read [[divine message]]s into [[natural disaster]]s and other [[random occurrence]]s, our [[vision]]s of the [[afterlife]], and our [[curiosity]] about how [[moral behavior]] and [[immoral behavior]] are rewarded or punished in this [[life]] .
[[Heterogeneous information network]]s can provide abundant [[knowledge]] about [[relationship]]s among different [[types of entiti]]es including [[data samples]] and [[class label]]s.
[[He]] writes of his experience in the [[German army]] on the [[Russian front]], where three [[bullet]]s left him [[crippled]], [[impotent]], and in [[lifelong pain]] .
[[Hierarchical Bayesian model]]s are used to connect three elements in [[visual surveillance]]: [[low-level visual feature]]s, simple [["atomic" activiti]]es, and [[interaction]]s.
[[Hierarchical database]]s typically offer some [[relational feature]]s, such as [[foreign reference]]s and [[multiple]] [[index]]es.
[[Hierarchical multilabel classification]] ([[HMC]]) is an extension of [[binary classification]] where an [[instance]] can be [[labelled]] with multiple [[classes]] that are organised in a [[hierarchy]] .
[[High-dimensional regression]] / [[classification]] continues to be an important and [[challenging problem]], especially when [[feature]]s are highly [[correlated]] .
Highly [[optimized]] [[DL reasoners]] like [[Pellet]], [[Hermit]], [[Fact++]] can be used for [[reasoning]] over [[knowledge base]] represented using [[classical]] [[DL]] .
[[High support]] requires that [[patterns appear frequently]] in the [[database]], while high [[occupancy]] requires that [[pattern]]s occupy a large [[portion]] of the [[transaction]]s they appear in.
His research areas include [[nonparametric inference]], [[asymptotic theory]], [[multiple testing]], and [[applications to astrophysics]], [[bioinformatics and genetics]] .
[[Histogram construction]] is a fundamental [[problem]] in [[data management]], and a good [[histogram]] supports [[numerous mining]] [[operation]]s.
Historically, [[AI]] has suffered from [[insularity]] and [[fragmentation]] .
Historically, [[GE Capital America]]s [[sales rep]]s identified leads by [[manually searching]] through [[news report]]s and [[financial statement]]s either in [[print]] or [[online]] .
[[Historical price]]s are important [[information]] that can help [[consumer]]s decide whether the [[time is right]] to [[buy]] a [[product]] .
[[Historical user activity]] is key for building [[user profile]]s to [[predict]] the [[user behavior]] and affinities in many [[web application]]s such as [[targeting]] of [[online advertising]], [[content personalization]] and [[social recommendation]]s.
[[HMMs]] exhibited excellent performance for [[name extraction]] ([[Bikel et al., 1999]]).
[[Holland and Leinhardt (1981)]] proposed the [[<math>p_1</math> model]] for the [[analysis]] of [[binary directed graph data]] in [[network studi]]es.
[[Hospitalization]] for [[ischemic stroke]], determined by [[Medicare claims data]] .
[[House 1]]; [[House 2]]: The [[Governor's budget recommendation]]s for the next [[fiscal]] [[year.
How can one build a [[distributed framework]] that allows [[efficient deployment]] of a wide [[spectrum]] of modern advanced [[machine learning (ML) program]]s for [[industrial-scale problem]]s using [[Big Model]]s (100s of billions of [[parameter]]s) on [[Big Data]] ([[terabyte]]s or [[petabyte]]s) - [[Contemporary parallelization strategi]]es employ [[fine-grained operation]]s and [[scheduling]] beyond the [[classic]] [[bulk-synchronous processing paradigm]] popularized by [[MapReduce]], or even [[specialized operator]]s relying on [[graphical representation]]s of [[ML program]]s.
How can we [[automatically]] [[spot]] all [[outstanding observations]] in a [[data set]]?
How can we optimize the [[topology]] of a [[networked system]] to bring a [[flu]] under [[control]], propel a [[video]] to [[popularity]], or stifle a [[network malware]] in its [[infancy]]?
How can we [[spot similar user]]s, e.g., [[virtual identical twin]]s, in [[Cleveland]] for a [[New Yorker]]?
How can we uncover the [[co-cluster]]s of [[origination]]s and [[destination]]s while constructing the [[dual predictive model]]s for the two [[stage]]s?
How does the [[Bayesian approach]] relate to alternative [[model]]s based on [[neural network]]s, [[radial basis function]]s, or other [[technique]]s?
However a [[click]] is only the [[starting point]] of a [[user's journey]] and subsequent [[downstream utiliti]]es such as [[time-spent]] and [[revenue]] are important.
However, [[algorithm]]s using this [[measure]] achieve limited [[accuracy]] .
However, a [[linear scan]] of a [[large number]] of [[base classifier]]s in the [[ensemble]] during [[prediction]] incurs significant [[cost]]s in [[response time]], preventing [[ensemble learning]] from being practical for many [[real world time-critical]] [[data stream application]]s, such as [[Web traffic stream monitoring]], [[spam detection]], and [[intrusion detection]] .
However, an often neglected [[source]] of [[spatial information]] are [[location mention]]s expressed in [[tweet content]]s.
However, [[appliance detection]] is a challenging [[task]], especially given a very [[low granularity]] and [[partial labeled]] even [[unlabeled data]] .
However, a [[real-world]] [[network]] may consist of more than two [[types]], and the [[interactions]] among [[multi-typed]] [[object]]s play a key [[role]] at disclosing the rich [[semantics]] that a [[network]] carries.
However, as [[online data]] gets more [[unstructured]], these [[assumption]]s are no longer [[valid]] .
However, as the [[min-max kernel]] is [[nonlinear]] and might be difficult to be used for [[industrial application]]s with [[massive data]], [[we]] propose to [[linearize]] the [[min-max kernel]] via [[0-bit CWS]], a [[simplification]] of the original [[CWS method]] .
However, as [[the repositories]] grow and [[the technology]] improves, [[researcher]]s are adding new [[entiti]]es to these [[repositori]]es to develop a [[richer model]] of the [[scholarly domain]] .
However, as the [[scale]] of these [[system]]s increases, [[examining]] the [[raw data]] yields diminishing [[insight]] .
However, besides [[file content]]s, [[relation]]s among file [[sample]]s, such as a "[[Downloader]] " is always associated with many [[Trojan]]s, can provide [[invaluable information]] about the [[properti]]es of [[file sample]]s.
However, beyond [[smoothness]], a desirable [[ranking function]] should vary [[monotonical]]ly along the [[geodesic]]s of the [[data manifold]], such that the [[ranking order]] along the [[geodesic]]s is preserved.
However, both [[categori]]es of [[method]]s are difficult to [[scale]] up to handle large [[network]]s (with billions of [[node]]s).
However, comparing with [[sponsored search]] and [[contextual advertising]], [[this problem]] in the [[RTB context]] is [[less understood]] yet more critical for [[publisher]]s because 1) bidders have to submit a [[bid]] for each [[individual impression]], which mostly is associated with [[user data]] that is subject to [[change]] over [[time]] .
However, [[connection]]s in [[social media]] are often [[multi-dimensional]] .
However, [[controlling for]] [[examiner]] largely removed the [[geographical east-west trend]] .
However, current [[approach]]es – mostly focused on [[lexico-syntactic pattern]]s – suffer from both [[low]] [[recall]] and [[precision]], as [[definitional sentence]]s occur in [[highly variable]] [[syntactic structure]]s.
However, current [[tensor decomposition algorithm]]s are not [[scalable]] for [[large tensor]]s with billions of [[size]]s and [[hundreds million]]s of [[nonzero]]s: the largest [[tensor]] in the [[literature]] remains thousands of [[size]]s and hundreds thousands of [[nonzero]]s.
However, [[data]] from [[passive monitoring]] is often too [[noisy]] and does not effectively capture [[silent]] or [[gray failure]]s, whereas [[active monitoring]] is potent in [[detecting]] [[fault]]s but limited in its ability to isolate the exact [[fault location]] depending on its [[scale]] and [[granularity]] .
However, despite growing interest in this field, no [[previous approach]] [[mines]] [[proportional]] [[FTFI]]s with their exact [[support]] ([[FT-support]]).
However, [[determining]] the optimal [[set]] of [[labels]] to acquire is [[intractable]] under relatively [[general assumptions]], which forces us to resort to [[approximate]] and [[heuristic technique]]s.
However, due to [[cost]]ly [[edit]] and [[maintenance]], [[WordNet]]'s [[capability]] of keeping up with the [[emergence]] of new [[concept]]s is poor compared with [[on-line encyclopedia]]s such as [[Wikipedia]] .
However, due to the reason of [[profit]] or fame, [[people]] try to [[game the system]] by [[opinion spamming]] (e.g., [[writing fake review]]s) to [[promote]] or to demote some [[target product]]s.
However, elements of [[human supervision]] strongly [[bias]] the [[learning]] [[process]] .
However, [[ES]] can be considered a [[gradient-based algorithm]] because it performs [[stochastic gradient descent]] via an [[operation similar]] to a [[finite-difference approximation]] of the [[gradient]] .
However, [[event]]s in [[social network]]s may have different underlying [[reason]]s.
However, existing [[approach]]es for [[location prediction]] fail to organically combine both the [[regularity]] and [[conformity]] of [[human mobility]] in a [[unified model]], and lack the [[capacity]] to incorporate [[heterogeneous]] [[mobility dataset]]s to boost [[prediction performance]] .
However, existing [[methods]] on [[SimRank]] [[computation]] suffer from two [[limitations]]: 1) the [[computing cost]] can be very high in [[practice]]; and 2) they can only be applied on [[static graphs]] .
However, [[existing research work]]s on [[interpolation]] either cause a [[large number]] of [[complex calculation]]s or are lack of [[high accuracy]] .
However, existing [[tag prediction methods]] do not consider this important [[factor]], in which their [[training]] and [[test datasets]] are either split by a [[fixed time stamp]] or [[randomly sampled]] from a [[larger]] [[corpus]] .
However, [[extracting]] a pure [[reputation signal]] from [[rating]]s is difficult because of [[data sparseness]] and several [[confounding factor]]s in [[users' voting behavior]] .
However, [[extracting]] [[green knowledge]] from [[location trace]]s is [[not a trivial task]] .
However, [[extreme sparsity]] of [[user-POI matrice]]s creates a severe [[challenge]] .
However, [[extreme sparsity]] of [[user-POI matrices]] creates a severe [[challenge]] .
However, [[few publications]] [[systematically study]] how [[social actions]] evolve in a [[dynamic social network]] and to what extent different [[factors]] affect the [[user actions]] .
However, few [[research]] and [[industrial effort]]s have been reported on providing [[manufacturer]]s with [[integrated data analytical solution]]s to [[reveal potential]]s and [[optimize]] the [[production process]] from [[data-driven perspective]]s.
However, for [[big data set]]s with a [[large number]] of [[ID-level coefficient]]s, [[fitting a GLMix model]] can be [[computationally challenging]] .
However, for many [[domains]], the inherent [[streaming nature]] of [[time series]] demands [[online discovery]] and [[maintenance]] of [[time series motifs]] .
However, for many [[problem]]s the [[actual value]]s are [[irrelevant]], whereas the <i>[[shape]]</i> of the [[current time series pattern]] may [[foretell the future]] .
However, [[GAN]]s have [[limited progress]] with [[natural language processing]] .
However, [[Grafting]] performs a [[greedy step]] to [[optimize]] over [[free parameters]] once new [[features]] are included.
However, how to [[encode]] [[constraints]] into [[spectral clustering]] remains a developing [[area]] .
However, [[human performance]] (86.8%) is much [[higher]], indicating that the [[dataset]] presents a good [[challenge problem]] for future [[research]] .
However, if [[search cost]]s continue to fall, [[seller]]s are made [[worse off]] since [[buyer]]s can more easily find the [[lowest-cost seller]], while [[buyer]]s benefit from the [[lower price]]s and their improved ability to find [[products]] that fit their [[need]]s.
However, if the [[data are sensitive]] (e.g., [[patient health records]], [[user behavior records]]) [[releasing information]] about [[significant patterns]] or [[trends]] carries significant [[risk to privacy]] .
However, if the [[variable]]s are [[correlated]] in some way, then their [[covariance]] will be [[nonzero]] .
However, in a [[real-world malicious URL detection task]], the [[ratio]] between the number of [[malicious URL]]s and [[legitimate URL]]s is [[highly imbalanced]], making it very inappropriate for simply [[optimizing]] the [[prediction]] [[accuracy]] .
However, [[individual strategi]]es for following other [[user]]s are highly [[heterogeneous]] .
However, in [[industry]], when the [[online service]]s are [[deployed]], the [[ultimate performance of interest]] is the [[success of the business]], and [[the real user]]s are the [[core of the business]] .
However, in many [[application]]s, [[data]] contain large amount of [[sample]]s that does not help improve the [[quality of model]], but still [[cost]] much [[I/O]] and [[memory]] to [[process]] .
However, in many [[application]]s such as [[mechanical system]]s and [[biology system]]s, the [[temporal dependencies]] might [[change over time]] .
However, in many [[application]]s, the [[set of class labels]] are [[organized]] in a [[hierarchical tree structure]], with the [[leaf node]]s as [[output]]s and the [[internal node]]s as [[clusters of output]]s at multiple [[levels of granularity]] .
However, in many [[domain]]s, [[knowledge]] is [[dispersed]] through several [[partial]] and/or [[overlapping ontologi]]es.
However, in many [[real world]] [[application]]s, the [[label space]]s for both the [[label]]ed / [[training]] and [[unlabeled]] / [[testing example]]s can be different.
However, in [[online social network]]s the [[low cost]] of [[link formation]] can lead to [[network]]s with [[heterogeneous relationship]] strengths (e.g., [[acquaintance]]s and best [[friend]]s mixed together).
However, in practice, [[demographic information]] such as [[age]], [[gender]], and [[location]] is usually [[unavailable]] due to [[privacy]] and other reasons.
However, in reality, [[paper]]s have distinct [[citation behavioral pattern]]s when looking for different [[reference]]s, [[depending on paper content]], [[author]]s and [[target venue]]s.
However, in recent years there have been several research efforts investigating the application of [[machine learning method]]s to inducing [[information extractor]]s ([[Riloff 1996]]; [[Soderland 1996]]; [[Califf 1998]]; [[Freitag 1998]]; [[Soderland 1999]]).
However, in recent years [[the Web]] has evolved from a [[global information space]] of [[linked documents]] to one where both [[document]]s and [[data]] are [[linked]] .
However, in [[sector]]s where [[full-time schedule]]s do not dominate, total [[hour]]s matter for [[job quality]] and [[worker outcome]]s.
However, in [[social media]], [[user]]s may have various [[interest]]s and the [[connection]]s among them are usually [[multi-faceted]] .
However, [[instance]]s in a [[network]] can be linked for various [[causal reason]]s, hence treating all [[links]] in a [[homogeneous way]] can limit the [[performance]] of [[relational classifier]]s.
However, it cannot be applied to a [[large number]] of [[user]]s in [[real social network]]s, since much of such [[information]] is missing, [[outdated]] and [[non-standard]] .
However, it cannot make [[recommendations]] for so-called [[cold start users]] that have [[rated]] only a very [[small number]] of [[items]] .
However, it is very [[resource intensive]] to [[create]] and [[maintain]] large [[annotated collection]]s.
However, [[its]] [[computation cost]] is high; it is [[quadratic]] in the [[number of data point]]s.
However, [[labeling graph data]] is quite [[expensive]] and [[time consuming]] for many [[real-world application]]s.
However, [[LBS]] introduces [[problematic issue]]s for [[location privacy]] due to the [[nature]] of the [[service]] .
However, [[learning]] from [[social metadata]] presents several [[challenges]], since it is [[sparse]], [[shallow]], [[ambiguous]], [[noisy]], and [[inconsistent]] .
However, many [[applications in the real world]] have [[imperfect information]]: each [[agent]] observes different [[event]]s.
However, many consider more [[complex linguistic phenomena]] such as [[negation]], [[hedges]], [[dependencies]] and [[semantic relations]] at either the [[token]] or [[sentence level]] ([[Vincze et al., 2008]]; [[Medlock and Briscoe, 2007]]; [[McIntosh and Curran, 2009]]) and at the [[sentence level]] for [[discourse-based categories]] ([[Hirohata et al., 2008]]; [[Teufel et al., 2009]]).
However, many [[real world application]]s require an [[integrated approach]] for creating, [[refining]] and [[selecting feature]]s.
However, many [[smart meter]] [[deployment]]s support [[sample rate]]s at most 1/900 [[Hz]], which challenges [[activity analysis]] with [[occurrence]]s of [[parallel activiti]]es, difficulty of aligning [[event]]s, and lack of [[consumption feature]]s.
However, [[mobile App]]s are highly [[varied]] and [[often poorly understood]], particularly for their [[activiti]]es and [[function]]s related to [[privacy]] and [[security]] .
However, most [[algorithm]]s typically just find a [[single]] (possibly [[non-novel]]/[[actionable]]) [[interpretation]] of the [[data]] even though [[alternatives]] could exist.
However, most [[current]] [[work]] assumes the [[noise-free]] [[OPSM model]] and thus is not practical in many [[real situations]] when [[sample contamination]] exists.
However, most [[driver]]s fear of being [[monitor]]ed once the [[government]] installs the [[device]]s in their [[vehicle]]s to collect [[GPS data]] .
However, most existing [[methods]] suffer from [[limited]] [[labeled data]] and [[expensive training]] .
However, most of [[these models]] are built with only [[local context]] and one [[representation]] [[per]] [[word]] .
However, no [[existing work]] has considered the [[temporal information]] for [[POI recommendation]]s in [[LBSN]]s.
However, none of these [[method]]s explicitly [[maximize]] the [[heritability]] of the derived [[trait]]s.
However, [[off-the-shelf]] [[resource]]s are not always sufficient for [[specialized domain]]s, such as [[medicine]], [[chemistry]], or [[microelectronics]] .
However, other [[experimental result]]s suggest that [[distance vector]]s contain some different [[semantic information]] from [[co-occurrence vector]]s.
However, [[participants]] in [[on-line]] [[data collection]] [[applications]] are naturally distrustful of the [[data collector]] as well as their [[peer respondents]], resulting in [[inaccurate data]] [[collected]] as the [[respondents]] refuse to provide [[truthful data]] in fear of [[collusion attacks]] .
However, [[periodic behaviors]] could be [[complicated]], involving [[multiple]] [[interleaving periods]], [[partial time span]], and [[spatiotemporal]] [[noises]] and [[outliers]] .
However, practical [[adaptation]] of [[early classification]] of [[time series]] requires an [[easy to understand explanation (interpretability)]] and a [[measure]] of [[confidence]] of the [[prediction]] results ([[uncertainty estimate]]s).
However, [[previous work]]s balance [[relevance]] and [[diversity]] mostly by a [[predefined fixed way]] .
However, recent [[experiment]]s indicated that the existing [[GLMNET]] [[implementation]] may not be [[stable]] for [[large-scale]] [[problem]]s.
However, [[research]] on [[statistical learning methods]] for fully [[personalized email prioritization (PEP)]] has been [[sparse]] due to [[privacy issues]], since [[people]] are reluctant to share [[personal messages]] and [[importance judgments]] with the [[research community]] .
However, [[research paper]]s are often hard to understand by [[non-researcher]]s, and few [[research paper]]s cover every [[aspect]] of the [[topic]] .
However, [[residence information]] has its [[limitation]]s because [[people]] are exposed to numerous [[disease risk]]s as they [[spend time outside]] of their [[residence]]s.
However, [[rule-based methods]] usually require more [[human efforts]] to [[rewrite]] these [[rules]] for each new [[domain]] since [[address data]] are very [[irregular]] and [[varied]] with [[countries]] and [[regions]] .
However, [[scheduled maintenance]] is [[labor-intensive]] and [[ineffective]] in [[identifying problem]]s that develop between [[technician's visit]]s.
However, simply [[assigning]] different [[weights]] for different individual [[pages]] is usually [[inefficient]] in [[crawling]] [[forum sites]] because of the different [[characteristics]] between [[forum sites]] and general [[websites]] .
However, since [[estimating]] [[item factors]] is [[computationally intensive]], it poses a challenge for [[time-sensitive]] [[recommender problem]]s where it is important to rapidly [[learn factors]] for [[new item]]s (e.g., [[news articles]], [[event updates]], [[tweets]]) in an [[online fashion]] .
However, [[spreadsheet]]s are difficult to [[integrate]] with other [[data source]]s.
However, [[structured (complex) data]], such as [[graph]]s, [[sequence]]s, [[network]]s, [[text]], [[image]], [[multimedia]] and [[relational data]], are receiving an increasing amount of interest in [[data mining]] .
However, [[such data]] is usually [[biased]] --- it has a <i>[[selection bias]]</i> since the [[shared item]]s can only be [[seen]] and [[responded]] to by [[user]]s connected to the [[sharer]] in most [[social network]]s, and it has a <i>[[response bias]]</i> since the response is usually influenced by the [[relationship]] between the [[sharer]] and the [[recipient]] (which may not indicate whether the [[shared content]] is good).
However such [[tool]]s for [[regression]] and [[classification]] do not capture [[model uncertainty]] .
However, [[supervised methods]] need [[large-scale]] [[labeled training data]] .
However, [[synergetic aspect]]s of [[dialog]] provide the best [[statistical predictor]]s of [[collective performance]] and [[adding aspect]]s of the [[alignment approach]] does not improve the [[model]] .
However, the [[assumption]] made by [[existing approaches]], that the [[marginal]] and [[conditional probabilities]] are directly [[related]] between [[source]] and [[target domains]], has limited [[applicability]] in either the original [[space]] or its [[linear transformations]] .
However, the [[dimension]]ality of [[whole-brain data]], usually on the [[order]] of [[hundreds of thousand]]s, poses significant [[computational challenge]]s.
However, the [[efficient alignment]] of [[large-scale]] [[knowledge base]]s still poses a considerable [[challenge]] .
However, the [[estimated intrinsic structure]]s are [[unreliable]] / [[inaccurate]] when the [[redundant]] and [[noisy feature]]s are not removed.
However, the existing [[GANs]] restrict the [[discriminator]] to be a [[binary classifier]], and thus limit [[their]] [[learning capacity]] for [[tasks that need to synthesize output]] with [[rich structure]]s such as [[natural language description]]s.
However, the existing [[M<sup>3</sup>N]] [[formulation]] does not enjoy [[primal sparsity]], which is a desirable [[property]] for [[selecting]] significant [[features]] and reducing the [[risk]] of [[over-fitting]] .
However, the existing [[news extraction methods]] based on [[template-level wrapper]] [[induction]] have three serious [[limitations]] .
However, the existing [[parallel programming paradigm]]s are too [[low-level]] and [[ill-suited]] for implementing [[ML-DM algorithm]]s.
However, the [[hierarchical constraint]]s make it a [[non-convex problem]] and the existing [[method]] finds the [[solution]] of its [[convex relaxation]], which needs additional [[condition]]s to [[guarantee]] the [[hierarchical structure]] .
However, the [[ILP formulation]] is [[computationally]] [[expensive]] even for [[small-size]] [[matrice]]s.
However, the [[interaction]] of [[independently tuned pipeline component]]s yields poor [[end-to-end performance]] as [[error]]s introduced by one [[component]] [[cascade]] through the whole [[pipeline]], affecting overall [[accuracy]] .
However, the [[k-means method]] inherently relies on the [[Euclidean metric]] in the [[embedded]] [[space]] and does not account for additional [[topology]] underlying the [[distribution]] .
However, the [[large-scale data sets]] make traditional [[conjoint analysis]] coupled with sophisticated [[Monte Carlo simulation]] for [[parameter estimation]] [[computationally prohibitive]] .
However, the [[manual maintenance]] of [[WordNet]] is an [[expensive endeavour]] which requires significant [[effort]] and [[time]] .
However, the [[modularity-based methods]] for [[community discovery]] assume that the [[network structure]] is given [[explicitly]] and is [[correct]] .
However, the [[paradigm]] requires a new and different [[set]] of [[concept]]s and [[analytic tool]]s, beyond those provided by [[standard]] [[quantitative]] (particularly, [[statistical]]) [[method]]s.
However, the [[patient]] [[EMR]]s are typically [[sparse and noisy]], which creates significant [[challenge]]s if [[we]] use them directly to represent [[patient phenotype]]s.
However, the [[projected gradient descent]] [[GMKL optimizer]] is inefficient as the [[computation]] of the [[step size]] and a reasonably accurate [[objective function value]] or [[gradient direction]] are all [[expensive]] .
However, [[the proposed technique]] requires the [[label]]s of all <i>n</i> [[input]] [[pair]]s in the [[worst-case]] .
However, the [[rating]]s are often very [[sparse]] in many [[application]]s, causing [[CF-based method]]s to degrade significantly in their [[recommendation performance]] .
However, there are many other [[resource]]s that are undoubtedly useful in [[NLP]], including [[lexical resource]]s like [[WordNet]] and [[Wiktionary]] and [[knowledge base]]s like [[Wikipedia]] and [[Freebase]] .
However, there are three challenges to [[scale]] such [[method]]s: a) [[scientific]] b) [[infrastructure]] and c) [[organizational]] .
However, there exists [[mature result]]s and [[package]]s in the [[field]]s of [[constraint satisfaction language]]s and [[solver]]s that the [[constrained clustering]] [[field]] has yet to [[explore]] .
However, there is a surprising lack of studies on how to [[effectively]] and [[efficiently]] [[organize]] [[social event]]s for a [[large]] group of people through such [[platform]]s.
However, these [[approaches]] are less well suited to the [[identification]] of [[content]] that [[spreads]] widely and then [[fades]] over [[time scales]] on the order of [[days]] --- the [[time scale]] at which we perceive [[news]] and [[events]] .
However these [[assumption]]s are often [[violated]] in [[practice]], and [[poor performance]] can result.
However, [[these Bayesian methods]] apply [[Bayes’ rule]] to [[observed data]] to [[infer the latent distribution]]s, whereas [[our model]] works directly in the [[space of probability distribution]]s and [[discriminatively train]]s them.
However, these [[method]]s assume that a [[source]] has the same [[reliability degree]] on all the [[question]]s, but ignore the [[fact]] that [[sources' reliability]] may vary [[significantly]] among different [[topic]]s.
However, the [[short-term effect]] is not always [[predictive]] of the [[long-term effect]], i.e., the [[final impact]] once the [[product has fully launched]] and [[user]]s have changed their [[behavior]] in response.
However, the [[significance]] of this [[problem]] can better be acknowledged when the potentially undesirable [[consequence]]s of incorrectly [[classifying]] a [[food pathogen]] as a [[nonpathogen]] are considered.
However, the [[task of refining]], [[extending]] and [[homogenizing knowledge]] and its [[structure]] is very [[complex]] .
However, they often include many [[rare]] [[senses]] while missing [[corpus]]/[[domain-specific senses]] .
However, [[this approach]] can [[overfit]] during [[stationary period]]s where the [[underling structure]] does not change but there is [[random noise]] in the [[graph]] .
However, this [[dichotomy]] of [[entiti]]es conceals the [[fact]] that, even though an [[entity]] may not be in the [[reported cover]], it may still participate in many other [[optimal]] or [[near-optimal solution]]s.
However [[this model]] misses an important [[aspect]] of [[ad delivery]]: [[time homogeneity]] .
However, [[this probability information]] may be [[unavailable]] or [[incomplete]] .
However, [[this success]] has not been translated to [[application]]s like [[question answering]] that may involve [[complex arithmetic]] and [[logic reasoning]] .
However, this way of [[modeling]] only [[incompletely]] [[represents]] real [[data semantics]] .
However, those [[method]]s suffer from potential [[inconsistency]] of [[matching]]s between [[multiple network]]s.
However, treed [[model]]s go further than [[conventional tree]]s (e.g. [[CART]], [[C4.5]]) by [[fitting model]]s rather than a simple [[mean]] or [[proportion]] within each [[subset]] .
However, unlike [[conventional strategi]]es that emphasize [[analysis]], [[strategi]]es that aim to [[discover]] and exploit new [[model]]s must engage in significant [[experimentation]] and [[learning]] - a "[[discovery driven]]," rather than [[analytical approach]] .
However, unlike [[split-plot models]], [[repeated measures models]] usually include an [[underlying]] [["functional" relationship]] between at least one of the [[predictor variable]]s and the [[observation]]s within [[individual]]s.
However, using this [[knowledge]] beyond simple [[keyword matching]], for example in [[inference]]s, requires it to be [[linked, or ontologized,]] into [[semantic repositories]] such as [[ontologies]] or [[term banks]] like [[WordNet]] .
However, we argue that given sufficient [[social media data]], [[users' collective behavior]]s could be [[sensed]], [[studied]], and even [[predicted]] in a certain [[circumstance]] .
However, [[we]] show that [[RDD]]s are expressive enough to capture a wide [[class of computation]]s, including recent specialized [[programming model]]s for [[iterative job]]s, such as [[Pregel]], and new [[application]]s that these [[model]]s do not capture.
However, [[we]] show that the combination of [[VEWS]] and [[ClueBot NG]] can give a [[fully automated]] [[vandal]] [[early warning system]] with even [[higher accuracy]] .
However, what if there are [[multiple labeling]] [[source]]s (`[[oracle]]s' or `[[expert]]s') with different but unknown [[reliabilities]]?
However, when [[scaling]] to [[large dataset]]s, [[such methods]] often result in [[quality loss]] .
However, when the [[distribution]] in the [[source domain]] and the [[target domain]] are not [[identical]] but [[related]], there may exist a shared [[concept space]] to preserve the [[relation]] .
However, [[Wikipedia]] can only provide [[information]] for [[celebrities]] because of its [[neutral point of view (NPOV)]] [[editorial policy]] .
How [[local governments organize]] and manage their [[operation]]s is appropriately the [[responsibility of local policymaker]]s (e.g., [[expenditure account]]s focus on [[function]] and [[activiti]]es rather than [[department]]s).
How to automatically [[spot]] the major [[trend]]s in [[large amount]]s of [[heterogeneous data]]?
How to compute [[this score]] quickly, on huge, [[disk-resident]], [[real graph]]s?
How to differentiate the [[social]] [[influence]]s from different angles ([[topics]])?
How to [[represent]] [[users]]' [[long-term]] and [[short-term preference]]s?
How to utilize the [[structure information]] in the [[source]] [[network]] for [[predicting link]]s in the [[target network]]?
How would [[we]] [[define]], as well as [[automate]], this seemingly [[self-contradiction request]]?
[[HRMS]]: The [[payroll system]] through which [[UMass employee]]s are paid.
[[HTH]] simultaneously [[learn]]s [[hash function]]s [[embedding]] [[heterogeneous media]] into different [[Hamming space]]s, and [[translator]]s aligning these [[space]]s.
[[Human being]]s by nature are also [[norm-creating]] and [[norm-following creature]]s.
[[Human brain]]s [[flexibly combine]] the [[meanings of word]]s to compose [[structured thought]]s.
[[Human emotional state]]s are not [[independent]] but rather proceed along systematic [[path]]s governed by both [[internal]], [[cognitive factor]]s and [[external]], [[social one]]s.
[[Humans]] seamlessly [[integrate]] [[perception]], [[cognition]] and [[action]] .
[[Human]]s will [[play a pivotal role]] in formulating the [[intricate]] [[complex of laws]] that will govern [[corporate behavior]] .
[[Human task]]s that have proved most [[amenable]] to [[computerization]] are those that follow [[explicit, codifiable procedure]]s — such as [[multiplication]] — where [[computer]]s now vastly exceed [[human labor]] in [[speed]], [[quality]], [[accuracy]], and [[cost efficiency]] .
I can ask [[doctor]]s who [[prescribe]] me [[drug]]s whether they have any [[financial interest]] in the [[pharmaceutical company]]; [[financial adviser]]s whether they [[get paid]] by the [[management]] of particular [[fund]]s they are recommending; and [[life insurance]] [[salespeople]] what kind of [[commission]] they are working on — and seek to [[establish relationship]]s with [[provider]]s who do not have [[conflicts of interest]] (or at least get a second [[independent opinion]]).
::# (<i>[[computing programming]]</i>) A [[variant]] of a non-[[standardize]]d [[programming language]] .
[[I]] conclude by highlighting the constructive role for [[public policy]] in [[fostering]] [[skills formation]] and [[preserving]] [[economic mobility]] .
[[<i>Conditional Random Fields</i> (CRFs)]] ([[Lafferty et al., 2001]]) are [[undirected graphical models]] used to [[calculate]] the [[conditional probability]] of [[values]] on designated [[output nodes]] given [[values]] assigned to other designated [[input nodes]] .
Ideal as a quick reference, [[Regular Expression Pocket Reference]] covers the [[regular expression API]]s for Perl 5.8, Ruby (including some upcoming 1.9 features), [[Java]], [[PHP]], [[.NET]] and [[C#]], [[Python]], [[vi]], [[JavaScript]], and the [[PCRE regular expression librari]]es.
Ideally a [[learning algorithm]] would [[train]] a [[ranking model]] that could directly [[optimize]] the [[performance measures]] with respect to the [[training data]] .
Ideally these [[embedding]]s should capture a [[rich variety of information]] about that [[word]], including [[topic]], [[part of speech]], [[word feature]]s such as [[animacy]], [[sentiment]], [[gender]], whether the numbers are years or small numbers, and the [[direction of sentiment]] ([[happy]] vs. [[sad]]).
Ideally, we would like to [[compute]] and [[maintain]] a [[sample]] without [[aggregation]] .
[[Ideal point estimation]] that [[estimate]]s [[legislator]]s' [[ideological position]]s and [[understand]]s their [[voting behavior]] has attracted studies from [[political science]] and [[computer science]] .
[[Idea]]s and [[behavior]]s that proved most [[adaptive]] -- [[making tool]]s, for example, or [[using language]] -- [[survived]] and [[flourished]], [[replicating]] themselves in as many [[mind]]s as possible.
[[Identifying]] and [[classifying]] [[personal]], [[geographic]], [[institutional]] or other [[names in a text]] is an important [[task]] for numerous [[applications]] .
[[Identifying]] and [[forecasting research trend]]s is of critical importance for a variety of [[stakeholder]]s, including [[researcher]]s, [[academic publisher]]s, [[institutional funding bodi]]es, [[compan]]ies operating in the [[innovation space]] and others.
[[Identifying]] [[modules]], or [[natural communities]], in [[large]] [[complex networks]] is fundamental in many [[fields]], including [[social science]]s, [[biological science]]s and [[engineering]] .
Identifying [[reference]]s to these [[entities]] in [[text]] was recognized as one of the important [[sub-tasks]] of [[IE]] and was called “[[Named Entity Recognition and Classification]] ([[NERC]])”.
Identifying [[similar]] [[keywords]], known as [[broad matches]], is an important [[task]] in [[online advertising]] that has become a standard [[feature]] on all [[major]] [[keyword advertising platforms]] .
Identifying [[user]]s from multiple [[heterogeneous]] [[social network]]s and [[integrating]] the different [[network]]s is a [[fundamental issue]] in many [[application]]s.
:::(i) Direct [[United States Government]] [[cash assistance]] to an [[individual]];<br>
[[<i>Discriminative</i> methods]] directly [[model]] the [[conditional distribution]], without assuming anything about the input [[distribution]] <math>p(x)</math>.
[[IES]] is an [[interdisciplinary science]] that will require bringing together [[expert]]s working in areas such as [[cognitive science]], [[computer science]], [[social science]], [[security]], [[marketing]], [[political campaigning]], [[public policy]], and [[psychology]] to develop a [[theoretical]] as well as an [[applied engineering methodology]] for managing the [[full spectrum]] of [[information environment security issue]]s.
[[IEThresh]] estimates a [[confidence interval]] for the [[reliability]] of each [[expert]] and [[filters out]] the one(s) whose estimated [[upper-bound]] [[confidence interval]] is below a [[threshold]] - which jointly optimizes [[expected accuracy (mean)]] and need to better estimate the [[expert's accuracy (variance)]] .
If all [[variable]]s are [[nominal categorical]], the [[relative entropi]]es are standardized to take a [[maximum]] of 1 and then [[transformed]] so that in the [[bivariate case]], there is a [[relative reduction]] in [[variability interpretation]] like that for the [[correlation coefficient]] .
If an [[account]] needs to spend at a [[greater rate]] than the [[periodic allotment]], the [[agency]] submits an [[allotment request]] .
If an [[itemset]] [[satisfi]]es [[minimum support]], then it is a [[frequent itemset (frequent pattern)]] .
If a [[node]] has no [[parents]], then a [[prior-probability function]], ''[[P]]''(''x</i><sub>i</sub>), is specified.
If [[data-ism]] is today's rising [[philosophy]], [[this book]] will be its [[bible]] .
If <i>S<sub>x</sub></i> or <i>S<sub>y</sub></i> is [[empty]], no [[attachments]] are [[produced]] .
If not, what are the most [[apparent deviation]]s from [[randomness]] -- a [[dense block]] of [[actor]]s that [[persist]]s [[over time]], or perhaps a [[star]] with many [[satellite node]]s that appears with some [[fixed periodicity]]?
If [[properly analyzed]], [[this data]] can be a [[source of rich intelligence]] for providing [[real-time]] [[decision making]] and for the [[provision]] of [[travel tour]] [[recommendation]]s.
[[<i>Fran </i> (Functional Reactive Animation]]) is a [[collection of data types]] and [[functions]] for composing [[richly interactive]], [[multimedia animation]]s.
If [[robot]]s take the [[good job]]s at [[high pay]] and [[human]]s get the [[low-pay leftover]]s, the [[living standard]]s of [[persons dependent on labor income]] will [[fall]] .
If [[sample]] [[data]] are not consistent with the [[statistical hypothesis]], the [[hypothesis is rejected]] .
If [[spatial feature]]s serve as [[actuating factor]]s for [[crime]], either because of the [[people]] who or [[faciliti]]es that are located there, then interventions designed to [[alter]] those [[person]]s and [[activiti]]es might well affect [[crime]] .
[[IFSS]] makes it practical for [[virus]] [[analysts]] to [[identify]] [[malware]] [[samples]] from the [[huge]] [[gray list]] and [[improves]] the [[detection]] ability of [[anti-virus software]] .
If the [[body]] [[speeds up]], a [[force]] has been applied in the [[direction]] of [[motion]] .
If the focus is on only one of the above three [[sequence]]s in attempting to analyze such [[hidden]] [[group based behavior]], or if they are [[merged]] into [[one sequence]] as per an [[investor]], the [[coupling relationships]] among them indicated through [[trading actions]] and their [[price]]s/[[volumes]]/[[times]] would be [[missing]], and the resulting [[findings]] would have a [[high probability]] of [[mismatching]] the genuine [[fact]] in [[business]] .
If the [[population]]s are non-[[normal]], particularly for small [[sample]]s, then the [[t-test]] may not be valid.
If the [[product names]] are not [[explicitly mentioned]] in the [[sentence]] but are [[implied]] due to the use of [[pronouns]] and [[language conventions]], [[we]] need to [[infer]] the [[products]] .
If the [[range]] of the [[mapping]] contains either a [[finite]] or a [[countably infinite]] number of [[value]]s, the [[random variable]] is said to be ''[[discrete]]''; if the [[range]] includes an [[interval of real numbers]], [[bounded]] or [[unbounded]], the [[random variable]] is said to be ''[[continuous]]''.
If the [[reconstructed data]] are [[statistically similar]] to the original [[individual-level data]], [[off-the-shelf]] [[individual-level model]]s can be [[readily]] and [[reliably applied]] for [[subsequent predictive]] or [[descriptive analytic]]s.
If the [[topology of the network]] is known but the [[label]]s of the [[node]]s are [[hidden]], [[we]] would like to select a small [[subset]] of [[node]]s such that, if we knew their [[label]]s, [[we]] could accurately [[predict]] the [[label]]s of all the other [[node]]s.
If the [[training data]] are [[linearly separable]], an appealing approach is to ask for the [[decision boundary]] {''x'' : ''ƒ''(''x'') = 0} that [[maximize]]s the [[margin]] between the [[two]] [[classes]] ([[Vapnik, 1996]]).
If we apply [[active learning]] on each [[domain]] separately, some [[data instance]]s selected from different [[domain]]s may contain [[duplicate knowledge]] due to the [[common feature]]s.
If we are trying to [[evaluate]] the [[likelihood]] that [[user]] A is [[linked]] to [[user]] B, [[we]] [[sum]] the [[number]] of [[item]]s the two [[user]]s have [[in common]] .
If we could [[limit]] the [[candidate]]s that should be [[investigated]], then we can [[speed up]] [[active learning]] considerably.
If we include this extra [[information]] into our [[graph model]], our [[classifier]] yields significantly [[higher accuracy levels]] than the [[vector model]]s.
If [[we]] integrate the effective [[local]] and [[holistic]] [[visual descriptor]]s via proper [[learning method]], we can achieve more [[accurate image annotation result]]s than using [[individual visual descriptor]] .
[[Ignoring]] or [[repairing significant section]]s of the [[data]] could fundamentally [[bias]] [[the results]] and [[conclusions drawn]] from [[analyse]]s.
I have a [[strong counter-current justified belief]] that there will be [[very high (~30%)]] and [[permanent]] [[world-wide]] [[underemployment rate]]s by [[2030]] enabled by [[increasing skill requirement]]s of [[good-paying job]]s and increasing concentration of [[means-of-production ownership]] that is precipitated by [[yet another]] [[sudden and deep recession]]s that will result in a period of [[high poverty]] which will be [[corrected]] by some new [[wealth distribution system]] .
[[I]] have used [[social media]] to [[crowd-source]] [[design]]s for [[swarming nanobots]] to treat [[cancer]] .
(ii) ~A new [[update rule]] for [[automatic learning rate adaptation]], to support [[learning]] from [[sparse]], [[high-dimensional data]], as well as the [[integration]] with [[adaptive regularization]] .
(i) Instead of using a [[bag-of-words representation]], [[our model]] exploits [[hierarchical structure]] and uses [[compositional semantics]] to [[understand sentiment]] .
(ii) [[Our system]] can be trained both on [[unlabeled domain data]] and on [[supervised sentiment data]] and does not require any [[language-specific sentiment lexica]], [[parser]]s, etc. (iii) Rather than [[limiting sentiment]] to a [[positive]] / [[negative scale]], [[we]] predict a [[multidimensional distribution]] over several [[complex]], [[interconnected sentiment]]s.
[[<i>l</i>1-minimization]] refers to [[finding the minimum]] [[l1-norm solution]] to an [[underdetermined linear system]] </math>b=Ax</math>.
[[Image annotation dataset]]s are becoming [[larger and larger]], with [[tens of millions]] of [[image]]s and [[tens of thousands]] of possible [[annotation]]s.
[[Image captioning]], [[speech synthesis]], [[music generation]], and [[video game playing]] all require that a [[model generate sequences of outputs]] .
[[Imperative program]]s describe [[computation]]s by repeatedly performing [[implicit effect]]s on a [[shared]] [[global state]] .
[[Imperfect information game]]s provide a [[microcosm]] of these [[social interaction]]s, while [[abstracting away]] the [[messiness]] of the [[real world]] .
[[Implicit feedback]] (e.g., [[click]]s, [[dwell time]]s, etc.) is an abundant [[source of data]] in [[human-interactive system]]s.
[[Implicit user feedback]], including [[click-through]] and subsequent [[browsing]] [[behavior]], is crucial for [[evaluating]] and improving the [[quality]] of [[results]] returned by [[search engines]] .
Important examples of [[ATM]]s include [[random forest]], [[adaboost]] (with [[decision tree]]s as [[weak learner]]s), and [[gradient boosted tree]]s, and they are often referred to as the best [[off-the-shelf]] [[classifier]]s.
Importantly, [[features]] for [[semi-CRFs]] can measure [[properties of segments]], and [[transitions]] within a [[segment]] can be [[non-Markovian]] .
Importantly, the [[gender]] [[composition]] ([[percentage]]s of [[women]]) in [[STEM field]]s reflects these [[gender difference]]s in [[interest]]s.
Importantly, their [[behavior]]s and [[interest]]s in different [[network]]s [[influence]] one another.
Importantly, [[we]] find that [[our approach]] is able to [[detect]] more quality [[biologically]] [[significant patterns]] with comparable efficiency with the [[counterparts]] of [[AOPC]] .
Improper [[treatment]] and [[post-discharge care]] of [[CHF]] [[patient]]s leads to repeat frequent [[hospitalization]]s (i.e., [[readmissions]]).
Improvements in [[hardware]], the [[availability]] of [[massive amounts of data]], and [[algorithmic upgrade]]s are among the [[factors supporting]] better [[machine translation]] .
In 1872, the value of [[fencing capital stock]] in the [[United State]]s was roughly equal to the value of all [[livestock]], the [[national debt]], or the [[railroads]], and [[annual]] [[fencing repair costs]] were greater than [[combined annual tax receipts]] at [[all levels of government]] ([[Hornbeck, 2010]]).
In 1881, [[Simon Newcomb]], an [[astronomer]] and [[mathematician]], [[published]] the first [[known article]] describing what has become known as [[Benford’s law]] in the [[American Journal of Mathematic]]s.
In [[1978]] [[airway cooling]] was identified as an important [[stimulus]] for [[EIA]]; however, severe [[EIA]] also occurred when [[hot]] [[dry air]] was inspired, and there was no abnormal cooling of the [[airway]]s.
In [[1986]] the [[thermal hypothesis]] proposed that cooling of the [[airways]] needed to be followed by rapid [[rewarming]] and that these two [[event]]s caused a [[vasoconstriction]] and a [[reactive hyperemia]] of the [[bronchial microcirculation]], together with [[edema]] of the [[airway wall]], causing the [[airways]] to narrow after [[exercise]] .
In [[1999]] [[health care expert]]s from [[Europe]] and the [[United State]]s met to confront the [[well-documented challenge]]s of [[implementing treatment guideline]]s and to [[identify strategi]]es for improvement.
In [[2008]], over 7,400 new [[vulnerabilities]] were disclosed - well over [[100]] [[per]] [[week]] .
In [[2008]], [[Rocket Fuel]]'s founders saw a [[gap]] in the [[digital advertising]] [[market]] .
In [[2008]] the [[shared task]] was dedicated to the [[joint parsing]] of [[syntactic]] and [[semantic dependenci]]es.
In a [[bibliographic database]] like [[DBLP]] or [[PubMed]], [[research paper]]s are [[explicitly linked with author]]s, venues, and terms.
In a [[cascading graph]], [[node]]s represent [[entiti]]es and [[weighted link]]s represent the [[causality effect]]s.
In [[accuracy]], [[AMM]] is somewhere between [[linear]] and [[kernel SVM]]s.
In a [[collaborative filtering setting]], the [[prediction]] may be based on the [[user's purchase history]] rather than [[rating information]], which may be [[unreliable]] or [[unavailable]] .
In a [[data set]] with [[imbalanced categories]], the main [[challenge]] is in [[identifying]] the [[rare categories]] or [[anomalies]]; hence, the [[task]] is often referred to as [[rare category detection]] .
In addition, an important [[contribution]] of [[BioCreAtIvE]] has been the [[creation]] and [[release]] of [[training]] and [[test data set]]s for both [[task]]s.
In addition, a novel [[theoretical analysis]] on the [[upper bound]] of different [[objective function]]s helps us understand their [[bias]] to different [[community size]]s, and [[experiment]]s are conducted on both [[real life]] and [[simulated data]] to [[validate]] [[our finding]]s.
In addition, being [[decision tree]]s, [[DET]]s perform [[automatic feature selection]] .
In addition, [[CPI]] improves the [[accuracy]] of [[MaxWalkSAT]] and [[maintain]]s the exactness of [[Integer Linear Programming]] .
In addition, different [[cascade]]s normally [[diffuse]] at different [[speed]]s and spread to diverse [[scale]]s, and hence show various [[diffusion pattern]]s.
In addition, it is often the case that the [[matrix]] representing [[pair]]s of [[origination]]s and [[destination]]s has a [[block structure]], i.e., the [[origination]]s and [[destination]]s can be [[co-clustered]] such that the [[predictive model]]s are [[similar]] within the same [[co-cluster]], and exhibit significant [[variation]] among different [[co-cluster]]s.
In addition, [[mass estimation]] has [[constant time]] and [[space complexities]] .
In addition, [[our experimental evaluation]] shows that our [[representative clustering]]s have a much smaller [[deviation]] from the [[ground truth clustering]] than [[existing approach]]es, thus reducing the [[effect]] of [[uncertainty]] .
In addition, our [[method]] is [[efficient]] and achieved 15-42% [[communication]] [[overhead reduction]] in [[comparison]] to the prior [[state-of-the-art methods]] .
In addition, our [[model]] [[handle]]s <i>[[overlapping view]]s </i>, where the [[mixture component]]s [[compete]] against each other in the [[data generation process]] .
In addition, [[product offer]]s might contain [[structured]] or [[semi-structured]] <i>[[product specification]]s</i> in the form of [[HTML table]]s and [[HTML list]]s.
In addition, [[raters]] can develop [[trust]] and [[distrust]] on [[object contributors]] depending on a few [[rating]] and [[trust related factors]] .
In addition, short [[biographi]]es of over [[100]] important [[statistician]]s are given.
In addition, since [[people]] have varied [[interest]]s, the ideal [[coverage algorithm]] should incorporate [[user preference]]s in order to tailor the selected [[posts]] to [[individual tastes]] .
In addition, the [[OntoDM-core module]] provides a [[representation]] of [[constraint]]s and [[constraint-based data mining task]]s and proposes a [[taxonomy thereof]] .
In addition, the [[output]]s of [[MLGF-MF]] is significantly more [[robust]] to [[skewed matrice]]s.
In addition, these [[methods]] work best if the [[network]] is [[unweighted]] and / or [[sparse]] .
In addition, to account for the [[characteristics]] of [[feature representations]], [[we]] propose a [[hybrid]] [[hierarchical clustering algorithm]] which [[combines]] the [[merits]] of [[hierarchical clustering]] and [[k-medoids algorithm]]s and a [[weighted subspace K-medoids algorithm]] to [[generate]] base [[clusterings]] .
In addition to achieving strong [[empirical results]] on the [[citation screening problem]], [[this work]] outlines many important [[steps]] for moving away from simulated [[active learning]] and toward [[deploy]]ing [[AL]] for [[real-world applications]] .
In addition to a [[direct maximization approach]], [[we]] propose three [[effective]] and [[efficient technique]]s for obtaining [[high-quality combiner]]s ([[consensus function]]s).
In addition, to avoid possible [[negative impact]], [[TLC]] introduces [[task-specific factor]]s to [[model task difference]]s.
In addition to describing the [[Swish prototype]] in detail, [[we]] validate it with 4 [[hours]] of [[user data]], obtaining [[task]] [[classification accuracies]] of about 70%.
In addition to developing [[relatively straightforward solution]]s to finding such [[outlier]]s based on the classical [[nested-loop join]] and [[index join algorithm]]s, [[we]] develop a [[highly efficient]] [[<i>partition-based</i> algorithm]] for [[mining outlier]]s.
In addition to [[extracting topics]], [[TIARA]] derives [[time-sensitive keywords]] to depict the [[content evolution]] of each [[topic over time]] .
In addition to [[information extraction task]]s, these [[annotation primitive]]s allow [[BRAT]] to be configured for use in various other [[task]]s, such as [[chunking]] (Abney, 1991), [[Semantic Role Labeling]] ([[Gildea and Jurafsky, 2002]]; [[Carrera]]s and [[M`arquez, 2005]]), and [[dependency annotation (Nivre, 2003]]) ([[See Figure 1 for example]]s).
In addition to its [[flexibility]] and [[tractability to mathematical analysis]], [[BLAST]] is an [[order of magnitude faster]] than existing [[sequence comparison tool]]s of comparable [[sensitivity]] .
In addition, to [[reduce]] the [[time complexity]] and the [[number of the parameter]]s, we decompose each [[slice]] of the [[factor tensor]]s into two smaller [[matrice]]s.
In addition to [[refining]] and [[learning]] the [[code relationships]], [[our classifier]] can also utilize this [[shared information]] to improve its [[performance]] .
In addition to [[robust]] [[accuracy]], [[our model]] is extremely efficient dealing with [[high volume]]s of [[training samples]] due to the [[independent learning]] paradigm among its multiple [[classifiers]] .
In addition to such "[[informal networks]]", [[we]] investigated the "[[formal networks]]", such as their [[hierarchical structure]], as well as the [[demographic profile data]] such as [[geography]], [[job role]], [[self-specified interests]], etc.
In addition to the [[experiment]] in the [[original paper]], ([[Rohloff et al., 2007]]) presents the results of [[benchmarking DAML DB]], [[SwiftOWLIM]], [[BigOWLIM]] and [[AllegroGraph]] using a [[LUMB (8000) dataset]] consisting of roughly [[one billion]] [[triple]]s.
In addition to the formal [[guarantees of convergence]], [[our algorithm]]s are [[accurate]]; in most cases, they converge to better [[quality solutions]] than [[existing]] [[methods]] in [[comparable time]] .
In addition to these [[inference methods]], [[we]] present [[SparseLDA]], an [[algorithm]] and [[data structure]] for evaluating [[Gibbs sampling distributions]] .
In addition to these [[requirement]]s, the concept of [[COA]] used in [[PFM]] reflects the specificities of [[government operation]]s and [[accountability requirement]]s.
In addition to the [[theoretical result]]s, we find that [[the algorithm]] [[learns quickly]], [[accurately]], and [[robustly]] in [[empirical evaluation]]s on two [[dataset]]s.
In addition, [[TPDA]] uses two [[phases]] to get [[approximate skeletons]] of [[Bayesian networks]], which is [[not efficient in practice]] .
In addition, we also develop [[extension]]s of [[our method]] to [[learn sparse]] and [[group sparse model]]s, often of [[interest]] in [[biological application]]s.
In addition, [[we]] apply a [[domain ontology]] to [[document clustering]] to investigate if the [[ontology]] such as [[MeSH]] improves [[clustering]] [[quality]] for [[MEDLINE articles]] .
In addition, [[we]] consider several [[heuristic technique]]s to speedup the [[construction]] of [[Cartesian contour]] .
In addition, [[we]] describe a [[method]] for [[scaling]] the [[algorithm]] to [[out-of-memory dataset]]s via [[multi-threaded deserialization]] of [[block-compressed data]] .
In addition, [[we]] describe [[our experiment]]s in incorporating [[dwell time]] into [[state-of-the-art]] [[learning to rank technique]]s and [[collaborative filtering model]]s that obtain [[competitive performance]]s in both [[offline]] and [[online setting]]s.
In addition, [[we]] extract [[advanced]] [[lexical feature]]s via [[natural language processing technique]]s to capture the [[quality measure]]s such as clarity of claims, [[originality]], and importance of [[cited prior art]] .
In addition, we further elevate the [[predictive power]] of [[our model]] by [[learning location profile]]s from [[heterogeneous]] [[mobility dataset]]s based on a [[gravity model]] .
In addition, [[we]] illustrate some [[interesting result]]s pertaining to [[disease-specific predictive feature]]s, some of which are not only [[consistent]] with existing [[medical domain knowledge]], but also contain [[suggestive hypothese]]s that could be [[validated]] by further [[investigation]]s in the [[medical domain]] .
In addition, [[we]] include the use of [[text feature]]s, based on [[medical]] [[noun phrase extraction]] and [[Statistical Topic Models]] .
In addition, [[we]] introduce [[neural program optimisation]]s based on [[symbolic computation]] and [[parallel branching]] that lead to significant [[speed improvement]]s.
In addition, we [[mine]] the [[content]] of the messages associated to an [[event]] to [[discover knowledge]] on its [[consequence]]s.
In addition, we observe that different [[factor]]s in [[social theori]]es [[influence]] the [[social role]] / [[status]] of an individual [[user]] to various extent, since these [[social principle]]s represent different aspects of the [[network]] .
In addition, [[we]] present a [[sequence]] of [[approximate estimator]]s that are [[simpler]] or more [[realistic]] or both, and analyze their [[performance]] .
In addition, [[we]] propose an [[estimation algorithm]] that considers the [[noise distribution]] of the [[private statistic]]s and offers better [[accuracy]] than performing [[standard parameter estimation]] using the [[private statistic]]s.
In addition, [[we]] propose a [[set]] of other [[measures]], that [[evaluate]] [[a priori]] the [[predictive power]] of a [[set]] of [[Trajectory Pattern]]s.
In addition, [[we]] propose [[structuring]] and [[modeling methods]] for [[multivariate]] [[time series]] using [[causal relationships]] of two [[time series]] .
In addition, [[we]] propose using [[frequent trajectory pattern]]s ([[mined]] from [[historical trajectori]]es) to [[scale down the candidate]]s of [[concatenation]] and a [[suffix-tree-based index]] to manage the [[trajectories]] received in the present [[time slot]] .
In addition, [[we]] shall present an [[effective procedure]] which helps a [[user]] to [[organize]] an [[event]] with [[proper attendee]]s with [[minimum total]] [[social distance]] and commonly available [[time]] .
In addition, [[we]] show [[concrete example]]s of how [[image information]] can successfully [[disentangle]] [[pair]]s of highly different [[item]]s that are [[ranked similarly]] by a [[text-only model]] .
In addition, [[we]] show significant [[improvement]]s in engagement by running an [[A/B test]] on a [[real-world application]] called [[Similar Profile]]s on [[LinkedIn]], world's largest [[online professional network]] .
In addition, [[we]] show that [[preselecting]] the [[actual number of]] [[customer group]]s does not always lead to higher [[predictive performance]] .
In addition, [[we]] show the [[surprising result]] that [[GA<sup>2</sup>M-model]]s have almost the same [[performance]] as the best [[full-complexity model]]s on a number of [[real dataset]]s.
In addition, [[we]] specify [[performance regions]] of the [[ROC curve]] that are naturally delineated by the [[class-specific]] [[strengths]] of the [[base classifiers]] and show that each of these [[regions]] can be associated with a [[unique set]] of [[guidelines]] for [[performance optimization]] of [[binary classifiers]] within unequal [[error cost regimes]] .
In addition, [[we]] summarize the major [[properties]] of these [[external]] [[measures]] .
In addition, with the existence of a [[large number]] of [[feature]]s, [[learning model]]s tend to [[overfit]] which may cause [[performance degradation]] on [[unseen data]] .
In a few [[sectors]], there exist [[marketplace sites]] that provide [[consumers]] with [[specifications]] [[forms]], which the [[consumer]] can [[fill out]] to learn the [[service term]]s of multiple [[service providers]] .
In a [[friendship attack]], an [[adversary]] utilizes the [[degrees of two vertice]]s [[connected]] by an [[edge]] to re-identify related [[victim]]s in a [[published social network data set]] .
In a [[full-employment economy]], any [[technological advance]] raises the [[pay]] for the [[input]], with [[inelastic supply]] relative to the input with [[elastic supply]] .
In a [[large]] [[online advertising system]], [[adversari]]es may attempt to profit from the creation of [[low quality]] or [[harmful advertisement]]s.
In a large-scale [[empirical study]], [[we]] show the effectiveness of [[FAST]] in [[ranking candidate]] [[pair]]s of [[feature]]s.
In a [[large scale experiment]] using [[data]] from the [[online marketplace Etsy]], [[we]] verify that moving to a [[multimodal representation]] significantly improves [[ranking quality]] .
In all, [[RTF]] directly [[optimizes]] for the actual [[problem]] using a correct [[interpretation]] of the [[data]] .
In all these [[application]]s, [[the data]] has a [[“normal” model]], and [[anomali]]es are recognized as deviations from this [[normal model]] .
In a [[maximum entropy bitext parsing model]], [[we]] define a distribution over [[source tree]]s, [[target tree]]s, and [[node-to-node alignment]]s between them.
In analogy to [[matrix factorization method]]s for [[collaborative filtering]], [[the algorithm]] does not require songs to be described by [[feature]]s [[a priori]], but it learns a [[representation]] from example [[playlist]]s.
In an [[earlier paper]] [9], [[we]] introduced a new [[“boosting” algorithm]] called [[AdaBoost]] which, theoretically, can be used to significantly reduce the [[error]] of any [[learning algorithm]] that consistently generates [[classifier]]s whose [[performance]] is a little better than [[random guessing]] .
In an effort to [[pro-actively monitor]] emerging food [[safety issue]]s and to stay abreast with developments related to [[food safety in the world]], [[NEA]] [[track]]s the [[World Wide Web]] as a [[source]] of [[news feed]]s to identify [[food safety]] related [[article]]s.
In an [[experiment]] using [[real data]], [[this model]] [[outperformed]] [[LDA]] in [[document modeling]] in terms of [[perplexity]] .
In an [[extensive experimental evaluation]] on [[real-world dataset]]s, we demonstrate that [[our algorithm]] is [[efficient]] and is able to [[accurately reconstruct]] the [[true matrix]] while asking only a [[small number]] of [[queri]]es.
In an increasingly complex [[marketing environment]] -- [[marketing organization]]s are being called upon to [[prove]] and [[optimize]] the [[return]] on [[marketing investment]] across different [[paid earned]] and [[owned marketing channel]]s.
In an [[online rating system]], [[raters]] assign [[ratings]] to [[objects]] contributed by other [[users]] .
In a [[paper]] that appeared in [[KDD-2008]], [[Brickell]] and [[Shmatikov]] proposed an [[evaluation methodology]] by [[comparing]] [[privacy gain]] with [[utility gain]] resulted from [[anonymizing the data]], and [[concluded]] that "even modest [[privacy gain]]s require almost complete [[destruction]] of the [[data-mining utility]]".
In a [[parallel group trial]], each [[experimental unit]] is [[randomized]] to receive one [[experimental treatment]] .
In a parallel [[thread]], recent [[work]] from the [[NLP]] community suggests that for [[tasks]] such as [[natural language]] [[disambiguation]] even a simple [[algorithm]] can outperform a sophisticated one, if it is provided with large quantities of [[high]] quality [[training data]] .
In [[application]]s, besides [[graph structure]], [[rich information]] on [[node]]s and [[edge]]s and [[explicit]] or [[implicit]] [[human supervision]] are often available.
In [[application]]s such as [[computational journalism]], [[user]]s are interested in claims of the form: <i>[[Karl Malone]] is one of the only two players in [[NBA history]] with at least 25,000 [[point]]s, 12,000 [[rebound]]s, and 5,000 [[assist]]s in [[one's career]] </i>.
In a [[real manufacturing pilot application]], our [[automated model]]s [[selected]] 125 [[fast wafer]]s in [[real-time]] .
In a single [[task]], [[we]] use [[co-regularization]] to obtain [[function]]s that are [[in-agreement]] with each other on the [[unlabeled sample]]s and achieve [[low classification error]]s on the [[labeled sample]]s simultaneously.
In a typical [[discussion post]], the [[author]] may give [[opinions]] on multiple [[products]] and also compare [[them]] .
In a [[user study]] and an [[experimental evaluation]] on [[real data]], [[we]] demonstrate that [[our framework]] is [[efficient]] and provides useful and [[intuitive result]]s.
In a [[user study]], [[researcher]]s found the [[paper]]s [[recommend]]ed by our [[method]] to be more useful, [[trustworthy]] and [[diverse]] than those selected by popular alternatives, such as [[Google Scholar]] and a [[state-of-the-art]] [[topic modeling approach]] .
In [[automatic]] [[tests]] using [[Treebank-derived]] [[data]], this [[technique]] achieved [[recall]] and [[precision rates]] of roughly [[92%]] for [[baseNP chunks]] and [[88%]] for somewhat more [[complex]] [[chunks]] that partition the [[sentence]] .
In [[average case]], for each [[edge insertion]] and [[deletion]], [[our algorithm]] [[update]]d the [[personalized PageRank]] in 3[[us]] in a [[web graph]] with 105[[M vertice]]s and 3.7B [[edge]]s, and 20[[ms]] in a [[social network]] with 42[[M vertice]]s and 1.5B [[edge]]s.
In [[biological]] [[article]]s, a typical [[figure]] often comprises multiple [[panel]]s, accompanied by either [[scope]]d or [[global]] [[captioned text]] .
In [[BioSnowball]], [[biography]] [[ranking]] and [[fact extraction]] are performed together in a single [[integrated training]] and [[inference process]] using [[Markov Logic Networks (MLNs)]] as its underlying [[statistical model]] .
In both [[setting]]s, [[our model]]s yield [[improvement]]s of [[statistical]] and [[practical significance]] over ones that [[classify]] each [[text independently]] of its [[emotional]] or [[social context]] .
In [[budgetary politics]], the [[true bottom line]] is a [[political judgment]] about [[political issue]]s and [[outcome]]s.
In [[capital market surveillance]], an [[emerging trend]] is that a group of [[hidden manipulators]] [[collaborate]] with each other to manipulate [[three]] [[trading sequence]]s: [[buy-orders]], [[sell-orders]] and [[trades]], through carefully arranging their [[price]]s, [[volumes]] and [[time]], in order to mislead other [[investors]], affect the [[instrument movement]], and thus [[maximize]] [[personal benefits]] .
In classic [[pattern recognition problem]]s, [[class]]es are [[mutually exclusive]] by definition.
In [[classification]], if a [[small number]] of [[instance]]s is added or [[removed]], [[incremental]] and [[decremental technique]]s can be applied to quickly [[update the model]] .
Includes [[regression method]]s for [[least squares]], [[absolute loss]], [[quantile regression]], [[logistic]], [[Poisson]], [[Cox proportional hazards partial likelihood]], and [[AdaBoost exponential loss]] .
Inclusion of [[local topic]]s in [[selected location]]s into the [[global pool]] of [[topic]]s resulted in more than 6% [[relative increase]] in [[user engagement]] with the [[recommendation system]] compared to using the [[global topic]]s exclusively.
In ''[[clustering]]'', groups of [[examples]] that [[belong together]] are sought.
In [[cognitive science]], [[prototype theory]] often makes use of [[vector]]s.
In coming to [[understand the world]] --- in [[learning concepts]], [[acquiring language]], and [[grasping causal relations]] --- our [[mind]]s make [[inference]]s that appear to go far beyond the [[data available]] .
In comparisons with [[real-time recurrent learning]], [[back propagation]] through time, [[recurrent cascade correlation]], [[Elman net]]s, and [[neural sequence chunking]], [[LSTM]] leads to many more successful runs, and [[learns much faster]] .
In comparison to [[sliding window]]s, [[fading factor]]s are faster and [[memory-less]], a [[requirement]] for [[streaming application]]s.
In comparison with [[conventional workflow modeling]] based on [[passive workflow log]]s, one salient [[feature]] of [[our approach]] is that it can proactively unravel the [[workflow patterns hidden]] in the [[location trace]]s, by [[automatically constructing]] the [[workflow state]]s and [[estimating parameter]]s describing the [[transition pattern]]s of [[moving object]]s.
[[Incomplete data]] present serious problems when integrating [[large-scale]] [[brain imaging data set]]s from different [[imaging modaliti]]es.
In [[computational linguistics]] and [[computer science]], an [[ontology]] is a [[formal representation]] of [[knowledge]] .
In conclusion, [[we]] underline the [[relationship]]s between [[innovation and ontology]], and discuss how we can improve the [[effectiveness]] of [[smart city application]]s, [[combining expert]] and [[user-driven ontology design]] with the [[integration]] and [[or-chestration of application]]s over [[platform]]s and [[larger city entiti]]es such as [[neighborhood]]s, [[district]]s, [[cluster]]s, and [[sectors of city activiti]]es.
In consequence, [[KnowWE]] can be used as a [[web-based knowledge engineering tool]] for [[building]] ([[diagnostic]]) [[decision-support system]]s.
In contrast, [[conventional algorithm]]s (e.g., [[PageRank]] and [[HITS]]) compute [[ranking score]]s by only resorting to [[graph structure information]] .
In contrast, much of the [[textual content]] on the [[web]], and especially [[social media]], is [[temporally sequenced]], and comes in short [[fragment]]s, including [[microblog post]]s on sites such as [[Twitter]] and [[Weibo]], [[status update]]s on [[social networking site]]s such as [[Facebook]] and [[LinkedIn]], or comments on [[content sharing site]]s such as [[YouTube]] .
In contrast, some [[recent approaches]] treat the [[task]] as a [[text classification problem]], where they [[learn]] to [[classify sentiment]] based only on [[labeled training data]] .
In contrast, [[spam attack]]s are usually [[bursty]] and either [[positively]] or [[negatively correlated]] to the [[rating]] .
In contrast to [[community discovery]], which [[finds group]]s of [[highly connected node]]s, [[role discovery]] [[finds group]]s of [[node]]s that share similar [[topological structure]] in the [[graph]], and hence a common [[role]] (or [[function]]) such as being a [[broker]] or a [[periphery node]] .
In contrast to [[existing technique]]s, which determine [[correlation]]s based on the [[full-space]], our [[method]] is able to exclude [[locally irrelevant dimension]]s, enabling more precise [[detection]] of the [[correlated feature]]s.
In contrast to most [[conventional approach]]es to [[de-biasing the data]] using [[click model]]s, this allows [[training]] of [[ranking function]]s even in [[setting]]s where [[queri]]es do not repeat.
In contrast to [[multi-label learning]] which models the [[object]]’s ambiguities ([[complicated semantic]]s) in [[output (label) space]], [[multi-instance learning]] can be viewed as modeling the [[object]]’s ambiguities in [[input (instance) space]] [113].
In contrast to other [[TF]] [[method]]s like [[higher order singular value decomposition (HOSVD)]], [[our method]] [[RTF (`ranking with tensor factorization')]] directly [[optimizes]] the [[factorization model]] for the best [[personalized ranking]] .
In contrast to some [[previous efforts]] that [[implicitly encode]] [[Must-Link]] and [[Cannot-Link constraints]] by modifying the [[graph Laplacian]] or the resultant [[eigenspace]], [[we]] present a more [[natural]] and [[principled formulation]], which preserves the original [[graph Laplacian]] and [[explicitly]] [[encodes]] the [[constraints]] .
In contrast to [[state-of-the-art]] [[sequence classifier]]s, our [[model]]s are simply lists of [[weighted discriminative subsequence]]s and can thus be [[interpreted]] and related to the [[biological problem]] - a crucial requirement for the [[bioinformatics]] and [[medical communiti]]es.
In contrast to [[statistical model]]s [[we]] present a [[distribution independent formulation]] of [[the problem]] together with [[uniform bound]]s of the [[risk functional]] .
In contrast to [[SVM]]s, [[FMs model]] all [[interactions between variable]]s using [[factorized parameter]]s.
In contrast to the [[document-wise model]], [[sentence-wise co-occurrence]] does not consider [[whole document]]s, and only concerns itself with the [[number of times]] that two [[word]]s [[occur]] in the same [[sentence]] .
In contrast to the existing [[algorithm]] which requires [[prior knowledge]] on the [[target graph]] and [[appropriately set parameter]]s, [[MASCOT]] requires only one [[simple parameter]], the [[edge sampling probability]] .
In contrast to the [[state-of-the-art]], [[this paper]] proposes a novel perspective [[in terms of]] [[learning shapelet]]s.
In contrast to traditional [[content-only classification method]]s, [[relational learning]] succeeds in improving [[classification performance]] by leveraging the [[correlation]] of the [[label]]s between [[linked instance]]s.
In contrast to traditional [[structured model]]s like [[Markov random field]]s, which become [[intractable]] and [[hard to approximate]] in the presence of [[negative correlation]]s, [[DPPs]] offer [[efficient]] and [[exact algorithm]]s for [[sampling]], [[marginalization]], [[conditioning]], and other [[inference task]]s.
In contrast, we can efficiently [[train]] and [[test]] [[much larger data set]]s using [[linear SVM]] without [[kernel]]s.
In contrast, [[we]] combine [[state-space model]]s with [[term volume]]s with a [[supervised learning model]], enabling us to effectively [[predict]] the [[volume]] in the future, even without new [[document]]s.
In contrast, [[we]] show that if a [[graph]] satisfies a [[restricted-growth condition]] on the [[growth rate]] of [[neighborhood]]s, then there exists a [[natural clustering algorithm]], based on [[vertex neighborhood]]s, for which the [[variance of the estimator]] can be [[upper bounded]] by a [[linear function]] of the [[degree]]s.
In contrast with [[global community detection]] ([[graph partitioning]] or [[covering]]), [[seed expansion]] is best suited for [[identifying communities locally]] concentrated around [[nodes of interest]] .
In [[conventional GAN]]s [8], the [[discriminator]] with [[multilayer perceptrons output]]s a [[binary probability distribution]] to suggest whether the [[unknown sequence]]s come from the [[real data]] rather than the [[data synthesized]] by a [[generator]] .
[[Increase]]s in [[labor productivity]] — the most commonly used [[productivity measure]] — reflect [[investments in capital equipment]] and [[information technology]], and the [[hiring]] of more [[highly skilled worker]]s.
Increasing [[labour flexibility]] means [[reducing the constraint]]s on the [[movement of worker]]s into and out of [[job]]s previously constrained by [[labour laws]], [[union agreement]]s, [[training system]]s or [[labour market]]s that protect [[workers' income]] and [[job security]].3
Increasingly, companies create wealth by converting these"raw" intangibles into the [[institutional skill]]s, [[patent]]s, [[brand]]s, [[software]], [[customer base]]s, [[intellectual capital]], and [[network]]s that raise [[profit per employee]] and [[ROIC]] .
Increasingly, [[human translator]]s are incorporating the [[output]] of [[machine translation (MT) system]]s such as [[Google Translate]] into their [[work]] .
[[Incremental budgeting]]: A [[budgeting technique]] where a [[base budget]] is established using uniform criteria for all [[agenci]]es, and [[addition]]s or [[deletion]]s are added to or [[subtracted]] from that base.
In [[crowdsourced]] [[data aggregation task]], there exist [[conflict]]s in the [[answer]]s provided by [[large numbers]] of [[source]]s on the same [[set of questions]] .
In [[data mining application]]s such as [[crowdsourcing]] and [[privacy-preserving data mining]], one may wish to obtain [[consolidated prediction]]s out of [[multiple model]]s without access to [[feature]]s of the [[data]] .
In [[data publishing]], [[anonymization technique]]s such as [[generalization]] and [[bucketization]] have been designed to provide [[privacy protection]] .
Indeed, a primary [[function]] of any [[racial caste system]] is to [[define the meaning]] of [[race]] in its [[time]] .
Indeed, [[Blackmore]] shows that once our distant [[ancestor]]s acquired the crucial [[ability to imitate]], a second kind of [[natural selection]] began, a [[survival of the fittest]] amongst competing [[idea]]s and [[behavior]]s.
Indeed, most of [[feature]]s that [[we]] use are contained in the [[published baseline recognizer]] for the [[ICDM-2012]] [[CPROD1 contest]] [10].
[[Indeed]], one of the [[joys of coding]] is that [[computer]]s are the [[opposite of mysterious]]: they operate in an [[unforgivingly predictable]], consistent and [[deterministic manner]] .
Indeed, the feel is that of a modern [["scripting" language]] like [[Perl]], [[Python]], or [[Ruby]] .
Indeed, the [[variation]] in [[SEMG parameter]]s from [[subject]] to [[subject]] creates differences in the [[data distribution]] .
In [[defining]] the [[task]], [[people]] noticed that it is essential to [[recognize]] [[information units]] like [[names]], including [[person]], [[organization]] and [[location names]], and [[numeric expressions]] including [[time]], [[date]], [[money]] and [[percent expressions]] .
[[Independent]] of this [[research area]], [[semi-supervised clustering technique]]s have shown to substantially improve [[clustering result]]s for <i>[[single-view clustering]]</i> by [[integrating]] [[prior knowledge]] .
In detailed [[experiment]]s using various [[network diffusion properti]]es over multiple [[synthetic]] and [[real dataset]]s, we demonstrate that [[the proposed approach]] is significantly more [[accurate]] than a [[frequentist plug-in baseline]] .
In details, [[we]] treat [[communiti]]es of a [[network]] as [[super node]]s, and their [[interaction]]s as [[link]]s among those [[super node]]s.
In [[digital advertising]], [[attribution]] is the [[problem of assigning credit]] to one or more [[advertisement]]s for driving the [[user]] to the [[desirable action]]s such as [[making a purchase]] .
[[Indirect cost]]: [[Overhead expense]], including [[space rental]] and other [[administrative support cost]]s, but not including [[employee]] [[fringe benefit]]s.
In [[distributed streaming setting]]s, however, [[periodically recomputing]] the [[global model]] is wasteful: communicating new [[observation]]s or [[model update]]s is required even when [[the model]] is, in practice, [[unchanged]] .
[[Individual Characterization]], [[Hidden Markov Model]], [[Cascading Non-homogeneous Poisson Process]], [[Forward-Backward Algorithm]]
In [[document modeling]], a [[record]] indicates a [[document]] [[represented as]] a "[[bag of words]]," meaning that the [[order]] of [[words]] is ignored, an [[item]] indicates a [[word]] and a [[latent factor]] indicates a [[topic]] .
In doing so, [[we]] also propose an [[evaluation methodology]] made possible by [[ALE]], reporting [[empirical result]]s on over 55 different [[game]]s.
In [[domain]]s such as [[consumer product]]s or [[manufacturing]] amongst others, [[we]] have [[problem]]s that warrant the [[prediction]] of a [[continuous target]] .
In [[Drosophila]] [[gene expression]] [[pattern research]], the [[in situ hybridization (ISH)]] [[image]] has become the standard technique to [[visualize]] and [[study]] the [[spatial distribution]] of [[RNA]] .
[[Inductive Logic Programming (ILP)]] has been applied to some [[natural language processing task]]s, including parsing ([[Mooney, 1997]]), [[POS disambiguation]] ([[Cussens, 1996]]), [[lexicon construction]] ([[Claveau et al., 2003]]), [[WSD]] ([[Specia et al., 2007]]), and so on.
In each [[boosting round]], the [[base learner]] for a [[label]] is [[generated by]] not only [[learning]] on its own [[task]] but also reusing the [[hypothese]]s from other [[label]]s, and the amount of [[reuse across label]]s provides an [[estimate]] of the [[label relationship]] .
In each case [[we]] highlight the different kinds of [[model]]s for [[capturing the diversity]] of clues driving the [[recognition process]] and the [[algorithm]]s for [[training]] and [[efficiently deploying]] the [[model]]s.
In each [[iteration]], it uses the current [[set]] of [[rules]] to [[assign labels]] to [[unlabeled data]] .
In each [[iteration]], we choose one [[coordinate]] and only [[update]] the corresponding [[parameter]], with all others remaining [[fixed]] .
In earlier [[experimental work]]s, [[such algorithms]] were found to [[outperform]] the [[competing algorithm]]s.
In [[economics]], [[Bandit problem]]s have first been used to [[model]] [[search process]]es.
In [[economic]]s, these two [[type]]s of [[recommendation]]s are referred to as [[substitute]]s and [[complement]]s: [[substitute]]s are [[product]]s that can be purchased instead of each other, while [[complement]]s are [[product]]s that can be purchased in addition to each other.
In [[EMM]] we look for [[subgroup]]s of the [[data]] for which a [[model fitted]] to the [[subgroup]] differs substantially from the same [[model fitted]] to the entire [[dataset]] .
In [[empirical evaluation]], [[we]] collect a [[data stream]] of [[user-generated comment]]s on a [[commercial news portal]] in 30 consecutive days, and carry out [[offline evaluation]] to compare various [[sampling strategi]]es, including [[unbiased active learning]], [[biased variant]]s, and [[random sampling]] .
In [[entity matching]], a fundamental issue while [[training]] a [[classifier]] to [[label]] [[pair]]s of [[entiti]]es as either [[duplicate]]s or [[non-duplicate]]s is the one of [[selecting]] informative [[training example]]s.
In [[ERP 2]], the [[DSMS server]] is maintained by the [[government]], i.e., [[data user]] .
In essence, [[our SAMer]] is a [[meta-bidder]] that [[hedges advertisers' risk]] between [[CPA(cost per action)]] - [[based campaign]]s and [[CPM(cost per mille impressions)]] - based [[ad inventori]]es; it [[statistically assess]]es the potential [[profit]] and [[cost]] for an [[incoming CPM bid request]] against a [[portfolio]] of [[CPA campaign]]s based on the [[estimated conversion rate]], [[bid landscape]] and other [[statistics]] [[learn]]ed from [[historical data]] .
In [[event pattern matching]] a [[sequence]] of [[input event]]s is [[match]]ed against a complex [[query pattern]] that specifies [[constraint]]s on [[extent]], [[order]], [[value]]s, and [[quantification]] of [[matching event]]s.
In everyday life it usually means some [[degree of closeness]] of two [[physical objects or ideas]], i.e., [[length]], [[time interval]], [[gap]], [[rank difference]], [[coolness]] or [[remoteness]], while the [[term metric]] is often used as a standard for a [[measurement]] .
In [[Experiment]]s 1 and 2 [[we]] demonstrate the [["offensive" function]] of [[unfalsifiability]]: that it allows [[religious adherent]]s to hold their [[belief]]s with more [[conviction]] and [[political partisan]]s to polarize and criticize their opponents more extremely.
In [[experiments]] involving over a hundred [[manually-annotated]] [[Web pages]] and tens of thousands of [[spots]], our [[approaches]] significantly outperform [[recently-proposed]] [[algorithms]] .
In [[experiments]] on several [[recommendation]] and [[retrieval problem]]s using two [[large]] [[scientific publications corpora]] [[we]] show [[speedups]] of [[factors]] of 2 to [[100]] with little [[loss]] in [[accuracy]] .
In extensive [[experiment]]s with [[subtopic retrieval]], [[social network search]], and [[document summarization]], [[our approach]] convincingly surpasses [[recently-published]] [[diversity algorithm]]s like [[subtopic cover]], [[max-marginal relevance (MMR)]], [[Grasshopper]], [[DivRank]], and [[SVMdiv]] .
In fact, a [[quarter]] of the [[industry-track paper]]s for [[KDD in 2012]] were based on [[data generated]] by [[online action]]s.
In fact, [[our model]] provides a single [[unified framework]] to address both [[cold]] and [[warm start scenarios]] that are commonplace in [[practical applications]] like [[recommender systems]], [[online advertising]], [[web search]], etc. [[We]] provide [[scalable]] and [[accurate]] [[model fitting methods]] based on [[Iterated Conditional Mode]] and [[Monte Carlo EM algorithms]] .
In fact, the [[Glottolog]] / [[Langdoc resource]]s have been developed [[primarily]] for [[language documentation]] and [[typological studi]]es, but their [[application]]s as described below naturally extends for [[less-resourced language]]s.
In fact, [[TriMine]] consistently [[outperform]]s the best [[state-of-the-art]] existing [[method]]s in terms of [[accuracy]] and [[execution speed]] (up to 74x faster).
In fact, when [[transcribed]], [[dialogical utterance]]s often appear [[elliptic]] to the extent of becoming [[ungrammatical]] ([[Clark, 1996]]; [[Linell, 1998, 2005]]).
[[Inference algorithm]]s for [[Markov logic]] combine ideas from [[satisfiability]], [[Markov chain Monte Carlo]], [[belief propagation]], and [[resolution]] .
[[Inference]] and [[learning]] are carried out [[efficiently]] via [[variational algorithm]]s.
[[Inference]] and [[learning]] are [[NP-hard]], but we give [[practical solution]]s.
[[Inference]] in the [[simpified]] [[model]] provides [[bounds on probabilities]] of interest in the original [[model]] .
[[Inference]] in [[this setting]] is a [[continuous optimization task]], which can be solved [[efficiently]] .
[[Inference]] in [[topic model]]s typically involves a [[sampling step]] to associate [[latent variable]]s with [[observation]]s.
[[Inference]] is performed using a [[computationally efficient]] [[online]] [[variational Bayes (VB) procedure]] .
[[Inference]] on [[segmentation models]] involves [[dynamic programming]] [[computations]] that in the [[worst case]] can be [[cubic]] in the [[length of a sequence]] .
[[Infer]]ring [[causal networks]] behind [[observed data]] is an [[active area of research]] with wide [[applicability]] to areas such as [[epidemiology]], [[microbiology]] and [[social science]] .
[[Inferring diffusion network]]s from [[trace]]s of [[cascade]]s has been extensively studied to better [[understand]] [[information diffusion]] in many [[domain]]s.
[[Influence maximization]], defined by [[Kempe, Kleinberg, and Tardos (2003)]], is the [[problem]] of finding a [[small set]] of [[seed nodes]] in a [[social network]] that [[maximize]]s the [[spread of influence]] under certain [[influence cascade models]] .
[[Influence maximization]] is the [[problem]] of finding a small [[subset]] of [[nodes]] ([[seed node]]s) in a [[social network]] that could [[maximize]] the spread of [[influence]] .
In [[Focus]], [[he]] delves into the [[science of attention]] in all its [[varieti]]es, presenting a long overdue discussion of this [[little-noticed]] and [[under-rated]] [[mental asset]] that matters enormously for how we [[navigate life]] .
Informally, [[shapelets]] are [[time series]] [[subsequence]]s which are in some sense [[maximally representative]] of a [[class]] .
[[Information]] about [[urban air quality]], e.g., the [[concentration]] of [[PM2.5]], is of great importance to protect [[human health]] and [[control air pollution]] .
[[Information diffusion]] and [[virus propagation]] are fundamental [[processes]] talking place in [[networks]] .
[[Information diffusion]], [[viral marketing]], [[graph-based semi-supervised learning]], and [[collective classification]] all attempt to [[model]] and exploit the [[relationships]] among [[nodes]] in a [[network]] to improve the [[performance]] of [[node labeling algorithms]] .
[[Information extraction]] is the [[process of analyzing]] [[unrestricted text]] with the purpose of [[picking out information]] about [[pre-specified]] [[types of entities]], the [[event]]s in which the [[entities]] are engaged, and the [[relationships between entities]] and [[events]] .
[[Information extraction]] [[populates]] [[slots]] in a [[database]] by [[identifying]] [[relevant]] [[subsequence]]s of [[text]], but is usually not aware of the emerging patterns and regularities in the [[database]] .
[[Information Extraction]] refers to the [[automatic extraction]] of [[structured information]] such as [[entities]], [[relationships between entities]], and [[attributes describing entities]] from [[unstructured source]]s.
[[Information extraction technique]]s automatically create [[structured database]]s from [[unstructured data source]]s, such as [[the Web]] or [[newswire documents]] .
[[Information network]] contains abundant [[knowledge]] about [[relationships]] among [[people]] or [[entities]] .
[[Information network]]s, such as [[social media]] and [[email network]]s, often contain [[sensitive information]] .
[[information network]]s, [[text mining]], [[link analysis]], [[topic modeling]], [[phrase extraction]], [[role discovery]], [[clustering]], [[ranking]], [[relationship mining]], [[probabilistic model]]s, [[real-world application]]s, [[efficient and scalable algorithm]]s
[[Information processing]] across [[ontologies]] is not possible without knowing the [[semantic mappings]] between their [[elements]] .
[[Information retrieval]], [[information extraction]] and [[question answering]] can be improved by performing appropriate [[query expansion]]s.
[[Information space]]s and [[information mapping]]s, [[sensing uncertainty]], [[discrete]] and [[continuous sensor]]s, [[POMDP]]s, [[Kalman filtering]], [[particle filtering]], [[information space]]s in games.
[[Information Theory]], [[Entropy]], [[Autoregressive Model]], [[Markov Chain]], [[Time Series Prediction]]
[[Information Warehouse]]: A [[sizeable database]] which contains an extensive and [[expanding set]] of [[financial]] and [[payroll data]] .
" [[Informativeness]]: How can one extract the most [[informative]] [[relative constraint]]s from given [[knowledge source]]s?
[[Infrastructure]] and [[operations (I&O)]] [[leader]]s will be at the forefront of efforts to [[choose]], [[pilot]], [[implement]], and [[evaluate]] these [[technologi]]es - and to make sure these [[technologi]]es don't merely [[cut cost]]s but drive [[customer value]] .
In [[frequent pattern mining]], a recent [[effort]] has been to incorporate <i>[[utility]]</i> into the [[pattern selection framework]], so that high [[utility]] ([[frequent]] or [[infrequent]]) [[pattern]]s are mined which address typical business concerns such as [[dollar value]] associated with each [[pattern]] .
In general, the [[MBTI]] and its [[scale]]s yielded [[score]]s with strong [[internal consistency]] and [[test-retest reliability estimate]]s, although [[variation was observed]] .
In general, the [[production data]] comes from [[product]]s with different [[level]]s of [[quality]], [[assembly line]] with [[complex]] [[flow]]s and [[equipment]]s, and [[processing craft]] with massive [[controlling parameter]]s.
In general, the target [[consumers]] of the [[NER system]]s are [[machines]], and common applications include [[question answering]], [[summarization]], and [[automatic correction]] of [[missing case information]] or [[misspellings in text]] .
In [[health care]], as elsewhere, [[scarcity]] is the mother of [[allocation]].<sup>1</sup> Although the extent is debated,<sup>2,3</sup> the [[scarcity]] of many specific [[intervention]]s — including [[beds in intensive care units]],<sup>4</sup> [[organ]]s, and [[vaccine]]s during [[pandemic influenza]]<sup>5</sup> — is widely acknowledged.
Inherited from [[synchronization]], [[Sync]] has several [[desirable properties]]: The [[clusters]] revealed by [[dynamic synchronization]] truly reflect the [[intrinsic structure]] of the [[data set]], [[Sync]] does not rely on any [[distribution assumption]] and allows [[detecting clusters]] of [[arbitrary number]], [[shape]] and [[size]] .
In [[hidden state sequence model]]s such as [[HMMs]], [[CMMs]], and [[CRFs]], it is standard to use the [[Viterbi algorithm]], a [[dynamic programming algorithm]], to [[infer]] the [[most likely]] [[hidden state sequence]] given the [[input]] and [[the model]] (see, e.g., [[Rabiner (1989)]]).
In [[highly uncertain]], [[complex]] and [[fast-moving environment]]s, strategies are as much about insight, [[rapid experimentation]] and [[evolutionary learning]] as they are about the traditional [[skills of planning]] and [[rock-ribbed execution]] .
In [[ISH image]] [[annotation]]s, the [[image content representation]] is crucial to achieve [[satisfactory result]]s.
[[Initial result]]s are encouraging with a [[reduction]] of [[congestion]] and [[underuse]], while in more [[locations rate]]s were decreased than [[increased]] .
[[Initiative]] is important, but not all [[initiative]] is [[productive]] .
In its [[4th]] and [[final stage]] [[IPLoM]] [[produce]]s [[cluster descriptions]] or [[line formats]] for each of the [[clusters]] [[produced]] .
In [[its]] basic [[gameplay]], two [[team]]s of five [[player]]s compete against each other to destroy the [[enemy]]’s [[base]] .
In its most [[primitive form]], [[image data]] are [[represented as]] [[pixel]]s.
In [[knowledge-lean approach]]es, [[coreference resolver]]s employ only [[morpho-syntactic cue]]s as [[knowledge source]]s in the [[resolution process]] (e.g., [[Mitkov (1998)]], [[Tetreault (2001)]]).
In [[knowledge-sharing]] [[OSNs]], such as [[blogs]] and [[question answering systems]], issues on how [[users]] participate in the [[network]] and how [[users]] "[[generate]]/[[contribute]]" [[knowledge]] are vital to the sustained and healthy [[growth]] of the [[networks]] .
In [[large]] [[social network]]s, [[nodes]] ([[users]], [[entities]]) are influenced by others for various reasons.
In light of this, [[we]] propose [[Geo-SAGE]], a [[geographical sparse additive generative model]] for [[spatial item recommendation]] in [[this paper]] .
In [[Limit Texas Holdem]], a [[poker game]] of [[real-world scale]], [[NFSP]] learnt a [[strategy]] that approached the [[performance]] of [[state-of-the-art]], [[superhuman algorithm]]s based on significant [[domain expertise]] .
In many [[application]]s, [[classification label]]s may not be associated with a single [[instance]] of [[record]]s, but may be associated with a <i>[[data set]]</i> of [[record]]s.
In many [[application]]s, e.g., [[recommender system]]s and [[traffic monitoring]], the [[data]] comes in the form of a [[matrix]] that is only [[partially observed]] and [[low rank]] .
In many [[application]]s, [[fitting]] a [[regression model]] with only [[linear effect]]s may not be sufficient for [[predictive]] or [[explanatory]] purposes.
In many [[application]]s such as [[image]] and [[video processing]], the [[data matrix]] often possesses simultaneously a [[low-rank structure capturing]] the [[global information]] and a [[sparse component]] capturing the [[local information]] .
In many [[application]]s we have a [[social network]] of [[people]] and would like to [[identify]] the [[member]]s of an [[interesting]] but [[unlabeled group]] or [[community]] .
In many cases, a [[relational graphical model]] is [[learned]] over all the [[attributes]] and [[labels]] in the [[graph]], and a [[joint probability distribution]] over these [[attributes]] and [[labels]] is [[learned]] and [[optimized]] .
In many cases, [[feature set]]s characterizing the [[object]]s are not available, hence, [[clustering of the object]]s depends solely on the [[link]]s and [[relationship]]s amongst [[object]]s.
In many [[classification problem]]s [[label]]s are relatively [[scarce]] .
In many interesting [[application]]s, the [[domain]] [[dimensionality]] is such as to prevent [[state-of-the-art]] [[statistical learning technique]]s from [[delivering]] [[accurate]] [[model]]s in reasonable [[time]] .
In many [[natural language task]]s, such as [[information extraction]] and [[semantic [[lexicon]] building]], individual [[entiti]]es and [[relation]]s of interest may be found in multiple [[context]]s within the [[corpus]] .
In many [[randomised trial]]s [[researcher]]s [[measure]] a [[continuous variable]] at [[baseline]] and again as an [[outcome assessed at follow up]] .
In many [[real-life application]]s, however, [[data instance]]s in one [[domain]] may correspond to multiple [[instance]]s in another [[domain]] .
In many [[real world application]]s, however, the [[information]] may come [[sequentially]], and as a consequence, the [[truth of object]]s as well as the [[reliability]] of [[source]]s may be [[dynamically evolving]] .
In many [[real-world application]]s, information such as [[web click data]], [[stock ticker data]], [[sensor network data]], [[phone call records]], and [[traffic monitoring data]] appear in the form of [[data stream]]s.
In many [[real world application]]s such as [[satellite image analysis]], [[gene function prediction]], and [[insider threat detection]], the [[data collected]] from [[heterogeneous source]]s often exhibit multiple [[types of heterogeneity]], such as [[task heterogeneity]], view [[heterogeneity]], and [[label heterogeneity]] .
In many [[real-world network]]s, [[node]]s have [[class label]]s or [[variable]]s that affect the [[network's topology]] .
In marked [[deviation]] from [[prior work]], our [[edge resistance]]s are learnt from [[training data]] .
In [[matrix factorization]], [[we]] represent [[user]]s and [[item]]s in a [[shared latent low-dimensional space]] of [[dimension]] <math>K</math> — [[user]] <math>i</math> is [[represented by]] a [[latent vector]] <math>u_j \in \mathbb{R}^K</math> and item <math>j</math> by a [[latent vector]] <math>v_j \in \mathbb{R}^K</math>.
In [[matters of great importance]] that have [[financial]], [[medical]], [[social]], or other implications, [[we]] often seek a [[second opinion]] before [[making a decision]], sometimes a [[third]], and sometimes many more.
In [[multilabel learning]], each [[instance]] in the [[training set]] is associated with a [[set of labels]] and the [[task]] is to [[output]] a [[label set]] whose [[size]] is [[unknown a priori]] for each [[unseen instance]] .
In [[multi-label learning]], each [[training example]] is associated with a [[set of labels]] and the [[task]] is to [[predict]] the proper [[label set]] for the [[unseen example]] .
In multiple [[clinical cohort]]s, we have developed the [[pathophysiological basis]] for [[studying]] [[probabilistic variation]]s in [[long-term]] [[ECG]] and demonstrated the ability of [[this information]] to effectively [[risk stratify patient]]s at [[risk of dying following heart attack]]s.
In [[multi-task learning (MTL)]], multiple [[related task]]s are [[learned jointly]] by [[sharing information]] according to [[task relation]]s.
In [[multi-task learning]], the [[prediction]] of [[cognitive score]]s at each [[time point]] is considered as a [[task]], and multiple [[prediction task]]s at different [[time point]]s are performed simultaneously to capture the [[temporal smoothness]] of the [[prediction model]]s across different [[time point]]s.
In [[natural language]] we often work with [[structured data]] of [[arbitrary size]]s, such as [[sequence]]s and [[tree]]s.
In [[non-stationary environment]]s [[data]] arrives]]incrementally]], however the underlying [[generating function]] may change over [[time]] .
In nowadays [[social network]]s, a huge [[volume]] of [[content]] containing [[rich information]], such as [[review]]s, [[rating]]s, [[microblog]]s, etc., is being [[generated]], [[consumed]] and [[diffused]] by [[user]]s all the [[time]] .
In [[offline analysis]] on the [[samples]] collected from a [[random]] [[bucket]] of [[Yahoo! Front Page]] [[Today Module]], [[we]] [[compare]] [[tensor segmentation]] against other [[segmentation]] [[schemes]] using [[demographic information]], and [[study]] [[user preference]]s on [[article]] [[content]] within [[tensor]] [[segments]] .
In [[online advertising]], [[response prediction]] is the [[problem of estimating the probability]] that an [[advertisement]] is [[clicked]] when [[displayed]] on a [[content publisher's webpage]] .
In [[online experimentation]], it is straightforward to [[measure]] the [[short-term effect]], i.e., the impact [[observed]] during the [[experiment]] .
In order for [[the future]] to [[energize]] and [[motivate]] [[current action]], it must [[feel imminent]] .
In order to [[annotate]] the [[semantics of the paper]] automatically, we modeled the [[rhetorical structure]] of an [[abstract]] by [[linguistic clue]]s and [[position information]] .
In order to apply these [[diversity maximization algorithm]]s in the context of [[aggregator website]]s and as a [[preprocessing step]] for our [[diversity maximization tool]], [[we]] develop [[greedy clustering algorithm]]s that [[maximize]] [[weighted coverage]] of a [[predefined set]] of [[topic]]s.
In order to capture the rich [[contextual structure]]s in a [[query]] or a [[document]], [[we]] start with each [[word]] within a [[temporal context window]] in a [[word sequence]] to directly capture [[contextual feature]]s at the [[word n-gram level]] .
In order to correctly find all [[duplicates]], [[we]] may need to make [[multiple passes]] over the [[data]]; as [[linkages]] are [[discovered]], they may in turn allow us to [[discover]] additional [[linkages]] .
In order to determine when two [[entities]] are [[spatially related]] in an [[adaptive]] and [[non-parametric way]], [[we]] propose a [[Voronoi-based]] [[neighbourhood definition]] upon which [[spatial literals]] can be built.
In order to develop ideas about the feasibility of [[WBE]], ground [[technology foresight]] and [[stimulate]] [[interdisciplinary exchange]], the [[Future of Humanity Institute]] hosted [[a workshop]] on May 26 and 27, [[2007]], in [[Oxford]] .
In order to [[evaluate]] how well [[language model]]s can exploit [[longer context]]s and deal with more [[realistic vocabulari]]es and [[larger corpora]] we also introduce the [[freely available]] [[WikiText corpus]] .
In order to [[extract attribute-value pair]]s from the [[HTML fragment]]s identified by the [[specification detector]], we again use [[supervised learning to classify]] [[column]]s as [[attribute column]] or [[value column]] .
In order to find a good [[trade-off]], [[we]] propose a [[random walk model]] combining the [[trust-based]] and the [[collaborative filtering approach]] for [[recommendation]] .
In order to generate desired [[clustering]], [[we]] propose to use <i>[[meta-path]] </i>, a path that connects [[object type]]s via a [[sequence of relation]]s, to [[control clustering]] with distinct [[semantics]] .
In order to [[learn]] [[hybrid random field]]s from [[data]], [[we]] [[develop]] the [[Markov Blanket Merging algorithm]] .
In order to make good [[predictions]], it is [[necessary]] to [[identify]] the appropriate [[causal relationships]] .
In order to [[maximize]] the benefit of [[location promotion]], [[we]] formalize it as an [[influence maximization problem]] in an [[LBSN]], i.e., given a [[target location]] and an [[LBSN]], which a [[set]] of <i> k</i> [[user]]s (called [[seed]]s) should be advertised initially such that they can successfully [[propagate]] and attract most other [[user]]s to visit the [[target location]] .
In order to [[minimize redundancy]] and [[optimize coverage]] of multiple [[user interest]]s, [[search engine]]s and [[recommender system]]s aim to diversify their [[set of result]]s.
In order to more [[efficiently]] perform the [[rule learning]] and exploit powerful [[multi-processor machines]], a [[scalable]] [[parallelized method]] capable of reducing the [[runtime]] by several [[factors]] is presented.
In order to perform [[this task]], we must [[reason jointly]] about [[candidate fact]]s and their associated [[extraction confidence]]s, [[identify]] [[co-referent entiti]]es, and [[incorporate]] [[ontological constraint]]s.
In order to present the [[organizational structure]] in a [[dynamic]] [[social network]], [[we]] propose a [[tree learning algorithm]] to derive an [[evolving]] [[community tree]] .
In order to [[prove the value]] of [[the model]], [[we]] [[automatically]] [[extract]] [[parallel sentence]]s from the [[comparable collection]]s and use them to train [[statistical machine translation engine]]s for specific [[domain]]s.
In [[order]] to stabilize our system in [[small dataset]]s, [[we]] construct a [[probabilistic framework]] for [[document-query pair]]s to [[maximize the likelihood]] of the objective [[permutation]] of top-<math>\tau</math> [[document]]s.
In [[ordinal regression]], [[we]] consider a [[problem]] which shares properties of both [[classification]] (i) and [[metric regression]] (ii).
In other papers ([[Schuster and Paliwal, 1997]], [[Socher et al., 2014]], [[Karpathy and Fei-Fei, 2014]]), [[biological inspiration]] is downplayed in favor of achieving [[empirical result]]s on [[important task]]s and [[dataset]]s.
In other words, a [[dictionary]] is a [[finite set]] of [[word]]s, each of which is [[defined]], and each of its [[defining]] [[word]]s is likewise defined somewhere in the [[dictionary]] .
In other words, a successful [[Boolean]] [[matrix decomposition]] gives a [[set of concepts]] and shows how every [[column]] of the [[input data]] can be [[expressed]] as a [[union]] of some [[subset]] of those [[concepts]] .
In other words, every [[party]] follows such [[protocol]] properly with the [[exception]] that it keeps a [[record]] of all its [[intermediate computations]] without [[sharing]] the [[record]] with [[others]] .
In [[our approach]], a [[hierarchy]] is [[created]] by [[repeatedly applying]] [[mean shift]] with an increasing [[bandwidth]] on the [[data]] .
In [[our approach]], the [[space]] of [[input attribute]]s of a [[deep web data source]] is [[stratified]] for capturing the [[relationship]] between the [[ input]] and the [[output attribute]]s.
In [[our approach]], [[topic models]] are used as efficient [[dimension reduction technique]]s, which are able to capture [[semantic relationships]] between [[word-topic]] and [[topic-document]] interpreted in terms of [[probability distributions]] .
In [[our approach]] we combine [[dynamic symbolic execution]] with [[fuzzing technique]]s.
In [[our calculus]], every [[expression]] denotes a [[probability distribution]] yet evaluates to a [[regular value]] .
In [[our experimental study]] [[we]] show the [[effectiveness]] of our [[method]] and we present interesting [[discoveries]] on [[multiple]] [[real world dataset]]s.
In our [[experiments]], an [[wrapper]] learned from 40 [[pages]] from a single [[news site]] achieved an [[accuracy]] of 98.1% on 3,973 [[news pages]] from 12 [[news sites]] .
In [[our experiment]]s, both [[distributed algorithm]]s exhibit significant [[improvement]]s compared to the [[state-of-the-art]] [[distributed method]] [13].
In [[our experiments]] on three [[real-world dataset]]s, [[we]] find that [[our algorithm]] outperforms the [[state-of-the-art]] [[approaches]] .
In [[our experiments]], [[we]] built a simple [[machine learning-based]] [[pronoun resolution system]], and [[evaluated]] [[the system]] on three different [[corpora]]: [[MUC]], [[ACE]], and [[GENIA]] .
In [[our experiments, we]] extensively evaluate the effectiveness of [[our approach]] on several [[application]]s, including [[training]] [[large-scale]] [[kernel method]]s and [[exemplar-based clustering]], on [[million]]s of [[data point]]s.
[[ In our experiments, we]] show that [[our algorithm]] can achieve the same [[level]] of [[prediction accuracy]] as [[Gibbs sampling]] an [[order of magnitude]] faster.
In [[our experiment]]s, [[Westfall-Young light]] dramatically outperforms the current [[state-of-the-art]] [[approach]], both in terms of [[runtime]] and [[memory efficiency]] on popular [[real-world benchmark dataset]]s for [[pattern mining]] .
In [[our experiment]]s, [[we]] use several [[real dataset]]s with various [[graph properti]]es to [[evaluate]] the [[effectiveness]] of [[our model]] using four [[quality measure]]s and a [[case study]] .
In [[our experiment]]s, [[we]] were able [[to map disease]] for a [[simulated population]] with 1.6 million [[people]] and a [[spatial grid]] with 65 thousand [[location]]s in several [[minute]]s.
[[In our experiment]]s with multiple [[real-life dataset]]s, [[we]] show that [[WPAM]] outperforms [[state-of-the-art]] [[baseline]]s by as much as 16% in terms of [[disambiguation accuracy]] .
" In our extensive [[evaluation]] on [[twenty time series dataset]]s [[we]] showed that [[the proposed method]] has several advantages over the [[state-of-the-art method]] that provides [[reliability estimate]]s in [[early classification]] .
In [[our formalization]], [[utterance]]s, that only change the [[belief]]s and [[intention]]s, are [[observation]]s.
In [[our framework]], it is assumed that [[multiple]] [[roles]] exist, and each [[data point]] corresponds to an [[entity]] (such as a [[retail customer]], or an [[email]], or a [[news article]]) that selects various [[roles]] which compete to influence the various [[attributes]] associated with the [[data point]] .
In [[our framework]], [[we]] do our best to utilize available resources including [[dictionari]]es, [[web corpora]], and [[lexical analyzer]]s, and represent them as [[linguistic feature]]s in the [[CRF model]] .
[[In Our Hands]] describes the financial feasibility of the Plan and its effects on [[retirement]], [[health care]], [[poverty]], [[marriage and family]], [[work]], neighborhoods and [[civil society]] .
In [[our method]], [[we]] combine both [[<math>L_1</math> norm]] and [[Laplacian]] based [[<math>L_2</math> penalty]] with [[Logit loss function]] of [[Logit Boost]] .
In [[our method]], [[we]] first [[compute]] the [[travel speed]] of each road [[segment]] using the [[GPS trajectori]]es received recently.
In [[our model]]s, [[we]] use [[hierarchical softmax]] where the [[vocabulary]] is [[represented as]] a [[Huffman binary tree]] .
In [[our model]], the [[learning algorithm]] presents a [[ranking]] to the [[user]] at each [[step]], and uses the [[set of document]]s from the presented [[ranking]], which the [[user]] reads, as [[feedback]] .
In our [[MVMT learning method]]s, [[we]] learn a [[linear mapping]] for each [[view]] in each [[task]] .
In [[our new approach]], these cases and others are explained by [[explicitly defining]] [[modeling goal]]s and [[analyzing]] the [[broader framework]] of the [[data mining problem]] .
In [[our opinion]], [[user]]s [[click]] those ads that can convince them to take further actions, and the [[critical factor]] is if those [[ad]]s can trigger [[users' desire]]s in their [[heart]]s.
In our previous [[work]], [[we]] proposed a [[scalable]] [[log-linear model]] called [[LMMH (Log-Linear Models for Multiple Hierarchies)]] that combats [[data sparsity]] at [[granular level]]s through small [[sample size correction]]s that borrow strength from [[rate estimate]]s at [[coarser resolution]]s.
In our problem, [[item]]s are [[scientific article]]s and [[user]]s are [[researcher]]s.
In [[our scheme]], [[location semantics]] are first [[learn]]ed from [[location data]] .
In [[our solution]], a [[shared subspace]] is first [[learned]] to [[represent]] common [[latent feature]]s of different [[domain]]s.
In our [[two-year-long]] [[field study]] in Bangladesh, we collected the [[health checkup]] results of 15, 075 [[subject]]s, the [[data]] of 6, 607 [[prescription]]s, and the [[follow-up examination]] results of 2, 109 [[subject]]s.
In our [[work]], [[we]] address the [[problem of modeling]] [[social network]] [[generation]] which explains both [[link]] and [[group formation]] .
In [[our work]] [[we]] develop an [[approach]] that takes advantage of several [[unlabeled protein]]s, along with multiple [[data source]]s and [[multiple]] [[functions of protein]]s.
In [[our work]], [[we]] use a slightly different [[loss function]] than [[Skip-Gram word2vec embeddings]] .
In [[outsourcing]], the [[data privacy]] is a critical [[issue]] for some [[legal]] or [[commercial reasons]] since there may be [[sensitive information]] contained in the [[data]] .
In particular, [[botnet]]s such as [[Conficker]] have been known to [[encrypt]] the [[communication packet]]s exchanged between [[bot]]s and their [[command-and-control server]], making it [[costly]] for existing [[botnet detection system]]s that rely on [[deep packet inspection (DPI) method]]s to [[identify]] [[compromised machine]]s.
In particular, by rewriting the [[empirical]] [[partial AUC risk]] as a [[maximum]] over [[subset]]s of [[negative instance]]s, [[we]] derive a new [[formulation]], where a modified form of the earlier [[optimization objective]] is evaluated on each of these [[subset]]s, leading to a [[tighter hinge relaxation]] on the [[partial AUC loss]] .
In particular, by using [[knowledge guidance constraint]]s, [[Rubik]] can also discover [[sub-phenotype]]s for several major [[disease]]s.
In particular, given [[prediction]]s from any [[regression model]] of the [[target]] on the [[test data]], [[we]] elucidate a [[provable method]] for improving these [[prediction]]s in terms of [[mean squared error]], given [[exact]] (or [[accurate]] enough) [[information]] of the [[aggregated target]]s.
In particular, [[HySAD]] introduces [[MC-Relief]] to [[select]] effective [[detection metric]]s, and [[Semi-supervised Naive Bayes (SNB_lambda)]] to precisely separate [[Random-Filler model attacker]]s and [[Average-Filler model attacker]]s from normal [[user]]s.
In particular, instead of [[optimizing]] a [[deterministic]] [[hard cascade]], [[we]] optimize a [[sotchastic soft cascade]] where each [[stage]] accepts or rejects [[sample]]s according to a [[probability distribution]] induced by the previous [[stage-specific]] [[classifier]] .
In particular, motivated by the [[characteristic]]s of [[heat pump consumption pattern]], we [[extract]] novel [[feature]]s that are [[highly relevant]] to [[heat pump usage]] from [[smart meter data]] and [[weather data]] .
In particular, [[our goal]] is to both [[qualitatively]] and [[quantitatively analyze]] when [[social context]] actually helps with [[TDE]] .
In particular recent [[research]] has focused on [[identifying]] how [[information propagate]]s through the [[Internet]] .
In particular, [[TAP]] can take results of any [[topic modeling]] and the existing [[network]] [[structure]] to perform [[topic-level]] [[influence propagation]] .
In particular, [[the brief]] examines the changing incidence of both [[distressed neighborhood]]s, in which at least [[40 percent]] of [[resident]]s live [[below poverty]], and [[high-poverty neighborhood]]s, where at least [[20 percent]] of [[resident]]s are [[poor]] .
In particular, the [[integrated framework]] defines [[monitoring condition]]s and [[the optimal corresponding delay time]]s based on an [[off-line analysis]] of [[historical alert]]s and [[incident ticket]]s.
In particular, [[traditional]] [[method]]s utilize an [[index]] to [[search]] in a [[reduced-dimensionality feature space]]; however, for [[high time-series length]], search with such an [[index]] yields many [[false hit]]s that need to be eliminated by accessing the [[full record]]s.
In particular, [[we]] address the problem of [[learning]] [[binary code]]s for [[collaborative filtering]], which enables us to [[efficiently]] make [[recommendations with time complexity]] that is [[independent]] of the [[total number of item]]s.
In particular, we compute a [[failure probability]] for [[device]]s and [[link]]s in near [[real time]] using [[data]] from [[active monitoring]], and look for [[statistically]] significant increases in the [[failure probability]] .
In particular, [[we]] consider a [[general problem]] in [[PPDM]] - [[multiparty]] [[secure computation]] of some [[functions]] of [[secure summations]] of [[data spreading]] around multiple [[parties]] .
In particular, [[we]] emphasize two fundamental [[qualiti]]es for the [[goal metrics (or Overall Evaluation Criteria)]] of any [[online service]]: [[directionality]] and [[sensitivity]] .
In particular we explore [[supervision]] to enforce i) [[sparsity]], ii) [[diversity]], and iii) [[alternativeness]] in the [[role]]s.
In particular, [[we]] focus on [[constructing]] [[Bayesian belief networks]] .
In particular, [[we]] formalise how to [[encode]] [[sequential data]] using [[set]]s of [[serial episode]]s, and use the [[encoded length]] as a [[quality score]] .
In particular, [[we]] [[formalize]] the [[networked bandit problem]] and [[propose]] an [[algorithm]] that considers not only the [[selected arm]], but also the [[relationships between arm]]s.
In particular, [[we]] formulate it as a [[regularized optimization problem]] and propose [[effective]] and [[scalable algorithm]]s to solve [[it]] .
In particular, [[we]] illustrate that an [[optimal solution]] of the [[LP problem]] can be [[directly obtained]] <i>without calling a [[solver]] </i>.
In particular, [[we]] lack substantial evidence on which [[product]]s [[consumer]] [[dislike]] .
In particular, [[we]] model the problem as a [[constrained optimization problem]], which [[maximize]]s the expected [[advertiser]] [[revenue]] subject to the [[constraint]]s of the [[total budget]] of the [[advertiser]] and the [[range]]s of [[bid price]] change.
In particular, [[we]] [[observe]] a typical [[lag]] of 2.5 [[hours]] between the [[peaks]] of attention to a [[phrase]] in the [[news media]] and in [[blogs]] respectively, with divergent [[behavior]] around the overall [[peak]] and a "[[heartbeat]]"-like [[pattern]] in the handoff between [[news]] and [[blogs]] .
In particular, [[we]] propose a [[coupled]] [[Hidden Markov Models (HMM)-based approach]] to [[detect abnormal]] [[group-based]] [[trading]] [[behaviors]] .
In particular, [[we]] propose a [[family of algorithm]]s for the [[prediction problem]] by modeling three key aspects, i.e., [[non-linearity]], [[question / answer coupling]], and [[dynamics]] .
In particular, [[we]] propose a [[framework]] of [[evolution trust]], [[eTrust]], which exploits the [[dynamics of user preference]]s in the context of [[online product review]] .
In particular, [[we]] propose a [[probabilistic framework]] to [[simultaneously]] construct [[dual predictive model]]s and uncover the [[co-cluster]]s of [[origination]]s and [[destination]]s.
In particular, [[we]] propose a [[time-constrained probabilistic factor graph model (TPFG)]], which takes a [[research publication network]] as [[input]] and [[models]] the [[advisor-advisee relationship]] [[mining problem]] using a [[jointly likelihood objective function]] .
In particular, [[we]] propose [[Orthogonal Sparse LOgistic Regression (OSLOR) method]] to [[jointly optimize]] [[node selection]] and [[outbreak prediction]], where the [[prediction loss]] are combined with an [[orthogonal regularizer]] and [[L1 regularizer]] to guarantee good [[prediction accuracy]], as well as the [[sparsity]] and [[low-redundancy]] of selected [[sensor]]s.
In particular, [[we]] propose [[two]] innovative [[unsupervised methods]] for [[keyword]] and [[sentence extraction]], and show that the [[results]] obtained compare favorably with previously [[published results]] on [[established benchmarks]] .
In particular, [[we]] prove the [[NP-hardness]] of the [[optimal action extraction problem]] for [[ATMs]] and formulate [[this problem]] in an [[integer linear programming formulation]] which can be [[efficiently]] solved by existing [[package]]s.
In particular, [[we]] provide a [[taxonomy]] of [[social network activiti]]es, describe a [[system architecture]] (with a number of [[key components open-sourced]]) that supports [[fast iteration]] in [[model development]], demonstrate a number of [[key factor]]s for [[effective ranking]], and [[report experimental result]]s from extensive [[online bucket test]]s.
In particular, [[we]] provide the first [[linear time approximation scheme]] ([[LTAS]]) for the [[rank-one NMF]] .
In particular, [[we]] show how [[post-result]] [[behavior]] depends on various [[properties]] of the [[queries]], [[advertisement]], [[sites]], and [[users]], and [[build]] a [[classifier]] using [[properties]] such as these to [[predict]] certain aspects of the [[user behavior]] .
In particular, [[we]] show that if there exists a [[polynomial time]] [[approximation algorithm]] for [[damks]] with [[approximation ratio]] γ, then there is a [[polynomial time]] [[approximation algorithm]] for [[dks]] with [[approximation ratio]] γ 2/8.
In particular, [[we]] show that on [[real advertising data]] this [[model]] combines [[past exposure]] together with the [[user profile]] to identify better [[target user]]s over the [[conventional targeting model]]s.
In particular, [[we]] study the [[problem]] of determining the [[semantic orientation]]s ([[positive]], [[negative]] or [[neutral]]) of [[opinion]]s expressed on [[product feature]]s in [[review]]s.
In particular, [[we]] [[study]] the [[stability problem]] of [[randomized clustering method]]s (which usually [[produce]] different results at each [[run]]).
In particular, [[we]] will study [[candidate generate-and-test algorithms]], [[hyper-structure algorithm]]s and [[pattern growth algorithms]] .
In [[performance]] display [[advertising]] a key [[metric]] of a [[campaign effectiveness]] is its [[conversion rate]] -- the [[proportion of user]]s who take a [[predefined action]] on the [[advertiser website]], such as a [[purchase]] .
In plenty of [[scenarios]], [[data]] can be [[represented as]] [[vectors]] and then [[mathematically abstracted]] as [[points]] in a [[Euclidean space]] .
In [[pLSI]], each [[document]] is [[represented as]] a [[list of numbers]] (the [[mixing proportions]] for [[topics]]), and there is no [[generative probabilistic model]] for these [[numbers]] .
In [[point estimation]], the [[estimand]] is often some [[function]] of the basic [[parameter]] θ.
In practice, these [[deviation]]s indicate [[pattern]]s -- for example, [[botnet attacker]]s forming a [[bipartite core]] with their [[victim]]s over the [[duration]] of an [[attack]], [[family members bonding]] in a [[clique-like fashion]] over a [[difficult period]] of [[time]], or [[research collaborations forming]] and [[fading away]] over the [[year]]s.
In practice, with the help of [[our model]], [[hypothese]]s on how to improve a [[collaborative network]] can be [[tested quickly]] and [[reliably]], thereby significantly easing [[performance]] improvement of [[collaborative network]]s.
In [[principle]], this [[pattern]] could be [[endlessly iterated]] unless there exists a final [[enforcement level]], whose [[norm]]s are all [[regimented]], or whose [[violation]]s are not punished (see [[Figure 1]]).
In [[proposed scheme]], the [[data]] sent to the [[service provider]] is [[perturbed]] by a [[random transformation]], and the [[service provider]] [[trains]] the [[SVM]] for the [[data owner]] from the [[perturbed data]] .
In [[ranking creation]], [[given a request]], [[one want]]s to generate a [[ranking list of offering]]s based on the [[feature]]s derived from the [[request]] and the [[offering]]s.
In [[realistic setting]]s the [[prevalence]] of a [[class]] may change after a [[classifier]] is [[induced]] and this will degrade the [[performance]] of the [[classifier]] .
In reality, many [[text collection]]s are from [[specific]], [[dynamic]], or [[emerging domain]]s, which poses significant new challenges for [[entity recognition]] with increase in [[name ambiguity]] and [[context sparsity]], requiring [[entity detection]] without [[domain restriction]] .
In [[real-life settings]], this [[tool]] was used by [[beta-testers]] in the [[IBM]] [[IP]] [[department]] .
In [[real world]], however, [[human behavior]]s are [[actually complex]] and [[dynamic]]: they include [[correlation]]s between [[user]] and multiple types of [[object]]s and also continuously evolve along [[time]] .
In [[recent work]], [[we]] proposed a [[structural SVM based approach]] for [[optimizing]] this [[performance measure]] ([[Narasimhan]] and [[Agarwal, 2013]]).
In recent work where more than a million [[human judgment]]s were [[collected]] on [[unstructured text]] and [[imagery data]] around [[natural disaster]]s, I will present [[observation]]s that [[debunk]] recent [[over-optimistic claim]]s about the utility of [[social media]] following [[disaster]]s.
In recent years, [[boosting]], [[neural networks]], [[support vector machines]], and many other [[technique]]s have been applied.
In [[recent years]], [[deep artificial neural network]]s (including [[recurrent]] ones) have won numerous [[contest]]s in [[pattern recognition]] and [[machine learning]] .
In recent years, there are emerging [[research work]]s focusing on building [[the platform]] and [[scaling]] [[it]] up, [[best practice]]s and [[lessons learned]] to obtain [[trustworthy result]]s, and [[experiment design technique]]s and various issues related to [[statistical inference and testing]] .
In recent years there has been a surge of interest in [[learning compact distributed representations or embeddings]] for many [[machine learning task]]s, including [[collaborative filtering]] ([[Koren et al., 2009]]), [[image retrieval]] ([[Weston et al., 2011]]), [[relation extraction]] ([[Riedel et al., 2013]]), [[word semantics]] and [[language modeling]] ([[Bengio et al., 2006]]; [[Mnih & Hinton, 2008]]; [[Mikolov et al., 2013]]), and many others.
In recent years there has been significant [[research effort]] spent on [[efficiently]] [[discovering these motifs]] in [[static]] [[offline databases]] .
In recent years, with the [[widespread usage]] of [[Web 2.0 technique]]s, [[crowdsourcing]] plays an important [[role]] in offering [[human intelligence]] in various [[service website]]s, such as [[Yahoo ! Answer]] and [[Quora]] .
In [[rehabilitation research]], change is commonly measured in such [[dependent variable]]s as [[employment status]], [[income]], [[empowerment]], [[assertiveness]], [[self-advocacy skills]], and [[adjustment to disability]] .
In return for these [[advantage]]s, however, there is generally a [[sacrifice]] in [[accuracy]], i.e., [[flexibility]] of [[model representation]] (e.g., [[linear]], [[rule-based]], etc.) and [[model complexity]] needs to be restricted in order for [[user]]s to be able to understand [[the results]] .
In [[Rise of the Robots]], [[Ford]] details what [[machine intelligence]] and [[robotics]] can accomplish, and implores [[employer]]s, [[scholar]]s, and [[policy maker]]s alike to face the implications.
In [[Rules of Play]] [[Katie Salen]] and [[Eric Zimmerman]] present a much-needed [[primer]] for [[this emerging field]] .
In [[SAM]], (i) [[functional optimisation]] is utilised to seek for [[optimal bidding]] to [[maximise]] the [[expected]] [[arbitrage net profit]], and (ii) a [[portfolio-based risk management solution]] is leveraged to reallocate [[bid volume]] and [[budget]] across the [[set]] of [[campaign]]s to make a [[risk]] and return [[trade-off]] .
[[InsDif]] works by transforming single-instances into the [[MIML representation]] for [[learning]], while [[SubCod]] works by transforming [[single-label example]]s into the [[MIML representation]] for [[learning]] .
In [[self-training]], the existing model first [[labels]] [[unlabeled data]] .
In [[semi-supervised multi-view learning]], [[unlabeled sample complexity (u.s.c.)]] specifies the [[size]] of [[unlabeled training sample]] that [[guarantee]]s a [[desired]] [[learning error]] .
In [[sentiment prediction task]]s these [[representation]]s outperform other [[state-of-the-art]] [[approach]]es on commonly used [[dataset]]s, such as [[movie review]]s, without using any [[pre-defined]] [[sentiment lexica]] or [[polarity shifting rule]]s.
In services such as [[Foursquare.com]], whether a [[person]] likes an [[article]] is considered [[private]] and therefore not disclosed; only the [[aggregative statistics]] of [[article]]s (i.e., how many [[people]] like [[this article]]) is [[revealed]] .
In several [[applications]] involving [[regression]] or [[classification]], along with making [[prediction]]s it is important to assess how [[accurate]] or reliable [[individual]] [[prediction]]s are.
In several [[setting]]s, one can leverage [[borrowed resource]]s with which [[task]]s can be accomplished more [[efficiently]] and [[cheaply]] .
In [[short]], perhaps we need a [[model]] that [[predict]]s the [[intergroup weather]]: [[Stereotype content]] may respond to [[systematic principle]]s, just as [[stereotyping process]]es do.
In simulations [[we]] show that an [[aggregate shock process]] proxies reasonably well the [[cyclical behaviour]] of [[job creation]] and [[job destruction]] in the [[United States]] .
In [[social choice]], a [[preference function (PF)]] takes a [[set of vote]]s ([[linear order]]s over a [[set of alternatives]]) as [[input]], and [[produce]]s one or more [[ranking]]s (also [[linear order]]s over the [[alternatives]]) as [[output]] .
Insofar as those efforts are directed at [[stimulating buying]] by [[people with stagnant incomes]], they must lead to [[increase]]d [[debt]] and eventually to [[increase]]d [[bankruptcy rate]]s.
In some [[experiment]]s, [[DeepWalk's representation]]s are able to [[outperform]] all [[baseline method]]s while using 60% less [[training data]] .
In [[speech processing]], [[phoneme]] or [[word lattice]]s ([[Campbell et al., 2007]]; [[Mathias and Byrne, 2006]]; [[Collins et al., 2004]]) are used as an interface between [[speech recognition]] and [[understanding]] .
In [[speech recognition]], [[confidence measures (CM)]] are used to [[evaluate]] [[reliability]] of [[recognition result]]s.
Inspired by [[empirical studies]] of [[networked system]]s such as [[the Internet]], [[social network]]s, and [[biological network]]s, [[researcher]]s have in recent years developed a variety of [[technique]]s and [[models]] to help us understand or [[predict the behavior]] of these [[systems]] .
Inspired by [[human transitive inference]] and [[learning ability]], whereby two seemingly [[unrelated concept]]s can be connected by a [[string of intermediate bridge]]s using [[auxiliary concept]]s, [[in this paper we]] study a novel [[learning problem]]: <i>[[Transitive Transfer Learning]]</i> (abbreviated to [[TTL]]).
Inspired by the powerful [[synchronization]], [[we]] propose [[Sync]], a [[novel]] [[approach to clustering]] .
Inspired by the success of [[myopic method]]s for [[active learning]] and [[bandit problem]]s, [[we]] propose a [[myopic method]] for [[active search]] on [[graph]]s.
Inspired from the recent [[developments]] on [[manifold learning]] and [[L1-regularized models]] for [[subset selection]], [[we]] propose in [[this paper]] a [[new approach]], called [[<i>Multi-Cluster Feature Selection</i> (MCFS)]], for [[unsupervised feature selection]] .
In spite of extensive [[research]] in recent years, finding [[exact]] [[time series motifs]] in [[massive databases]] is an [[open problem]] .
In spite of this additional [[power]], [[exact learning]] and [[inference algorithm]]s for [[semi-CRFs]] are [[polynomial-time]] — often only a [[small]] [[constant factor]] slower than [[conventional CRFs]] .
[[Instance]]s arriving in a [[data stream]] are usually [[time-dependent]], and the underlying [[concept]] that they represent may [[evolve over time]] .
In [[statistics]], this [[classifier]] is motivated by the [[assumption]] that the [[log probability]], <math>\log p(y \vert x)</math>, of each [[class]] is a [[linear function]] of x, plus a [[normalization constant]] .
Instead, [[domain expert]]s interact with [[DeepDive]] by defining [[feature]]s or [[rule]]s about the [[domain]] .
Instead he met [[Ruth]], a [[woman]] who [[taught]] him a series of [[exercise]]s to ease his own [[suffering]] and manifest his [[greatest desire]]s.
Instead, it has also become more [[clustered]] and [[concentrated]] in [[distressed]] and [[high-poverty neighborhood]]s, [[eroding]] the brief [[progress]] made against [[concentrated poverty]] during the [[late 1990s]] .
Instead of a commonly used [[mutual information measure]] based on [[Kullback-Leibler divergence]], [[we]] use a [[quadratic divergence measure]], which allows us to make an [[efficient]] [[non-parametric implementation]] and requires no [[prior assumption]]s about [[class densiti]]es.
Instead of complete [[enumeration]], which is typically [[infeasible]] for this [[class]] of [[pattern]]s, [[we]] develop effective [[sampling method]]s to extract a representative subset of the minimal [[Boolean pattern]]s (in [[disjunctive normal form - DNF]]).
Instead of eagerly [[matching]] [[incoming event]]s, the [[pre-processing step]] [[buffers event]]s in a [[match window]] to apply different [[pruning technique]]s ([[filtering]], [[partitioning]], and [[testing]] for [[necessary match condition]]s).
Instead of focusing on [[arrival rates]], [[we]] [[reconstruct]] [[bursts]] as a [[dynamic phenomenon]], using [[kinetics concepts]] from [[physics]] -- [[mass]] and [[velocity]] -- and [[derive]] [[momentum]], [[acceleration]], and [[force]] from these.
Instead of learning a single [[model]] on a [[sliding window]] or [[ensemble learning]], [[SyncStream]] captures [[evolving concept]]s by [[dynamically]] maintaining a [[set of prototype]]s in a new [[data structure]] called the [[P-tree]] .
Instead of [[learning textual model]]s to [[predict content polarity]] (i.e., the traditional [[sentiment analysis approach]]), [[we]] first [[measure]] the [[bias]] of [[social media user]]s toward a [[topic]], by solving a [[relational learning task]] over a [[network of user]]s connected by [[endorsement]]s (e.g., [[retweet]]s in [[Twitter]]).
Instead of merely searching over [[patent content]], [[we]] focus on [[studying]] the [[heterogeneous patent network]] derived from the [[patent database]], which is [[represented by]] several [[types of object]]s ([[compani]]es, [[inventor]]s, and [[technical content]]) [[jointly]] [[evolving over time]] .
Instead of navigating through [[candidate record]]s via an [[index]], [[we]] access their [[feature]]s, obtained by a [[multi-resolution transform]], in a [[stepwise sequential-scan]] manner, one [[level of resolution]] at a [[time]], over a [[vertical representation]] .
Instead of [[ranking documents]] by their [[relevance score]], here we [[rank]] [[field]]s (and [[record]]s) by their [[confidence score]], where a [[correctly labeled]] [[field]] is analogous to a [[relevant document]] .
Instead of requiring multiple [[pass]]es of [[training]], the [[constrained formulation]] allows the [[cross-validation]] be handled in one [[pass]] of [[constrained optimization]] .
Instead of treating each individual [[page]] [[independently]], [[we]] propose a [[list-wise strategy]] by taking into account the [[site-level]] [[knowledge]] .
Instead, programs written in a particular [[“high level” language]] such as [[C++]], [[Python]] or [[Java]] are translated by a special piece of [[software]] (a [[compiler]] or an [[interpreter]]) into [[low-level instruction]]s which a [[computer]] can actually run.
Instead, [[we]] conduct [[this study]] by implementing a [[privacy-preserving]] [[large]] [[distribute]] [[social sensor system]] in a [[large]] [[global IT company]] to capture the [[multifaceted activities]] of 30,000+ [[people]], including [[communications]] (e.g., [[emails]], [[instant messaging]], etc.) and [[Web 2.0]] [[activities]] (e.g., [[social bookmarking]], [[file sharing]], [[blogging]], etc).
Instead, [[we]] formulate the [[problem to model]] multiple [[network]]s as "[[composite network knowledge transfer]] ".
Instead, [[we]] look for a [[sparse set]] of [[center node]]s so that the effective [[conductance]] from the [[center]] to the rest of the [[graph]] has [[maximum entropy]] .
Instead, [[we]] use a [[probabilistic context free grammar (PCFG)]] based [[unit extractor]] on the [[table]]s, and retain several [[top-scoring extraction]]s of [[quantity]] and [[numeral]]s.
In [[stock market]]s, an emerging challenge for [[surveillance]] is that a group of [[hidden manipulator]]s [[collaborate]] with each other to manipulate the [[price movement]] of [[securiti]]es.
In [[streaming partitioning]], [[graph]]s are [[partitioned serially]] in a single [[pass]] .
In structuralism, the [[morpheme]]s of the [[open]] and [[closed class]]es are called [[<i>free</i> morpheme]]s, in contradistinction to [[<i>bound</i> morpheme]]s.
In Study 1, over 200 [[college student]]s [[estimated]] how much their own [[chance of]] [[experiencing]] 42 [[event]]s differed from the [[chance]]s of their [[classmate]]s.
In [[such cases]], we need to collect additional [[evidence]]s [[scattered]] in [[internal and external corpus]] to augment the [[knowledge base]]s and enhance their [[disambiguation power]] .
In such cases, we seek to [[adapt existing model]]s from a [[resource-rich]] [[source domain]] to a [[resource-poor]] [[target domain]] .
In such [[instance]]s, [[we]] show that [[the algorithm]] is a small [[constant]] [[factor]] [[approximation]] of the [[optimum]] .
[[Integrating]] these [[heterogeneous data source]]s, to automatically [[infer]] the [[function of protein]]s, is a fundamental challenge in [[computational biology]] .
[[Integrity]] in [[engineering research]] and [[practice]] depends on [[value]]s.
[[Intelligence analyst]]s grapple with many [[challenge]]s, chief among them is the need for [[software support]] in [[storytelling]], i.e., automatically ' [[connecting the dots]]' between [[disparate entiti]]es (e.g., [[people]], [[organization]]s) in an effort to form [[hypothese]]s and suggest [[non-obvious relationship]]s.
[[Intelligence Quotient (IQ) Test]] is a [[set of standardized question]]s designed to [[evaluate]] [[human intelligence]] .
[[Intelligent information system]]s rely on [[advanced representations of application domain]]s, [[task]]s, [[method]]s to solve those [[task]]s, and the available [[data]] .
[[Interaction Network]]s, [[Neighborhood Analysis]], [[Activation Function]]s
[[Interactive alignment]]; [[Interpersonal synergies]]; [[Communication]]; [[Decision-making]]; [[Distributed cognition]]; [[Social cognition]]; [[Pragmatics]]; [[Dynamical systems]] .
Interesting [[activity-level consumption pattern]]s have been [[identified]], and the [[evaluation]] on both [[real]] and [[synthetic dataset]]s has shown high [[accuracy]] on [[discovering]] washer and shower.
Interestingly, [[our approach]] resembles some [[actual]] [[selection practice]]s that have recently received [[legal scrutiny]] .
Interestingly, [[we]] show that the proposed [[approximate formulation]] can be [[transformed]] into an instance of the [[minimum s-t cut problem]], which can be solved [[efficiently]] by [[finding]] [[maximum]] [[flows]] .
Interest in [[SSL]] has increased in recent years, particularly because of [[application domains]] in which [[unlabeled data]] are plentiful, such as [[images]], [[text]], and [[bioinformatics]] .
In terms of [[matching subgraph count]]s, our [[method]] [[outperform]]s [[similar algorithm]]s used with the [[Stochastic Kronecker Graph model]] .
In terms of [[word error rate]], relative improvements of 9.2% ([[Cantonese]], [[ML training]]), 9.3% ([[Tagalog]], [[BMMI-SAT training]]), 12% ([[Tagalog]], [[confusion network]] combinations with [[MFCC]]s), and 8.7% ([[Switchboard]]) are achieved.
[[Internet-based electronic marketplace]]s leverage information technology to [[match]] [[buyer]]s and [[seller]]s with [[increased]] [[effective]]ness and [[lower]] [[transaction cost]]s, leading to more [[efficient]], [["friction-free" market]]s.
[[Internet display advertising]] is a [[critical revenue source]] for [[publisher]]s and [[online content provider]]s, and is supported by [[massive amount]]s of [[user]] and [[publisher data]] .
[[Internet users]] regularly have the need to [[find]] [[biographies]] and [[facts]] of [[people]] of interest.
[[Interpretable model]]s provide [[significant insight]]s on [[data]] and [[model behavior]]s and may convince [[end-user]]s to employ certain [[model]]s.
[[Interpretation]] of these [[factor]]s suggested that [[courage]] may be classified by more [[complex]], [[context-based situation]]s.
[[Inter-sentence similarity]] is replaced by [[rank]] in the [[local context]] .
In [[text]], this usually amounts to examining [[pairs]] of [[entities in a document]] and determining (from [[local]] [[language cues]]) whether a [[relation]] exists between [[them]] .
In that case, [[mass-produced]], [[fully educated]] [[robot scientist]]s working [[diligently]], [[cheaply]], [[rapidly]] and increasingly effectively will ensure that most of what science knows in [[2050]] will have been discovered by our [[artificial progeny]]!
In the [[ABECOS electronic commerce system]], [[buyer agent]]s, [[seller agent]]s, and [[directory agent]]s liaise with one another in [[e-commerce activiti]]es.
In the [[baseNP]] [[experiments]] aimed at [[non-recursive NP structures]], [[we]] use the [[chunk]] [[tag set]] (I, O, B}, where [[words]] marked [[I]] are [[inside]] some [[baseNP]], those marked [[O]] are [[outside]], and the [[B]] tag is used to mark the [[left most item]] of a [[baseNP]] which immediately follows another [[baseNP]] .
In the [[biomedical domain]], [[entiti]]es of interest are usually [[reference]]s to [[gene]]s, [[protein]]s, [[cell type]]s, and the like.
In the case of [[text]], examples of such [[summaries]] include [[newswire articles]], [[headlines]], and the [[information snippets]] returned by [[Google]] .
In the competitive [[environment]] of the [[internet]], [[retaining]] and growing one's [[user base]] is of major concern to most [[web service]]s.
In the [[context]] of [[information flow]], [[we]] studied the [[event recommendation problem]] .
In the context of [[online learning]], a [[greedy]], [[fast]] and [[provably]] [[near-optimal algorithm]] is employed to [[sequentially]] [[maximize]] the [[mutual information]] between [[past]] and [[future observation]]s, taking advantage of [[submodularity properti]]es.
In the current [[social network]], a [[user]] may have hundreds of [[friend]]s and find it very time consuming to [[categorize]] and [[tag]] every [[friend]] manually.
[[In the current study, we]] investigated the [[gender difference]]s in interests as an [[explanation]] for the [[differential distribution]] of [[women]] across [[sub-disciplines of STEM]] as well as the overall [[underrepresentation of women]] in [[STEM field]]s.
In the design of [[learning]] formulation, [[we]] propose a local [[regularization]] [[framework]] that can incorporate the [[correlation]]s among [[term]]s explicitly.
In [[the DG network]]s, we aim to use the [[disease network]] to [[predict]] the [[association]]s between [[gene]]s.
In the [[economic difficulties experienced]] in the former [[Soviet Union]] and [[countri]]es in [[Eastern Europe]], the absence of [[institutional structure]]s and [[behavioral code]]s that are central to [[successful]] [[capitalism]] has been particularly important.
In the [[era of]] [[big data]], [[information]] regarding the same [[object]]s can be collected from increasingly more [[source]]s.
In the era of [[EHR]]s, it is possible to examine the [[outcomes of decision]]s made by [[doctor]]s during [[clinical practice]] to [[identify]] [[patterns of care]] --- [[generating evidence]] based on the [[collective practice]] of [[expert]]s.
In the [[experimental section]], [[we]] test [[the algorithm]] for [[dalks]] on [[large publicly available]] [[web graph]]s.
In the [[experiment]]s, the proposed [[method]] reduces [[feature set]] sizes by >90 % and >30 % compared to complete [[tree]] [[mining]] and open [[tree]] [[mining]], respectively.
In the [[experiment]]s, [[we]] use two existing [[knowledge base]]s as our [[source]]s of [[world knowledge]] .
In the [[field of natural language processing]], [[SVM]]s are applied to [[text categorization]], and are reported to have achieved high [[accuracy]] without falling into [[over-fitting]] even with a [[large number]] of [[word]]s taken as the [[feature]]s ([[Joachims, 1998]]; [[Taira and Haruno, 1999]])
In the first [[method]], we divide our [[sample]]s according to the [[availability]] of [[data source]]s, and we [[learn shared set]]s of [[feature]]s with [[state-of-the-art]] [[sparse learning method]]s.
In the first part of [[this work]], [[we]] outline a [[supervised learning approach]] to [[attribute value extraction]] where we [[train]] a [[sequential classifier]] and evaluate the [[extraction performance]] on a set of [[hand-labeled]] [[listing]]s.
In the first [[stage]], [[Assember]] [[filter]]s [[trivial fluctuation]]s using [[wavelet transform]] and [[detect]]s [[frequent evolution]]s for [[individual sensor]]s via a [[segment-and-group approach]] .
In the first [[we]] used [[Reliability Analysis]] and [[Exploratory Factor Analysis]] to identify six [[attributes of engagement: Perceived Usability]], [[Aesthetic]]s, [[Focused Attention, Felt Involvement]], [[Novelty]], and [[Endurability]] .
In the [[foundational paper]]s, generally published in [[cognitive science]] and [[computational neuroscience journal]]s ([[Hopeld, 1982]], [[Jordan, 1997]], [[Elman, 1990]]), [[biologically plausible mechanism]]s are emphasized.
In the [[half century]] between [[1965]] and [[2015]], [[work rate]]s for the [[American male]] spiraled relentlessly [[downward]], and an ominous migration commenced: a “[[flight from work]],” in which [[ever-growing number]]s of [[working-age men]] [[exited the labor force altogether]] .
In the [[ICA model]], each [[node in the graph]] is [[represented as]] a [[vector]] that is a combination of [[node features]], Xi, and [[features]] that are [[constructed]] using the [[labels]] of the [[nodes]]’ [[immediate neighbors]] .
In their [[basic gameplay]], two [[teams of multiple player]]s compete against [[each other]] to destroy the [[enemy's base]], controlling a [[powerful unit]] known as "[[hero]]".
In their respective pages, the [[Planet]]s are listed along with their basic properties such as the year of planet’s discovery, [[mass]], [[radius]], [[orbital period]], [[semi-major axis]], [[eccentricity]], [[inclination]], [[longitude of periastron]], [[time of periastron]], [[maximum time variation]], and [[time of transit]], including all error range values.
In [[their work presented here]], [[Siu]] ([[University of British Columbia]]) and [[Jaimovich]] ([[Duke University]]) begin by looking at previous [[recession]]s and uncovered a striking pattern.
In [[the last decade]], the main focus in the [[study of]] [[flexible work]] has been [[job insecurity]], 10 a concept defined as “the [[discrepancy]] between the [[level of]] [[job security]] a [[person experience]]s and the [[level]] she might [[prefer]]”.11 Studies of [[self‐reported]] [[job insecurity]] after [[plant closing]]s present consistent evidence that [[job insecurity]] can have significant [[adverse effect]]s on [[self‐reported physical]] and mental health.12
In the late [[twentieth Century]], [[Artificial Intelligence (AI)]] adopted the term in the sense of a “[[specification]] of a [[conceptualization]]” — an [[ontology]] defines a [[set of]] [[representational primitive]]s to [[model]] a [[domain of knowledge]] .
In the [[mainstream framework]] of [[compressed sensing (CS)]], Ã is recovered from <i>[[M</i> [[linear measurement]]s <i>y</i> = <i>xS</i> â <i>[[R]]</i> / 1Ã <i>M </i>, where <b> [[S]] </b> â <i>[[R]]</i> <i>[[N]]</i> [[Ã]] <i>M</i> is often a [[Gaussian (or Gaussian-like) design matrix]] .
In the [[NER problem]], [[we]] regard each [[word]] in a [[sentence]] as a [[token]] .
In the [[Netherland]]s, there is a [[research tradition]] that [[measures fraud]] against [[regulation]]s by [[interviewing eligible individuals using a survey]] .
In the [[parallel case]], most [[algorithm]]s perform a [[sum-reduction]] at the end of each [[pass]] to construct the [[global count]]s, also incurring high [[synchronization cost]] .
In the past [[fifteen]] [[year]]s, [[employment]] has shifted due to [[a major recession]] and increasing [[globalization]], particularly with [[China]]’s introduction to the [[world economy]], as well as enormous changes in [[non-AI digital technology]] .
In the past [[half-century]], more than 150 [[article]]s have been [[published]] about [[Benford’s law]], a [[quirky law]] based on the number of times a particular [[digit]] occurs in a particular [[position in number]]s ([[Nigrini 1999]]).
In the past, [[ontologi]]es were perceived as [[arcane]], [[over-complicated]], and perhaps [[over-hyped]] .
In the past several years, a number of different [[language modeling]] improvements over simple [[trigram model]]s have been found, including [[caching]], [[higher-order n-gram]]s, [[skipping]], [[interpolated Kneser-Ney smoothing]], and [[clustering]] .
In the past three years, [[research]] in [[this area]] has been accelerated by the availability of the [[Semantic Vectors package]], a [[stable]], [[fast]], [[scalable]], and [[free software package]] for [[creating]] and [[exploring concepts]] in [[distributional model]]s.
In the present work, [[we]] show that [[CRFs]] beat all reported [[single-model]] [[NP chunking]] [[results]] on the standard [[evaluation dataset]], and are [[statistically indistinguishable]] from the previous best performer, a [[voting arrangement]] of 24 [[forward-]] and [[backward-looking support-vector classifiers]] (Kudo and Matsumoto, 2001).
In the previous [[work]], [[we]] developed [[FREyA]], an [[interactive Natural Language Interface]] for [[querying ontologi]]es.
In the [[probability product kernel]], [[data point]]s in the [[input space]] are [[mapped]] to [[distribution]]s over the [[sample space]] and a general [[inner product]] is then evaluated as the [[integral of the product]] of [[pair]]s of [[distribution]]s.
In the [[process]] also known as [[graph simplification]], [[node]]s and (unweighted) [[edge]]s are grouped to [[supernode]]s and [[superedge]]s, respectively, to obtain a smaller [[graph]] .
In the process, [[we]] create a [[word–category]] [[co-occurrence matrix]], which can be used for [[unsupervised word sense disambiguation]] and [[estimating]] [[distributional similarity]] of [[word senses]], as well.
In [[the proposed framework]], the [[SEMG data]] of a [[subject]] represent a [[domain]]; [[data]] from multiple [[subject]]s in the [[training set]] form the [[multiple source domain]]s and the [[test subject data]] form the [[target domain]] .
In the [[sampling model]], [[we]] focus on [[all-pair shortest path distance]]s, [[node similariti]]es, and [[correlation clustering]] .
In these [[application]]s, [[data stream]]s usually arrive at a [[speed]] of [[GB/second]], and it is necessary to [[classify]] each [[stream record]] in a [[timely manner]] .
In [[these approaches]] [[input object]]s (such as [[image]]s, [[relation]]s or [[word]]s) are [[mapped to dense vectors]] having [[lower-dimensionality]] than the [[cardinality of the input]]s, with the goal that the geometry of his [[low-dimensional]] [[latent embedded space]] be [[smooth]] with respect to some [[measure of similarity]] in the [[target domain]] .
In these cases, [[Collective Entity Linking]], in which the [[name mention]]s in the same [[document]] are [[linked]] [[jointly]] by exploiting the [[interdependence]] between [[them]], can improve the [[entity linking accuracy]] .
In these cases, [[local government]]s are expected to adhere to the [[financial reporting]] and [[accounting requirement]]s of the appropriate [[regulatory agenci]]es or [[professional]] [[association]]s.
In the [[second setting]], [[we]] [[pre-train]] the [[word vector]]s with an [[unsupervised neural language model]] ([[Bengio et al., 2003]]; [[Collobert and Weston, 2008]]).
In the second [[stage]], [[Assember]] [[generate]]s [[SCP]]s by [[assembling]] the [[frequent evolution]]s of [[individual sensor]]s.
In the second stage, <i>quantitative</i> [[representation]] of [[coupled behavior]]s is [[learn]]ed via proper [[method]]s.
In these [[datasets]] (called "[[information networks]]"), [[closely related]] [[objects]] that share the same [[properties]] or [[interests]] form a [[community]] .
In [[these method]]s, a [[network]] is treated as a [[view]] or [[domain]].The [[key assumption]] is that there is a [[common clustering structure]] shared across all [[view]]s ([[domain]]s), and different [[view]]s ([[domain]]s) provide [[compatible]] and [[complementary]] [[information]] on this [[underlying clustering structure]] .
In the sequel, [[we]] use [[table]]s and [[relation]]s interchangeably, but [[domain]]s and [[column]]s differently.
In these [[tasks]], the <i>[[prequential error]]</i> estimated using [[fading factor]]s provide reliable [[estimator]]s.
In the [[sketching model]], [[we]] show how to [[efficiently]] [[approximate]] the [[neighborhood function]], which in turn can be used to [[approximate]] various notions of [[centrality]] .
In the [[statistics community]], [[outlier detection]] for [[time series data]] has been studied for [[decade]]s.
In [[the system]], the [[impression]]s that satisfy the [[campaign targeting attribute]]s are [[partition]]ed into multiple [[mutually exclusive samples]] .
In the talk, some typical [[research work]] in the [[field]] of [[uncertain graph mining]] will also be introduced, including [[frequent subgraph pattern mining]], [[dense subgraph detection]], [[reliable subgraph discovery]], and [[clustering]] on [[uncertain graph data]] .
In the years ahead, [[technological improvement]]s in [[robotics]] and [[automation]] will boost [[productivity]] and [[efficiency]], implying significant [[economic gain]]s for [[compani]]es.
In this [[approach]], [[we]] enforce [[model sparsity]] and [[smoothness]] in the [[functional space]] [[spanned]] by the [[basis functions]] .
In this [[article]], [[we]] describe the [[ranking technique]]s and [[system architecture]] of [[COA]] .
In [[this article]], [[we]] give the first [[algorithm]] for [[2-locus genome-wide association studi]]es that is [[subquadratic]] in the number, <i>n </i>, of [[SNP]]s.
[[In this article we]] present a [[multi-level algorithm]] for [[graph clustering]] using [[flow]]s that [[deliver]]s significant [[improvements]] in both [[quality]] and [[speed]] .
In [[this]] article, [[we]] present a [[thorough analysis]] of the [[literature]] on [[duplicate record detection]] .
In [[this article]], we present [[OntoDM-core]], an [[ontology]] of core [[data mining entiti]]es.
In [[this article]], [[we]] propose a [[parallel framework]] for solving [[this problem]] in [[metric space]]s.
[[In this article, we]] report our efforts in [[mining]] the [[information]] [[encoded]] as [[clickthrough]] [[data]] in the [[server log]]s to [[evaluate]] and [[monitor]] the [[relevance]] [[ranking]] [[quality]] of a [[commercial]] [[web search engine]] .
In [[this article]] [[we]] show that [[text understanding]] can be handled by a [[deep learning system]] without [[artificially imbedding]] [[knowledge about words]], [[phrases]], [[sentence]]s or any other [[syntactic]] or [[semantic structure]]s associated with a [[language]] .
In [[this book]], [[Denis Serre]] begins by providing a [[clean]] and [[concise introduction]] to the basic [[theory of matrice]]s.
In this [[case]] a [[sample]] from a missing [[class]] is incorrectly [[classified]] to one of the existing [[class]]es.
In [[this case]], each single [[task]] needs [[large amount]]s of [[data]] to provide [[accurate]] [[estimation]]s.
In [[this chapter]], [[I]] will explore the [[complex]] [[link]]s between [[economic culture]] and [[economic progress]] .
In [[this context]], [[we]] introduce [[ClickRank]], an efficient, [[scalable]] [[algorithm]] for [[estimating]] [[web page]] and [[web site]] importance from [[browsing information]] .
In [[this extended abstract we]] introduce the [[Arcade Learning Environment (ALE)]]: both a [[challenge problem]] and a [[platform]] and [[methodology for evaluating]] the development of [[general]], [[domain-independent AI technology]] .
In [[this fascinating]] and [[witty account]], [[Bloom draw]]s on [[child development]], [[philosophy]], [[neuroscience]], and behavioral economics in order to address [[pleasures noble]] and [[seamy]], [[highbrow]] and [[lowbrow]] .
In this [[feature space]] a [[linear decision surface]] is [[constructed]] .
[[In this follow-on paper, we]] focus on [[pitfalls]] [[we]] have seen after running numerous [[experiments]] at [[Microsoft]] .
In this [[interactive setting]], [[the algorithm]] asks the [[user]] to [[label]] a [[query]] [[data point]] under an existing [[category]] or [[declare]] the [[query]] [[data point]] to belong to a [[previously undiscovered]] [[category]] .
In this light, [[we]] propose a novel [[collaborative boosting framework]] comprising a [[text-to-activity classifier]] for each [[user]], and a [[mechanism for collaboration]] between [[classifier]]s of [[user]]s having [[social connection]]s.
In [[this model]], a [[sentence]] [[connectivity matrix]] is constructed based on [[cosine similarity]] .
[[In this model, we]] allow the [[autoregressive model]] to [[change]] over [[time]] .
In [[this model]] we are given a [[corpus]] of [[words]] <math>w</math> and their [[contexts]] <math>c</math>.
In [[this model]], we have a [[public graph]] and each [[node]] in the [[public graph]] has an associated [[private graph]] .
In [[this monograph]], [[we]] investigate the [[principle]]s and [[methodologies of mining latent entity structure]]s from [[massive unstructured and interconnected data]] .
In this new [[problem setting]], [[cluster]]s and [[outlier]]s are [[then simultaneously mined]] according to this [[user preference]] .
In this new world, [[social capital]] is as important as [[financial capital]], [[access]] trumps [[ownership]], [[sustainability]] supersedes [[consumerism]], [[cooperation]] ousts [[competition]], and "[[exchange value]]" in the [[capitalist marketplace]] is increasingly replaced by "[[sharable value]]" on the [[Collaborative Commons]] .
[[In this paper]], and based on an [[exhaustive]] and [[collaborative]] [[case study research]] in a large [[software-intense company]] with highly developed [[experimentation culture]], we present the [[evolution process]] of moving from [[ad-hoc customer data analysi]]s towards [[continuous controlled experimentation]] at [[scale]] .
In [[this paper]], an [[efficient]] [[two-stage approach]] is proposed to solve a [[class]] of [[dimensionality reduction technique]]s, including [[Canonical Correlation Analysis]], [[Orthonormal Partial Least Squares]], [[linear Discriminant Analysis]], and [[Hypergraph Spectral Learning]] .
[[In this paper]], based on the [[matrix factorization approach]], [[we]] propose a [[method]] called [[bounded nonnegative matrix tri-factorization (BNMTF)]] .
[[In this paper]], by [[observing]] that many [[data-driven application]]s in [[search engine]]s highly rely on [[online mining]] of [[search log]]s, [[we]] develop an [[OLAP system]] on [[search log]] which serves as an [[infrastructure]] supporting various [[data-driven application]] .
[[In this paper]], in a departure from recent [[efforts]], [[we]] consider the [[problem]] of [[analyzing patterns]] and [[critical events]] that affect the [[dynamic graph]] from the [[viewpoint]] of a [[single node]], or a selected [[subset]] of [[nodes]] .
In [[this paper]], I present a simple [[unsupervised learning algorithm]] for [[classifying]] a [[review]] as [[recommended]] or not [[recommended]] .
[[In this paper]], [[maximum entropy]] is used for [[text classification]] by [[estimating]] the [[conditional distribution]] of the [[class variable]] given the [[document]] .
[[In this paper]], motivated by the [[ubiquity]] of [[graph representation]]s in [[real-world application]]s, [[we]] propose to study [[selective sampling]] on [[graph]]s.
In [[this paper]], resting on the [[analysis]] of [[instruction frequency]] and [[function-based]] [[instruction sequence]]s, [[we]] develop an [[Automatic Malware Categorization System (AMCS)]] for [[automatically grouping]] [[malware]] [[sample]]s into [[families]] that share some common [[characteristics]] using a [[cluster ensemble]] by aggregating the [[clustering solutions]] [[generated by]] different [[base]] [[clustering algorithms]] .
[[In this paper]], the [[state-of-the-art]] of [[multi-label learning]] is reviewed in terms of paradigm [[formalization]], [[learning algorithm]]s and related [[learning setting]]s.
[[In this paper we]] adapt a well known [[streaming algorithm]] for [[approximating]] [[item frequenci]]es to the [[matrix sketching setting]] .
[[In this paper, we]] address several [[critical task]]s which can facilitate [[information sharing]] and [[collaboration]] between both [[private]] and [[public sector]] [[participant]]s for major [[disaster recovery planning]] and [[management]] .
In [[this paper, we]] address several key [[problem]]s which inhibit better [[information sharing]] and [[collaboration]] between both [[private]] and [[public sector]] participants for [[disaster management]] and [[recovery]] .
In [[this paper, we]] address the issue of [[training]] [[very large CRFs]], containing up to [[hundreds]] [[output labels]] and [[several billion]] [[features]] .
[[In this paper, we]] address the [[observation]] of [[noisy]], [[stochastic processes]] and attempt to [[detect]] [[temporal]] [[segments]] that are [[related]] to [[inconsistencies]] and [[irregularities]] in its [[dynamics]] .
[[In this paper, we]] address the problem of "[[consensus learning]]" among competing [[hypotheses]], which either rely on [[outside knowledge]] ([[supervised learning]]) or [[internal structure]] ([[unsupervised clustering]]).
[[In this paper we]] address the [[problem]] of [[modeling text stream]]s from two [[news source]]s - [[Twitter]] and [[Yahoo! New]]s.
[[In this paper we]] address the [[problem]] where the [[class]] [[distribution]] changes and only [[unlabeled examples]] are available from the new [[distribution]] .
[[In this paper, we]] address these [[concerns]] in the [[general]] [[FTFI mining problem]] .
In [[this paper, we]] address these two [[issues]] through employing [[abundant]] [[unlabeled data]] and pursuing [[sparsity of metrics]], resulting in a novel [[metric learning approach]] called [[semi-supervised sparse metric learning]] .
[[In this paper, we]] advance [[state-of-the-art technique]]s by [[adaptively modeling task relatedness]] and [[view consistency]] via a [[nonparametric Bayes model]]: we model [[task relatedness]] using [[normal penalty]] with [[sparse covariance]]s, and [[view consistency]] using [[matrix Dirichlet process]] .
[[In this paper, we]] advocate a [[recommendation support]] for [[active friending]], where a [[user actively]] specifies a [[friending target]] .
[[In this paper, we]] aim to [[automatically discovering]] and [[profiling users'communiti]]es by taking into account both the [[contact]]s and the [[topic]]s.
[[In this paper, we]] aim to [[detect]] [[electric heat pump]]s from [[coarse grained]] [[smart meter data]] for a [[heat pump]] [[marketing campaign]] .
[[In this paper, we]] aim to [[discover]] <i>[[top-k]]</i> such representative [[locally densest subgraph]]s of a [[graph]] .
[[In this paper, we]] aim to [[learn]] a [[ranking function]] that varies [[linearly]] and therefore [[monotonical]]ly along the [[geodesic]]s of the [[data manifold]] .
[[In this paper, we]] aim to [[predict]] the [[long-term impact]] of [[question]]s / [[answer]]s shortly after they are posted in the [[CQA site]]s.
[[In this paper, we]] analyze a flexible [[stochastic process model]], the <i>[[generalized linear auto-regressive process(GLARP)]]</i> and [[identify]] the [[condition]]s under which the [[impact of hidden variable]]s appears as an [[additive term]] to the [[evolution matrix]] [[estimated]] with the [[maximum likelihood]] .
In [[this paper]] we analyze for the first time the [[economic impact]] of [[industrial robot]]s, using new data on a [[panel]] of [[industri]]es in 17 [[countri]]es from [[1993]]-[[2007]] .
[[In this paper, we]] analyze [[Meta Algorithms (MAs)]] which work by adaptively combining [[iterate]]s from a [[pool]] of [[base optimization algorithm]]s.
In [[this paper, we]] analyze the [[accuracy/diversity trade-off]] and prove that [[classifiers]] that are more [[accurate]] and make more [[predictions]] in the [[minority group]] are more important for [[subensemble construction]] .
[[In this paper, we]] analyze the fundamental [[characteristic]]s of [[privacy]] and [[utility]], and show that it is inappropriate to directly [[compare]] [[privacy]] and [[utility]] .
[[In this paper we]] analyze [[time-stamp data]] from [[social media service]]s and find that the [[distribution]] of postings [[inter-arrival times (IAT)]] is characterized by four [[pattern]]s: (i) [[positive correlation]] between [[consecutive IAT]]s, (ii) [[heavy tail]]s, (iii) [[periodic spike]]s and (iv) [[bimodal distribution]] .
[[In this paper, we]] analyze various [[training criteria]] which directly [[optimize]] [[translation quality]] .
[[In this paper, we]] apply a [[trust region Newton method]] to [[maximize]] the [[log-likelihood]] of the [[logistic regression model]] .
[[In this paper, we]] apply [[our approach]] to [[biological network]]s to [[evaluate]] [[our approach]] and to understand how the [[biosystem]]s [[change over time]] .
[[In this paper, we]] approach both challenges by constructing a [[relationship]] to the [[concept]] of [[data compression]] using the [[Minimum Description Length principle]]: a [[detect]]ed [[cluster structure]] is the better the more [[efficient]] it can be exploited for [[data compression]] .
[[In this paper, we]] attempt [[harnessing]] [[historical cascade data]], propose a novel [[data driven approach]] to select important [[nodes as sensor]]s, and [[predict the outbreak]]s based on the [[cascading behavior]]s of these [[sensor]]s.
[[In this paper, we]] attempt to re-shape each [[input]] [[feature]] so that it is appropriate to use with a [[linear]] [[weight]] and to [[scale]] the different [[features]] in proportion to their [[predictive value]] .
[[In this paper, we]] build up a [[large human mobility database]] ([[GPS record]]s of 1.6 million [[user]]s over one year) and several different [[dataset]]s to capture and analyze [[human]] [[emergency behavior]] and their [[mobility]] following the [[Great East Japan Earthquake]] and [[Fukushima nuclear accident]] .
[[In this paper, we]] call the [[problem of recommending link]]s to [[jointly optimize]] for [[click]]s and [[post-click downstream utiliti]]es <i>[[click shaping]] </i>.
[[In this paper, we]] combine [[classic idea]]s in [[topic modeling]] with a [[variant]] of the [[mixed-membership block model]] recently developed in the [[statistical physics community]] .
[[In this paper, we]] combine [[ranking]] and [[classification]] in order to perform more [[accurate analysis]] of a [[heterogeneous information network]] .
[[In this paper we]] compare a [[latent factor]] ([[PureSVD]]) and a [[memory-based model]] with our novel [[PCA-based model]], which we call [[Eigenapp]] .
[[In this paper, we]] compare [[representative algorithm]]s from two [[approach]]es: working with individual [[reported location]]s vs. [[segment]]s between [[consecutive location]]s.
[[In this paper, we]] conduct [[extensive analyse]]s and [[comparisons to evaluate the effectiveness of task trail]]s in [[three search application]]s: determining [[user satisfaction]], [[predicting user search interest]]s, and [[query suggestion]] .
[[In this paper, we]] consider an [[instance]] of [[this problem]], where the [[object of interest]] is the structure of a [[social network]], i.e., a [[graph]] describing [[user]]s and their [[link]]s.
[[In this paper, we]] consider a [[nonconvex least squares formulation]], which seeks to [[minimize]] the [[least squares loss function]] with the [[rank]] [[constraint]] .
[[In this paper, we]] consider a novel [[optimization problem]]: given a [[training dataset]] of existing [[item]]s with their [[user-submitted tag]]s, and a [[query set]] of desirable [[tag]]s, design the k best new [[item]]s expected to attract the [[maximum number]] of desirable [[tag]]s.
[[In this paper, we]] consider a novel [[scheme]] referred to as [[Cartesian contour]] to [[concisely represent]] the [[collection]] of [[frequent itemset]]s.
[[In this paper, we]] consider designing [[clustering algorithm]]s that can be used in [[MapReduce]], the most popular [[programming environment]] for [[processing]] [[large dataset]]s.
[[In this paper, we]] consider the ability of an [[attacker]] to use [[data]] meeting [[privacy definition]]s to build an [[accurate]] [[classifier]] .
[[In this paper we]] consider the design options for [[migrating OSes]] [[running]] [[services with liveness constraints]], focusing on [[data center]] and [[cluster environment]]s.
[[In this paper, we]] consider the [[matrix completion problem]] when the [[observation]]s are [[one-bit measurement]]s of some [[underlying matrix]] <i> M </i>, and in particular the [[observed sample]]s consist only of [[one]]s and no [[zero]]s.
[[In this paper, we]] consider the [[problem]] of combining [[link]] and [[content analysis]] for [[community detection]] from [[networked data]], such as [[paper citation networks]] and [[World Wide Web]] .
In [[this paper, we]] consider the problem of [[extracting a relation]] of [[books]] {([[author]], [[title]]) [[pairs]] from [[the Web]] .
[[In this paper, we]] consider the [[problem of extracting structured data]] from [[web page]]s taking into account both the [[content]] of individual [[attribute]]s as well as the [[structure]] of [[page]]s and [[site]]s.
In [[this paper, we]] consider these [[factors]] by first motivating the use of a [[supervised framework]] through a careful [[investigation]] of [[issues]] such as [[network observational period]], [[generality of existing methods]], [[variance reduction]], [[topological causes]] and [[degrees of imbalance]], and [[sampling approaches]] .
[[In this paper, we]] consider the [[task of selecting]] [[initial seed user]]s of a [[topic]] with [[minimum size]] so that {\em with a [[guaranteed]] [[probability]]} the number of [[user]]s discussing the topic would reach a given [[threshold]] .
[[In this paper, we]] construct a large [[human mobility database]] that [[store]]s and manages [[GPS record]]s from [[mobile device]]s used by [[approximately]] 1.6 million [[people]] throughout [[Japan]] from [[1 August]] 2010 to 31 [[July 2011]] .
In [[this paper, we]] [[define]] and [[study]] a new opinionated [[text data analysis problem]] called [[Latent Aspect Rating Analysis (LARA)]], which aims at [[analyzing]] [[opinions]] expressed about an [[entity]] in an [[online review]] at the level of [[topical aspects]] to discover each individual [[reviewer]]'s [[latent opinion]] on each [[aspect]] as well as the [[relative emphasis]] on different [[aspects]] when forming the [[overall judgment of the entity]] .
[[In this paper, we]] define a novel [[graph density function]], which gives [[subgraph]]s of much [[higher quality]] than [[densest subgraph]]s: the [[graph]]s found by [[our method]] are [[compact]], [[dense]], and with smaller [[diameter]] .
[[In this paper we]] demonstrate a novel [[visualization system]] for [[analyzing]] how [[Twitter users]] [[collectively talk]] about [[space]] and for uncovering [[correlation]]s between [[geographical location]]s of [[Twitter user]]s and the [[location]]s they [[tweet]] about.
[[In this paper we]] demonstrate how to [[speed up]] the [[performance]] of the [[computation]] of [[frequent item]]s by almost one [[order of magnitude]] over the best [[published result]]s by [[matching]] the [[algorithm]] to the underlying [[hardware architecture]] .
[[In this paper, we]] describe a [[discriminatively-trained model]] that reasons about [[schema matching]], [[coreference]], and [[canonicalization]] [[jointly]] .
[[In this paper, we]] describe a [[framework]], based on [[possible-worlds semantics]]; when applied on an [[uncertain dataset]], it computes a [[set]] of [[representative clustering]]s, each of which has a [[probabilistic guarantee]] not to exceed some [[maximum distance]] to the [[ground]] [[truth clustering]], i.e., the [[clustering]] of the actual (but unknown) [[data]] .
In [[this paper, we]] describe a [[generative model]] for [[dyadic events]], where each [[event]] arises from one of <math>C</math> [[latent classes]], and the [[properties]] of the [[event]] ([[sender]], [[recipient]], and [[type]]) are chosen from [[distributions]] over these [[entities]] conditioned on the [[chosen class]] .
[[In this paper, we]] describe an [[approach]] to this [[problem]] based on [[automated URL classification]], using [[statistical methods]] to discover the [[tell-tale]] [[lexical]] and [[host-based properties]] of [[malicious Web site]] [[URLs]] .
[[In this paper, we]] describe and compare two [[user-friendly system]]s that seek to make the [[universal knowledge]] of [[Web KB]]s available to [[user]]s who neither know [[SPARQL]], nor the [[internals]] of the [[KB]]s.
[[In this paper, we]] describe an efficient [[dataflow]] [[implementation]] of a [[collaborative filtering (CF) solution]] to the [[Netflix Prize]] [[problem]] [1] based on [[weighted]] [[coclustering]] [5].
[[In this paper we]] describe an [[experiment]] in which [[colonies of naked mole rat]]s were [[tag]]ged with [[RFID transponder]]s.
In [[this paper]],[[we]] describe a [[supervised learning]] [[framework]], [[Dynamic Hierarchical Classification (DHC)]] for [[patient's risk-of-readmission prediction]] .
[[In this paper we]] describe a very important [[primitive]] for [[PEGASUS]], called [[GIM-V]] ([[generalized iterated matrix-vector multiplication]]).
[[In this paper, we]] describe methods for [[building]] and [[evaluation]] of [[limited domain question-answering character]]s.
[[In this paper, we]] describe [[our experience building]] and deploying [[GOODS]], a [[system]] to manage [[Google's internal data lake]] .
[[In this paper, we]] describe our [[method]]s and [[experiment]]s for deploying two new [[style-based recommender system]]s on the [[Etsy site]] .
[[In this paper, we]] describe [[PEGASUS]], an [[open source]] [[peta]] [[graph mining library]] which performs typical [[graph mining tasks]] such as [[computing]] the [[diameter of the graph]], computing the [[radius of each node]] and finding the [[connected components]] .
[[In this paper we]] describe several [[algorithm]]s designed for [[this task]], including techniques based on [[correlation coefficients]], [[vector-based similarity calculations]], and [[statistical Bayesian methods]] .
[[In this paper, we]] describe the [[architecture]] and main [[components]] of the [[system]] .
[[In this paper, we]] describe the first [[fast]] [[algorithm]] for [[computing PageRank]] on general [[graph]]s when the [[edge weight]]s are [[personalized]] .
In [[this paper, we]] describe the [[implicit]] [[social graph]] which is formed by [[users' interactions]] with [[contacts]] and [[groups of contacts]], and which is distinct from [[explicit social graphs]] in which [[users]] explicitly add other [[individuals]] as their "[[friends]]".
[[In this paper, we]] describe the [[TensorFlow dataflow model]] and demonstrate the compelling [[performance]] that [[TensorFlow]] achieves for several [[real-world application]]s.
In [[this paper, we]] describe two [[system]]s: one that [[extracts implicit (semantic) attributes]] and one that [[extracts explicit attributes]] from [[product description]]s and [[populates a knowledge base]] with these [[product]]s and [[attributes]] .
[[In this paper we]] [[design]] and implement a [[benchmarking framework]] for [[fair]] and exhaustive [[comparison]] of [[entity-annotation system]]s.
In [[this paper, we]] design a new [[heuristic algorithm]] that is easily [[scalable]] to [[millions]] of [[nodes]] and [[edges]] in [[our experiment]]s.
[[In this paper, we]] [[design]], [[implement]] and [[deploy]] an [[integrated solution]], named [[PDP-Miner]], which is a [[data analytics platform]] customized for [[process optimization]] in [[Plasma Display Panel (PDP) manufacturing]] .
[[In this paper, we]] develop a [[dynamic]] [[temporal graphical model]]s based on [[hidden Markov model]] [[regression]] and [[lasso]]-type [[algorithms]] .
[[In this paper, we]] develop a [[latent graphical model]] to [[infer]] [[patent quality]] from related [[measurement]]s.
[[In this paper, we]] develop a [[mechanistic framework]] to model [[social influence]] of [[prior collective opinion]]s (e.g., [[online product rating]]s) on subsequent [[individual decision making]] .
[[In this paper, we]] develop an [[algorithm]] to recommend [[scientific article]]s to [[user]]s of an [[online community]] .
[[In this paper, we]] develop and apply a [[methodology]] that is able to accurately [[assess]], both [[qualitatively]] and [[quantitatively]], the impacts of various [[content-agnostic factor]]s on [[video popularity]] .
In [[this paper, we]] develop an [[effective]] [[social network]] [[compression approach]] achieved by a novel [[Eulerian data structure]] using [[multi-position linearizations of directed graphs]] .
In [[this paper, we]] develop a new [[hashing algorithm]] to create [[efficient codes]] for [[large scale data]] of [[general formats]] with any [[kernel function]], including [[kernels on vectors]], [[graphs]], [[sequence]]s, [[sets]] and so on.
[[In this paper we]] develop a new [[theoretical framework]] casting [[dropout training]] in [[deep neural networks (NNs)]] as [[approximate Bayesian inference]] in [[deep Gaussian processes]] .
[[In this paper, we]] develop an [[integrated data mining approach]] to give [[early deterioration warning]]s for [[patient]]s under [[real-time monitoring]] in [[ICU]] and [[RDS]] .
[[In this paper, we]] develop an [[intelligent file scoring system]] ([[IFSS]] for short) for [[malware detection]] from the [[gray list]] by an [[ensemble]] of [[heterogeneous]] [[base-level classifiers]] derived by different [[learning methods]], using different [[feature]] [[representations]] on [[dynamic]] [[training sets]] .
[[In this paper, we]] develop a novel [[product recommender system]] called [[METIS]], a [[MErchanT Intelligence recommender System]], which detects [[users' purchase intent]]s from their [[microblog]]s in near [[real-time]] and makes [[product recommendation]] based on matching the [[users' demographic information extracted]] from their [[public profile]]s with [[product demographics learned]] from [[microblog]]s and [[online review]]s.
[[In this paper, we]] develop a [[semantic annotation technique]] for [[location-based social network]]s to [[automatic]]ally [[annotate]] all [[place]]s with [[category tag]]s which are a crucial [[prerequisite]] for [[location search]], [[recommendation service]]s, or [[data cleaning]] .
[[In this paper we]] develop [[density estimation trees (DETs)]], the natural analog of [[classification tree]]s and [[regression tree]]s, for the [[task]] of [[density estimation]] .
[[In this paper we]] develop [[method]]s to [[mine]] for exceptional [[regression model]]s.
[[In this paper, we]] develop novel [[multi-task learning technique]]s to [[predict]] the [[disease progression]] measured by [[cognitive score]]s and select [[biomarker]]s predictive of the [[progression]] .
In [[this paper, we]] develop the first [[online motif discovery algorithm]] which [[monitors]] and [[maintains]] [[motifs]] exactly in [[real time]] over the most [[recent history]] of a [[stream]] .
In [[this paper]] we develop the first [[unsupervised approach]] to [[semantic parsing]], using [[Markov logic]] ([[Richardson and Domingos, 2006]]).
[[In this paper, we]] develop two [[algorithm]]s, [[MinRPset]] and [[FlexRPset]], for [[finding minimum representative pattern set]]s.
[[In this paper we]] discuss an [[interesting]] and [[useful property]] of [[clickstream data]] .
In [[this paper, we]] discuss [[a novel approach]] based on the [[theory of multiple kernel learning]] to [[detect]] [[potential]] [[safety anomalies]] in very [[large data bases]] of [[discrete]] and [[continuous data]] from [[world-wide]] [[operations]] of [[commercial fleets]] .
[[In this paper, we]] discuss [[real time]] [[algorithm]]s for reducing the [[volume of the data]] collected in [[sensor network]]s.
[[In this paper, we]] [[empirically]] [[evaluate]] the [[performance]] of several [[methods]] for [[topic inference]] in [[previously unseen documents]], including [[methods]] based on [[Gibbs sampling]], [[variational inference]], and a new [[method]] inspired by [[text classification]] .
[[In this paper, we]] employ the [[MDL principle]] to [[identify]] the [[set]] of [[sequential pattern]]s that [[summarises the data]] best.
[[In this paper, we]] employ [[transfer learning]], which [[borrows knowledge]] from [[auxiliary historical task]]s to improve the [[data veracity]] in a given [[target task]] .
In [[this paper, we]] enrich [[interactive]] [[sensor querying]] with [[statistical modeling technique]]s.
In [[this paper]] we examine how to [[learn the structure]] of a [[DPN]] from [[data]] .
[[In this paper, we]] examine the scenario of a [[heterogeneous network]] with [[node]]s and [[content]] of various [[type]]s.
[[In this paper, we]] examine the [[utility]] of this concept for [[identifying]] [[malicious network source]]s.
[[In this paper, we]] expand the use of [[browsing information]] for [[web search]] [[ranking]] and other [[applications]], with an emphasis on analyzing [[individual]] [[user session]]s for creating [[aggregate model]]s.
In [[this paper, we]] exploit the inherent [[parallelism]] and [[high memory bandwidth]] of [[graphics processing units (GPU)]] to accelerate the [[computation]] of [[SimRank]] on [[large graphs]] .
[[In this paper, we]] explore a [[model-based approach]] for [[recommendation]] in [[social network]]s, employing [[matrix factorization technique]]s.
[[In this paper we]] explore [[MapReduce]] for [[clustering]] this kind of [[data]] .
In [[this paper we]] explore the [[application]] of [[SAME]] to [[graphical model]] [[inference]] on [[modern hardware]] .
[[In this paper, we]] explore [[this problem]] and [[design]] a [[technique]] called [[MINDS]] to [[mine minimally discriminative subgraph]]s from [[large global-state network]]s.
[[In this paper, we]] [[extract]] [[social event]]s from [[data stream]]s in [[social network]]s, and then use the [[extracted]] [[social event]]s to improve the [[learning]] of [[information diffusion model]]s.
[[In this paper, we]] fill [[this gap]] by proposing a [[semi-supervised representation learning method]] for [[text data]], which we call the <i>[[predictive text embedding]]</i> ([[PTE]]).
[[In this paper, we]] first [[characterize]] the [[expected cost]]s of [[vertex]] and [[edge partition]]s with and without [[aggregation of message]]s, for the commonly [[deployed policy]] of placing a [[vertex]] or an [[edge uniformly]] at [[random]] to one of the [[partition]]s.
[[In this paper, we]] first demonstrate, through an extensive [[classification study]] using [[kernel machines]], that the [[min-max kernel]] often provides an [[effective]] [[measure]] of [[similarity]] for [[nonnegative data]] .
[[In this paper, we]] first formulate a novel [[Graph Coarsening Problem]] to find a [[succinct representation]] of any [[graph]] while preserving key [[characteristic]]s for [[diffusion process]]es on that [[graph]] .
[[In this paper, we]] first propose a [[bivariate metric]], one [[measure]]s the [[variability of the estimate]], and the other [[measure]]s the [[accuracy of classifying]] the [[positive]] and [[negative user]]s.
[[In this paper, we]] first provide [[quantitative analysis]] showing that [[bounce rate]] is an effective [[measure]] of [[user satisfaction]] .
[[In this paper, we]] first [[study]] [[user behavior]] on [[sponsored search results]] (i.e., the [[advertisements]] displayed by [[search engines]] next to the ordinary, [[organic results]]), and [[compare]] this [[behavior]] to that of [[organic]] [[results]] .
[[In this paper we]] focus on a [[family]] of [[poly-time solvable formulation]]s, known as the <i>[[k-clique densest subgraph problem]]</i> ([[k-Clique-DSP]]) [57].
[[In this paper, we]] focus on [[fine-grained mining]] of [[contention]]s in discussion / [[debate forum]]s.
[[In this paper, we]] focus on [[linear classifier]]s including [[logistic regression]] and [[linear SVM]] because of their [[simplicity]] over [[kernel]] or other [[method]]s.
In [[this paper]] we focus on [[the latter]]: we tackle the [[problem of automatically]] [[detecting]] the [[tasks]] that the [[user]] is involved in, by [[identifying]] which of the [[windows]] on the [[user's desktop]] are related to each other.
[[In this paper, we]] focus on the [[problem of performing multi-label classification]] on [[networked data]], where the [[instance]]s in the [[network]] can be [[assigned multiple label]]s.
[[In this paper, we]] focus on the [[problem]] of [[profiling users' home location]]s in the context of [[social network]] ([[Twitter]]).
In [[this paper, we]] focus on the task of determining [[coreference relation]]s as defined in [[MUC-6]] ([[MUC-6 1995]]) and [[MUC-7]] ([[MUC-7 1997]]).
[[In this paper, we]] focus on the two [[definitions quoted]] from ([[Barnett and Lewis, 1994]]) above and do not consider the [[dual class-membership problem]] or [[separating]] [[noise]] and [[outlier]]s.
[[In this paper, we]] focus on the widely studied [[linear threshold diffusion model]], and [[prove]], for the [[first time]], that the [[network modification problem]]s under this [[model]] have [[supermodular objective function]]s.
[[In this paper, we]] focus our attention on the problem of [[collusions]], in which some [[parties]] may [[collude]] and share their [[record]] to deduce the [[private information]] of other [[parties]] .
[[In this paper, we]] formalize [[the problem]] as [[entity matching]] across [[heterogeneous source]]s and propose a [[probabilistic topic model]] to solve [[the problem]] .
In [[this paper, we]] [[formally define]] the [[problem of popular event tracking]] in [[online communities]] ([[PET]]), focusing on the [[interplay]] between [[texts]] and [[networks]] .
[[In this paper, we]] formulate a new [[research problem]] of [[learning]] from [[vaguely labeled]] [[one-class]] [[data streams]], where the [[main objective]] is to allow [[users]] to [[label]] [[instance groups]], instead of single [[instance]]s, as [[positive samples]] for [[learning]] .
[[In this paper, we]] generalize [[discriminative clustering]] to structured and [[complex output variable]]s that can be [[represented as]] [[graphical model]]s.
[[In this paper, we]] generalize the "[[influential node]]s "[[problem]] .
[[In this paper, we]]: give a [[taxonomy]] for [[classifying attack]]s against [[online machine learning algorithm]]s; discuss [[application-specific factor]]s that limit an [[adversary's capabiliti]]es; introduce two models for [[modeling]] an [[adversary's capabiliti]]es; explore the limits of an [[adversary's knowledge]] about [[the algorithm]], [[feature space]], [[training]], and [[input data]]; explore [[vulnerabiliti]]es in [[machine learning algorithm]]s; discuss [[countermeasures against attack]]s; introduce the [[evasion challenge]]; and discuss [[privacy-preserving learning technique]]s.
[[In this paper, we]] incorporate [[disease-specific context]]s into [[mortality modeling]] by formulating the [[mortality prediction problem]] as a [[multi-task learning problem]] in which a [[task]] corresponds to a [[disease]] .
[[In this paper, we]] incorporate [[utility]] into [[sequential pattern mining]], and a [[generic framework]] for [[high utility sequence mining]] is defined.
[[In this paper, we]] infer the [[real-time]] and [[fine-grained air quality information]] throughout a city, based on the ([[historical]] and [[real-time]]) [[air quality data]] reported by existing [[monitor station]]s and a variety of [[data source]]s we observed in the city, such as [[meteorology]], [[traffic flow]], [[human mobility]], [[structure of road network]]s, and [[point of interests (POIs)]] .
[[In this paper, we]] integrate both the [[extraction]] of [[meaningful topic]]s and the [[filtering of messages]] over the [[Twitter stream]] .
[[In this paper we]] introduce a [[game theoretic framework]] ([[GHIN]]) for defining and [[mining cluster]]s in [[heterogeneous information network]]s.
[[In this paper we]] introduce a [[method]] for [[identifying]] [[informative subset]]s of [[adjacent voxel]]s, corresponding to [[brain patch]]es that distinguish [[subset]]s of [[class]]es.
[[In this paper we]] introduce a [[methodology]] for [[extracting]] [[mobility profile]]s of [[individual]]s from [[raw digital trace]]s (in particular, [[GPS trace]]s), and study [[criteria]] to [[match individual]]s based on [[profile]]s.
In [[this paper we]] introduce a [[modular]], highly [[flexible]], [[open-source environment]] for [[data generation]] .
[[In this paper, we]] introduce an [[asynchronous parallel version]] of the [[algorithm]], analyze its [[convergence properti]]es, and propose a [[solution]] for [[primal-dual]] [[synchronization]] required to achieve [[convergence]] in practice.
[[In this paper we]] introduce a new [[approach]] that combines the [[positive properties]] of both [[directions]] : it provides a detailed [[description]] of the complete [[database]] using a small [[set]] of [[patterns]] .
In [[this paper]], [[we]] introduce an [[extension]] of [[itemsets]], called [[regular]], with an [[immediate semantics]] and [[interpretability]], and a conciseness comparable to [[closed itemsets]] .
[[In this paper, we]] introduce a [[real-world task]], [[tracking trend]]s of [[term]]s, to which [[temporal topic model]]s can be applied.
[[In this paper, we]] introduce a [[survey]] of [[contemporary]] [[techniques for outlier detection]] .
In [[this paper, we]] introduce [[cold start link prediction]] as the [[problem]] of [[predicting]] the [[structure]] of a [[social network]] when the [[network]] itself is totally [[missing]] while some other [[information]] regarding the [[nodes]] is available.
[[In this paper, we]] introduce [[delta-relevance]], a definition of a more strict [[criterion]] of [[relevance]] .
In [[this paper, we]] introduce new [[probabilistic]] formulations of [[frequent itemsets]] based on [[possible world]] [[semantics]] .
[[In this paper, we]] introduce [[TechMiner]], a new approach, which combines [[NLP]], [[machine learning]] and [[semantic technologi]]es, for [[mining technologi]]es from [[research publication]]s and [[generating an OWL ontology]] describing their [[relationship]]s with other [[research entiti]]es.
In [[this paper, we]] introduce [[TextRank]] - a [[graph-based]] [[ranking model]] for [[text processing]], and show how [[this model]] can be successfully used in [[natural language applications]] .
[[In this paper we]] introduce the first [[scalable]] [[end-to-end approach]] to [[learning approximate Nash equilibria]] without [[prior domain knowledge]] .
[[In this paper, we]] introduce the <i>[[network lasso]]</i>, a [[generalization of]] the [[group lasso]] to a [[network setting]] that allows for simultaneous [[clustering]] and [[optimization on graphs]] .
[[In this paper, we]] introduce the [[notion]] of [[Empirical Glitch Explanations - concise]], [[multi-dimensional description]]s of [[subset]]s of potentially [[dirty data]] - and propose a [[scalable method]] for [[empirically generating]] such [[explanatory characterization]]s.
[[In this paper, we]] introduce the [[problem of graph identification]], i.e., the [[discovery]] of the [[true]] [[graph structure]] underlying an [[observed network]] .
In this [[paper, we]] investigate [[algorithm]]s for [[generic schema matching]], [[outside]] of any particular [[data model]] or application.
[[In this paper, we]] investigate and compare various [[online recruiting system]]s from a [[product]] [[perspective]] .
[[In this paper, we]] investigate a [[problem of predicting]] <i>what [[image]]s are likely to appear on the [[Web]] at a [[future time point]] </i>, given a [[query word]] and a [[database]] of [[historical image stream]]s that potentiates [[learning]] of [[uploading pattern]]s of previous [[user image]]s and associated [[metadata]] .
[[In this paper, we]] investigate [[entity recognition (ER)]] with [[distant-supervision]] and propose a novel [[relation phrase-based ER framework]], called [[ClusType]], that runs [[data-driven phrase mining]] to generate [[entity mention candidate]]s and [[relation phrase]]s, and enforces the [[principle]] that [[relation phrase]]s should be softly [[clustered]] when [[propagating type information]] between their [[argument entiti]]es.
[[In this paper, we]] investigate how different [[pre-processing]] [[decisions]] and different [[network]] [[force]]s such as [[selection]] and [[influence]] affect the [[modeling]] of [[dynamic]] [[networks]] .
In [[this paper, we]] investigate [[methods]] for [[automatically]] [[connecting the dots]] -- providing a [[structured]], [[easy way]] to [[navigate]] within a [[new topic]] and [[discover hidden connections]] .
[[In this paper, we]] investigate the [[feasibility]] of [[mining the relationship]] between [[action]]s and their [[outcome]]s from the [[aggregated timeline]]s of [[individual]]s [[posting]] [[experiential microblog report]]s.
[[In this paper, we]] investigate the [[highly reliable subgraph]] [[problem]], which arises in the [[context]] of [[uncertain graph]]s.
In [[this paper, we]] investigate the [[key technique]]s that can help [[business]]es promote their [[location]]s by [[advertising]] wisely through the underlying [[LBSN]]s.
[[In this paper, we]] investigate the most [[common scenario]] with [[implicit feedback]] (e.g. [[click]]s, [[purchase]]s).
[[In this paper, we]] investigate the [[social role]]s and [[status]]es that [[people]] act in [[online social network]]s in the perspective of [[network structure]]s, since the [[uniqueness]] of [[social network]]s is connecting [[people]] .
[[In this paper, we]] investigate the [[sparse inverse covariance estimation technique]] for [[identifying]] the [[connectivity]] among different [[brain region]]s.
In [[this paper]] we investigate ways to learn for [[sub-document classification]] when only [[page level]] [[labels]] are available - these [[labels]] only indicate if the [[relevant]] [[content]] exists in the given [[page]] or not.
[[In this paper, we]] join both [[research area]]s and present a [[solution]] for [[integrating]] [[prior knowledge]] in the [[process of detecting]] multiple [[clustering]]s.
[[In this paper we]] model a [[job-specific shock process]] in the [[matching model]] of [[unemployment]] with [[non-cooperative wage behaviour]] .
In [[this paper, we]] perform a comprehensive [[comparison study]] of various [[document clustering approaches]] such as three [[hierarchical methods]] ([[single-link]], [[complete-link]], and [[complete link]]), [[Bisecting K-means]], [[K-means]], and [[Suffix Tree Clustering]] in terms of the [[efficiency]], the [[effectiveness]], and the [[scalability]] .
In [[this paper]] we present a [[case study]] of a [[deployed]] [[data-driven system]] that grows a [[product-related terms dictionary]] by [[parsing]] [[product offering title]]s in order to improve an [[affiliate-marketing link insertion service]] that inserts [[affiliated hyperlink]]s to relevant [[offer]]s .
[[In this paper, we]] present a [[community-based effort]] on [[technical advancement]]s in [[Hive]] .
[[In this paper, we]] present a [[computationally efficient algorithm]] based on [[multiple instance learning]] for [[map]]ping [[informal settlement]]s (slums) using [[very high-resolution remote sensing imagery]] .
[[In this paper, we]] present a detailed [[analysis]] of [[CCA]] that yields an effective [[initialization]] and [[iterative algorithm]]s for the problem.
[[In this paper, we]] present a [[dynamic pattern]] driven [[approach]] to [[summarize data]] produced by [[Twitter]] [[feed]]s.
In [[this paper, we]] present a fast [[algorithm]] called <i>[[Grafting-Light]]</i> to solve the [[<math>\ell_1</math>-norm regularized maximum likelihood estimation]] of [[MRFs]] for [[efficient]] [[feature selection]] and [[structure learning]] .
[[In this paper, we]] present a first [[approach]] for [[real time]] and [[streaming classification]] of [[such data]] .
[[In this paper, we]] present a [[formal definition]] of the [[competitiveness]] between two [[item]]s.
[[In this paper we]] present a [[formalism]] of [[clustering probability distribution]]s, and its application to [[query clustering]] where each [[query]] is [[represented as]] a [[probability density]] of [[click-through rate (CTR) weighted bid and distortion]] is measured by [[KL divergence]] .
[[In this paper, we]] present a formulation for [[hedging]] [[online resource allocation]]s with [[leverage]] and propose an [[efficient]] [[data mining algorithm]] ([[SHERAL]]).
[[In this paper we]] present a [[framework]] for studying [[graph pattern mining]] on [[time-varying stream]]s.
In [[this paper]], we present a [[general end-to-end approach]] to [[sequence learning]] that makes [[minimal assumption]]s on the [[sequence structure]] .
[[In this paper, we]] present a general [[mining framework]] [[SympGraph]] for [[modeling]] and [[analyzing]] [[symptom relationship]]s in [[clinical note]]s.
In [[this paper, we]] present a generic, [[Bayesian framework]] for [[capturing]] exactly [[this situation]] .
[[In this paper we]] present a [[generic optimization criterion]] [[BPR-Opt]] for [[personalized ranking]] that is the [[maximum posterior estimator]] derived from a [[Bayesian analysis]] of the [[problem]] .
[[In this paper, we]] present a [[large scale]] [[data mining]] effort that [[detect]]s and [[block]]s such [[adversarial advertisement]]s for the [[benefit]] and [[safety]] of our [[user]]s.
[[In this paper, we]] present a [[latent topic-based clustering algorithm]] to [[discover patterns]] in the [[trajectori]]es of [[geo-tagged text message]]s.
[[In this paper, we]] present a [[learning-based approach]] to [[broad matching]] that is based on exploiting [[implicit]] [[feedback]] in the form of [[advertisement]] [[clickthrough logs]] .
[[In this paper we]] present a [[methodology]] to find [[trendsetter]]s in [[information network]]s according to a specific [[topic of interest]] .
[[In this paper, we]] present a [[method]] to [[dynamically estimate]] the [[probability of mortality]] inside the [[Intensive Care Unit (ICU)]] by combining [[heterogeneous data]] .
[[In this paper we]] present an [[algorithm]] to balance the need to keep a [[website]] fresh with new [[content]] with the desire to [[present]] the best [[content]] to the most [[visitors]] at times of [[peak]] [[traffic]] .
[[In this paper, we]] present an alternative, [[projective sampling]] [[strategy]], which [[fit]]s [[function]]s to a [[partial]] [[learning]] [[curve]] and a [[partial]] [[run-time]] [[curve]] obtained from a small [[subset]] of potentially available [[data]] and then uses these [[projected function]]s to [[analytical]]ly [[estimate]] the [[optimal]] [[training set]] size.
In [[this paper]] we present an [[analysis]] of an [[AltaVista]] [[Search Engine query log]] consisting of [[approximately]] 1 billion entries for [[search request]]s over a period of six [[week]]s.
[[In this paper, we]] present an approach for [[translating]] [[keyword queries]] to [[DL]] [[conjunctive]] [[queries]] using [[background knowledge]] available in [[ontologies]] .
[[In this paper, we]] present an [[approach]] that leverages [[user click]]s during [[Web search]] to [[automatically generate training data]] for [[entity matching]] .
[[In this paper, we]] present an [[efficient]] [[anonymous]] [[data collection]] [[protocol]] for a [[malicious environment]] such as the [[Internet]] .
[[In this paper, we]] present an [[experimental evaluation]] of various [[collaborative filtering algorithm]]s on a [[real-world dataset]] of [[purchase history]] from [[customer]]s in a store of a French [[home improvement]] and [[building supplies chain]] .
In [[this paper, we]] present a novel [[exploratory visual analytic system]] called [[TIARA (Text Insight via Automated Responsive Analytics)]], which [[combines]] [[text analytics]] and [[interactive visualization]] to help [[users]] [[explore]] and [[analyze]] [[large collections]] of [[text]] .
[[In this paper, we]] present a novel [[flow-based approach]], called [[flowNet Algorithm]], to efficiently analyze [[large-sized]], [[complex networks]] .
[[In this paper, we]] present a novel [[framework]] for [[monitoring anomalies]] over [[continuous]] [[trajectory stream]]s.
[[In this paper, we]] present a novel [[method]] for testing the [[statistical significance]] of the [[class]] of [[spatio-temporal teleconnection pattern]]s called as [[dipole]]s.
[[In this paper we]] present a novel [[probabilistic]] [[topic model]] to [[analyze]] [[text corpora]] and [[infer]] [[descriptions]] of its [[entities]] and of [[relationships]] between those [[entities]] .
[[In this paper, we]] present a novel [[text clustering method]] to address these two issues by enriching [[document representation]] with [[Wikipedia concept]] and [[category]] [[information]] .
[[In this paper we]] present an [[unsupervised approach]] to [[extracting]] [[semantic networks]] from [[large volumes of text]] .
[[In this paper we]] present a paradigm for combining [[formal specification]] with [[implementation]], called [[monitoring-oriented programming (MoP)]], providing a [[light-weighted formal method]] to [[check conformance]] of [[implementation]] to [[specification]] at [[runtime]] .
[[In this paper, we]] present a [[pattern discovery]] based [[approach]] to [[detect]] [[fraudulent event]]s at the [[POS]] .
[[In this paper we]] present a [[recommendation system]] for [[picking heroes]] in a [[MOBA game]] .
[[In this paper, we]] present a [[social influence]] based [[clustering framework]] for analyzing [[heterogeneous information network]]s with three unique [[feature]]s.
In [[this paper, we]] present a [[technique to predict]] the [[inference quality]] by utilizing (1) [[network analysis]] and [[network autocorrelation modeling]] of [[informal]] and [[formal network]]s, and (2) [[regression model]]s to [[predict]] [[user interest]] [[inference quality]] from [[network characteristics]] .
[[In this paper we]] present a [[text-based framework]] for [[quantifying]] how [[diverse]] a [[document]] is in terms of its [[content]] .
In [[this paper, we]] present a[ text mining system]] capable of [[populating]] a [[software ontology]] with [[information]] detected in [[documents]] .
[[In this paper, we]] present a [[unified framework]] in which one can use [[background]] [[lexical information]] in terms of [[word-class]] [[association]]s, and refine this [[information]] for specific [[domains]] using any available [[training examples]] .
In [[this paper, we]] present a unified [[generative model]], the [[Optimized Network Model (ONM)]], that characterizes the [[lifecycle]] of a [[ticket]], using both the [[content]] and the [[routing sequence]] of the [[ticket]] .
[[In this paper, we]] present [[Deep Graph Kernel]]s, a [[unified framework]] to [[learn latent representation]]s of [[sub-structure]]s for [[graph]]s, inspired by latest [[advancement]]s in [[language modeling]] and [[deep learning]] .
In [[this paper, we]] present [[Espresso]], a [[weakly-supervised]], [[general-purpose]], and [[accurate algorithm]] for [[harvesting semantic relations]] .
[[In this paper, we]] present [[FoodSIS]], a [[system]] for [[end-to-end web information gathering]] for [[food safety]] .
[[In this paper, we]] present [[FUNNEL]], a unifying [[analytical model]] for [[large scale]] [[epidemiological data]], as well as a novel [[fitting algorithm]], [[FUNNELFIT]], which solves the above [[problem]] .
[[In this paper, we]] present <i>[[Wise Market]]</i> as an [[effective framework]] for [[crowdsourcing]] on [[social media]] that motivates [[user]]s to participate in a [[task]] with care and correctly [[aggregate]]s their [[opinion]]s on [[pairwise choice problem]]s.
[[In this paper we]] present [[Morphy]], an [[integrated tool]] for [[German morphology]], [[part-of-speech tagging]] and [[context-sensitive lemmatization]] .
[[In this paper, we]] present [[OQA]], the first [[approach]] to leverage both [[curated]] and extracted [[KB]]s.
[[In this paper, we]] present [[our approach]] for [[geographic personalization]] of a [[content recommendation system]] .
[[In this paper, we]] present [[our approach]] to [[conversion rate estimation]] which relies on utilizing [[past performance observation]]s along [[user]], [[publisher]] and [[advertiser data hierarchi]]es.
[[In this paper, we]] present [[SmartDispatch]], a [[learning-based tool]] that seeks to [[automate]] the [[process]] of [[ticket dispatch]] while maintaining [[high accuracy level]]s.
[[In this paper we]] present the [[design]], [[implementation]] and [[evaluation]] of [[SOBA]], a [[system]] for [[ontology-based information extraction]] from [[heterogeneous]] [[data resource]]s, including [[plain text]], [[tables]] and [[image captions]] .
In [[this paper]] we present the [[MultiFarm dataset]], which has been designed as a [[benchmark]] for [[multilingual]] [[ontology matching]] .
[[In this paper, we]] propose a [[citywide]] and [[real-time model]] for [[estimating]] the [[travel time]] of any [[path]] (represented as a sequence of [[connected road segment]]s) in [[real time]] in a [[city]], based on the [[GPS trajectori]]es of [[vehicles received]] in current [[time slot]]s and over a period of history as well as [[map data source]]s.
[[In this paper, we]] propose a [[co-factorization model]], [[CoFactor]], which jointly decomposes the [[user-item interaction matrix]] and the [[item-item co-occurrence matrix]] with shared [[item latent factor]]s.
[[In this paper, we]] propose a [[comprehensive method]], [[FACETS]], to [[simultaneously model]] <i>all</i> these three [[challenge]]s.
[[In this paper, we]] propose a [[data driven phenotyping framework]] called [[Pacifier (PAtient reCord densIFIER)]], where we interpret the [[longitudinal EMR data]] of each [[patient]] as a [[sparse matrix]] with a [[feature dimension]] and a [[time dimension]], and derive more robust [[patient phenotype]]s by exploring the [[latent structure]] of those [[matrice]]s.
[[In this paper, we]] propose a [[data structure]], called [[Community Tree]], to [[represent]] the [[organizational structure]] in the [[social network]] .
In [[this paper, we]] propose a [[discriminatively-trained model]] that [[jointly]] performs [[coreference resolution]] and [[canonicalization]], enabling [[features]] over [[hypothesized]] [[entities]] .
[[In this paper, we]] propose a [[factorization]] based [[sparse learning framework]] termed [[FHIM]] for [[identifying]] [[high-order feature interaction]]s in [[linear]] and [[logistic regression model]]s, and study several [[optimization method]]s for solving them.
In [[this paper, we]] propose a [[flexible]] and [[generalized framework]] for [[constrained spectral clustering]] .
[[In this paper, we]] propose a [[flexible]] and [[robust framework]], [[CGC (Co-regularized Graph Clustering)]], based on [[non-negative matrix factorization(NMF)]], to tackle these [[challenge]]s.
[[In this paper, we]] propose a [[flexible]] and [[robust framework]] that allows [[multiple underlying clustering structure]]s across different [[network]]s.
[[In this paper, we]] propose a [[framework]] for [[detecting change]]s in [[multidimensional data stream]]s based on [[principal component analysis]], which is used for [[projecting data]] into a [[lower dimensional space]], thus facilitating [[density estimation]] and [[change-score calculation]]s.
[[In this paper, we]] propose a [[framework]] that overcomes the above [[drawback]]s by integrating the [[ranking]] and [[selection paradigm]]s.
[[In this paper, we]] propose a [[framework]] (titled [[DRoF]]) that [[Discovers Regions of different Function]]s in a city using both [[human mobility]] among [[region]]s and [[points of interests (POIs)]] located in a [[region]] .
In [[this paper, we]] [[propose a framework]] to solve the [[similarity search problem]] given [[user-defined]] [[instance-level constraints]] for [[tropical cyclone events]], [[represented by]] [[arbitrary length]] [[multidimensional]] [[spatio-temporal data sequence]]s.
[[In this paper, we]] propose a general, [[disk-based graph engine]] called [[TurboGraph]] to process [[billion-scale]] [[graph]]s very [[efficiently]] by using [[modern hardware]] on a single [[PC]] .
[[In this paper, we]] propose a [[general framework]] for robustly [[discovering entity synonym]] with two novel [[similarity function]]s that overcome the [[limitation]]s of [[prior technique]]s.
[[In this paper, we]] pro - pose a [[generic stream sampling framework]] for [[big-graph analytic]]s, called [[Graph Sample and Hold (gSH)]], which [[sample]]s from [[massive graphs sequentially]] in a [[single pass]], one [[edge]] at a [[time]], while maintaining a [[small state]] in [[memory]] .
In [[this paper, we]] propose a holistic [[lexicon-based approach]] to solving the [[problem]] by exploiting [[external evidence]]s and [[linguistic convention]]s of [[natural language expression]]s.
[[In this paper, we]] propose a [[kernel-boundary-alignment algorithm]], which considers [[training-data imbalance]] as [[prior information]] to augment [[SVMs]] to improve [[class-prediction accuracy]] .
In [[this paper, we]] propose a [[knowledge-based approach]] to [[literature mining]] and focus on [[reference metadata]] [[extraction methods]] for [[scholarly publication]]s.
[[In this paper we]] propose [[algorithm]]s which construct [[outlier causality tree]]s based on [[temporal]] and [[spatial properti]]es of [[detected outlier]]s.
[[In this paper, we]] propose a [[method]] for [[tag recommendation]] based on [[tensor factorization (TF)]] .
[[In this paper we]] propose a [[model]] of the [[user response]] to an [[ad campaign]] as a [[function]] of both the [[interest match]] and the [[past exposure]], where the [[interest match]] is [[estimated]] using [[historical search]] / browse [[activiti]]es of the [[user]] .
[[In this paper we]] propose a [[model]] that captures the [[compositional structure]] of [[textual relation]]s, and [[jointly optimize]]s [[entity]], [[knowledge base]], and [[textual relation representation]]s.
[[In this paper, we]] propose a [[multi-task learning formulation]] for [[predict]]ing the [[disease progression]] [[measure]]d by the [[cognitive score]]s and [[selecting marker]]s [[predictive]] of the [[progression]] .
[[In this paper, we]] propose an [[adaptive]] [[line search scheme]] which allows to [[tune]] the [[step size]] adaptively and meanwhile guarantees the [[optimal]] [[convergence rate]] .
[[In this paper we]] propose an [[algorithm]] for [[recursively constructing]] a [[hierarchy of topic]]s from a [[collection]] of [[content-representative document]]s.
In [[this paper, we]] propose an [[algorithm]] to [[automatically discover program workflows]] from [[event trace]]s that [[record]] [[system events]] during [[system execution]] .
[[In this paper, we]] propose an [[approach]] for [[discovering]] [[anomalous windows]] using [[Scan Statistics for Linear Intersecting Paths (SSLIP)]] .
[[In this paper, we]] propose an [[approach]] to [[evaluate]] the [[correlation]] between [[time series data]] and [[event data]] .
In [[this paper, we]] propose an [[efficient algorithm]], namely [[<i>UP-Growth</i> (<i>Utility Pattern Growth</i>)]], for [[mining]] [[high utility itemsets]] with a [[set of technique]]s for [[pruning]] [[candidate itemsets]] .
[[In this paper, we]] propose an [[efficient]] [[hierarchical document clustering method]] based on a [[new algorithm]] for [[rank-2 NMF]] .
[[In this paper, we]] propose an [[efficient online algorithm]] for [[tracking]] [[personalized PageRank]] in an [[evolving network]] .
[[In this paper, we]] propose an [[evolutionary co-clustering formulation]] for [[identifying]] [[co-cluster structure]]s from [[time-varying data]] .
[[In this paper, we]] propose an [[evolutionary]] [[multi-branch tree clustering method]] for [[streaming text data]] .
[[In this paper, we]] propose a new [[latent semantic model]] that incorporates a [[convolutional-pooling structure]] over [[word sequence]]s to [[learn]] [[low-dimensional]], [[semantic vector representation]]s for [[search queri]]es and [[Web document]]s.
In [[this paper, we]] propose a new method for [[indexing]] [[large amounts]] of [[point]] and [[spatial data]] in [[high-dimensional space]] .
[[In this paper, we]] propose a new [[method]] for [[quantifying]] the [[strength]] of the [[causal influence]] from one [[time series]] to another.
[[In this paper, we]] propose an improved [[GLMNET]] to address some [[theoretical]] and [[implementation issue]]s.
[[In this paper, we]] propose an innovative [[EM]] [[clustering algorithm]] particularly suited for the [[GPU platform]] on [[NVIDIA's Fermi architecture]] .
[[In this paper, we]] propose an [[integrated data mining framework]] for [[identify]]ing the [[precursor]]s to [[precipitation event cluster]]s and use [[this information]] to [[predict]] extended [[period]]s of [[extreme precipitation]] and [[subsequent flood]]s.
In [[this paper, we]] propose an [[method]] for [[extracting]] [[bibliographic]] [[attributes]] from [[reference strings]] captured using [[Optical Character Recognition (OCR)]] and an extended [[hidden Markov model]] .
In [[this paper, we]] propose a [[Noise Tolerant Time-varying Factor Graph Model (NTT-FGM)]] for [[modeling]] and [[predicting]] [[social actions]] .
[[In this paper, we]] propose a [[non-transitive hashing method]], namely [[Multi-Component Hashing (MuCH)]], to [[identify]] the [[latent similarity component]]s to cope with the [[non-transitive similarity relationship]]s.
[[In this paper, we]] propose a novel [[adversarial learning framework]], [[RankGAN]], for [[generating highquality language description]]s.
In [[this paper, we]] propose a novel algorithm [[uHARMONY]] which [[mines discriminative patterns]] directly and effectively from [[uncertain data]] as [[classification features]]/[[rules]], to help [[train]] either [[SVM]] or [[rule-based classifier]] .
In [[this paper, we]] propose a novel [[approach]], namely [[DPMFS]], to address [[this issue]] .
[[In this paper, we]] propose a novel [[clustering model]] to [[learn]] the [[data similarity matrix]] and [[clustering structure]] simultaneously.
[[In this paper, we]] propose a novel [[community detection algorithm]], which utilizes a [[dynamic process]] by contradicting the [[network]] [[topology]] and the [[topology-based propinquity]], where the [[propinquity]] is a [[measure]] of the [[probability]] for a [[pair]] of [[nodes]] involved in a [[coherent]] [[community]] [[structure]] .
[[In this paper we]] propose a novel [[criterion]] which achieves [[good generalization performance]] of a [[classifier]] by specifically selecting a [[set]] of [[query sample]]s that [[minimize]]s the [[difference]] in [[distribution]] between the [[labeled]] and the [[unlabeled data]], after [[annotation]] .
[[In this paper, we]] propose a novel [[data mining paradigm]] called [[Statistical Arbitrage Mining(SAM)]] focusing on [[mining]] and exploiting [[price discrepanci]]es between two [[pricing scheme]]s.
[[In this paper, we]] propose a novel [[generative adversarial network]], [[RankGAN]], for [[generating]] [[high-quality]] [[language description]]s.
[[In this paper, we]] propose a novel <i>[[multi-domain active learning]]</i> [[framework]] to [[jointly select data instance]]s from all [[domain]]s with [[duplicate information]] considered.
[[In this paper, we]] propose a novel [[model]] of [[Topic-specific Authority Analysis (TAA)]], which addresses the [[limitation]]s of [[the previous approach]]es, to identify [[authorities]] specific to given [[query topic]] (s) on a [[content sharing service]] .
[[In this paper, we]] propose a novel [[probabilistic measure]] for [[periodicity]] and design a [[practical method]] to detect [[period]]s.
[[In this paper we]] propose a novel [[topic model]], [[Temporal-LDA]] or [[TM-LDA]], for [[efficiently]] [[mining text stream]]s such as a [[sequence]] of [[post]]s from the same [[author]], by [[modeling]] the [[topic transition]]s that naturally arise in these [[data]] .
[[In this paper, we]] propose an [[unsupervised approach]] to automatically [[detect circles]] in an [[ego network]] such that each [[circle]] represents a [[densely knit community]] of [[researchers]] .
[[In this paper, we]] propose an [[unsupervised approach]] to [[learning]] for [[Relation Detection]], based on the use of [[massive]] [[clustering ensembles]] .
[[In this paper we]] propose a [[principled method]] for making an arbitrary [[classifier]] [[cost-sensitive]] by wrapping a [[cost-minimizing procedure]] around [[it]] .
[[In this paper, we]] propose a [[probabilistic model]] for [[discovering latent influence]] from [[sequence]]s of [[item adoption event]]s.
[[In this paper, we]] propose a [[probabilistic model]] named [[COM (COnsensus Model)]] to model the [[generative process]] of [[group activiti]]es, and make [[group recommendation]]s.
[[In this paper, we]] propose a [[prototype-based classification model]] for [[evolving data stream]]s, called [[SyncStream]], which [[dynamically model]]s [[time-changing concept]]s and [[makes prediction]]s in a [[local fashion]] .
In [[this paper, we]] propose a [[relaxed]] [[OPSM model]] called [[ROPSM]] .
[[In this paper, we]] propose a [[sampling method]] that [[provably]] and accurately [[estimate]]s the [[similarity]] between [[vertice]]s.
In [[this paper, we]] propose a [[scalable]] [[distributed]] [[Bayesian matrix factorization]] [[algorithm]] using [[stochastic gradient MCMC]] .
In [[this paper, we]] propose a [[scheme]] for [[privacy-preserving outsourcing]] the [[training]] of the [[SVM]] without [[disclosing]] the [[actual content of the data]] to the [[service provider]] .
In [[this paper, we]] propose a [[semantic measure]] which incorporates [[citation semantics]] ([[Citonomy]]) into [[literature (document) clustering]] .
[[In this paper, we]] propose a simple [[statistical model]] towards making [[inference]]s on the [[test set]] about the various [[performance metric]]s of [[predictive association rule]]s.
[[In this paper, we]] propose a [[transfer learning framework]] based on the [[multi-source domain adaptation methodology]] for [[detect]]ing different [[stages of fatigue]] using [[SEMG signal]]s, that addresses the [[distribution]] [[difference]]s.
In [[this paper, we]] propose a [[two-phase algorithm]] with [[partial correlation-based]] [[CI tests]]: the [[first phase]] of [[the algorithm]] constructs a [[Markov random field]] from [[data]], which provides a [[close approximation]] to the structure of the [[true]] [[Bayesian network]]; at the [[second phase]], [[the algorithm]] removes [[redundant edges]] according to [[CI tests]] to get the [[true]] [[Bayesian network]] .
[[In this paper, we]] propose a [[two-stage method]] called [[Assember]] .
In [[this paper, we]] propose a [[unified]] [[algorithmic framework]] for solving many known [[variant]]s of [[MDS]] .
[[In this paper, we]] propose a [[unified generative model]] for [[LARA]], which does not need [[pre-specified aspect keyword]]s and [[simultaneously mine]]s 1) [[latent topical aspect]]s, 2) [[rating]]s on each [[identified aspect]], and 3) [[weight]]s placed on different [[aspect]]s by a [[reviewer]] .
[[In this paper, we]] propose a variant of the [[calibrated]] [[multi-task feature learning]] [[formulation]] by including a [[squared norm regularizer]] .
In [[this paper, we]] propose [[Discriminative Topic Model (DTM)]] that [[separates]] [[non-neighboring pairs]] from each other in addition to bringing [[neighboring pairs]] [[closer together]], thereby preserving the [[global]] [[manifold structure]] as well as improving the [[local consistency]] .
[[In this paper, we]] propose [[GIGATENSOR]], a [[scalable distributed algorithm]] for [[large scale tensor decomposition]] .
[[In this paper we]] propose <i>[[WhereNext]]</i>, which is a [[method]] aimed at [[predicting]] with a certain level of [[accuracy]] the next [[location]] of a [[moving object]] .
[[In this paper, we]] propose [[KAURI]], a [[graph-based framework]] to [[collectively link]] all the [[named entity mention]]s in all [[tweet]]s posted by a [[user via modeling]] the [[user's topics of interest]] .
[[In this paper, we]] propose [[Lassplore]] for solving [[large-scale]] [[sparse logistic regression]] .
[[In this paper, we]] propose [[LCARS]], a [[location-content-aware recommender system]] that offers a particular [[user]] a [[set]] of [[venue]]s (e.g., [[restaurants)]] or [[event]]s (e.g., [[concert]]s and [[exhibitions]]) by giving consideration to both [[personal interest]] and [[local preference]] .
[[In this paper, we]] propose [[LIEGE]], the first [[general framework]] to [[Link the entiti]]es in [[web list]]s with the [[knowledge base]] to the best of [[our knowledge]] .
[[In this paper, we]] propose [[MASCOT]], a [[memory-efficient]] and [[accurate]] [[method for local triangle estimation]] in a [[graph stream]] based on [[edge sampling]] .
[[In this paper we]] propose [[MetaFac (MetaGraph Factorization)]], a [[framework]] that extracts [[community]] [[structures]] from various [[contexts]] and [[interactions]] .
[[In this paper, we]] propose [[metric]]s of [[influence]], coverage and [[connectivity]] for [[scientific literature]] .
[[In this paper, we]] propose novel [[sparse PCA method]]s to perform [[anomaly detection]] and [[localization]] for [[network data stream]]s.
In [[this paper, we]] propose [[Paragraph Vector]], an [[unsupervised framework]] that learns [[continuous distributed vector representation]]s for [[pieces of texts]] .
In [[this paper]] [[we]] propose [[Support Vector Random Fields (SVRFs)]], an extension of [[Support Vector Machines (SVMs)]] that [[explicitly models]] [[spatial correlations]] in [[multi-dimensional data]] .
[[In this paper, we]] propose the [[Bayesian Browsing Model (BBM)]], a new [[modeling technique]] with following [[advantages]] : (a) it does <i>[[exact inference]]</i>; (b) it is [[single-pass]] and [[parallelizable]]; (c) it is [[effective]] .
[[In this paper, we]] propose the [[MAHR approach]], which is able to [[automatically discover]] and [[exploit]] [[label relationship]] .
[[In this paper]], [[we]] propose the [[MIML (Multi-Instance Multi-Label learning]]) [[framework]] where an [[example]] is described by multiple [[instance]]s and associated with [[multiple]] [[class label]]s.
[[In this paper, we]] propose the [[problem of estimating]] an [[overlapping decomposition]] for [[Gaussian graphical model]]s of a [[large scale]] to generate [[overlapping sub-graphical model]]s.
[[In this paper, we]] propose the [[Semantic Wiki]] [[KnowWE]] as a [[knowledge engineering environment]] for [[decision-support system]]s.
[[In this paper, we]] propose three [[learning strategi]]es to automatically [[adjust]] the [[tradeoff parameter]] for [[Banditron]] .
In [[this paper, we]] propose to use a [[Bayesian network structure]] to [[efficiently]] [[encode]] the [[conditional dependencies]] of the [[labels]] as well as the [[feature set]], with the [[feature set]] as the [[common parent]] of all [[labels]] .
[[In this paper, we]] propose [[Word-Class Lattices (WCLs)]], a generalization of [[word lattice]]s that [[we]] use to model [[textual definition]]s.
In [[this paper]] we provide a [[unifying view]] of [[sparse approximation]]s for [[GP regression]] .
[[In this paper we]] provide improved [[bound]]s on the [[dot product]] under [[random projection]] that matches the [[optimal bound]]s on the [[Euclidean distance]] .
[[In this paper, we]] report a successful [[large-scale]] [[case study]] of [[conjoint analysis]] on [[click through stream]] in a [[real-world application]] at [[Yahoo!]] .
[[In this paper we]] review [[Sweet IM]] [[A/B testing system]], with focus on [[statistical aspect]]s and [[methodology]] .
[[In this paper, we]] show how [[response prediction]] can be viewed as a [[problem]] of [[matrix completion]], and propose to [[solve]] it using [[matrix factorization technique]]s from [[collaborative filtering (CF)]] .
[[In this paper, we]] show how [[uncertain extraction]]s about [[entiti]]es and their [[relation]]s can be transformed into a <em>[[knowledge graph]]</em>.
In [[this paper, we]] show that it is possible to design [[efficient]] [[inference algorithm]]s for a [[conditional random field]] using [[features]] that depend on [[long]] [[consecutive]] [[label sequence]]s ([[high-order features]]), as long as the [[number]] of [[distinct]] [[label sequence]]s used in the [[features]] is [[small]] .
In [[this paper]] [[we]] show that once the [[Wikipedia entiti]]es [[mention]]ed in a [[corpus of textual assertions]] are [[linked]], this can further enable the [[detection]] and [[fine-grained typing]] of the [[unlinkable entiti]]es.
[[In this paper, we]] show that [[personalized level]]s can be [[automatically generated]] for [[platform game]]s.
[[In this paper, we]] show that the [[rank-one]] [[binary]] [[matrix approximation]] can be [[reformulated]] as a [[0-1 integer linear program (ILP)]] .
[[In this paper, we]] show that [[topic model]]s, such as [[Latent Dirichlet Allocation (LDA)]] and its [[hierarchical variant]]s, form a [[natural]] [[class of model]]s for [[learning]] [[accurate]] [[entity disambiguation model]]s from [[crowd-sourced]] [[knowledge base]]s such as [[Wikipedia]] .
In [[this paper we]] study a [[query-dependent]] [[variant]] of the [[community-detection problem]], which [[we]] call the <i>[[community-search problem]]</i>: given a [[graph]] <math>G</math>, and a [[set]] of <i>[[query nodes]]</i> in the [[graph]], [[we]] seek to find a [[subgraph]] of <math>G</math> that contains the [[query]] [[nodes]] and it is [[densely connected]] .
[[In this paper, we]] study a [[variant]] of the [[social network maximum influence problem]] and its application to [[intelligently approaching]] individual [[gang member]]s with [[incentive]]s to leave a [[gang]] .
[[In this paper, we]] [[study]] [[clustering]] of [[multi-typed]] [[heterogeneous network]]s with a <i>[[star network schema]]</i> and propose a novel [[algorithm]], [[NetClus]], that utilizes [[link]]s across [[multi-typed]] [[object]]s to generate [[high-quality]] [[net-clusters]] .
[[In this paper we]] study <i>[[link prediction with explanations]]</i> for [[user recommendation]] in [[social network]]s.
[[In this paper, we]] study [[IM]] in the [[absence of complete information]] on [[influence probability]] .
[[In this paper, we]] study [[massive]] [[real-world]] [[social network]]s formed by [[direct contacts]] among [[people]] through various [[personal communication service]]s, such as [[Phone-Call]], [[SMS]], [[IM]] etc.
[[In this paper, we]] [[study]] `[[networked bandit]]s', a new [[bandit problem]] where a [[set]] of [[interrelated arm]]s varies over [[time]] and, given the [[contextual information]] that selects one arm, invokes other [[correlated arm]]s.
In [[this paper, we]] study the [[data annotation bias]] when [[data item]]s are presented as [[batch]]es to be judged by [[worker]]s [[simultaneously]] .
[[In this paper, we]] study the differences between [[social media data]] and [[traditional]] [[attribute-value data]], investigate if the [[relation]]s revealed in [[linked data]] can be used to help [[select]] relevant [[feature]]s, and propose a novel [[unsupervised feature selection framework]], [[LUFS]], for [[linked social media data]] .
In [[this paper, we]] study the [[discovery of <i>frequent patterns</i>]] and <i>[[association rules]]</i> from [[probabilistic data]] under the <i>[[Possible World Semantics]]</i>.
[[In this paper, we]] study the [[dynamics]] of [[news event]]s and their [[relation]] to [[changes of sentiment expressed]] on [[relevant topic]]s.
[[In this paper, we]] [[study]] the efficient [[influence maximization]] from two [[complementary]] directions.
[[In this paper, we]] study the key [[computational problem]] involved in [[organization]] of [[social event]]s, to our [[best knowledge]], for the first [[time]] .
In [[this paper, we]] study the [[linking patterns]] and [[discussion topics]] of [[political bloggers]] .
[[In this paper we]] [[study]] the [[mutual dependencies]] between these two [[step]]s and propose to [[combine]] them by using a [[model]] of [[Conditional Random Fields (CRFs)]] .
[[In this paper, we]] study the [[pattern]]s of [[user]] [[participation]] [[behavior]], and the [[feature]] [[factor]]s that [[influence]] such [[behavior]] on different [[forum]] [[dataset]]s.
[[In this paper we]] study the [[problem of maximizing achievable reciprocity]] for an [[ensemble of digraphs]] with the same [[prescribed]] [[in-]] and [[out-degree sequence]]s.
[[In this paper, we]] study the [[problem of modeling]] the [[dynamics of composite network]]s, where the [[evolution process]]es of different [[network]]s are [[jointly considered]] .
[[In this paper, we]] study the problem of [[opinion summarization]] for [[entiti]]es, such as [[celebriti]]es and [[brand]]s, in [[Twitter]] .
In [[this paper, we]] study the problem of [[semi-supervised feature selection]] for [[graph classification]] and [[propose a novel solution]], called [[gSSC]], to [[efficiently]] [[search for optimal]] [[subgraph features]] with [[labeled]] and [[unlabeled graphs]] .
[[In this paper, we]] [[study]] these [[problem]]s and propose two effective [[methods]] to solve the [[problems]] .
[[In this paper, we]] study the [[speed]] and [[scalability]] of its [[algorithm]]s.
In [[this paper]] we study the [[support vector machine (SVM)]] ([[Vapnik, 1996]]; [[Schölkopf and Smola, 2001]]) for [[two-class classification]] .
[[In this paper, we]] study three different [[collaborative filtering frameworks]] : [[Low-rank]] [[matrix approximation]], [[probabilistic]] [[latent semantic analysis]], and [[maximum-margin]] [[matrix factorization]] .
[[In this paper, we]] tackle the [[problem]] of [[template-independent]] [[Web news extraction]] in a [[user-friendly way]] .
In [[this paper, we]] take a [[computer science bibliographic network]] as an [[example]], to [[analyze]] the [[roles]] of [[authors]] and to discover the likely [[advisor-advisee relationships]] .
In [[this paper, we]] therefore propose [[Trust Antecedent Factor (TAF) Model]], a novel [[probabilistic model]] that [[generate]] [[ratings]] based on a number of [[rater]]'s and [[contributor]]'s [[factors]] .
[[In this paper, we]] unify [[vector-based]] and [[graph-based approach]]es.
[[In this paper, we]] use a [[recurrent network]] to generate the [[model descriptions of neural network]]s and [[train]] [[this RNN]] with [[reinforcement learning]] to [[maximize the expected accuracy]] of the [[generated architecture]]s on a [[validation set]] .
In [[this paper, we]] use the term [[semantics]] in a general sense, as the [[meaning of a word]], a [[phrase]], a [[sentence]], or any [[text]] in [[human language]], and the study of such [[meaning]] .
[[In this paper we]] utilize a [[keyword propagation algorithm]] based on [[manifold structure]] to enrich the [[keyword information]] and remove the [[noise]] for [[video]]s.
[[In this paper, we]] want to [[study]] the [[IOC(Inference of Organizational Chart) problem]] to [[identify]] [[company internal organizational chart]] based on the [[heterogeneous online ESN]] launched in [[it]] .
In this [[probabilistic]] [[context]], an [[itemset]] <i>X</i> is called [[frequent]] if the [[probability]] that <i>X</i> occurs in at least <i>[[minSup]]</i> [[transaction]]s is above a given [[threshold]] <math>\tau</math>.
In this [[problem]], [[we]] assume that our [[data]] is [[represented as a graph]] with [[nodes]] and [[edges]], <math>G</math> = (''V'', ''E'').
:::: in this [[regression function]] <math>f(X)=f(x_i,\boldsymbol\beta)</math> is a [[parametric regression function]] which is a [[linear combination]] between <math>p</math> [[regression coefficient]]s <math>\beta_j</math> ([[parameter]]s) and [[basis function]]s <math>\phi_j(x_i)</math>.
In [[this research]], a [[research model]] was proposed to [[integrate]] these [[theori]]es and two [[experiment]]s were conducted to examine the [[theoretical relationship]]s.
In this [[research]], [[we]] aim to address some critical issues related to [[privacy protection]]: Would the highest [[privacy setting]]s [[guarantee]] a [[secure protection]]?
In this section, [[we]] discuss the differences between [[generative]] and [[discriminative modeling]], and the advantages of [[discriminative modeling]] for many [[task]]s.
In this section, [[we]] extend [[RAE]]s to a [[semi-supervised setting]] in order to [[predict]] a [[sentence]] - or [[phrase-level target distribution]] t.1
In this section, [[we]] provide [[empirical evidence]] that combining [[labeled]] and [[unlabeled training document]]s using [[EM]] [[outperform]]s [[traditional]] [[naive Bayes]], which trains on [[labeled document]]s alone.
In this section, [[we]] set the scope of [[ontology mapping]] and [[ontology mapping tool]]s, and outline meanings of [[ontology mapping]], [[integration]], [[merging]], and [[alignment]] .
In [[this sense]], the [[labeled training data]] only draws from the [[positive class]], and the [[unlabeled data]] is a [[mixture]] of [[positive]] and [[negative sample]]s, a problem usually referred to as [[positive-unlabeled (PU) learning]] ([[Hsie et al., 2015]]] .
In [[this sense]] we improve [[the result]] in [4] ([[Best paper]] of [[STOC 2013]]).
In [[this setting]] it is natural to consider sets of [[Dirichlet processes]], one for each [[group]], where the well-known [[clustering property]] of the [[Dirichlet process]] provides a [[nonparametric prior]] for the [[number]] of [[mixture component]]s [[within]] each [[group]] .
In [[this study, we]] [[compare]] the [[Cox proportional hazards model]] with a [[machine learning approach]] for [[stroke]] [[prediction]] on the [[Cardiovascular Health Study (CHS) dataset]] .
In [[this study]] we evaluate the use of [[approximations]] to the [[computation]] of the [[PCRW]] [[distributions]], including [[fingerprinting]], [[particle filtering]], and [[truncation strategies]] .
[[In this study, we]] formalize a [[multi-focal learning problem]], where [[training data]] are [[partition]]ed into several different [[focal group]]s and the [[prediction model]] will be [[learned]] within each [[focal group]] .
In [[this study]], [[we]] investigate [[predictive modeling]] for providing a [[low-cost preventive medicine program]] .
In [[this study]], [[we]] investigate the problem in the context of [[heterogeneous bibliographic network]]s and propose a novel [[cluster-based citation recommendation framework]], called [[ClusCite]], which explores the [[principle]] that [[citation]]s tend to be [[softly clustered]] into [[interest group]]s based on multiple [[types of relationship]]s in the [[network]] .
In [[this study]], [[we]] propose a [[method]] called <i>[[LiSM (Line-in-the-Sand Miner)]]</i> to discover [[trajectories]] from [[untrustworthy sensor data]] .
In [[this study]], [[we]] propose a novel [[method]], called [[STROD]], that allows [[efficient]] and consistent modification of [[topic hierarchi]]es, based on a [[recursive generative model]] and a [[scalable tensor decomposition inference algorithm]] with [[theoretical performance guarantee]] .
In [[this study]], [[we]] propose a simple and yet effective [[method]] to provide [[uncertainty estimate]]s for an [[interpretable]] [[early classification method]] .
In [[this study]] we strive to develop a series of new [[latent semantic models]] with a [[deep structure]] that [[project]] [[queri]]es and [[document]]s into a common [[low-dimensional space]] where the [[relevance]] of a [[document]] given a [[query]] is readily computed as the distance between them.
In [[this study]], [[we]] take a first step towards better [[disease management]] of [[diabetic patient]]s by applying [[state-of-the art method]]s to anticipate the [[patient's future health condition]] and to [[identify]] [[patients at high risk]] .
In [[this survey]] we overview the [[definition]]s and [[methods for graph clustering]], that is, [[finding]] [[sets of ''related'' vertice]]s in [[graph]]s.
In [[this survey]], we provide a comprehensive and [[structured overview]] of a [[large set]] of interesting [[outlier definition]]s for various forms of [[temporal data]], novel [[technique]]s, and [[application scenario]]s in which specific [[definition]]s and [[technique]]s have been widely used.
In [[this system]], [[GPS-equipped taxicab]]s are employed as [[mobile sensor]]s constantly probing the [[traffic rhythm]] of a [[city]] and [[taxi drivers' intelligence]] in choosing [[driving direction]]s in the [[physical world]] .
In this talk, [[Drew]] will examine [[data science]] through the [[lens]] of the [[social scientist]] .
In this talk, first [[I]] will review the [[statistical approach]], especially [[Bayesian approach]], for the [[relational data analysis]] with recent advancements in [[machine learning]] [[literature]] .
[[In this talk]] [[I]] introduce [[cloud computing]] based [[cross-media knowledge discovery]] .
In this talk, [[information processing]] in [[social network]]s will first be reviewed in three [[phrase]]s, namely (i) from [[content]] to [[social relationship]], (ii) [[mining]] on [[social relationship]], and (iii) from [[social relationship]] to [[content organization]] .
In this talk, [[Sprint's Head of Predictive Modeling]], [[Tracey De Poalo]], will talk about the [[process]] she developed using [[SAS]] and [[logistic regression]] to build a [[wide range]] of [[model]]s.
In this talk we will show how [[data science]] and [[optimization technique]]s can be applied to [[cross channel data]] to [[attribute marketing effectiveness]], [[drive media planning]] and [[real-time optimization]] of [[campaign]]s.
[[In this theory]], [[discourse structure]] is composed of three separate but [[interrelated component]]s: the [[structure]] of the [[sequence of utterances]] (called the [[linguistic structure]]), a [[structure of purposes]] (called the [[intentional structure]]), and the [[state of focus of attention]] (called [[the attentional state]]).
In this [[tutorial]], [[we]] review [[modeling]], [[reference]], and [[parameter estimation]] in [[CRFs]], both on [[linear chains]] and on [[general graphical structures]] .
In this way, [[MetricForensics]] [[scales]] to highly [[volatile]] [[graphs]] by only [[allocating resource]]s for [[computationally expensive analysis]] when an [[interesting event]] is discovered at a [[coarser resolution]] first.
In this way we can [[learn latent community function]]s and the corresponding [[portfolio]]s of [[estate]]s from [[human mobility data]] and [[Point of Interest (POI)]] [[data]] .
In [[this work]], a novel [[training scheme]] for [[generating]] [[bottleneck feature]]s from [[deep neural network]]s is proposed.
In this [[work]], [[we]] address [[software reliability issues]] by proposing a novel [[method]] to [[classify]] [[software behaviors]] based on [[past history]] or [[runs]] .
In [[this work]], [[we]] address [[the latter problem]] by introducing a novel [[algorithm]] that finds [[shapelet]]s in less time than current [[method]]s by an [[order of magnitude]] .
[[In this work, we]] address the new [[problem]] of [[cold-start local event recommendation]] in [[EBSN]]s.
[[In this work, we]] address [[this problem]] by proposing an [[active version]] of [[matrix completion]], where [[queri]]es can be made to the [[true underlying matrix]] .
[[In this work, we]] attempt to deduce the [[cognitive process]] of [[task routing]], and [[model]] the [[decision making]] of [[expert]]s as a [[generative process]] where a [[routing decision]] is made based on [[mixed routing pattern]]s.
In [[this work]], [[we]] begin by presenting a [[formal framework]] for this general [[problem]] .
In this work we collected a [[representative]] enough portion of the [[music]] [[social network]] [[last.fm]], capturing [[explicitly expressed]] [[bonds of friendship]] of the [[user]] as well as [[social tag]]s.
In [[this work]], [[we]] combine [[supervised NER]] with [[bootstrapping]] to expand the [[seed list]], and [[output]] [[normalized result]]s.
In [[this work]], [[we]] consider how to [[sanitize data]] to prevent the [[disclosure]] of [[sensitive pattern]]s during [[sequential pattern mining]], while ensuring that the [[nonsensitive pattern]]s can still be [[discovered]] .
[[In this work, we]] consider [[this problem]] through [[the framework of Bayesian optimization]], in which a [[learning algorithm's generalization performance]] is [[modeled]] as a [[sample]] from a [[Gaussian process (GP)]] .
In [[this work]], [[we]] define a [[logic]] formalizing [[induction]] over [[well-founded set]]s and [[monotone]] and [[iterated induction]] .
In [[this work]], we describe [[DeepDive]], a [[system]] that combines [[database]] and [[machine learning idea]]s to help develop [[KBC system]]s, and we present techniques to make the [[KBC process]] more [[efficient]] .
In [[this work]] [[we]] describe the [[design]], [[implementation]] and [[deployment]] of a [[decision support system]] for the [[detection]] and the [[damage assessment]] of [[earthquake]]s in [[Italy]] .
In this [[work]], [[we]] [[empirically study]] [[workloads]] from three popular [[knowledge-sharing]] [[OSNs]], including a [[blog system]], a [[social]] [[bookmark sharing network]], and a [[question answering]] [[social network]] to examine these [[properties]] .
In this [[work]], [[we]] explore a [[principled]] and [[flexible framework]] in order to find [[alternative]] [[clusterings]] of the [[data]] .
[[In this work, we]] explore techniques that can [[automatically classify]] [[malware variant]]s into their [[corresponding famili]]es.
In [[this work]] we extend the [[recent sampling advance]]s for [[unsupervised LDA model]]s to [[supervised task]]s.
In [[this work]], [[we]] first propose classes of novel [[trajectory outlier definition]]s that [[model]] the [[anomalous behavior]] of [[moving object]]s for a large [[range]] of [[real time application]]s.
In [[this work]], [[we]] focus on [[binary classification problem]]s and study [[selective labeling]] in [[data stream]]s where a [[decision]] is required on each [[sample sequentially]] .
In [[this work]], [[we]] focus on the following [[limitation]]s of our former [[work]] in [[retrieving relevant concept]]s at a given turn with the [[term vector similarity]] between each [[pair of dialogue segment]] and [[Wikipedia article]] .
In this [[work]], [[we]] focus on [[WordNet 2.0]] [[senses]], though any similar [[term bank]] would apply.
[[In this work we]] identify [[unique properti]]es of [[implicit]] [[feedback dataset]]s.
In [[this work]] [[we]] identify [[unique properti]]es of [[implicit]] [[feedback dataset]]s.
[[In this work, we]] instead derive [[sense vector]]s by [[embedding]] the [[graph structure]] of a [[semantic network]] in the [[word space]] .
[[In this work, we]] introduce a new class of [[indexable]] <i>[[fault signatures]]</i> that [[encode]] [[temporal]] [[evolution]] of [[events]] [[generated by]] a [[network]] [[fault]] as well as [[topological relationships]] among the [[nodes]] where these [[events]] occur.
In [[this work]] we introduce a new [[method]], [[Key-Value Memory Network]]s, that makes [[reading documents]] more viable by utilizing different [[encoding]]s in the [[addressing]] and [[output stage]]s of the [[memory read operation]] .
[[In this work we]] introduce a new [[time series]] [[primitive]], [[time series shapelets]], which addresses these [[limitations]] .
In [[this work]] [[we]] introduced a [[method to embed word types]] into the [[space of Gaussian distributions]], and [[learn the embeddings]] [[directly]] in [[that space]] .
In [[this work]] [[we]] introduce [[restreaming]] [[graph partitioning]] and [[develop algorithm]]s that [[scale similarly]] to [[streaming partitioning algorithm]]s yet empirically perform as well as fully [[offline algorithm]]s.
In [[this work]], [[we]] introduce the [[multi-layer coherent subgraph (MLCS) model]], which defines [[clusters of vertice]]s that are densely connected by [[edge]]s with similar [[label]]s in a [[subset]] of the [[graph layer]]s.
[[In this work, we]] introduce the novel [[paradigm]] of [[subspace correlation clustering]]: we [[analyze]] <i>[[subspace projection]]s</i> to find <i>[[subsets of object]]s</i> showing <i>[[linear correlation]]s</i> among this [[subset of dimension]]s.
In [[this work]] [[we]] introduce the recent [[temporal pattern mining framework]] for finding [[predictive pattern]]s for [[monitoring]] and [[event detection problem]]s in [[complex]] [[multivariate time series data]] .
In [[this work]], [[we]] investigate how such [[team]]s can be formed on a [[social network]] .
In [[this work]], [[we]] present a [[statistical model]] to [[estimate]] [[worker reliability]] and [[task clarity]] without resorting to the single [[gold standard assumption]] .
[[In this work, we]] present the first [[learning algorithm]] for [[mKPGM]]s.
[[In this work, we]] present the first [[online outlier exploration platform]], called [[ONION]], that enables [[analyst]]s to effectively explore [[anomali]]es even in [[large dataset]]s.
In [[this work]], [[we]] propose a generalized [[cross domain collaborative filtering]] [[framework]] that integrates [[social network information]] seamlessly with [[cross domain data]] .
In [[this work]], [[we]] propose a [[generative model]] and an [[incremental algorithm]] to automatically [[mine]] useful [[evidence]]s across [[document]]s.
In [[this work]] [[we]] propose [[algorithm]]s for privately [[estimating the parameter]]s of [[exponential random graph models (ERGMs)]] .
In [[this work]] [[we]] propose a [[method]] for [[automatically establishing]] the [[credibility]] of [[user-generated medical statement]]s and the [[trustworthiness]] of their [[author]]s by exploiting [[linguistic cue]]s and [[distant supervision]] from [[expert source]]s.
In [[this work]], [[we]] propose a new [[holistic approach]] called [[SPEAGLE]] that utilizes [[clue]]s from all [[metadata]] ([[text]], [[timestamp]], [[rating]]) as well as [[relational data (network)]], and [[harness]] them [[collectively]] under a <i>[[unified framework]]</i> to spot suspicious [[user]]s and [[review]]s, as well as [[products]] [[targeted]] by [[spam]] .
In [[this work]], [[we]] propose a novel [[methodology]] using [[graph clustering]] to analyze [[average treatment effect]]s under [[social interference]] .
In [[this work]] [[we]] propose a [[probabilistic model]] based on [[collaborative filtering]] and [[topic modeling]] .
In [[this work]], [[we]] propose a [[translation method]] that [[learn]]s the [[mapping]] of the [[contextual information]] to the [[textual features of ad]]s by using [[past click data]] .
In this [[work]], [[we]] propose a very [[simple algorithm]] to [[automatically]] [[identify community structures]] from these two [[types of data]] .
In [[this work]], [[we]] proposed the use of the [[probability distribution]] resulting from a random walk with restart over a suitable [[entity graph]] to [[represent]] the [[semantics]] of [[entities]] and [[document]]s in a unified way.
In [[this work]], [[we]] propose instead to [[minimize]] a [[regularized distance criterion]], which is motivated by the [[minimum distance functional]]s used in [[nonparametric method]]s for their excellent [[robustness properti]]es.
In [[this work]], we propose [[Neural Programmer]], an [[end-to-end]] [[differentiable neural network]] augmented with a [[small set]] of [[basic arithmetic]] and [[logic operation]]s.
[[In this work, we]] propose to [[extract]] [[latent]] [[social dimension]]s based on [[network]] [[information]], and then utilize them as [[features]] for [[discriminative]] [[learning]] .
In [[this work]], [[we]] provide a novel [[theoretical analysis]] of [[deterministic]] leverage [[score sampling]] .
In [[this work]], [[we]] show why these [[idea]]s are not directly suitable for [[rule discovery]] in [[time series]] .
[[In this work, we]] [[streamline]] an important step in the [[operation]]s of the [[NGO]] by developing and deploying a [[data-driven system]] for [[locating village]]s with [[extreme poverty]] in [[Kenya]] and [[Uganda]] .
In [[this work]], [[we]] study [[max-min LP]]s and [[min-max LP]]s in a distributed setting.
[[In this work, we]] study the [[music listening histori]]es of [[Last.fm]] [[user]]s focusing on the changes in their [[preference]]s based on their [[choice]]s for different [[artist]]s at different [[point]]s in [[time]] .
[[In this work we]] try to overcome these [[limitation]]s by explicitly including [[time]] in the extracted [[knowledge]], thus making the [[temporal information]] a [[first-class citizen]] of the [[analysis]] [[process]] .
In [[this work]] [[we]] use [[conditional random fields]] ([[Lafferty et al., 2001]]), a type of [[undirected graphical model]], to [[automatically]] [[label fields of contact records]] .
In [[thorough experiment]]s [[we]] demonstrate the strength of [[our novel paradigm]] in [[comparison]] to existing [[method]]s.
In [[thorough experiment]]s [[we]] demonstrate the strengths of [[MiMAG]] in [[comparison]] with related [[approach]]es on [[synthetic]] as well as [[real-world dataset]]s.
In those [[applications]], [[training data]] occurs naturally in text corpora, and [[high]] quality [[training data]] [[sets]] running into billions of [[words]] have been reportedly used.
In those described by the [[sociologist]] [[Max Weber]] as “[[patrimonial]],” the [[polity]] is considered a [[type of]] [[personal property]] of the [[ruler]], and [[state administration]] is essentially an extension of the [[ruler]]’s [[household]] .
In [[TML]], [[we]] [[learn the metric]] and the [[task covariance]]s between the [[source tasks]] and the [[target task]] under a unified [[convex formulation]] .
In today's [[computerized]] and [[information-based society]], [[individual]]s are constantly presented with [[vast amounts of text data]], ranging from [[news article]]s, [[scientific publication]]s, [[product review]]s, to a wide range of [[textual information]] from [[social media]] .
In [[topic modelling]], various alternative [[prior]]s have been developed, for instance [[asymmetric]] and [[symmetric prior]]s for the [[document-topic]] and [[topic-word]] [[matrices respectively]], the [[hierarchical Dirichlet process]] prior for the [[document-topic matrix]] and the [[hierarchical Pitman-Yor process prior]] for the [[topic-word matrix]] .
[[Intractable]] [[publication volume]]s; the [[difficulty]] of [[verifying evidence]]; and [[observed problem]]s in [[evidence]] and [[citation chain]]s suggest a need for a [[web-friendly]] and [[machine-tractable model]] of [[scientific publication]]s.
In traditional [[application]]s, both [[academic]] and [[industrial]], [[statistical model]]s or [[inference procedure]]s tend to be of a [[`throw-away' nature]] .
In [[traditional]] [[classification task]]s [1]: [[Class]]es are [[mutually exclusive]] by definition.
In traditional [[text clustering methods]], [[documents]] are [[represented as]] "[[bags of words]]" without considering the [[semantic information]] of each [[document]] .
[[Intrusion Detection System]]s: In many [[host-based]] or [[networked computer system]]s, different kinds of [[data]] are collected about the [[operating system call]]s, [[network traffic]], or other [[activity]] in [[the system]] .
[[Intuitions abound]], but a [[coherent framework]] for [[understanding]], [[analyzing]], and [[synthesizing]] [[deep learning architecture]]s has remained elusive.
Intuitively, [[arbitrary length]] [[multidimensional data sequence]]s are [[projected]] into a [[fixed dimensional manifold]] for [[LCSS parameter learning]] .
[[Intuitively]], a reputable [[user]] posts [[high quality comment]]s and is [[highly rated]] by the [[user community]] .
Intuitively, a [[user]] may trust different [[subset]]s of [[friend]]s regarding different [[domain]]s.
Intuitively, one can "<i>explore</i>" each [[candidate item]] by displaying it to a [[small fraction]] of [[user visit]]s to [[estimate]] the [[item]]'s [[click-through rate (CTR)]], and then "<i>exploiy</i>" [[high CTR item]]s in order to [[maximize click]]s.
Intuitively, to represent a [[dense region]], the [[subgraph identified]] should be the [[subgraph]] with [[highest density]] in its [[local region]] in the [[graph]] .
[[Intuitively]], [[user]]s in a group may have different [[influence]]s, and those who are expert in [[topic]]s relevant to the [[group]] are usually more [[influential]] .
In turn the [[recommendation system]] [[optimize]]s a [[pool]] of [[trending local]] and [[global topic]]s by exploiting [[user feedback]] .
In turn, this ability enables us to [[reduce model complexity]] compared to [[full-fledged]] [[fluid dynamics model]]s, while maintaining [[forecast accuracy]]; (iii) Unlike previous [[simulation-based studi]]es, [[we]] perform [[experiment]]s in a [[production data center]] .
In typical [[classification task]]s, [[we]] seek a [[function]] which assigns a [[label]] to a single [[object]] .
In typical [[data sets]], [[our utility criterion]] implies [[low]] [[false positive]] and [[false negative rates]] in the [[reported lists]] .
In [[USpan]], [[we]] introduce the [[lexicographic quantitative sequence]] tree to extract the complete [[set]] of [[high utility sequence]]s and [[design]] [[concatenation mechanism]]s for calculating the [[utility]] of a [[node]] and its [[children]] with two effective [[pruning strategi]]es.
In various [[application]]s, [[graph]]s are enriched by additional [[information]] .
[[Inventories]] of [[manually compiled]] [[dictionaries]] usually serve as a source for [[word senses]] .
Investigating [[this problem]], [[we]] consider the [[sequence]] of [[queri]]es and their [[click]]s in a [[search session]] as a [[task]] and propose a [[task-centric click model (TCM)]] .
[[Investigation]]s into [[brain connectivity]] aim to [[recover network]]s of [[brain region]]s connected by [[anatomical tract]]s or by [[functional]] [[association]]s.
In view of this, [[we]] [[shall]] also examine how this [[paradigm]] shift will affect the [[information processing]] in [[social network]]s.
In [[Web Mining]], [[data]] can be collected at the [[server-side]], [[client-side]], [[proxy server]]s, or obtained from an [[organization's database]] (which contains [[business data]] or [[consolidated]] [[Web data]]).
[[IOC]] is very challenging to address as, to [[guarantee]] smooth [[operation]]s, the [[internal organizational chart]]s of [[compani]]es need to meet certain [[structural requirement]]s (about its [[depth]] and [[width]]).
I plan to take a [[proactive step]] by looking more closely at my [[relationship]]s with [[physician]]s, [[lawyer]]s, [[banker]]s, [[accountant]]s, [[financial adviser]]s, and the other [[professional]]s to whom I turn to for [[expert advice]] .
I review [[deep supervised learning]] (also recapitulating the history of [[backpropagation]]), [[unsupervised learning]], [[reinforcement learning]] & [[evolutionary computation]], and [[indirect search]] for [[short program]]s [[encoding]] [[deep and large network]]s.
[[I]] [[review]] some of the recent [[work]] in this [[area]], [[outlining]] some of the [[open question]]s and [[problem]]s.
[[IR Query Suggestion Task]], [[Interactive Task]], [[Context-Aware Algorithm]], [[Personalized Algorithm]] .
Irrespective of the [[learning algorithm]] itself, the final [[classifier]] has a [[weight]] to [[multiply]] by each [[feature]] .
[[ISIS]] has been used in over a [[dozen case studi]]es defined by the [[DoD]], [[DHHS]], [[NIH]], [[BARDA]] and [[NSC]] .
[[ISIS]] is comprised of the following [[basic component]]s: (<i> i </i>) a [[web app]] that serves as the [[user-interface]], (<i> ii </i>) a [[middleware]] that [[coordinate]]s [[user interaction]] via the [[web app]] with [[backend model]]s and [[database]]s, (<i> iii </i>) a [[backend computational modeling framework]] that is comprised of [[highly resolved epidemic simulation]]s combined with [[highly realistic control strategi]]es that include [[pharmaceutical]] as well as [[non-pharmaceutical intervention]]s and (<i> iv </i>) a [[backend data management framework]] that manages [[complex unstructured]] and [[semi-structured data]] .
[[ISIS]] is designed to [[support networked epidemiology]] -- [[study]] of [[epidemic process]]es over [[social contact network]]s.
Is my [[marketing]] working, how much is [[marketing]] helping the [[business]] and which [[campaign]]s and [[channel]]s are [[effective]]?
Issues such as solving [[SVM optimization problem]]s, [[theoretical convergence]], [[multiclass classification]], [[probability estimates]] and [[parameter selection]] are discussed in detail.
• Is the [[data]] contained within an [[object]] of [[static]] or [[dynamic layout]]?
[[I]] study large [[charitable gift]]s by [[Chairmen]] and [[CEO]]s of [[public compani]]es using their own [[company stock]] as the [[donation currency]] .
[[It]] allows for the [[integration of knowledge]] at different [[levels of formality]], and therefore tries to weaken the [[flexibility]] / [[productivity]] [[dilemma]] described above.
It allows us to capture the [[interest distribution]] of [[user]]s and the [[content distribution]] for [[movi]]es; it provides a [[link]] between [[interest]] and [[relevance]] on a [[per-aspect basis]] and it allows us to differentiate between [[positive]] and [[negative sentiment]]s on a [[per-aspect basis]] .
[[It]] also achieves [[state-of-the-art results]] on the existing [[WikiQA benchmark]] .
It also deals with many [[special word]]s, [[phrase]]s and [[language construct]]s which have impacts on [[opinion]]s based on their [[linguistic pattern]]s.
[[It]] also includes a number of [[exercise]]s with selected [[answer]]s, making it appropriate as a [[textbook]] for a [[senior year]] or [[graduate level course]] in [[AI knowledge representation]] .
[[It]] also made it difficult for [[local government]]s interested in comparing their [[financial]] and [[service delivery]] situations to make [[valid]] and [[useful comparison]]s — a [[complaint registered periodically]] with [[DCA]] by [[local government official]]s from [[around the state]] .
[[It]] also outperforms related [[model]]s on [[similarity task]]s and [[named entity recognition]] .
[[It]] also studies a number of domain-specific scenarios such as [[stream mining]], [[web graph]]s, [[social network]]s, [[chemical]] and [[biological data]] .
It also suggests an [[elegant]] and [[thorough integration]] of [[syntax]], [[semantics]], and [[pragmatics]] .
[[It]] amounts to an [[incremental method]] for [[dynamic programming]] which imposes limited [[computational demand]]s.
It can be [[categorized]] mainly into into two [[main component]]s: [[feature extraction]] and [[feature selection]] .
It can be thought of as a [[type-of relation]]; for example [[car]], [[ship]] and [[train]] are all [[hyponym]]s of [[vehicle]] .
It can improve the [[user's experience]] in previewing a video's [[timeline structure]] compared to [[traditional]] [[scheme]]s that [[tag]] an entire [[video clip]] .
It can support studies in [[cyber-security]], [[document management]], hypertext / hypermedia, IR, [[knowledge management]], [[LIS]], multimedia, and [[machine learning]] .
It combines a [[lexical taxonomy structure]] with [[corpus statistical information]] so that the [[semantic distance]] between [[nodes]] in the [[semantic space]] constructed by the [[taxonomy]] can be better [[quantified]] with the [[computational evidence]] derived from a [[distributional analysis]] of [[corpus data]] .
[[It]] compares favorably with well [[tuned implementation]]s of other known [[method]]s.
[[It]] considers the [[science]], [[engineering]], and [[deployment]] of [[AI-enabled computing system]]s.
It consists of [[input]], [[projection]], [[hidden]] and [[output layer]]s.
[[It]] contains extensive surveys on important [[graph topic]]s such as [[graph language]]s, [[indexing]], [[clustering]], [[data generation]], [[pattern mining]], [[classification]], [[keyword search]], [[pattern matching]], and [[privacy]] .
It contains many worked examples and homework exercises and will appeal to students, [[researcher]]s and [[practitioner]]s in fields such as [[engineering]], [[computer science]], [[mathematics]], [[statistics]], [[finance]] and [[economics]] .
It could be [[taxable]], or [[non-taxable]], and divvyed up on a [[continual basis]], [[monthly]], or [[annually]] .
[[Item-based technique]]s first analyze the [[user-item matrix]] to [[identify relationship]]s between different [[item]]s, and then use these [[relationship]]s to indirectly compute [[recommendations for user]]s.
It employs a [[method]], referred to as [[Weakly Supervised Latent Dirichlet Allocation (WS-LDA)]], to accurately [[learn]] the [[topic model]] with [[partially labeled]] [[named entities]] .
[[Item recommendation]] is the [[task of predicting]] a [[personalized ranking]] on a [[set of item]]s (e.g. [[website]]s, [[movi]]es, [[product]]s).
[[Itemset mining]], [[exponential model]]s, [[decomposable model]]s, [[junction tree]]s, [[Bayesian model selection]], [[MCMC]] .
[[Itemwise score regression]], [[pairwise preference satisfaction]], and [[listwise structured learning]] are the major [[technique]]s in use.
[[It]] encompasses the material objects from the molecular to the [[macroscopic level]]s that constitute the body and associates with them [[nonmaterial entiti]]es ([[space]]s, [[surface]]s, [[line]]s, and [[point]]s) required for describing [[structural relationship]]s.
[[It]] examines the [[live]]s and [[work]] of six [[20th century]] [[moral leader]]s who pursued [[moral cause]]s ranging from [[world peace]] to [[social justice]] and [[human right]]s.
It explains how [[machine learning]] can be used to [[identify]] [[significant term]]s within [[unstructured text]], and [[enrich]] it with [[links]] to the appropriate [[Wikipedia articles]] .
[[It]] extends such an [[engine]] in several ways, including [[column-oriented in-memory storage]] and [[dynamic mid-query replanning]], to effectively [[execute SQL]] .
It follows that, for good [[generalisation]] of [[FLD]], the [[required projection dimension]] grows [[logarithmically]] with the [[number of classes]] .
It forms part of [[SPAR]], a [[suite]] of [[Semantic Publishing]] and [[Referencing Ontologies]] .
It gives the [[highest]] [[correlation value]] (r = 0.828) with a [[benchmark]] based on [[human]] [[similarity judgements]], whereas an [[upper bound]] (r = 0.885) is observed when [[human subjects]] replicate the same [[task]] .
It goes beyond the traditional focus on [[data mining problem]]s to introduce [[advanced data type]]s such as [[text]], [[time series]], [[discrete sequence]]s, [[spatial data]], [[graph data]], and [[social network]]s.
It has been successfully applied to various [[task]]s, e.g., [[classification]], [[clustering]] and [[information retrieval]] .
It has important [[applications]] in [[information]] [[tasks]] such as [[search]] and [[recommendation]] .
It has provided a [[set of mathematical tool]]s for [[representing]] and [[analyzing]] [[information structure]]s.
I think that [[robot]]s will [[complement]] [[human]]s, not supplant them.
It involves several steps, including [[identifying]] the [[domain-relevant terminology]], [[defining]] each [[term]], and [[harmonizing]] the results.
It involves two [[sub-problems]]: how to [[detect the periods]] in [[complex movement]], and how to [[mine]] [[periodic movement behaviors]] .
It is a big [[challenge]] to [[guarantee]] the [[quality]] of [[discovered]] [[relevance features]] in [[text documents]] for describing [[user preference]]s because of the [[large number]] of [[terms]], [[patterns]], and [[noise]] .
It is a challenging problem due to its [[sparse]], [[high-dimensional]], and [[large-volume]] [[characteristic]]s.
It is a [[labor-intensive]] and [[time-consuming task]] to build a [[large-scale]] [[annotated corpus]] for each [[target domain]] .
:: It is a [[Linear Model]] trained with an [[L1]] [[prior]] as [[regularizer]] .
[[It]] is also a [[practical]], [[modern introduction]] to [[scientific computing]] in [[Python]], tailored for [[data-intensive application]]s.
[[It]] is also representative of a [[class of parallel computation]]s whose [[memory access]]es and [[work distribution]] are both [[irregular]] and [[data-dependent]] .
It is a new [[statistics]], [[machine learning]], [[data management]] and [[databases]], [[pattern recognition]], [[artificial intelligence]], and other [[areas]] .
It is a [[well-established fact]] that in [[rich societi]]es the [[poor]] have [[shorter live]]s and [[suffer]] more from almost every [[social problem]] .
It is based on a novel [[insight]] on the [[relationship]] between [[screening]] and the so-called [[proximal operator]] that we first [[establish]] in [[this paper]] .
It is becoming increasingly common for [[epidemiologist]]s to consider [[randomizing intact clusters]] (e.g. [[famili]]es, [[school]]s, [[communiti]]es) rather than [[individual]]s in [[experimental trial]]s.
It is believed that a [[joint learning]] of multiple [[data source]]s is beneficial as different [[data source]]s may contain [[complementary information]], and [[feature-pruning]] and [[data source selection]] are critical for [[learning]] [[interpretable model]]s from [[high-dimensional data]] .
It is challenging for traditional [[model]]s to capture the [[heterogeneous]] [[user interaction]]s and diverse [[patterns of cascade]]s in [[social media]] .
It is challenging to [[simultaneously]] [[perform]] the above two [[task]]s taking into account the [[nonstationarity]] and strong [[correlation]]s between [[nodes]] .
It is closely related to the [[BHK Interpretation]], a view of logic developed by the [[intuitionist]]s [[Brouwer]], [[Heyting]], and [[Kolmogorov]] in the [[1930s]] .
[[It]] is [[deploy]]ed [[commercially]] and reads several million [[cheques per day]] .
It is extremely important in many [[application domain]]s to have [[accurate]] [[model]]s of [[user behavior]] .
It is [[flexible]] since the [[user]] can formally specify [[positive]] and [[negative feedback]] based on the [[existing]] [[clustering]], which ranges from which [[clusters]] to keep (or not) to making a [[trade-off]] between [[alternativeness]] and [[clustering quality]] .
It is fully [[data-driven]], requiring only the [[text]] from a [[corpus]] of [[document]]s as [[input]], it produces [[human-readable explanation]]s, and it can be generalized to [[score diversity]] of other [[entiti]]es such as [[author]]s, [[academic department]]s, or [[journal]]s.
It is integrated into a [[QlikView App]] (with [[EMR]] [[integration]] planned for [[Q2]]) and currently scores [[patient]]s everyday, helping to mitigate [[readmission]]s and improve [[quality of care]], leading to healthier [[outcome]]s and [[cost saving]]s.
[[It]] is made out to be an [[all-pervasive fact]] of [[our live]]s - be it of [[housing]], [[food]], [[water]] or [[oil]] .
It is natural to look for distinct [[class]]es in such [[dataset]]s by [[clustering]] the [[data]] .
It is necessary for the purpose of [[accountability]] and [[reporting requirement]]s to [[group]] similar [[transaction]]s by like [[account]]s.
It is possible, for example, to define a [[database type]] for a [[complex document]], such as [[a journal article]], but the [[meaning of the text]], [[figure]] and [[table]] [[component]]s of that type are much [[less well-defined]] than a typical [[component]] of an [[employee record type]], such as the [[salary]] .
[[It]] is proposed that the regulatory [[volume]] [[increase]], after [[cell]] [[shrinkage]], is the key [[event]] resulting in release of [[inflammatory mediator]]s that cause [[airway]] [[smooth muscle]] to [[contract]] and the [[airway]]s of [[asthmatic subject]]s to narrow.
It is [[remotely executing]] and [[global]], [[always on]], and endlessly [[configurable]] .
It is [[self-configuring]], meaning it constantly [[reconfigure]]s itself on the fly, and increasingly it is also [[self-organizing]], [[self-architecting]], and [[self-healing]] .
[[It]] is the main [[method]] for [[deciding which algorithms are correct and efficient tools]] for [[automating]] [[statistical inference]] .
It is therefore important to develop and [[evaluate]] [[PEP methods]] under the [[assumption]] that only [[limited]] [[training examples]] can be available, and that the [[system]] can only have the [[personal email]] [[data]] of each [[user]] during the [[training]] and [[testing]] of the [[model]] for that [[user]] .
It is traditionally a [[challenge]] for [[home buyer]]s to [[understand]], [[compare]] and contrast the [[investment value]]s of [[real estate]]s.
It is used widely in the [[social]] and [[behavioral sciences]], as well as in [[political science]], [[economics]], [[organizational science]], and [[industrial engineering]] .
It is well known that [[connectivity analysis]] of [[linked documents]] provides significant [[information]] about the [[structure]] of the [[document space]] for [[unsupervised learning task]]s.
It is well known that [[users' behaviors (actions)]] in a [[social network]] are [[influenced]] by various [[factors]] such as [[personal interests]], [[social influence]], and [[global trends]] .
It is [[well researched]] with [[reference]]s, and well known [[scholar]]s and [[professional]]s are quoted to back up [[conclusion]]s, but none seemed hard and [[fast]] .
It is well studied that under certain [[assumption]]s, [[convex optimization]] using the [[trace norm]] and [[ℓ1-norm]] can be an [[effective computation surrogate]] of the [[difficult]] [[RPCA problem]] .
It [[iteratively updates]] [[message]]s between [[data point]]s until [[convergence]] .
It justifies granting to each [[citizen]] an [[unconditional basic income]] at the [[highest]] [[sustainable level]] consistent with two conditions: [[respect]] for [[everyone]]'s [[formal freedom]] and an appropriate level of [[resources target]] at the [[less able]] .
[[It]] leverages a novel [[distributed memory abstraction]] to provide a [[unified engine]] that can run [[SQL queri]]es and sophisticated [[analytics function]]s (e.g. [[iterative machine learning]]) at [[scale]], and [[efficiently]] [[recover]]s from [[failure]]s [[mid-query]] .
[[It]] makes use of [[structured English]] and follows the [[structure proposed]] in ([[Melli & McQuinn, 2008]]), where each [[concept record]] contains
[[It]] maps the [[nodes of a dataflow graph]] across many [[machine]]s in a [[cluster]], and within a [[machine]] across multiple [[computational device]]s, including [[multicore CPU]]s, [[general-purpose GPU]]s, and [[custom-designed ASIC]]s known as [[Tensor Processing Units (TPUs)]] .
It [[maximize]]s the [[conditional probability]] of observing the [[response]]s from both the [[quotation stage]] and the [[decision stage]], given the [[feature]]s and the [[co-cluster]]s.
[[It]] maximizes the [[total influence]] in the end of the [[process]] instead of [[myopically]] pursuing [[short term gain]] .
[[ITMC]] identifies a `[[unique]]' [[cluster]] whose [[distribution]] [[diverge]]s significantly from the entire [[dataset]] .
It naturally occurs in many [[setting]]s in [[web search]], [[web mining]] and [[web advertising]] .
[[It]] offers numerous [[research challenge]]s but promises [[insight]] useful to anyone interested in [[opinion analysis]] and [[social media analysis]] .
It performs [[iterative]], [[random-coordinate update]]s to [[maximize]] the [[dual objective]] .
It powers ongoing [[research project]]s, [[large-scale industrial application]]s, and [[startup prototype]]s in [[vision]], [[speech]], and [[multimedia]] .
It presents the general [[mathematical background]] to [[DPPs]] along with a range of [[modeling extension]]s, [[efficient algorithm]]s, and [[theoretical result]]s that aim to enable [[practical modeling]] and [[learning]] .
It provides an [[interpretable]] [[latent structure]] for [[users and item]]s, and can form [[recommendation]]s about both existing and [[newly published article]]s.
[[It]] provides a [[representational framework]] for the [[description of mining structured data]], and in addition provides [[taxonomies of dataset]]s, [[data mining task]]s, [[generalization]]s, [[data mining algorithm]]s and [[constraint]]s, based on the [[type of data]] .
It provides the [[capability]] for the [[user]] to view [[clustering behaviour]] from different [[perspectives]] and thus explore new [[hypotheses]] .
[[It]] refers to the [[phenomenon]] that [[data]] becomes sparser in [[high dimensional space]], [[adversely affecting algorithm]]s designed for [[low dimensional space]] .
It relies at its [[lowest level]] on [[Latent Dirichlet Allocation]], while making use of [[latent]] side [[feature]]s for [[cross-property integration]] .
[[It]]'s actually a very simple idea: [[Everyone in society]] receives a single [[basic income]] to provide for a [[comfortable living]] whether they [[choose to]] [[work]] or [[not]] .
[[Its author]]s give [[proof positive]] that [[government]] does not have to be a [[gigantic]] and [[inefficient bureaucracy]] .
It's been codified as [[myth]]s, [[proverb]]s, [[cliché]]s, [[epigram]]s, [[parable]]s; the [[skeleton]] of every great [[story]] .
Its [[computational complexity]] is [[sub-linear]] in number of [[instance]]s and number of [[variable]]s and [[subquadratic]] in number of [[class]]es.
It seeks to [[retrieve]] the [[structure]] of the investigation from the [[paper]] as [[generic]] [[high-level]] [[Core Scientific Concepts (CoreSC)]] .
its [[ego neighborhood]], using a [[label propagation algorithm]]; finally, the [[local communiti]]es are [[merged]] into a [[global collection]] .
It selects those [[instance]]s on which the [[base learner]]’s [[predictions]] are most [[confident]], and [[constructs]] a [[labeled training set]] from them.
[[It]] sets forth a [[theory]] to explain how some [[human]]s try to [[solve]] some [[simple formal problem]]s.
[[Its forward-backward algorithm]]s can be used to [[estimate / update]] [[the model parameter]]s, determine the [[predicted]], [[filtered]] and [[smoothed probabiliti]]es, [[evaluate goodness]] of an [[observation sequence fitting]] to the [[model]], and [[find the best state sequence]] of the underlying [[stochastic process]] .
It shows that the proposed [[coupled]] and [[adaptive HMMs]] [[outperform]] a standard [[HMM]] only modeling any [[single sequence]], or the [[HMM]] combining [[multiple]] [[single sequence]]s, without considering the [[coupling relationship]] .
[[It]]'s ideal for [[analysts]] new to [[Python]] and for [[Python programmer]]s new to [[scientific computing]] .
It signifies a [[synchronized]] [[value rise]] or [[fall]] of all [[variables]] within one [[set]] whenever all [[variables]] in the other [[set]] go jointly at the [[opposite trend]] .
It's insufficient to simply match a [[shopper]] with [[popular item]]s from the [[category in question]]; a successful [[shopping experience]] also identifies [[product]]s that match those [[aesthetic]]s.
[[Its membership]] was appointed by the [[Speaker of the House]], [[Lieutenant Governor]], [[Association County Commissioners of Georgia]], [[Georgia Municipal Association]], [[County Officer’s Association]], and [[Georgia Chamber of Commerce]] and included [[local government official]]s, [[state legislator]]s, and [[business leader]]s.
Its [[recommendation]]s contributed significantly to the [[content]] of [[this document]] .
It’s simple: [[profits]] have surged as a [[share of]] [[national income]], while [[wages]] and other [[labor compensation]] are down.
Its use in solving the [[local pragmatics problem]]s of [[reference]], [[compound nominal]]s, [[syntactic ambiguity]], and [[metonymy]] is described and illustrated.
It then [[trains]] a new [[classifier]] using the [[label]]s for all the [[document]]s, and [[iterate]]s to [[convergence]] .
It took about 1 day to [[train]] 231 [[classifier]]s with [[PC-Linux]] ([[Celeron]] [[500Mhz]], [[512MB]]).
[[It]] underpins much of [[modern economics]] and is widely used as an [[explanation]] for [[social organisation]], [[social conflict]] and the [[resource crunch]] confronting [[humanity's survival on the planet]] .
It uses [[convolutional neural network character recognizer]]s combined with [[global training technique]]s to provide [[record accuracy]] on [[business]] and [[personal cheque]]s.
It uses one [[truthful review]] as a [[template]], and replaces its [[sentence]]s with those from other [[review]]s in a [[repository]] .
It uses the [[topic model]] to [[resolve ambiguities]] of [[named entity]] [[classes]] by representing the [[classes]] as [[topics]] .
It utilizes [[hidden unit]]s to [[discover]] the [[latent topic]]s and can [[learn]] [[compact semantic representation]]s for [[document]]s which greatly facilitate [[document retrieval]], [[clustering]] and [[classification]] .
It was also similar to the [[percentage seen]] in [[prehistoric band]]s and [[tribe]]s, indicating that [[we]] were as [[lethally violent]] then as common [[mammalian evolutionary history]] would [[predict]] .
[[It]] was built to support the [[Rexplore system]] ([[Osborne et al, 2013]]) and to [[enhance semantically]] a number of [[analytics]] and [[data mining algorithm]]s.
[[It]] was only where [[transmission capacity]] was a [[scarce asset]], like in the [[access portion]] of [[radiotelephony]], that [[speech compression]] became a useful [[tool]] .
It will actually be within your power to experience a crowded, hot, slow, [[consumer-hell]] type situation as not only [[meaningful]], but [[sacred]], on [[fire]] with the same [[force that made the stars]]: [[love]], [[fellowship]], the [[mystical oneness]] of [[all thing]]s [[deep down]] .
It will also be significant in [[market]]s where [[product]]s are [[differentiated]], reducing the [[monopoly power]] enjoyed by [[seller]]s, and leading to [[lower price]]s and [[seller profit]]s.
[[It]] will be a valuable resource for [[researcher]]s and [[practitioner]]s in [[natural language processing]], [[computer science]], [[management science]]s, and the [[social science]]s.
It will cover the importance of [[anchoring]] in [[application]]s, [[minimization]] of [[design-to-testing time]], [[development]] with [[users-in-the-loop]], [[error tolerance]] of [[machine learning]], [[design]] for diverse [[user population]]s, and the [[necessity]] of [[open source software]] [[integration]] .
[[It]] works by successively improving its [[evaluation]]s of the [[quality of particular action]]s at particular [[state]]s.
It works in the [[Bayesian setting]], which [[facilitates modeling spamicity]] of [[author]]s as [[latent]] and allows us to exploit various [[observed behavioral]] footprints of [[reviewer]]s.
I use [[bold]] [[upper case letter]]s to represent [[matrice]]s <math>(\mathbf{X}, \mathbf{X}, \mathbf{X})</math>, and [[bold]] [[lower-case letter]]s to represent [[vector]]s (b).
I will describe several [[approach]]es, including an [[algorithm]] based on [[discrete optimization]], and an [[algorithm]] based on [[Bayesian analysis]] .
I will present examples of how [[natural language processing]] and [[distributed human computing]] are [[improving the lives of speaker]]s of all the [[world's language]]s, in areas including [[education]], [[disaster-response]], [[health]] and access to [[employment]] .
[[I]] will provide an [[overview]] of these [[challenge]]s and [[the strategi]]es we have adopted at [[LinkedIn]] to address those.
I will provide [[quantifiable improvement]]s in [[key]] [[metric]]s driven by [[Fatafat]] and interesting, [[unsolved problem]]s / [[challenge]]s that need to be addressed for [[fast]]er and wider adoption of [[OSS]] by these [[compani]]es.
I will show examples of [[interpretable model]]s for [[stroke prediction]] in [[medical patient]]s and [[prediction]] of [[violent crime]] in young people raised in [[out-of-home care]] .
[[Iyyer et al (2015)]] demonstrate that [[multilayer feed-forward network]]s can provide [[competitive result]]s on [[sentiment classification]] and [[factoid question answering]] .
Jaynes]] dispels the [[imaginary distinction]] between '[[probability theory]]' and '[[statistical inference]]', leaving a [[logical unity]] and [[simplicity]], which provides greater [[technical power]] and [[flexibility]] in [[application]]s.
[[Jeremy Howard]] will discuss his experience as a [[consultant]] at [[Sprint]] comparing [[R]] and [[random forest]]s to the [[existing process]], and will show the [[pros and cons]] of each [[approach]] .
[[Jimeng Sun]], [[Christos Faloutsos]], Spiros Papadimitriou, [[Philip S. Yu]]
[[Jim Hurford]] looks at the very varied aspects of this evolution, covering [[human prehistory]]; the [[relation]] between [[instinct]] and [[learning]]; [[biology]] and [[culture]]; [[trust]], [[altruism]], and [[cooperation]]; [[animal thought]]; [[human]] and [[non-human vocal anatomy]]; the [[meaning]]s and [[form]]s of the [[first words]]; and the growth of [[complex]] [[systems of grammar]] and [[pronunciation]] .
[[Job polarization]], [[technological change]], [[computerization]], [[job task]]s, [[skill demand]] .
Just as the [[elementary part]]s of the [[syntax]] are really the [[word form]]s (and not the [[word]]s), the elementary parts of [[morphology]] are really the [[allomorph]]s.
[[Karl Marx]] thought that the [[falling rate]] of [[profit]] would inevitably bring about the [[fall]] of the [[capitalist system]] .
[[KBC]] is a [[long-standing problem]] in [[industry]] and [[research]] that encompasses problems of [[data extraction]], [[cleaning]], and [[integration]] .
[[KBS]] are a specific [[class of]] [[computerized information system]] that supports [[business and organizational decision-making]] .
[[KB]]s are considered to be [[entities]] of the [[universe of discourse]] and are represented by [[frame]]s.
Keeping up with the [[ever-expanding flow]] of [[data]] and [[publication]]s is [[untenable]] and poses a [[fundamental bottleneck]] to [[scientific progress]] .
[[KERA]] extracts [[topic-representative term]]s from individual [[document]]s in a purely [[unsupervised fashion]] and is revealed to be significantly more [[effective]] than [[state-of-the-art]] [[method]]s.
[[Kernel-based regression]] represents an important [[family]] of [[learning technique]]s for solving challenging [[regression task]]s with [[non-linear pattern]]s.
[[Kernel]]s provide a [[flexible method]] for deriving new [[matrix factorization method]]s.
[[Kernel SVM]]s often reach [[state-of-the-art]] [[accuraci]]es, but suffer from the curse of [[kernelization]] due to [[linear model growth]] with [[data size]] on [[noisy data]] .
[[Kernel technique]]s have long been used in [[SVM]] to handle [[linearly inseparable problem]]s by [[transforming data]] to a [[high dimensional space]], but [[training]] and [[testing large data set]]s is often [[time consuming]] .
Key to [[our approach]] is the integration of [[lexicographic]] and [[encyclopedic knowledge]] from [[WordNet]] and [[Wikipedia]] .
[[Keywords Clustering]], [[Anomaly Detection]], [[Category Detection]], [[Mean Shift]]
Keywords: [[globalization]], [[labor-market adjustment]], [[local labor market]]s, [[inequality]]
[[keyword similarity]], [[keyword-based advertising]], [[online learning]] .
Keywords: [[Personalized Ranking]]; [[Adaptive Sampling]]; [[Pairwise Learning]]
Key words: [[Scholarly discourse]] - [[Scientific publishing]] – [[Ontologies]] – [[Knowledge-based system]]s – [[Argumentation]] – [[Visualisation]] – [[Eprint servers]] – [[Internet-digital-libraries]] .
[[Kinematic constraint]]s, [[Dubins car]], [[Reeds-Shepp car]], [[differential drive]]s, a [[car]] [[pulling]] [[trailer]]s, [[phase space]], [[rigid-body dynamic]]s, [[dynamics]] of a [[chain of bodi]]es, [[Newtonian mechanic]]s, [[Euler-Lagrange equation]], [[variational principle]]s, [[Hamilton's equation]]s, [[differential game]]s.
[[Kleinberg's (2002)]] [[burst-detection algorithm]] is adapted to identify emergent [[research-front concept]]s.
[[KnIT]] combines [[entity detection]] with [[neighbor-text feature analysis]] and with [[graph-based diffusion of information]] to [[identify]] potential new [[properties of entiti]]es that are strongly implied by [[existing relationship]]s.
[[KnowItAll]] [33] is a [[state-of-the-art]] [[Web extraction system]] that addresses the automation challenge by [[learning to label]] its own [[training examples]], and tackles issues pertaining to [[corpus]] [[heterogeneity]] by not relying on [[deep linguistic analysis]] or [[entity recognizers]] .
[[knowledge acquisition]], [[knowledge engineering tool]]s, [[decision-support system]]s
[[Knowledge acquisition]], [[Ontology learning]], [[Ontology]], [[Probabilistic topic models]] .
[[Knowledge appear]]s in different representations ranging from [[technical document]]s, [[construction plan]]s, [[sheet]]s, and [[experience]]s of [[human expert]]s, but also in the explicit form of [[rule]]s and [[model]]s.
[[Knowledge bases (KBs)]] encoded using [[RDF triple]]s deliver many benefits to [[application]]s and [[programmer]]s that access the [[KBs on the web]] via [[SPARQL endpoint]]s.
::[[Knowledge representation system]] based on [[DL]]s consists of two components - [[TBox]] and [[ABox]] .
[[Knowledge worker]]s, [[information appliance]]s, [[writing]], [[memory]] .
[[L1-Norm Max-Margin Markov Network]]s, [[Primal Sparsity]], [[Dual Sparsity]]
[[LAA define]]s a [[correlation factor]] that represents the [[connection]] between two [[document]]s, and considers the [[topic proportion]] of [[paired document]]s based on [[this factor]] .
[[Labeling]] [[text data]] is quite [[time-consuming]] but essential for [[automatic]] [[text classification]] .
[[Label sets]] of [[unseen examples]] are [[predicted]] [[recursively]] according to the [[label ordering]] given by the [[network]] .
[[Labor compensation]] relative to [[net value added]] ("[[net labor share]] ") may be more important in some [[setting]]s, however, because [[depreciation]] is not [[consume]]d.
[[Labor market]]s susceptible to [[computerization]] due to specialization in routine [[task-intensive activiti]]es experience significant [[occupational polarization]] within [[manufacturing]] and [[nonmanufacturing]] but no [[net employment]] [[decline]] .
[[Language use]] is a [[psychologically rich]], [[stable individual difference]] with [[well-established correlation]]s to [[personality]] .
[[Large enterprise]] [[IT (Information Technology) infrastructure component]]s generate [[large volume]]s of [[alert]]s and [[incident ticket]]s.
[[Large-scale]] [[astronomical surveys]], including the [[Sloan Digital Sky Survey]], and [[large simulations]] of the [[early universe]], such as the [[Millennium Simulation]], can contain [[millions]] of [[points]] and fill [[terabytes]] of [[storage]] .
[[Large-scale]] [[data center network]]s are [[complex]] --- comprising several thousand [[network device]]s and several hundred thousand [[link]]s --- and form the critical [[infrastructure upon]] which all [[higher-level service]]s depend on.
[[Large scale]] [[evaluation]]s on the [[click-through log]]s from a [[commercial search engine]] demonstrate that [[this approach]] can result in significant improvement in terms of [[click prediction]] [[accuracy]], for both the ads with rich [[historical data]] and those with [[rare one]] .
[[Large-scale information processing system]]s are able to [[extract]] [[massive collection]]s of [[interrelated fact]]s, but unfortunately transforming these [[candidate fact]]s into [[useful knowledge]] is a [[formidable challenge]] .
[[Large-scale]] [[logistic regression]] arises in many applications such as [[document classification]] and [[natural language processing]] .
[[large scale nonlinear optimization]], [[limited memory methods]], [[partitioned quasi-Newton method]], [[conjugate gradient method]]
[[Large-scale]] [[sensor deployments]] and an [[increased use]] of [[privacy-preserving transformations]] have led to an increasing interest in [[mining]] [[uncertain time series data]] .
[[Large text datasets]]; [[Information retrieval]]; [[Data mining]]; [[Text analytics]]; [[Automated semantic tagging]]
[[LaSA model]] is constructed to capture [[latent semantic association]] among [[words]] from the [[unlabeled corpus]] .
:: [[Lasso model]] fit with [[Lars]] using [[BIC]] or [[AIC]] for [[model selection]]
[[Lasso]] ([[Tibshirani, 1996]]), [[Information Gain]] ([[Cover and Thomas, 2012]]), [[Relief]] ([[Kira and Rendell, 1992a]]), [[MRMR]] ([[Peng et al., 2005]]), [[Fisher Score]] ([[Duda et al., 2012]]), [[Laplacian Score]] ([[He et al., 2005]]), and [[SPEC]] ([[Zhao and Liu, 2007]]) are some of the well known [[feature selection technique]]s.
[[LASTA]] generates over 50 distinct [[feature]]s derived from [[signal]]s such as [[user generated post]]s and [[profile]]s, [[user reaction]]s such as [[comment]]s and [[retweet]]s, [[user attribution]]s such as [[list]]s, [[tag]]s and [[endorsement]]s, as well as [[signal]]s based on [[social graph connection]]s.
[[Last]] but not the [[least]], there is considerable difference in [[users' behavior]] between [[Twitter]] and [[Sina Weibo]] .
Lastly, almost all [[previous work]] is based on [[single]], [[positive / negative categori]]es or [[scale]]s such as [[star rating]]s.
[[Lastly]], [[lack of semantic]] or [[discourse aspect]]s in [[concept retrieval]] could cause a [[limited capability]] of the tracker to deal with [[implicitly mentioned subject]]s.
Lastly, the proposed [[species-specific gene normalization module]], based on [[our previous work]] [3], is utilized to calculate the [[inference score]]s for [[candidate]] [[Entrez ID]]s from [[article]]s.
Last week, [[academic publisher]] [[Springer]] released [[SciDetect]], a [[freely available program]] to [[automatically detect]] [[automatically generated paper]]s.
Last, [[we]] examine the idea of wrapping the [[dropout technique]] in the [[state-of-art DML method]]s and [[observe]] that the [[dropout technique]] can significantly improve the [[performance]] of the original [[DML method]]s.
[[Latent Dirichlet allocation (LDA)]] is a widely used [[Bayesian topic model]] applying the [[Dirichlet distribution]] over the [[latent topic distribution]] of a [[document]] having multiple [[topic]]s.
[[Latent Dirichlet Allocation]]) to decompose [[free-text hospital note]]s into meaningful [[feature]]s, and the [[predictive power]] of these [[feature]]s for [[patient mortality]] .
[[Latent factor model]]s and [[decision tree based model]]s are widely used in [[task]]s of [[prediction]], [[ranking]] and [[recommendation]] .
[[Latent factor model]]s have the advantage of [[interpreting]] [[categorical feature]]s by a [[low-dimensional representation]], while such an [[interpretation]] does not naturally fit [[numerical feature]]s.
[[Latent semantic models]], such as [[LSA]], intend to [[map]] a [[query]] to its [[relevant document]]s at the [[semantic level]] where [[keyword-based matching]] often fails.
[[Latent topic analysis]] has emerged as one of the most [[effective method]]s for [[classifying]], [[clustering]] and [[retrieving textual data]] .
[[Lattice]]s are learned from a [[dataset of definitions]] from [[Wikipedia]] .
Launched in [[2012]], [[Robohub]] is dedicated to connecting the [[robotics community]] to [[the public]] .
; [[Law Enforcement]]: [[Outlier detection]] finds numerous [[application]]s to [[law enforcement]], especially in cases, where unusual [[pattern]]s can only be [[discovered]] over time through multiple [[action]]s of an [[entity]] .
[[LAWS]] might include, for example, [[armed]] [[quadcopter]]s that can search for and eliminate [[enemy combatant]]s in a [[city]], but do not include [[cruise missile]]s or [[remotely piloted drone]]s for which humans make all [[targeting decision]]s.
[[Lawyers]] found it very useful in [[patent]] [[rating]], specifically, in highlighting potentially [[valuable]] [[patents]] in a [[patent]] [[cluster]] .
[[L-BFGS]] [[estimates the curvature]] [[numerically]] from previous [[gradients]] and [[updates]] .
[[LCM]]: [[Labor Cost Management system]], an interface between [[HR]] / [[CMS]] and [[MMARS]] that allows [[agenci]]es to assign employee payroll expenses among different [[accounts and program]]s.
[[LDA]] also [[models]] a [[word distribution]] by using a [[multinomial distribution]] whose [[parameters]] follows the [[Dirichlet distribution]] .
[[LDA]] assumes that [[latent topics]], i.e., [[discrete latent variables]], are [[distributed]] according to a [[multinomial distribution]] whose [[parameters]] are generated from the [[Dirichlet distribution]] .
[[LDA]] is a three-level [[hierarchical Bayesian model]], in which each [[item]] of a [[collection]] is [[modeled]] as a [[finite mixture]] over an [[underlying]] [[set of topics]] .
[[Learning algorithm]]s are based on the [[voted perceptron]], [[pseudo-likelihood]] and [[inductive logic programming]] .
[[Learning algorithm]]s make use of [[conditional likelihood]], [[convex optimization]], and [[inductive logic programming]] .
[[Learning algorithm]]s that [[embed object]]s into [[Euclidean space]] have become the [[methods of choice]] for a wide range of [[problem]]s, ranging from [[recommendation]] and [[image search]] to [[playlist prediction]] and [[language modeling]] .
[[Learning]] for these [[targeting tasks]] is difficult since most [[training]] [[pages]] are [[multi-topic]] and need expensive [[human labeling]] at the [[sub-document level]] for [[accurate training]] .
[[Learning]] from [[data stream]]s is a [[research area]] of increasing importance.
[[Learning]] in [[non-stationary environment]]s is an increasingly important [[problem]] in a wide variety of [[real-world application]]s.
Learning [[MLN]] [[structure]] from a [[relational database]] involves learning the [[clauses]] and [[weights]] .
[[Learning]] of the [[information diffusion model]] is a fundamental [[problem]] in the study of [[information diffusion]] in [[social network]]s.
[[Learning ontologies]] requires the acquisition of relevant [[domain concepts]] and [[taxonomic]], as well as [[non-taxonomic]], [[relations]] .
[[Learning]] [[probabilistic graphical model]]s from [[high-dimensional dataset]] s is a [[computationally challenging task]] .
[[Learning]] [[temporal graph]] [[structure]]s from [[time series data]] reveals important [[dependency relationship]]s between current [[observation]]s and [[histories]] .
[[Learning]] the [[hierarchy]] of [[classifier]]s is often the most challenging [[component]] of such [[classification scheme]]s.
[[Learning]] the [[latent variable]]s of such [[process]], i.e., the [[influence strength]] along each [[link]], is a [[central question toward]]s understanding the [[structure]] and [[function]] of [[complex network]]s, [[modeling information cascade]]s, and [[developing application]]s such as [[viral marketing]] .
[[Learning to rank for Information Retrieval (IR)]] is a [[task]] to automatically [[construct a ranking model]] using [[training data]], such that the [[model]] can [[sort new objects]] according to their [[degrees of relevance]], [[preference]], or [[importance]] .
[[Learning to rank]] from [[relevance judgment]] is an [[active]] [[research area]] .
[[Learning to rank]] is an important [[area]] at the interface of [[machine learning]], [[information retrieval]] and [[Web search]] .
[[Learning to rank]] is useful for [[document retrieval]], [[collaborative filtering]], and many other [[application]]s.
[[Learning to rank]] is useful for many [[application]]s in [[information retrieval]], [[natural language processing]], and [[data mining]] .
[[Learning to store information]] over [[extended time interval]]s by [[recurrent backpropagation]] takes a [[very long time]], mostly because of [[insufficient]], [[decaying]] [[error backflow]] .
[[Least squares regression]] is widely used to [[understand]] and [[predict data behavior]] in many [[field]]s.
[[Lesson]]s for [[saving]]s, [[addiction]], and [[elsewhere]] are discussed.
[[Leveraging]] the [[conformity information]], [[Confluence]] can accurately [[predict actions of user]]s.
[[Leveraging]] the [[spatial constraint]], it conceptually organizes all the [[SCP]]s into a novel [[structure]] called the [[SCP search tree]], which facilitates the [[effective]] [[pruning]] of the [[search space]] to generate [[SCP]]s [[efficient]]ly.
[[Lexical cohesion]] refers to the [[reader-perceived]] [[unity of text]] achieved by the [[author]]’s usage of [[words]] with related [[meanings]] ([[Halliday and Hasan, 1976]]).
[[Lexical resource]]s are less structured resources such as [[thesauri]], [[lists of facts]], [[lexical relation instance]]s, [[lists of paraphrases]], and other [[flat lists of lexical objects]] .
[[Lexical resource]]s such as [[thesauri]] and [[ontologies]] form essential ingredients of many [[intelligent]] [[information access applications]] .
[[LIBLINEAR System]], [[Linear SVM Training System]], [[Logistic Regression Training System]] .
[[Li et al.]] [18] apply [[GAN]]s with the [[policy gradient method]] to [[dialogue generation]] .
Like [[AZ]], [[AZ-II]] follows the [[rhetorical structure]] of a [[scientific paper]] and the [[knowledge]] claims made by the [[authors]] .
Like [[Lasso]], it can [[efficiently]] [[learn a solution path]] based on a [[set]] of [[predefined parameter]]s and therefore provides strong support for [[model selection]] .
Like [[LSA]], it uses [[dimensionality reduction]] ([[SVD]]) to filter out [[noise]] in [[the system]] .
Like many other [[books on logic]], [[this one]] covers [[logical syntax]] and [[semantics]] and [[proof theory]] plus [[induction]] .
Like other similar [[organization]]s operating in [[dynamic environment]]s, the [[IBM Corporation]] strives to maintain such current and [[correct information]], specifically [[assessments of employees]] against [[job role]]s and [[skill set]]s from its [[expertise taxonomy]] .
Like [[SVM]]s, [[FM]]s are a general [[predictor working]] with any [[real valued feature vector]] .
Likewise, [[review selection]] (identifying a [[subset]] of ' helpful' or ' important' [[review]]s) leads to [[redundant]] or [[non-representative summari]]es.
Likewise, the same [[protein mutation]] can be encoded using [[single-letter]] or [[three-letter]] [[amino acid reference]]s.
[[Linear Classification]] has achieved [[complexity linear]] to the [[data size]] .
[[Line item]]s consist of an [[account]] number, [[language]] that outlines how the [[money]] may be spent, the amount, and the [[fund]] [[designation]] .
[[Line item]]: Unit by which the [[Legislature]] [[appropriates]] [[money]] .
[[Linguistic annotation]] and [[text analytics]] are active [[areas of research and development]], with [[academic conference]]s and [[industry event]]s such as the [[Linguistic Annotation Workshop]]s and the annual [[Text Analytics Summit]]s.
[[Linguistic annotations]] are [[notes]] about [[linguistic features]] of the [[annotated text]] that give [[information]] about the [[words]] and [[sentence]]s of the [[text]] .
[[Link creation behavior]]s can be summarized by [[classifying user]]s in different [[categori]]es with distinct [[structural]] and [[behavioral characteristic]]s.
[[LinkedIn]] [[dynamically deliver]]s [[update activiti]]es from a [[user's interpersonal network]] to more than 300 million [[member]]s in the [[personalized feed]] that [[rank]]s [[activiti]]es according their "[[relevance]] " to the [[user]] .
[[Linked or networked data]] are [[ubiquitous]] in many [[applications]] .
[[Link mining]] refers to [[data mining technique]]s that explicitly consider these [[links]] when building [[predictive]] or [[descriptive models]] of the [[linked data]] .
[[Link prediction]] is a key [[research direction]] within this [[area]] .
[[Link prediction]] is a [[technique]] used to [[predict]] the formation of [[tie]]s within a [[network]] .
[[Link prediction]] is the [[problem of predicting]] the [[presence]] or [[absence]] of [[edge]]s between [[node]]s of a [[graph]] .
[[Link prediction]], [[matrix factorization]], [[side information]], [[ranking loss]] .
[[Link Prediction]], [[Network]]s, [[Machine Learning]], [[Class Imbalance]]
[[Link prediction system]]s have been largely adopted to recommend new [[friend]]s in [[online social network]]s using [[data]] about [[social interaction]]s.
"[[Links]]," or more generically [[relationships]], among [[data instance]]s are ubiquitous.
[[Little work]] exists studying [[trust evolution]] in an [[online world]] .
[[Local aid]]: [[Moni]]es [[appropriated]] to be [[distributed to citi]]es and [[town]]s.
[[Local government]]s should supplement these [[classification]]s as necessary to provide information for [[policy]] and [[management purpose]]s.
[[Locality-sensitive hashing (LSH)]] is a [[basic]] [[primitive]] in several [[large-scale]] [[data processing]] [[application]]s, including [[nearest-neighbor search]], [[de-duplication]], [[clustering]], etc. [[In this paper we]] propose a new and simple [[method]] to [[speed]] up the [[widely-used]] [[Euclidean realization]] of [[LSH]] .
[[Location]], [[Location Entity Mention]], [[LocationOf Relation]], [[Entity Type]]
[[Location privacy protection method]]s based on [[k-anonymity]] and [[l-diversity]] have been proposed to provide [[anonymized use]] of [[LBS]] .
[[Locke]] argued that each [[person]] creates his own [[property]] by adding his [[labor]] to the [[raw material of nature]], transforming it into [[things of value]] .
[[Locke]] argued that [[right]]s were [[natural]] and [[inhered]] in [[human beings qua human beings]]; [[government]]s existed only to protect these [[right]]s and could be overturned if they violated them.
[[Locke]] then used [[his]] [[theory of the natural right to private property]] to tear apart the [[feudal property]] [[regime]] based on [[proprietary obligation]]s on [[the commons]] .
[[Logistic Regression]] is a well-known [[classification method]] that has been used widely in many [[applications]] of [[data mining]], [[machine learning]], [[computer vision]], and [[bioinformatics]] .
[[Logistic regression]] is a [[workhorse]] of [[statistics]] and is closely related to [[methods used in Machine Learning]], including the [[Perceptron]] and the [[Support Vector Machine]] .
[[logistic regression]], [[newton method]], [[trust region]], [[conjugate gradient]], [[support vector machines]]
[[Logistic Regression]], [[Sparse Learning]], [[L1-ball Constraint]], [[Nesterov’s method]], [[Adaptive Line Search]]
[[Log-likelihood analyse]]s and [[user studi]]es on [[large dataset]]s show that [[the proposed model]] [[outperform]]s several [[state-of-the-art baseline]]s in [[video tagging quality]] .
[[Longitudinal analysis]] is important in many [[discipline]]s, such as [[the study]] of [[behavioral transition]]s in [[social science]] .
[[Longitudinal data]]; [[Newton- — Raphson]]; [[Nonlinear least squares]]; [[Nonlinear models]]; [[Random effects]] .
[[LSTM]] is local in [[space]] and [[time]]; its [[computational complexity]] per [[time step]] and weight is O.
[[LUDIA]] is more robust to [[nonlinear estimate]]s and [[random effect]]s than other [[reconstruction algorithm]]s.
[[Machine learning]] and [[statistical modeling]] [[approach]]es that can obtain insights by [[continuous]]ly [[process]]ing [[large amount]]s of [[data emitted]] at very [[high frequency]] by these [[application]]s have emerged as the [[method of choice]] .
[[Machine learning approach]]es to [[this task]] require either [[manual annotation]] or, in the case of [[distant supervision]], existing [[structured source]]s of the same [[schema]] .
[[Machine learning]] is the [[automation]] of [[discovery]] - the [[scientific method]] on steroids - that enables [[intelligent robot]]s and [[computer]]s to [[program themselve]]s.
[[Machine learning method]]s extract value from [[vast data set]]s [[quick]]ly and with [[modest resource]]s.
[[Machine learning]] provides these, [[developing method]]s that can [[automatically detect pattern]]s in [[data]] and then use the [[uncovered pattern]]s to [[predict future data]] .
[[machine learning]] - [[summarization]] - [[indexing]] - [[keywords]] - [[keyphrase extraction]]
[[Machine learning systems]], such as [[Kylin]], use these [[infoboxes]] as [[training data]], accurately [[extracting]] even more [[semantic knowledge]] from [[natural language text]] .
[[Machine Learning technique]]s have been applied to [[reduce]] the [[cost]] of [[Information Extraction system]] [[adaptation]] .
[[Machine learning]] typically works with a [[data matrix]], where each [[row]] [[represents an object]] characterized by a [[feature vector of attributes]] (which might be [[numeric]] or [[categorical]]), and where the main [[task]]s are to [[learn a mapping]] from this [[feature vector]] to an [[output prediction]] of some form, or to perform [[unsupervised learning]] like [[clustering]] or [[factor analysis]] .
[[Machine]]s and [[people]] have [[complementary skill]]s in [[Knowledge Discovery]] .
[[MAHR]] implements the idea as a [[boosting approach]] with a <i>[[hypothesis reuse]]</i> [[mechanism]] .
Making [[complex topic]]s [[accessible]] and [[relevant]], [[this book]] lays out the [[purpose]]s and [[processes of budgeting]] in a [[democracy]] and identifies [[up-to-date problem]]s and [[directions of change]] .
[[Malicious Web sites]] are a [[cornerstone]] of [[Internet criminal activities]] .
[[Manager]]s can release the [[real]] [[power]] of [[computer]]s by challenging centuries-old notions about [[work]] .
[[Managing and mining]] such [[large-scale]] [[uncertain graph data]] is of both [[theoretical]] and [[practical significance]] .
[[Manually-curated gold standard annotation]]s are a [[prerequisite]] for the [[evaluation]] and [[training]] of [[state-of-the-art tool]]s for most [[Natural Language Processing (NLP) task]]s.
[[Manually]] finding such [[mappings]] is tedious, [[error-prone]], and clearly not possible at the [[Web scale]] .
Many [[algorithm]]s in [[speech]] and [[language processing]] can be viewed as instances of ''[[dynamic programming]]'' ([[DP]]) (Bellman, 1957).
Many [[application]]s (e.g., [[anomaly detection]]) concern [[sparse signal]]s.
Many [[application]]s, e.g., [[Web service composition]], [[complex system design]], [[team formation]], etc., rely on [[method]]s for [[identifying collections of objects or entities]] satisfying some [[functional requirement]] .
Many [[applications]] in [[surveillance]], [[monitoring]], [[scientific discovery]], and [[data cleaning]] require the [[identification]] of [[anomalies]] .
Many chapters in [[this book]] illustrate that applying a [[statistical method]] such as [[Latent Semantic Analysis]] ([[LSA]]; [[Landauer & Dumais, 1997]]; [[Landauer, Foltz, & Laham, 1998]]) to [[large databases]] can yield insight into [[human cognition]] .
Many [[complex disease]]s have [[heterogeneous phenotype]]s and are products of a variety of [[genetic]] and [[environmental factor]]s acting in [[concert]] .
Many connections to other well-known [[technique]]s from [[machine learning]] and [[statistics]] are discussed, including [[support-vector machine]]s, [[neural network]]s, splines, [[regularization network]]s, [[relevance vector machine]]s and others.
Many conventional [[anomaly detection approach]]es [[detect]] [[anomali]]es based on the [[distance]] between [[patterns]], and often provide only limited intuition about the [[generative process]] of the [[anomali]]es.
Many current [[NLP system]]s and techniques treat [[word]]s as [[atomic unit]]s - there is no notion of [[similarity]] between [[word]]s, as these are [[represented as]] indices in a [[vocabulary]] .
Many [[data]] are [[modeled]] as [[tensor]]s, or [[multi dimensional array]]s.
Many [[data set]]s contain [[rich information]] about [[object]]s, as well as [[pairwise relation]]s between them.
Many [[DML algorithm]]s suffer from the [[over-fitting problem]] because of a [[large number]] of [[parameter]]s to be determined in [[DML]] .
Many examples are sketched, including [[missing value situation]]s, [[application]]s to [[grouped]], [[censored]] or [[truncated data]], [[finite mixture models]], [[variance component estimation]], [[hyperparameter estimation]], [[iteratively reweighted least squares]] and [[factor analysis]] .
Many [[external factor]]s may degrade [[classification performance]] including changes in [[data distribution]], [[noise]] or [[bias]] in the [[source data]], and the [[evolution]] of [[the system]] itself.
Many [[graph mining operations]] ([[PageRank]], [[spectral clustering]], [[diameter estimation]], [[connected components]] etc.) are essentially a repeated [[matrix-vector multiplication]] .
Many important [[problem area]]s are both [[richly structured]] and [[large scale]], from [[social]] and [[biological network]]s, to [[knowledge graph]]s and [[the Web]], to [[image]]s, [[video]], and [[natural language]] .
Many important [[retrieval]] and [[mining tasks]] rely on [[ranking the data item]]s based on their [[centrality]] or [[prestige]] in the [[network]] .
Many interesting [[semantic relationship]]s, such as [[advisor-advisee between author]]s, are hidden in the [[publication record]]s; moreover, the [[research topic]]s of [[author]]s, [[venue]]s and [[term]]s are also hidden or unorganized, preventing insightful [[organization]] of the [[entiti]]es.
Many [[internet compani]]es, such as [[Yahoo]], [[Facebook]], [[Google]] and [[Twitter]], rely on [[content recommendation system]]s to deliver the most [[relevant content item]]s to [[individual user]]s through [[personalization]] .
Many kinds of [[data regarding public health]] are often [[skewed]], usually to the [[right]], and [[lognormal distribution]]s can often be applied to [[such skewed data]], for instance, [[surgical procedure time]]s, [[blood pressure]], and assessment of [[toxic compound]]s in [[environmental analysis]] .
Many [[methods]] [[implicitly]] [[assume]] [[Gaussian]] or [[uniformly distributed]] [[data]], and/or their [[result]] is difficult to [[interpret]] .
Many modern applications of [[description logics (DLs)]] require answering [[queri]]es over [[large data quantiti]]es, [[structured according]] to relatively simple [[ontologi]]es.
Many [[modern]] [[unsupervised]] or [[semi-supervised machine learning algorithm]]s rely on [[Bayesian probabilistic model]]s.
Many natural notions of "[[representativeness]] " satisfy [[submodularity]], an [[intuitive notion]] of [[diminishing return]]s.
Many of us spontaneously anticipate how [[friend]]s and colleagues will [[evaluate]] our [[choice]]s; the [[quality]] and [[content]] of these [[anticipated judgment]]s therefore matters.
Many [[online experiment]]s exhibit dependence between [[user]]s and [[item]]s.
:(m) Any other [[agency]] or [[instrumentality]] of a multi-, [[regional]], or [[intra-state]] or [[local government]] .
Many [[pattern recognition]] and [[machine learning approach]]es employ a [[distance metric]] on [[pattern]]s, or a [[generality relation]] to [[partially order]] the [[pattern]]s.
Many [[people]] share their [[activiti]]es with others through [[online communiti]]es.
Many practical [[data mining system]]s such as those for [[fraud detection]] and [[surveillance]] deal with building [[classifier]]s that are not [[autonomous]] but part of a larger [[interactive system]] with an [[expert]] in the [[loop]] .
Many [[real application]]s in various [[field]]s require [[efficient]] and [[effective management]] of [[large-scale]] [[graph structured data]] .
Many [[real-world domains]] are ''[[relational]]'' in nature, consisting of an [[set of objects]] [[related]] to each other in [[complex ways]] .
Many [[real-world graph]]s have [[complex]] [[label]]s on the [[node]]s and [[edge]]s.
Many [[recommendation]] and [[retrieval tasks]] can be [[represented as]] [[proximity queries]] on a [[labeled directed graph]], with [[typed nodes]] representing [[documents]], [[terms]], and [[metadata]], and [[labeled edges]] representing the [[relationship]]s between them.
Many [[resource]]s are invested in the [[task]] of [[fraud detection]] due to the [[high cost]] of [[fraud]], and being able to [[automatically detect]] potential [[fraud]] quickly and precisely allows [[human]] [[investigator]]s to work more efficiently.
Many [[rural communiti]]es lack [[stable employment]], [[opportunities for mobility]], [[investment in the community]], and [[diversity in the economy]] and other [[social institution]]s.
Many [[scalable]] [[data mining task]]s rely on [[active learning]] to provide the most useful [[accurately]] [[labeled instance]]s.
Many [[semantic theori]]es consider the [[meanings of words]] as relatively [[stable]] and [[independent]] of the [[communicative context]] .
Many [[social networks]] can be characterized by a [[sequence]] of [[dyadic interactions]] between [[individuals]] .
Many [[social Web sites]] allow [[users]] to [[annotate the content with descriptive metadata]], such as [[tags]], and more recently to [[organize]] [[content]] [[hierarchically]] .
Many [[statistical analyses]] distinguish between [[<i>response</i> variable]]s and [[<i>explanatory</i> variable]]s.
Many systems take the form of [[networks]], [[sets]] of [[nodes or vertice]]s joined together in [[pairs]] by [[links or edges]] (1).
Many [[web-facing compani]]es use [[online controlled experiment]] s to guide [[product development]] and [[prioritize idea]]s, including [[Amazon]] [1], [[eBay]], [[Etsy]] [2], [[Facebook]], [[Google]] [3], [[Groupon]], Intuit [4], [[LinkedIn]], [[Microsoft]] [5], [[Netflix]] [6], [[Shop Direct]] [ 7], [[StumbleUpon]] [8], [[Yahoo]], and [[Zynga]] [9].
[[Mapping]] out the [[challenge]]s and [[strategi]]es for the widespread adoption of [[service computing]] .
[[Mapping]] these [[phenotype]]s to [[pairs of genetic loci]] is hindered by the [[huge number]] of [[candidate]]s leading to enormous [[computational]] and [[statistical problem]]s.
[[Marble]] decomposes the [[observed tensor]] into two terms, a [[bias tensor]] and an [[interaction tensor]] .
( [[Marginal cost]] is the [[cost]] of [[producing]] additional [[unit]]s of a [[good or service]], if [[fixed cost]]s are not counted.
[[Marginal probabiliti]]es are entered as [[soft evidence]] in the [[network]] and through [[probabilistic inference]] become [[consistent]] with the [[discovered knowledge]] .
[[Marketer]]s often rely on a [[set of descriptive segment]]s, or [[qualitative subset]]s of the [[population]], to specify the [[audience]]s of [[targeted advertising campaign]]s.
[[Market]]s ([[electronic]] or otherwise) have three main [[function]]s, summarized in Table 1: [[matching]] [[buyer]]s and [[seller]]s; [[facilitating]] the [[exchange]] of [[information]], [[good]]s, [[service]]s and [[payment]]s associated with [[market transaction]]s; and providing an [[institutional infrastructure]], such as a [[legal]] and [[regulatory framework]], that enables the efficient functioning of the market.
[[Market]]s play a central role in the [[economy]], facilitating the [[exchange]] of [[information]], [[good]]s, [[service]]s, and [[payment]]s.
[[Markov chain]] in the space of [[possible variable assignment]]s (in this case, [[hidden state sequence]]s) such that the [[stationary distribution]] of the [[Markov chain]] is the [[joint distribution]] over the [[variable]]s.
[[Markov logic]] combines [[first-order logic]] with [[Markov networks]] .
[[Markov logic]] has been successfully applied to [[problem]]s in [[information extraction]] and [[integration]], [[natural language processing]], [[robot mapping]], [[social networks]], [[computational biology]], and others, and is the basis of the open-source [[Alchemy system]] .
[[Markov logic]] is a powerful new [[language]] that accomplishes this by attaching [[weights]] to [[first-order formulas]] and treating them as [[templates]] for [[features]] of [[Markov random fields]] .
[[Markov logic networks (MLNs)]] combine [[logic]] and [[probability]] by attaching [[weights]] to [[first-order clauses]], and viewing [[these]] as [[templates for features]] of [[Markov network]]s.
[[Markov logic]], [[statistical relational learning]], [[machine learning]], [[graphical models]], [[firstorder logic]], [[probabilistic logic,Markov networks,Markov random fields]], [[inductive logic programming]], [[satisfiability]], [[Markov chain Monte Carlo]], [[belief propagation]], [[collective classification]], [[link prediction]], [[link-based clustering]], [[entity resolution]], [[information extraction]], [[social network analysis]], [[natural language processing]], [[robot mapping]], [[computational biology]]
[[Marky]] grants solid management of [[inter-]] and [[intra-annotator work]] .
[[Marky]] is a novel [[environment]] for managing [[multi-user]] and [[iterative]] [[document annotation project]]s.
[[Marky]] is [[freely available]] for [[non-commercial use]] at http://sing.ei.uvigo.es/marky
[[MASCOT]] achieves both [[accuracy]] and [[memory-efficiency]] of the two [[algorithm]]s by an [[unconditional triangle counting]] for a new [[edge]], regardless of whether [[it]] is [[sampled]] or not.
[[MASCOT-C]] is based on [[constant]] [[edge sampling]], and [[MASCOT-A]] improves its [[accuracy]] by utilizing more [[memory space]]s.
[[Massive Online Open Course]]s have the potential to [[revolutionize]] [[higher education]] with their wide [[outreach]] and [[accessibility]], but they require [[instructor]]s to come up with [[scalable alternate]]s to traditional [[student evaluation]] .
[[Matching entiti]]es from different [[information source]]s is a very important [[problem]] in [[data analysis]] and [[data integration]] .
[[<math>\cal{Q}</math>-learning]] ([[Watkins, 1989]]) is a simple way for [[agent]]s to [[learn how to act optimally]] in [[controlled Markovian domain]]s.
[[Matrice]]s are ubiquitous in [[computer science]], [[statistics]], and applied [[mathematic]]s.
[[Matrix approximation]] is a common tool in [[recommendation system]]s, [[text mining]], and [[computer vision]] .
[[Matrix factorization]] is one of the [[fundamental technique]]s for analyzing [[latent relationship]] between two [[entiti]]es.
[[MaxEnt classifier]]s ([[Ratnaparkhi 1996]]; [[Ratnaparkhi 1998]]) have been applied to various [[NLP application]]s.
[[MDP]]s differ from [[general stochastic control problem]]s in that the [[action]]s are taken at [[discrete time point]]s, rather than [[continuously]] .
Meanwhile, a [[Cloud]] [[aggregate]]s and [[mine]]s the information from [[these taxis]] and other [[source]]s from the [[Internet]], like [[Web map]]s and [[weather forecast]] .
Meanwhile, [[Bayesian linear classifier]]s with [[weighted maximum likelihood]] are [[optimized online]] to [[estimate parameter]]s.
Meanwhile, [[employee]]s in [[compani]]es are usually organized into different [[hierarchi]]es according to the relative [[rank]]s of their [[position]]s.
Meanwhile, [[model-based approaches]] have difficulty in identifying a small, [[clustered]] [[set]] of [[anomali]]es.
Meanwhile, [[the proposed model]] takes into account the <i>[[non-i.i.d]]</i> nature of the [[data]] by [[estimating]] the [[within-individual correlation]]s.
[[Meanwhile]], [[user]]s nowadays are usually involved in multiple [[online social network]]s simultaneously to enjoy specific [[service]]s provided by different [[network]]s.
[[Measure]]s developed for [[static scenario]]s, namely [[structural measure]]s and [[ground-truth-based measure]]s, cannot correctly reflect [[error]]s attributable to [[emerging]], [[splitting]], or [[moving cluster]]s.
[[Mechanized Classifier]], [[Classification Function]], [[Human Classifier]] .
[[Medical coding or classification]] is the process of [[transforming information]] contained in [[patient medical records]] into standard [[predefined]] [[medical codes]] .
; [[Medical Diagnosis]]: In many [[medical applications the data]] is collected from a variety of devices such as [[MRI scan]]s, [[PET scan]]s or [[ECG]] [[time-seri]]es.
[[Meme-tracking]], [[Blog]]s, [[News Media]], [[News Cycle]], [[Information Cascade]]s, [[Information Diffusion]], [[Social Network]]s
[[MEMMs]] are able to model more [[complex]] [[transition]] and [[emission]] [[probability distribution]]s and take into account various [[text feature]]s.
[[Memory-based approach]]es that perform [[vector operation]]s in the original [[high dimensional space]] [[over-predict]] [[popular app]]s because they fail to capture the [[neighborhood of less popular app]]s.
[[Memory leaks]], [[heap profiling]], [[graph mining]], [[graph grammars]], [[dominator tree]] .
[[Merchants]] selling [[products]] on the [[Web]] often ask their [[customers]] to share their [[opinions]] and hands-on experiences on [[products]] they have [[purchased]] .
[[Metadata]] can be expressed in a wide variety of [[vocabularies]] and [[languages]], and can be [[created]] and [[maintained]] with a variety of [[tools]] .
[[Metadata]] is used to describe [[documents]] and [[applications]], improving [[information seeking]] and [[retrieval]] and its [[understanding]] and use.
[[MetaFac]], [[Metagraph Factorization]], [[Relational Hypergraph]], [[Nonnegative Tensor Factorization]], [[Community Discovery]], [[Dynamic Social Network Analysis]] .
[[Meta path]]s are good [[mechanism]]s to improve the [[quality]] of [[graph analysis]] on [[heterogeneous information network]]s.
[[Methodologically]], to emphasize the [[difference]]s and [[similariti]]es of most existing [[feature selection algorithms for generic data]], we generally categorize them into [[four group]]s: similarity based, [[information theoretical]] based, [[sparse learning]] based and [[statistical based method]]s.
[[Methodologically]], we build [[topic model]]s that are trained to [[automatically discover topics]] from [[product review]]s that are successful at [[predicting]] and [[explaining]] such [[relationship]]s.
[[Method]]s that [[generalize]] our [[procedure to estimate factors]] in an [[online fashion]] for [[dynamic applications]] are also considered.
[[MetricForensics]] combines a [[multi-level "drill down" approach]], a [[collection]] of [[user-selected]] [[graph metrics]], and a [[collection]] of [[analysis technique]]s.
[[Metric learning]] makes it plausible to [[learn]] [[semantically meaningful]] [[distance]]s for [[complex distributions of data]] using [[label]] or [[pairwise constraint information]] .
[[Metric learning]], [[parameter learning]], [[spatio-temporal data mining]], [[mining multi-dimensional data sequence]]s, [[similarity search]], [[ensemble method]], [[dimensionality reduction]], [[embedding method]], [[atmospheric event]]s, [[tropical cyclone]]s.
[[Metric learning]], [[semi-supervised sparse metric learning]], [[sparse inverse covariance estimation]], [[alternating linearization]] .
[[metric learning]], [[similarity learning]], [[Mahalanobis distance]], [[edit distance]], [[structured data]], [[learning theory]] .
[[Metric]]s like [[disk activity]] and [[network traffic]] are widespread [[source]]s of [[diagnosis]] and [[monitoring information]] in [[datacenter]]s and [[network]]s.
[[Metric space]]s, [[measure]], [[random sampling]], [[low-discrepancy sampling]], [[low-dispersion sampling]], [[grid]]s, [[lattice]]s, [[collision detection]], [[Rapidly-exploring Random Trees (RRTs)]], [[Probabilistic Roadmaps (PRMs)]], [[randomized potential field]]s.
[[Metric]]s used in past [[GN task]]s such as [[Precision]], [[Recall]], and [[F-measure]] do not take [[ranking]] into consideration.
[[MF model]]s decompose the [[observed user-item interaction matrix]] into [[user]] and [[item latent factor]]s.
[[Microblogging platform]]s such as [[Twitter]] have been recently frequently used for [[detecting]] [[real-time]] [[event]]s.
[[Micro-blogging service]]s, such as [[Twitter]], and [[location-based]] [[social network application]]s have generated [[short text message]]s associated with [[geographic information]], [[posting time]], and [[user id]]s.
[[Microblogging service]]s, such as [[Twitter]], have become popular [[channel]]s for [[people]] to express their [[opinion]]s towards a broad range of [[topic]]s.
[[Microblog]] s have become an important [[source of information]] for the purpose of [[marketing]], [[intelligence]], and [[reputation management]] .
[[Micropublication]]s, because they [[model]] [[evidence]] and allow [[qualified]], nuanced [[assertion]]s, can play essential roles in the [[scientific communications ecosystem]] in [[place]]s where simpler, [[formalized]] and purely [[statement-based model]]s, such as the [[nanopublications model]], will not be sufficient.
[[Micropublication]]s may be [[formalized]] and [[serialized]] in multiple ways, including in [[RDF]] .
[[Micropublication]]s support [[natural language statement]]s; [[data]]; [[method]]s and [[materials specification]]s; [[discussion]] and [[commentary]]; [[challenge]] and [[disagreement]]; as well as allowing many kinds of [[statement formalization]] .
[[Migrating]] [[operating system instance]]s across distinct [[physical host]]s is a useful [[tool]] for [[administrator]]s of [[data centers and clusters]]: It allows a [[clean separation]] between [[hardware]] and [[software]], and facilitates [[fault management]], [[load balancing]], and [[low-level system maintenance]] .
[[Migration Motif]], [[Financial Data Mining]], [[Spatial-temporal Pattern Mining]], [[Trajectory Mining]], [[Risk Factor]]
[[Millennials]], defined as those born between [[1985]] and [[1996]], entered the [[workforce]] during [[the greatest economic downturn in almost a century]], and faced enormous [[financial setback]]s as a result, according to a new [[analysis released]] today by [[New York City]] [[Comptroller]] [[Scott M. Stringer]] .
Millions of [[American]]s [[work]] [[full time]], [[year round]], for [[poverty-level wage]]s.
Millions of [[people]] use [[social network]]s everyday to talk about a variety of [[subject]]s, [[publish opinion]]s and [[share information]] .
[[Mincer (1966)]], [[Pencavel (1986)]], and [[Heckman and Killingsworth (1986)]] .
[[MindNet]] represents a general [[methodology]] for [[acquiring]], [[structuring]], [[accessing]], and [[exploiting]] [[semantic information]] from [[natural language text]] .
[[MineFleet]] analyzes [[high throughput data stream]]s onboard the [[vehicle]], generates the [[analytics]], sends those to the [[remote server]] over the [[wide-area wireless networks]] and offers them to the [[fleet managers]] using [[stand-alone]] and [[web-based user-interface]] .
[[Minimal]] enhancement to the [[ontology]] was performed once the [[annotation]] of the [[corpus]] was performed.
[[Mining cluster evolution]] from [[multiple]] [[correlated]] [[time-varying text corpora]] is important in [[exploratory text analytics]] .
[[Mining dense subgraph]]s from a [[large graph]] is a fundamental [[graph mining task]] and can be widely applied in a variety of [[application domain]]s such as [[network science]], [[biology]], [[graph database]], [[web mining]], [[graph compression]], and [[micro-blogging system]]s.
[[Mining]] [[discrete patterns]] in [[binary data]] is important for [[subsampling]], [[compression]], and [[clustering]] .
[[Mining discriminative subgraph]]s from [[global-state network]]s allows us to [[identify]] the [[influential sub-network]]s that have [[maximum impact]] on the [[global state]] and unearth the [[complex relationship]]s between the [[local entiti]]es of a [[network]] and their [[collective behavior]] .
[[Mining entity structures hidden]] in [[social media]] will help [[reorganize scattered information]] from [[hundreds of millions]] of [[individual]]s and improve the [[social network service]]s.
[[Mining frequent trees]] is very useful in [[domain]]s like [[bioinformatics]], [[web mining]], [[mining semi-structured data]], and so on.
[[Mining high utility itemsets]] from a [[transactional database]] refers to the [[discovery of itemsets]] with [[high utility]] like [[profits]] .
[[Mining high utility itemset]]s from [[database]]s is an [[emerging topic]] in [[data mining]], which refers to the [[discovery of itemset]]s with utilities higher than a [[user-specified]] [[minimum utility threshold]] <i>[[min_util]]</i>.
[[Mining]] only exact [[pattern]]s yields limited [[insight]]s, since it may be hard to [[find exact match]]es.
[[Mining]] such [[news data]] will enable us to extract multiple types of entities like [[people]], [[location]]s, [[organization]]s, and [[event]]s, for [[effective]] [[news understanding]] and [[analysis]] .
[[Mining time series data]] has been a very active [[research area]] in the past [[decade]], exactly because of its [[prevalence]] in many [[high-impact application]]s, ranging from [[environmental monitoring]], [[intelligent transportation system]]s, [[computer network forensics]], to [[smart building]]s and many more.
Misunderstanding the [[feature]]s and [[guarantee]]s of a [[database system]] can cause, at best, [[user]] consternation due to slowness or [[unavailability]] .
[[MIT]] [[robotics researcher]] [[Rodney Brooks]]'s [[classic 1986 paper]] on the '[[subsumption architecture]]', a [[bio-inspired]] way to [[program robots]] to [[react]] to their [[environment]], gathered nearly 10,000 citations in 30 years ([[R. Brooks]] IEEE J.
[[Mixed land use]] refers to the [[effort]] of putting [[residential]], [[commercial]] and [[recreational]] uses in [[close proximity]] to one another.
[[Mixture model]]s have been widely [[studied]] and applied to various [[application area]]s, including [[density estimation]], [[fraud]] / [[failure detection]], [[image segmentation]], etc. [[Previous research]] has been almost exclusively focused on [[mixture model]]s having [[component]]s of a single [[type]] (e.g., a [[Gaussian mixture model]].) However, recent growing needs for [[complicated]] [[data modeling]] necessitate the use of more [[flexible mixture model]]s (e.g., a [[mixture]] of a [[lognormal distribution]] for [[medical cost]]s and a [[Gaussian distribution]] for [[blood pressure]], for [[medical analytic]]s.) [[Our]] key ideas include: 1) separating [[marginal distribution]]s and their [[dependenci]]es using [[copula]]s and 2) [[online extension]] of a recently-developed "[[expectation minimization of description length]]", which enable us to efficiently [[learn]] [[type]]s of both [[marginal distribution]]s and [[copula]]s as well as their [[parameter]]s.
[[MLBE]] regards the [[binary latent factor]]s as [[hash code]]s in a common [[Hamming space]] .
[[MLGF-MF]] is designed with [[asynchronous I/O]] [[permeated]] in the [[algorithm]] such that [[CPU]] keeps [[executing]] without waiting for [[I/O]] to complete.
[[MLGF-MF]] uses [[Multi-Level Grid File (MLGF)]] for [[partitioning the matrix]] and [[minimizes the cost]] for [[scheduling parallel SGD update]]s on the [[partitioned region]]s by exploiting [[partial match queries processing]] .
[[MLN]]s allow us to model a [[wide range]] of rich [[structural feature]]s like [[proximity]], [[precedence]], [[alignment]], and [[contiguity]], using [[first-order clause]]s.
:: [[MLPClassifier]] supports [[multi-class classification]] by applying [[Softmax]] as the [[output function]] .
::* [[MLP]] requires [[tuning]] a number of [[hyperparameter]]s such as the number of [[hidden neuron]]s, [[layer]]s, and [[iteration]]s.
[[MLRF]] and [[LPSR]] [[learn a hierarchy]] to deal with the [[large number]] of [[labels]] but optimize [[task independent measure]]s, such as the [[Gini index]] or [[clustering error]], in order to [[learn the hierarchy]] .
[[MMARS]]: [[Massachusetts Management Accounting and Reporting System]], the [[official accounting system]] of the [[Commonwealth]] .
[[Mobility prediction]] enables appealing [[proactive experience]]s for [[location-aware service]]s and offers essential [[intelligence]] to [[business]] and [[government]]s.
[[Model budget]]: A [[preliminary budget estimate developed in-house]], by [[Administration]] and [[Finance]], prior to the [[formal budget development process]] .
[[Model induction from relational data]] requires [[aggregation]] of the [[values of attributes]] of related [[entiti]]es.
[[Modeling]] and [[predicting]] a [[spatial distribution]] for an individual is a [[challenging problem]] given both (a) the typical [[sparsity of data]] at the [[individual level]] and (b) the [[heterogeneity]] of [[spatial mobility pattern]]s across [[individual]]s.
[[Modeling disease progression]] based on [[real-world evidence]] is a very challenging [[task]] due to the [[incompleteness]] and irregularity of the [[observation]]s, as well as the [[heterogeneity]] of the [[patient condition]]s.
[[Modeling]] the [[dynamics]] of [[online social network]]s over [[time not]] only helps us understand the [[evolution]] of [[network structure]]s and [[user behavior]]s, but also improves the [[performance]] of other [[analysis task]]s, such as [[link prediction]] and [[community detection]] .
[[Modeling]] the [[movement of information]] within [[social media outlet]]s, like [[Twitter]], is key to understanding to how [[ideas spread]] but quantifying such [[movement run]]s into several [[difficulti]]es.
[[Model parameters]] can be fit on training motifs using a [[variational EM algorithm]] within an [[empirical]] [[Bayesian]] [[framework]] .
[[Model selection]] by our [[variational bound]] shows that a [[five layer hierarchy]] is justified even when [[modelling]] a [[digit data set]] containing only 150 [[example]]s.
[[Models]] for many [[natural language tasks]] benefit from the flexibility to use [[overlapping]], [[non-independent features]] .
[[Models of associative neural network]]s have an [[information storage capacity]] slightly under 1 [[bit per synapse]] depending on what kind of [[information]] is [[encoded]] ([[Nadal, 1991]]; [[Nadal and Toulouse, 1990]]).
[[Model]]s, such as the [[Kronecker model]], [[duplication-based model]]s, and [[preferential attachment model]]s, have been used for [[task]]s such as [[representing null model]]s, [[detecting anomali]]es, [[algorithm testing]], and developing an understanding of various [[mechanistic growth process]]es.
[[Models that learn]] to represent [[textual]] and [[knowledge base relation]]s in the same [[continuous latent space]] are able to perform [[joint inference]]s among the two kinds of [[relation]]s and obtain [[high accuracy]] on [[knowledge base completion]] ([[Riedel et al. 2013]]).
Moderately [[visible]] [[statement]]s were judged as true at [[chance level]], whereas [[highly visible]] [[statement]]s were judged as true significantly above [[chance level]] .
Modern [[AI research]] has addressed another important property of the [[real world]] — [[pervasive uncertainty]] about both its [[state]] and its [[dynamics]] — using [[probability-theory]] .
Modern [[communication networks]] generate [[massive volume]] of [[operational]] [[event data]], e.g., [[alarm]], [[alert]], and [[metrics]], which can be used by a [[network management system (NMS)]] to diagnose potential [[faults]] .
Modern [[computer hardware]] offers an elaborate [[hierarchy]] of [[storage subsystem]]s with different [[speed]]s, [[capaciti]]es, and [[cost]]s associated with them.
Modern [[data mining settings]] involve a combination of [[attribute-valued descriptors]] over [[entities]] as well as [[specified relationships]] between these [[entities]] .
Modern [[models of relation extraction]] for [[tasks like ACE]] are based on [[supervised learning of relations]] from [[small]] [[hand-labeled corpora]] .
[[Moni]]es in some [[budgetary account]]s, and in most [[non-budgetary account]]s, are not [[subsidiarized]], and [[expenditure]]s in those [[account]]s are not limited by [[object class control]]s.
[[Monitoring]] and [[mining]] [[BGP]] [[update messages]] can directly reveal the [[health]] and [[stability]] of [[Internet routing]] .
[[Monte Carlo tree search (MCTS)]] is a recently proposed [[search method]] that combines the [[precision]] of [[tree search]] with the [[generality]] of [[random sampling]] .
[[Moore's Law]] is one [[small component]] in an [[exponentially]] growing planetary computing [[ecosystem]] .
[[More advanced method]]s such as ([[Nakagawa et al., 2010]]) that can capture such phenomena use many [[manually constructed resource]]s ([[sentiment lexica]], [[parser]]s, [[polarity-shifting rule]]s).
More and more [[technologi]]es are taking advantage of the [[explosion]] of [[social media]] ([[Web search]], [[content recommendation service]]s, [[marketing]], [[ad targeting]], etc.).
More importantly, [[we]] show that its long [[term]] [[average]] [[performance]] will [[converge]] to the best [[rate]] achievable by any competing fixed [[ranking]] [[policy]] selected with the benefit of hindsight.
More in particular, [[we]] [[utilise]] a rich [[pattern language]] and show how a [[database]] can be [[encoded]] by such [[patterns]] .
More interestingly, it takes an [[first]] [[step]] in deriving a [[generative]] [[view]] of the [[frequent pattern formulation]], i.e., how a [[small]] [[number]] of [[patterns]] [[interact]] with each other and [[produce]] the [[complexity]] of [[frequent itemsets]] .
More [[names]] are added as new [[functional]] or [[structural information]] is [[discovered]] .
Moreover, as [[scientist]]s [[trust]] some [[author]]s more than others, [[result]]s are personalized to [[individual preference]]s.
Moreover, based on the observation that [[user]]s tend to [[visit]] [[nearby]] [[POI]]s, we further enhance the [[recommendation model]] by considering [[geographical information]] .
Moreover, by expressing the [[temporal graph]]s with the [[phenotype]]s, the expressing [[coefficient]]s can be used for [[application]]s such as [[personalized medicine]], [[disease diagnosis]], and [[patient segmentation]] .
Moreover, comparing to [[previous work]], [[the approach]] produces [[larger]], more [[accurate]] [[folksonomies]], and in addition, [[scales]] better.
Moreover, [[computations]] are [[fast enough]] that all [[processing]], from [[data retrieval]] through [[estimation]], [[testing]], [[validation]] and [[report generation]], proceeds in an [[automated pipeline]], without anyone needing to see the [[raw data]] .
Moreover, considering that in some cases [[other information]] about [[costs]] can be obtained in addition to [[cost intervals]], such as the [[distribution of costs]], [[we]] propose a [[general approach]] [[CODIS]] for using the [[distribution information]] to help improve [[performance]] .
Moreover, [[CTC]] can directly control the [[sparsity]] of [[inferred representation]]s by using appropriate [[regularization]] .
Moreover, [[experimental result]]s employing a variety of [[reward function]]s and several [[real world information network]]s illustrate that the [[GHIN framework]] produces more [[accurate]] and [[informative cluster]]s than the recently proposed [[NetClu]]s and [[state of the art]] [[MDC algorithm]]s.
Moreover, [[FEMA]] is able to [[discover]] quite a number of interesting [[multi-faceted temporal pattern]]s on [[human behavior]]s with [[good interpretability]] .
Moreover, [[GeBM]] is able to [[simulate]] [[basic psychological phenomena]] such as [[habituation]] and [[priming]] (whose definition we provide in the [[main text]]).
Moreover, given that the [[estimator]]s are [[unbiased]], the [[approximation error]] is also kept under [[control]] .
Moreover, in addition to [[estimate]]s of the [[aggregated target]]s, if we have [[exact]] (or [[approximate]]) [[estimate]]s of the [[mean]] and [[variance]] of the [[target distribution]], then based on our general strategy we provide an [[optimal]] way of incorporating [[this information]] so as to further improve the [[quality of prediction]]s of the individual [[target]]s.
Moreover, in [[our model]] [[event textual content]], [[organizer]], and [[location information]] are utilized to [[learn]] [[representation]] of [[cold-start]] [[event]]s for [[predicting]] [[user response]] to them.
Moreover it can [[update]] the [[personalized PageRank]] [[score]]s in [[amortized O]] (1/Îµ) [[iteration]]s for each [[graph modification]] .
Moreover, many [[technique]]s are limited to [[detect]] [[cluster]]s in the [[full-dimensional data space]] .
Moreover, [[our approach]] can [[extract]] not only [[texts]], but also [[images]] and [[animates]] within the [[news bodies]] and the [[extracted]] [[news articles]] are in the same [[visual style]] as in the original [[pages]] .
Moreover, [[purchase data]], in contrast to [[ratings data]], is [[implicit]] with [[non-purchase]]s not necessarily indicating [[dislike]] .
Moreover, [[RandNLA]] promises a [[sound algorithmic]] and [[statistical foundation]] for modern [[large-scale data analysis]] .
Moreover, [[relationship]]s between [[instance]]s in different [[domain]]s may be associated with [[weights based]] on [[prior (partial) knowledge]] .
Moreover, since inferring this [[graphical model]] requires [[parameter tuning]] of the [[prior of truth]], [[we]] propose an [[initialization]] [[scheme]] based upon a [[quantity]] named <i>[[truth existence score]] </i>, which [[synthesize]]s two [[indicator]]s, namely, <i>[[participation rate]]</i> and <i>[[consistency rate]] </i>.
Moreover, such a [[physical system]] is influenced by [[cyber effect]]s, including [[workload scheduling]] in [[server]]s.
Moreover, the [[error rate]] of [[APM]] is guaranteed to be very small when the [[database]] contains hundreds [[transaction]]s, which further affirms [[APM]] is a [[practical solution]] for [[summarizing]] [[probabilistic frequent pattern]]s.
Moreover, the [[implemented]] [[co-clustering]] [[framework]] supports a [[wide variety]] of other [[large-scale]] [[data mining application]]s and forms the [[basis]] for [[predictive modeling]] on large, [[dyadic]] [[dataset]]s [4, 7].
Moreover, [[the method]] is [[scalable]]: both [[computation time]] and required [[space grow linearly]] as [[corpus size]] increases.
Moreover, the [[RaHH framework]] [[encode]]s both [[homogeneous]] and [[heterogeneous relationship]]s between the [[data entiti]]es to design [[hash function]]s with improved [[accuracy]] .
Moreover, the [[recommendation model]] can effectively make use of [[user check-in count data]] as implicity [[user feedback]] for [[model]]ing [[user preference]]s.
Moreover, the [[results]] of [[BiMRF]] with [[two-star configurations]] suggest that the [[feature]] of [[user]] [[similarity]] defined by [[frequency]] of [[communication]] or [[number]] of [[common]] [[friends]] is inadequate to [[predict]] [[grouping]] [[behavior]], but adding [[node]]-level [[feature]]s can improve the [[fit of the model]] .
Moreover, the [[source]] and [[target]] [[network]]s are usually [[heterogeneous]] and have different types of [[node]]s and [[link]]s.
Moreover, the [[synchronization]] allows natural [[outlier handling]], since [[outliers]] do not synchronize with [[cluster objects]] .
Moreover, the [[text in the caption]] contains important [[semantic entities]] such as [[protein]] names, [[gene]] [[ontology]], [[tissues label]]s, etc., [[relevant]] to the [[image]]s in the [[figure]] .
Moreover, the use of [[adaptive downsampling]] with a modified [[DTW formulation]] achieves a 7 - to almost [[20-fold]] [[reduction]] in [[runtime]] relative to [[DTW]], without a [[significant change]] in [[biomarker discrimination]] .
Moreover, [[VEWS]] detects far more [[vandal]]s than [[ClueBot NG]] and on average, detects them 2.39 [[edit]]s before [[ClueBot NG]] when both [[detect the vandal]] .
Moreover, [[we]] apply [[Spine]] as a [[pre-processing step]] for the [[influence-maximization problem]], showing that [[computation]]s on [[sparsified model]]s give up little [[accuracy]], but yield significant [[improvement]]s in terms of [[scalability]] .
Moreover, [[we]] disclose that the [[label relationship]] is usually [[asymmetric]] .
Moreover, [[we]] incorporate [[efficient]] [[technique]]s, such as [[Nystrom approximation]], to further [[reduce]] [[time]] and [[space complexity]] for [[indexing]] and [[search]], making [[our algorithm]] [[scalable]] to [[huge data sets]] .
Moreover, [[we]] investigate [[this task]] in a [[multilingual setting]] and for some specific [[domain]]s.
Moreover, [[we]] investigate two [[method]]s to obtain the [[binary code]]s from the [[relaxed solution]]s.
Moreover, [[we]] propose a [[scalable algorithm]] ([[linear]] wrt the [[size of the graph]]) that generates a [[provably]] [[near-optimal solution]] .
Moreover, [[we]] propose a [[similarity measure]], called [[Multi-Component Similarity]], aggregating [[Hamming similarities]] in [[multiple hash table]]s to capture the [[non-transitive property]] of [[semantic similarity]] .
Moreover, [[we]] prove that [[our algorithm]]s are nearly [[optimal]], by proving an [[information-theoretic lower bound]] on the [[number of trace]]s that an [[optimal]] [[inference algorithm]] requires for performing [[this task]] in the general case.
Moreover, [[we]] show a faster [[approximation]] exists, when the [[alert]]s follow certain [[causal structure]] .
Moreover, [[we]] show that [[OWA]] aggregates of [[margin-based classification loss]]es have [[good generalization properti]]es.
Moreover, [[we]] show that [[SPARFA-Trace]] achieves comparable or better [[performance]] in [[predicting]] [[unobserved]] [[learner response]]s compared to existing [[collaborative filtering]] and [[knowledge tracing method]]s.
Moreover, [[we]] show that [[these corpora]] can help when [[translating]] [[out-of-domain text]]s.
Moreover when the [[solution]] is [[sparse]], as is often the [[case]] in [[real application]]s, [[our new method]] benefits by selecting important [[variable]]s to [[update]] more often, thus resulting in [[higher speed]] .
More precisely, [[logical definition]]s of [[threat]]s and [[reward]]s are proposed together with their [[weighting system]]s.
More recent [[approach]]es consider combining [[data exploration]] with traditional [[refinement technique]]s.
More [[recent models]], namely [[Laplacian PLSI (LapPLSI)]] and [[Locally-consistent Topic Model (LTM)]], have incorporated the [[local]] [[manifold structure]] into [[topic models]] and have shown the resulting [[benefits]] .
More specifically, a [[user]]'s [[action]] at time <math>t</math> is [[generated by]] her [[latent state]] at <math>t</math>, which is influenced by her [[attributes]], her own [[latent state]] at [[time]] <math>t-1</math> and her [[neighbors]]' [[states]] at time <math>t</math> and <math>t-1</math>.
More specifically, given [[the stimulu]]s, what are the [[interaction]]s between [[(groups of) neuron]]s (also known as [[functional connectivity]]) and how can we [[automatically infer]] those [[interaction]]s, given [[measurement]]s of the [[brain activity]]?
More specifically, [[our work]] focuses on [[recommending]] [[query topic]]s to [[user]]s.
More specifically, [[we]] contribute 1) an improved bound for correlated [[itemset]] miners, 2) a novel [[iterative pruning algorithm]] to exploit the [[bound]], and 3) an adaptation of [[this algorithm]] to [[mine]] all [[itemset]]s on the [[convex hull]] in [[ROC space]] .
More specifically, we exploit [[matrix factorization]] to [[model interaction]]s between [[user]]s and [[group]]s.
More specifically, [[we]] focus on the [[personalization model]]s by generating three kinds of [[affinity score]]s: [[Viewer-ActivityType Affinity]], [[Viewer-Actor Affinity]], and [[Viewer-Actor-ActivityType Affinity]] .
More specifically, [[we]] propose a [[community profiling model]] called [[COCOMP]], where the [[communities label]]s are [[latent]], and each [[social document]] corresponds to an [[information sharing activity]] among the most [[probable]] [[community member]]s regarding the most relevant [[community issue]]s.
Most [[algorithm work]] in [[data mining]] focuses on [[designing algorithm]]s to address a [[learning problem]] .
Most existing [[algorithm]]s [[cluster]] [[documents]] and [[words]] [[separately]] but not [[simultaneously]] .
Most [[existing approach]]es are based on [[burst detection]], [[topic modeling]], or [[clustering technique]]s, which cannot naturally model the [[implicit]] [[heterogeneous network structure]] in [[social media]] .
Most existing [[approaches]] combine [[link]] and [[content information]] by a [[generative model]] that [[generates]] both [[links]] and [[contents]] via a shared [[set]] of [[community]] [[memberships]] .
Most existing [[approach]]es require the [[label relationship]] as [[prior knowledge]], or exploit by [[counting]] the [[label co-occurrence]] .
Most existing [[blocking technique]]s do not provide [[control]] over the [[size of]] the [[generated block]]s, despite this [[control]] being important in many [[practical applications of ER]], such as [[privacy-preserving record linkage]] and [[real-time ER]] .
Most existing [[research]] about [[online trust]] assumes [[static trust relation]]s between [[user]]s.
Most [[existing solution]]s estimate the [[marginal utility]] of an [[item]] given a [[set of item]]s already in the [[response]], and then use [[variant]]s of [[greedy set cover]] .
Most [[existing studi]]es assume that all [[paper]]s adopt the same [[criterion]] and follow the same [[behavioral pattern]] in [[deciding relevance]] and [[authority of a paper]] .
Most [[existing work]] on the [[learning]] of [[diffusion model]]s does not distinguish the [[events caused]] by the [[social influence]] from those caused by [[external trend]]s.
Most frameworks such as [[TensorFlow]], [[Theano]], [[Caffe]] and [[CNTK]] have a static view of the world.
Most [[friending recommendation service]]s today aim to [[support passive friending]], where a [[user passively]] selects [[friending target]]s from the [[recommended candidate]]s.
Most importantly, [[metro map]]s explicitly show the [[relation]]s between [[paper]]s in a way which captures [[development]]s in the [[field]] .
Most [[measurements of performance]] are geared to the needs of [[20th-century]] [[manufacturing]] [[compani]]es.
Most [[modern]] [[commercial aircraft]] [[record]] several hundred [[flight parameters]] including [[information]] from the [[guidance]], [[navigation]], and [[control systems]], the [[avionics]] and [[propulsion systems]], and the [[pilot]] [[inputs]] into the [[aircraft]] .
Most modern [[convolutional neural networks (CNNs)]] used for [[object recognition]] are built using the same [[principle]]s: [[Alternating convolution]] and [[max-pooling layer]]s followed by a [[small number]] of [[fully connected layers]] .
Most of the [[algorithm]]s proposed in [[literature]] for [[NMF]] have been based on [[minimizing]] the [[Frobenius norm]] .
Most of the current [[MIL method]]s only deal with the case when each [[instance]] is [[represented by]] one [[type]] of [[feature]]s.
Most of the [[existing solution]]s for [[SSD]] and [[SSSP queri]]es, however, require that the [[input]] [[graph fit]]s in the main [[memory]], which renders them inapplicable for the [[massive disk-resident]] [[graph]]s commonly used in [[web]] and [[social application]]s.
Most of them learn [[decision model]]s that continuously [[evolve over time]], run in [[resource-aware environment]]s, [[detect]] and react to changes in the [[environment]] generating [[data]] .
Most of [[the proposed algorithm]]s assume that a [[knowledge base]] can provide enough [[explicit]] and useful [[information]] to help [[disambiguate]] a [[mention]] to the right [[entity]] .
Most of the [[research]] on [[episode]]s deals with special cases such as [[serial]], [[parallel]], and [[injective episode]]s, while [[discovering general episode]]s is understudied.
Most of these [[model]]s, including [[dynamic Bayesian networks (DBN)]] and [[user browsing models (UBM)]], use [[probabilistic model]]s to understand [[user click behavior]] based on individual [[queri]]es.
Most of [[this research]] focused on [[perfect information game]]s, in which all [[event]]s are observed by all [[player]]s, culminating in [[program]]s that beat [[human world champion]]s in [[checker]]s, [[chess]], [[Othello]], [[backgammon]], and most recently, [[Go]] .
Most [[people]], including [[philosopher]]s, tend to [[classify]] [[human motive]]s as falling into one of two [[categori]]es: the [[egoistic]] or the [[altruistic]], the [[self-interested]] or the [[moral]] .
Most [[previous work]] focuses on exploiting [[correlation]]s among different [[label]]s to facilitate the [[learning process]] .
Most previous [[work]] focuses on [[learning]] and [[predicting]] with "[[static]]" [[temporal graph]]s only.
Most previous [[work]] has focused on [[linking terms in formal text]]s (e.g. [[newswire]]) to [[Wikipedia]] .
Most [[previous work]] in this [[area]] uses [[prior]] [[lexical knowledge]] in terms of the [[sentiment-polarity]] of [[words]] .
Most [[regression model]]s are described in terms of the way the [[outcome variable]] is modeled: in [[linear regression]] the [[outcome]] is [[continuous]], [[logistic regression]] has a [[dichotomous outcome]], and [[survival analysis]] involves a [[time to event outcome]] .
Most retailers have [[data warehouse]]s of [[transaction data]] containing [[customer information]] and related [[transaction]]s.
Most [[semi-supervised learning model]]s [[propagate]] the [[label]]s over the [[Laplacian graph]], where the [[graph]] should be built [[beforehand]] .
Most [[state-of-the-art approach]]es consider the [[notion]] of [[structural balance]] in [[signed network]]s, [[building inference algorithm]]s based on [[information]] about [[link]]s, [[triad]]s, and [[cycle]]s in the [[network]] .
Most [[statistical models]] in wide use are special cases of [[Markov logic]], and [[first-order logic]] is its [[infinite]]-[[weight]] [[limit]] .
Most [[theories of motivation]] and [[self‐regulation]] converge on the idea that setting a [[behavioral]] or [[outcome goal]] is the key act of willing that promotes [[goal attainment]] (e.g., [[Ajzen, 1991]]; [[Atkinson, 1957]]; [[Bandura, 1991]]; [[Carver & Scheier, 1998]]; [[Gollwitzer, 1990]]; [[Locke & Latham, 1990]]).
Most traditional [[supervised learning method]]s are developed to [[learn]] a [[model]] from [[labeled example]]s and use this [[model]] to [[classify]] the [[unlabeled]] ones into the same [[label space]] predefined by [[the model]]s.
Motivated by [[applications]] in [[guaranteed delivery]] in [[computational advertising]], [[we]] consider the [[general problem of balanced allocation]] in a [[bipartite]] [[supply-demand setting]] .
Motivated by [[ensemble method]]s in general, and [[boosting algorithm]]s in particular, [[BART]] is defined by a [[statistical model]]: a [[prior]] and a [[likelihood]] .
Motivated by modern [[microblogging platform]]s, such as <b> [[twitter]] </b>, in [[this paper]] we study the [[problem]] of [[learning influence probabiliti]]es in a [[data-stream scenario]], in which the [[network topology]] is relatively [[stable]] and the [[challenge]] of a [[learning algorithm]] is to keep up with a [[continuous stream]] of [[tweet]]s using a [[small amount]] of [[time]] and [[memory]] .
[[Motivated]] by [[structural properties]] of the [[Web]] [[graph]] that support efficient [[data structure]]s for [[in memory]] [[adjacency]] [[queri]]es, [[we]] study the extent to which a large [[network]] can be [[compressed]] .
Motivated by the [[hierarchical structure]] of the [[brain network]]s, [[we]] consider the [[problem]] of [[estimating]] a [[graphical model]] with [[tree-structural regularization]] in [[this paper]] .
Motivated by the insufficiency of the existing [[quasi-identifier/sensitive-attribute (QI-SA) framework]] on [[modeling]] [[real-world]] [[privacy requirements]] for [[data publishing]], [[we]] propose a novel versatile [[publishing scheme]] with which [[privacy requirements]] can be specified as an [[arbitrary set]] of [[privacy rules]] over [[attributes]] in the [[microdata table]] .
Motivated by the [[problem]] of [[optimizing allocation]] in [[guaranteed display advertising]], [[we]] develop an [[efficient]], [[lightweight method]] of [[generating a compact]] <i>[[allocation plan]]</i> that can be used to guide [[ad server decision]]s.
Motivated by the [[prospect]] of creating [[computational models of humor]], we study the [[influence]] of the [[language]] of [[cartoon caption]]s on the [[perceived humorousness]] of the [[cartoon]]s.
Motivated by these [[argument]]s, [[we]] propose a new approach to [[second-order optimization]], the [[saddle-free Newton method]], that can rapidly escape [[high dimensional saddle point]]s, unlike [[gradient descent]] and [[quasi-Newton method]]s.
Motivated by these [[challenge]]s, we designed techniques that make both the [[batch execution]] and [[incremental execution]] of a [[KBC program]] up to two [[orders of magnitude]] more [[efficient]] and/or [[scalable]] .
Motivated by these [[patterns]], [[we]] argue that it is valuable to consider a richer [[framework]] for analyzing how recent changes in the [[earnings]] and [[employment distribution]] in the [[United States]] and other [[advanced economies]] are shaped by the interactions among [[worker skills]], [[job tasks]], [[evolving technologies]], and shifting [[trading]] [[opportunities]] .
[[Motivational analyse]]s should [[restrict themselve]]s, therefore, to the [[altruistic impulse]] and its [[knowable consequence]]s.
[[Motivation]], [[example]]s, [[application]]s, [[high-level planning concept]]s, overview of [[the book]] .
[[Moveover]], [[our approach]] inherits from [[logistic regression]] good [[interpretability]] of [[the model]], which is important for [[clinical application]]s but not offered by [[KLR]] and [[SVM]] .
[[MSN (Portal)]] and [[Bing (Search)]] generate the [[content]], [[traffic]] and [[data]], that make for an exciting fertile [[environment]] for [[large scale]] [[data mining practice]] and [[system development]] .
[[MTC]] captures the [[joint likelihood]] of [[multi-output]] by learning the [[marginal]] of each [[output]] firstly and then a [[sparse]] and [[smooth output dependency graph function]] .
[[MuCH]] generates [[multiple hash table]]s with each [[hash table]] corresponding to a [[similarity component]], and preserves the [[non-transitive similariti]]es in different [[hash table]] respectively.
Much of [[research]] in [[data mining]] and [[machine learning]] has led to numerous [[practical applications]] .
Much work is done on (1) the [[definition]] of [[standards for representation of knowledge]] (e.g. [[XML]], [[RDF]], [[OIL]]), (2) the [[definition]] of [[structures for knowledge organization]] (e.g. [[ontologies]]) and (3) the [[population]] of such [[knowledge structure]]s.
[[Multi-criteria decision making]]; [[Multi-expert decision making]]; [[Multi-step ranking]]; [[Recursive aggregation]]
[[Multidimensional distribution]]s are often used in [[data mining]] to describe and [[summarize]] different [[feature]]s of [[large dataset]]s.
[[Multi-instance multi-label learning (MIML)]] is a [[framework]] for [[supervised classification]] where the [[object]]s to be [[classified]] are [[bags of instance]]s associated with multiple [[label]]s.
[[Multi-label classification]] of [[heterogeneous]] [[information network]]s has received renewed attention in [[social network analysis]] .
[[Multi-label learning]] arises in many [[real-world task]]s where an [[object]] is naturally associated with multiple [[concept]]s.
[[Multimedia feature]]s can help us capture the [[attractiveness]] of the [[ad]]s with similar [[content]]s or [[aesthetic]]s.
[[Multimedia Interpretation]]; [[Ontology Evolution]]; [[Ontology Dynamics]]; [[Description Logics]]; [[OWL]]; [[Multimedia Ontologi]]es.
[[Multiple Kernel Learning (MKL)]] aims to [[learn]] the [[kernel]] in an [[SVM]] from [[training data]] .
Multiple [[supervised]] and [[unsupervised]] [[hypotheses]] can be drawn from these [[information source]]s, and negotiating their [[difference]]s and [[consolidating]] [[decisions]] usually yields a much more [[accurate]] [[model]] due to the [[diversity]] and [[heterogeneity]] of these [[models]] .
[[Multiplication]] of a given [[vector]] by a [[square matrix]] may result in a [[scalar multiple]] of the [[vector]] .
[[Multiplicative gate units learn]] to [[open]] and [[close access]] to the [[constant error]] [[flow]] .
[[Multi-task feature learning]] has been proposed to improve the [[generalization performance]] by [[learning]] the [[shared feature]]s among multiple [[related task]]s and it has been successfully applied to many [[real-world problem]]s in [[machine learning]], [[data mining]], [[computer vision]] and [[bioinformatics]] .
:: [[Multi-task Lasso model]] trained with [[L1/L2 mixed-norm]] as [[regularizer]]
[[Multi-task learning]] [10] studies the [[problem]] where multiple [[task]]s are [[trained in parallel]] such that [[training information]] of related [[task]]s are used as an [[inductive bias]] to help improve the [[generalization performance]] of other [[task]]s.
[[Multi-task learning]], [[Low-rank]] and [[sparse pattern]]s, [[Trace norm]]
[[Multi-touch attribution]] is one of the most important [[problem]]s in [[digital advertising]], especially when [[multiple media channel]]]s, such as [[search]], [[display]], [[social]], [[mobile]] and [[video]] are involved.
[[Multi-view graph clustering]] aims to enhance [[clustering performance]] by [[integrating]] [[heterogeneous information]] collected in different [[domain]]s.
[[Mutual information]], [[Hellinger distance]], [[relative entropy]], [[metric entropy]], [[minimax risk]], [[Bayes risk]], [[density estimation]], [[Kullback-Leibler distance]] .
[[My]] [[research team]] is trying to [[build robots]] that are capable of doing the same — with [[artificial sensor]]s ([[camera]]s, [[microphone]]s and [[scanner]]s), [[algorithm]]s and [[actuator]]s, which [[control]] the [[mechanism]]s.
[[Naive Bayes]] assumes a particular [[probabilistic generative-model]] for [[text]] .
[[Named entities]] are [[phrase]]s that contain the [[names]] of [[persons]], [[organizations]] and [[locations]] .
[[Named Entities Recognition (NER)]] is generally understood as the [[task]] of [[identifying]] [[mentions of rigid designator]]s from [[text]] belonging to [[named-entity type]]s such as [[person]]s, [[organization]]s and [[location]]s ([[Nadeau and Sekine, 2007]]).
[[Named entity disambiguation]] is the [[task of disambiguating]] [[named entity mention]]s in [[natural language text]] and [[link]] them to their corresponding [[entries in a knowledge base]] such as [[Wikipedia]] .
[[Named entity extraction]], [[Product name extraction]], [[Synonym generation]]
[[Named entity recognition]] is the [[task of identifying]] and [[categorizing]] [[textual reference]]s to [[objects in the world]], such as [[person]]s, [[organization]]s, [[companie]]s, and [[location]]s.
[[Named Entity Recognition (NER)]], the [[task of identifying and classifying]] the [[names]] of [[people]], [[organisations]], [[locations]] and other [[entities]] within [[text]], is central to many NLP tasks.
[[Named Entity Recognition]], [[Search Log Mining]], [[Topic Model]], [[Web Mining]]
[[Named entity recognition]] was first introduced in the [[sixth MUC]] [100] and consisted of three [[subtasks]]: [[proper names]] and [[acronym]]s of [[persons]], [[locations]], and [[organizations]] ([[ENAMEX]]), absolute temporal terms ([[TIMEX]]) and monetary and other numeric expressions ([[NUMEX]]).
[[NAMED IDENTITY]]; [[SURVEY]]; [[LEARNING METHOD]]; [[FEATURE SPACE]]; [[EVALUATION]]
[[Namely]], the [[modular structure]] of [[web graph]]s appears to motivate full [[offline optimization]], whereas the [[locally dense structure]] of [[social graph]]s [[precludes significant gain]]s from [[global manipulation]]s.
[[Namely]], [[the proposed algorithm]]s can [[sample]] [[(item -) set]]s according to [[frequency]], [[area]], [[squared frequency]], and a [[class discriminativity measure]] .
[[Namely]], [[the proposed method]] is more <i>[[effective]]</i> than the [[state-of-the-art method]], is <i>simple</i> to implement, and provides <i>[[interpretable]]</i> [[result]]s.
n-ary associations of [[annotation]]s are also supported, allowing the [[annotation]] of [[event structure]]s such as those [[targeted]] in the [[MUC (Sundheim, 1996]]), [[ACE]] ([[Doddington et al., 2004]]), and [[BioNLP]] ([[Kim et al., 2011]]) [[Information Extraction (IE) task]]s (Figure 2).
[[NASA]] has some of the largest and most [[complex]] [[data source]]s in the world, with [[data source]]s ranging from the [[earth sciences]], [[space sciences]], and massive [[distributed]] [[engineering data set]]s from [[commercial aircraft]] and [[spacecraft]] .
[[NASA]] is an [[adaptive variant]] of [[Mirror Descent Stochastic Approximation]] .
[[National income]] is defined as the [[sum]] of all [[income]] available to the [[resident]]s of a given [[country]] in a given [[year]], regardless of the [[legal classification]] of that [[income]] .
: [[National wealth]] = [[national capital]] = [[domestic capital]] + [[net foreign capital]]
[[Natural language processing]] employs [[computational technique]]s for the purpose of [[learning]], [[understanding]], and [[producing human language content]] .
[[Natural Language Processing]], [[machine learning]], [[Computational Linguistics]], [[multi-document summarization]], [[Rotten Tomatoes]],
[[Natural Selection]], [[Candidate Solution]], [[Fitness Function]]; [[Coevolution Learning]]; [[Compositional Coevolution]]; [[Evolutionary Clustering]]; [[Evolutionary Computation in Economics]]; [[Evolutionary Computation in Finance]]; [[Evolutionary Computational Techniques in Marketing]]; [[Evolutionary Feature Selection and Construction]]; [[Evolutionary Fuzzy Systems]]; [[Evolutionary Games]]; [[Evolutionary Kernel Learning]]; [[Evolutionary Robotics]]; [[Neuroevolution]]; [[Nonstandard Criteria in Evolutionary Learning]]; [[Test-Based Coevolution]]
[[Navigation function]]s, [[smooth manifold]]s, [[vector field]]s, [[numerical potential function]]s, [[optimal navigation function]]s, [[compositions of funnel]]s, [[dynamic programming]] on [[continuous space]]s.
[[Neelakantan et al. (2015)]] use [[RNNs]] to compose the [[distributed semantic]]s of [[multi-hop path]]s in [[KB]]s; however for multiple reasons, the approach lacks [[accuracy]] and [[practicality]] .
[[Negative]]: [[overwhelm]], [[anxiety]], [[insecurity]], [[impatience]], [[frustration]]
[[Negative Reinforcement]] describes a situation where the [[removal or termination]] of an [[existing stimulus]] (or [[existing aversive condition]]) as a [[consequence of an instance of behaviour]] makes that [[behaviour]] more likely to [[occur]] in that [[context]] in [[future]] .
[[Neighborhood communiti]]es give us a simple and powerful [[heuristic]] for [[speeding up]] [[local partitioning method]]s.
[[NEM]] is potentially useful in many [[applications]] including [[web search]], [[online advertisement]], and [[recommender system]] .
[[Neoclassical economics]] is built on very [[strong assumption]]s that, over time, have become “[[established fact]]s.” Most famous among these are that all [[economic agent]]s ([[consumer]]s, [[compani]]es, etc., are [[fully rational]], and that the so-called [[invisible hand]] works to create [[market efficiency]]).
[[NER]] is frequently accomplished with [[B-I-O tagging]], which classifies each token as being at the [[beginning]] of the [[named entity]] (B), [[continuing]] the [[entity]] (I), or [[outside]] of any [[entity]] to be tagged (O).
[[NER]] seeks to [[identify the words and phrases in text]] that [[reference entiti]]es in a given [[category]], such as [[people]], [[place]]s, or [[companies]], or in this application genes and [[protein]]s.
[[NetKit]] is based on a [[node-centric framework]] in which [[classifier]]s comprise a [[local classifier]], a [[relational classifier]], and a [[collective inference procedure]] .
[[Network data]] is ubiquitous, [[encoding]] [[collections]] of [[relationships]] between [[entities]] such as [[people]], [[place]]s, [[genes]], or [[corporations]] .
[[Networked data]], [[extracted]] from [[social media]], [[web page]]s, and [[bibliographic database]]s, can contain [[entiti]]es of [[multiple class]]es, [[interconnected]] through different types of [[link]]s.
[[Network interconnection]], [[information interoperability]], and [[crowds interaction]] on the [[Internet]] could inspire better [[computation model]]s than [[Turing machine]], since that [[human]] plays an important [[factor]] in [[Internet computing]], so that the [[human-machine]] and [[machine-machine interaction]]s have evolved to be the [[kernel of Internet computing]] .
[[Network science]] focuses on [[relationship]]s between [[social entities]] .
[[Network science]] is gaining [[recognition]] and standing in the general [[social]] and [[behavioral science]] [[communities]] as the [[theoretical basis]] for examining [[social structure]]s.
[[Neural networks]] are [[powerful]] and [[flexible model]]s that work well for many [[difficult learning task]]s in [[image]], [[speech]] and [[natural language understanding]] .
[[Neural Programmer]] can call these augmented [[operation]]s over several [[step]]s, thereby inducing [[compositional program]]s that are more [[complex]] than the [[built-in operation]]s.
Nevertheless, [[coming out]] with a [[high-quality]] comparable [[corpus]] of a specific [[topic]] is not [[straightforward]] .
Nevertheless, it is easier for a [[user]] to provide a [[few example]]s ("[[seeds]]") than a [[weighted combination]] of [[sophisticated meta-path]]s to specify her [[clustering preference]] .
Nevertheless, until recently [[this knowledge]] has overwhelmingly been [[represented by]] means of [[informal tool]]s, such as [[natural language]] and [[picture]]s.
New [[cloud based Machine Learning]] as a [[Service platform]]s help [[transform data]] into [[intelligence]] and build [[cloud-hosted intelligent API]]s for [[connected software]] [[application]]s.
[[Newcomb calculated]] that the [[probability]] that a [[number]] has any particular [[non-zero first digit]] is:
Newly emerged [[event-based online social service]]s, such as [[Meetup]] and [[Plancast]], have experienced increased [[popularity]] and [[rapid growth]] .
Newly revised and [[up-to-date]], [[this edition of The Social Animal]] is a brief, compelling [[introduction]] to [[modern]] [[social psychology]] .
New material on [[random forests]], the [[perceptron convergence theorem]], [[accuracy method]]s, and [[conjugate gradient optimization]] for the [[multi-layer perceptron]] .
New results are discussed, along with [[applications of probability theory]] to a wide variety of [[problem]]s in [[physic]]s, [[mathematic]]s, [[economic]]s, [[chemistry]] and [[biology]] .
[[Newton methods]] for [[nonlinear optimization]] use [[second order information]] to find [[search directions]] .
::[[Newton]] modified this idea, saying that the only way to [[change]] the [[motion]] of a [[body]] is to use [[force]] .
[[Newton]] thus added the idea that a [[force]] is needed to [[change]] the [[speed]] or the [[direction]] of [[motion]] of a body.
New types of [[work arrangement]]s can be as [[dangerou]]s as traditional [[unemployment]] for [[workers' health]]
Next, however [[we]] combined the [[AE]] and [[BK]] [[record]]s into one training set, [[BK&AE]], to determine whether these two [[categori]]es could be treated as a single [[record cluster]] .
Next, I shall say that a <i>[[person]]</i> is a [[woman]] or a [[man]] ([[young]] or [[old]]).
Next, [[we]] applied these [[association rule]]s to [[infer]] the [[direction of disease progression]] based on the [[support measure]]s of the [[association rule]]s.
Next, we build a [[classifier]] to [[identify]] [[funnier]] [[caption]]s [[automatically]] .
Next [[we]] demonstrate [[unfalsifiability]]'s [["defensive" function]]: When [[fact]]s [[threaten]] their [[worldview]]s, [[religious participant]]s frame specific reasons for their [[belief]]s in more [[unfalsifiable]] terms (Experiment 3) and [[political partisan]]s construe [[political issue]]s as more [[unfalsifiable]] ("[[moral opinion]]") instead of [[falsifiable]] ("a [[matter of facts]]"; Experiment 4).
Next, we develop two [[efficient latent link recommendation (ELLR)]] [[algorithm]]s in order to [[recommend]] [[link]]s by directly [[optimizing]] these two [[lower bound]]s, respectively.
Next, [[we]] review the most important [[classification technique]]s, including [[Bayesian Network]]s and [[Support Vector Machine]]s.
Next, [[we]] use the approach to the [[problem of graph classification]] via [[geometric embedding]]s such as the [[Laplacian]], [[pseudo-inverse Laplacian]] and the [[Lovász orthogonal labelling]]s.
[[Nicholas Eberstadt]], a [[political economist]] and a [[demographer]] by [[training]], holds the [[Henry Wendt Chair]] in [[Political Economy]] at [[American Enterprise Institute]] .
n [[imbalanced [[training data]]set]] poses serious problem for many [[real-world]] [[supervised learning tasks]] .
Nine varied samples rated [[gender]], [[ethnicity]], [[race]], [[class]], [[age]], and [[disability out-group]]s.
[[NMF]] is usually formulated as a [[constrain]]ed [[non-convex optimization problem]], and many [[algorithm]]s have been developed for solving [[it]] .
[[Noble]]'s [[counter-argument]] looks at the [[human cost]]s of [[unchecked technological growth]], along the way [[re-examining]] and redefining the meaning of [[Luddism]] .
[[Node]]s in a [[heterogeneous information network]] are regarded as [[object]]s derived from distinct [[domain]]s such as ' [[author]]s' and ' [[paper]]s'.
[[Noisy Labeler]]s, [[Estimation]], [[Active Learning]], [[Labeler Selection]]
[[Non-budgetary account]]s: [[Spending]] [[accounts]] other than [[direct appropriation]] and [[retained revenue]] .
[[Non-budgeted fund]]s: All [[funds]] except the [[General]], [[Highway]] and [[Health]] and [[Wellness Fund]]s, which are considered budgeted funds.
:::# [[Non-cash contribution]]s or [[donation]]s of [[property]] (including donated [[surplus property]]);<br>
:# [[Non-cash contribution]]s or [[donation]]s of [[property]] (including donated [[surplus property]]);<br>
None of this stuff is really about [[morality]] or [[religion]] or [[dogma]] or big fancy [[question]]s of [[life after death]] .
[[Non-exhaustive Learning]], [[Bayes Classifier]], [[Novelty Detection]], [[Anomaly Detection]], [[Bacteria Detection]] .
[[Nonnegative matrix factorization (NMF)]] is a [[dimension reduction method]] that has been widely used for numerous [[application]]s, including [[text mining]], [[computer vision]], [[pattern discovery]], and [[bioinformatic]]s.
[[Nonparametric Estimation Algorithm]], [[Categorial Data Statistical Analysis]], [[Longitudinal Statistical Analysis]], [[Multivariate Statistical Analysis]] .
[[Norma Haan]] of [[Berkeley]] conducted a fifty-year [[follow-up]] of [[people]] who had been studies while [[young]], and concluded that the subjects had become more [[outgoing]], [[self-confident]], and [[warm]] with [[age]] .
Normally, a [[chatbot]] works by a [[user]] asking a [[question]] or making a [[comment]], with the [[chatbot]] answering the [[question]], or making a [[comment]], or initiating a new [[topic]] .
[[Norm]]s are often given an [[intrinsic value]] and even [[worshipped]], as in the [[religious law]]s of many different [[societi]]es.
::Notable examples include [[LinkedIn]], [[Viadeo]], [[XING]], Ownmirror, [[Open Science Lab]], [[Wisestep.com]], [[C-Profs.com]], and [[Hall.com]] .
Notably, [[Perceived Usability]] played a mediating role in the [[relationship]] between [[Endurability]] and [[Novelty]], [[Aesthetic]]s, [[Felt Involvement]], and [[Focused Attention]] .
Notations used in this section are <math>Y_1</math> = [[pretest score]]s, <math>T</math> = [[experimental treatment]], <math>Y_1</math> = [[posttest score]]s, <math>D = Y_2 - Y_1</math> ([[gain score]]s), and [[RD]] = [[randomized design]] ([[random selection]] and [[assignment]] of [[participant]]s to [[group]]s and, then, [[random assignment]] of [[group]]s to [[treatment]]s).
Not in any sense that makes [[exploitation]] [[intrinsically]] [[unjust]] .
Not only is [[SNARE]] applicable in a wide variety of [[domain]]s, but it is also robust to the [[choice]] of [[parameter]]s and highly [[scalable]] [[linear]]ly with the [[number]] of [[edges]] in a [[graph]] .
Not [[surprisingly]], [[competition]] is central to the [[study]] of [[economic]]s, [[psychology]], [[sociology]], [[political science]], and more.
Novel [[features]] dedicated to [[news titles]] and [[bodies]] are developed.
Novel [[feature]]s of this [[algorithm]] are its handling of [[approximate]] [[pattern matching]] through a [[graph]] [[theoretical method]], [[maximal clique identification]], and incorporation of [[temporal]] and [[spatial constraint]]s.
[[Novelty detection]] is the [[identification]] of new or [[unknown data]] or [[signal]] that a [[machine learning system]] is not aware of during [[training]] .
[[Nowaday]]s, [[healthcare data management system]]s integrate [[large amount]]s of [[medical information]] on [[patient]]s, including [[diagnose]]s, [[medical procedure]]s, [[lab test result]]s, and more.
Nowadays, many [[online service compani]]es are using [[online metric]]s as the pointers toward the [[North Star]] (i.e. the success of [[the business)]] to improve their [[product]]s, including [[Amazon]] [ 21 ], [[eBay]], [[Etsy]] [ 28 ], [[Facebook]] [ 3 ], [[Google]] [ 34 ], [[Groupon]], [[Intuit]] [ 29 ], [[LinkedIn]] [ 30 ], [[Microsoft]] [ 27 ], [[Netflix]] [ 2 ], and [[Yahoo]] .
[[Nowaday]]s, [[streaming data]] with [[multi-label nature]] widely exist in [[real-world scenario]]s such as [[instant new]]s, [[email]]s, [[microblog]]s, etc [ 70].
Nowadays, the [[development]] of most [[leading web service]]s is [[controlled]] by [[online experiment]]s that [[qualify]] and [[quantify]] the [[steady stream]] of their [[update]]s.
[[Nowaday]]s, [[user]]s engage in multiple [[network]]s and form a "[[composite social network]] " by considering [[common user]]s as the [[bridge]] .
[[NTT-FGM]] simultaneously [[models]] [[social network structure]], [[user attributes]] and [[user action history]] for better [[prediction]] of the [[users]]' [[future actions]] .
[[Numerical data]] can conveniently be [[represented as]] [[geometric vector]]s.
Numerous [[difficulti]]es associated with [[quantitative risk assessment]]s (e.g., inadequate [[characterization]]s of [[invasion process]]es, [[lack of crucial data]], [[large uncertainti]]es associated with available [[data]], etc.) have hampered the usefulness of such [[estimate]]s in the [[task of supporting]] the [[authoriti]]es who are battling to manage [[invasion]]s with [[limited resource]]s.
Numerous [[model]]s have been proposed for [[modeling social networks]] to explore their [[structure]] or to address [[application problem]]s, such as [[community detection]] and [[behavior prediction]] .
[[NumPy]], short for [[Numerical Python]], is the [[foundational package]] for [[scientific computing]] in [[Python]] .
; [[NYC young]] worker [[unemployment spiked]] during [[recession]] but has since [[declined]] .
o 9.6.3 [[Jackknife]] and [[bootstrap estimation]] of [[classification accuracy]]
O are all possible [[assignment]]s of [[true]] and [[false]] to the [[symbol]]s.
[[Object code]]: [[Three-character identifier]] used in [[MMARS]] to classify all [[expenditure]]s.
[[Object-level search]] result [[clustering]] is challenging because [[we]] need to support diverse [[similarity]] [[notion]]s over [[object-specific feature]]s (such as the [[price]] and [[weight]] of a [[product]]) of [[heterogeneous domain]]s.
[[Object-oriented language]]s bind some notion of [[code]] and [[data]] together.
[[object]]s can be assigned to different [[cluster]]s in different [[subspace]]s and [[attribute]]s may contribute to different [[subspace]]s containing [[cluster]]s.
[[Object]]s with multiple [[numeric attribute]]s can be compared within any "[[subspace]]" ([[subset of attribute]]s).
[[Observations]] need not be [[independent]] and may [[overlap]] in [[space]] and [[time]] .
[[Observing]] the [[bottleneck]] in checking whether a [[pattern can probabilistically represent another]], which involves the [[computation of a joint probability of the supports of two pattern]]s, [[we]] introduce a novel [[approximation]] of the [[joint probability]] with both [[theoretical]] and [[empirical proof]]s.
Obtained [[perplexity reduction]]s against [[Good-Turing trigram]] [[baseline]] are over 50% and against [[modified Kneser-Ney smoothed 5-gram]] over [[40%]] .
[[Obviously]], these [[social connection]]s are [[non-transitive]] if people are [[connected]] due to different [[reason]]s.
[[O-CRF]] outperforms [[O-NB]], obtaining nearly double its [[recall]] and [[increased precision]] .
[[O-CRF]]’s gains are partly due to its lower [[false positive rate]] for [[relationship]]s [[categorized]] as “[[Other]].”
[[Office]] and [[Windows follow Sinofsky’s long planning]] and [[execution cycle]]s [13].
[[Offline evaluation metric]]s are [[indicator]]s of the [[expected model]] [[performance]] on [[real data]] .
[[Offline metric]]s certainly have great [[value]]s in guiding the [[service]]s when they are at relatively [[early stage]]s and [[not many user]]s are available for [[experimentation]] yet.
[[Offline retailer]]s have [[nailed discovery]], [[delight]], [[serendipity]], and [[impulse purchase]]s in [[person]] with [[greater success]] than [[online commerce site]]s.
Of note, some systems initially developed for [[extracting clinical information]] have later been adapted to [[extract relations]] among [[biological entiti]]es (e.g., [[MedLEE]] [8] / [[GENIES]] [9], [[SemRep]]/[[SemGen]] [10]).
Of special interest in [[information theory]], [[data compression]], [[mathematical finance]], [[computational learning theory]] and [[statistical mechanics]] is the special case when the [[loss]] <math>L(P_{θ^∗},\hat{P}_t)</math> is the [[relative entropy]] between the [[true distribution]] <math>P_{θ^∗}</math> and the [[estimated distribution]] <math>\hat{P}_t</math>.
Often, [[discourse relations]] are realized in [[text]] by [[explicit words]] and [[phrase]]s, called <i>[[discourse connectives]]</i>, but they can also be [[implicit]] .
Often, in the [[real world]], [[entiti]]es have two or more [[representation]]s in [[database]]s.
[[Often]], one is interested in [[detecting anomali]]es in [[discrete sequence]]s to find possible [[intrusion]]s, frauds, [[fault]]s, [[contamination]]s.
Often referred to by one of the following [[acronym]]s: "[[GL]] "," [[MGL]] "," [[MGLA]] ([[nnotated]]) ".
Often we want to [[classify documents]] by [[topic]], [[source]], or [[style]] .
Of these [[resource]]s, [[WordNet]] ([[Fellbaum, 1998]]), the de [[facto standard lexical database]] of [[English]], has remained in widespread use over the past two decades, with a broad range of applications such as [[Word Sense Disambiguation]] ([[Navigli, 2009]]), [[Query expansion]] and [[Information Retrieval]] ([[Varelas et al., 2005]]; [[Fang, 2008]]), [[sentiment analysis]] ([[Esuli and Sebastiani, 2006]]), and [[semantic similarity measurement]] ([[Budanitsky and Hirst, 2006a]]; [[Pilehvar et al., 2013]]).
('''OKBC''') is a [[protocol]] and an [[API]] for accessing knowledge in [[knowledge representation]] systems such as [[ontology]] repositories and [[object-relational database]]s.
[[OKBC]] is defined in a [[programming language independent fashion]], and has existing implementations in [[Common Lisp]], [[Java]], and [[C]] .
::OKBC provides a [[uniform model]] of [[KRS]]s based on a common conceptualization of [[class]]es, [[individual]]s, [[slot]]s, [[facet]]s, and [[inheritance]] .
[[OLTP applications]] typically automate [[clerical]] [[data processing tasks]] such as [[order entry]] and [[banking transactions]] that are the bread-and-butter [[day-to-day operations]] of an [[organization]] .
On 100,000 + [[queri]]es (across 7 [[micro-segment]]s) obtained from [[Bing log]]s, [[our system]] [[LaSEWeb]] [[answered queri]]es with an [[average recall]] of 71%.
On a [[domain-specific]] [[IT helpdesk dataset]], [[the model]] can find a solution to a [[technical problem]] via [[conversation]]s.
On a fairly [[complex]] [[synthetic]] [[table-comprehension dataset]], traditional [[recurrent network]]s and [[attentional model]]s perform poorly while [[Neural Programmer]] typically obtains nearly [[perfect accuracy]] .
On a largescale [[Freebase]] + [[ClueWeb]] [[prediction task]], [[we]] achieve 25% [[error reduction]], and a 53% [[error reduction]] on [[sparse relation]]s due to [[shared strength]] .
On a [[noisy]] [[open-domain]] [[movie transcript dataset]], [[the model]] can perform simple forms of [[common sense reasoning]] .
On a number of [[text data set]]s, [[our proposed method]] produces [[high-quality]] [[tree structure]]s in significantly [[less time]] compared to other [[method]]s such as [[hierarchical K-mean]]s, [[standard NMF]], and [[latent Dirichlet allocation]] .
On a [[public]] [[benchmark data set]] [[we]] are able to demonstrate a significant improvement in terms of [[relation extraction]] [[quality]] of our new [[kernels]] over other [[state-of-the-art]] [[kernels]] .
::On a [[radio]] [[transmitter]], frequency drift can cause a [[radio station]] to drift into an [[adjacent channel]], causing illegal [[interference]] .
On a [[regular basis]] [[we]] apply the parser to [[offering title]]s to produce a [[large set]] of [[labeled term]]s.
On a second [[problem]] using [[binned feature]]s, [[our model]] [[outperform]]s the [[baseline]] even after the latter sees 5x as much [[data]] .
On [[average]], our [[pattern-based classification technique]] outperforms the [[baseline approach]] by 24.68% in [[accuracy]] .
On both [[dataset]]s, the proposed [[CoupledLP framework]] outperforms several alternative [[method]]s.
On both [[synthetic]] and [[real data sets]], [[we]] show that [[Grafting-Light]] is much more efficient than [[Grafting]] for both [[feature selection]] and [[structure learning]], and performs comparably with the [[optimal]] [[batch method]] that [[directly optimizes]] over all the [[features]] for [[feature selection]] but is much more [[efficient]] and [[accurate]] for [[structure learning]] of [[MRFs]] .
Once all [[token]]s are [[label]]ed, [[Textpresso]] can perform “[[fact extraction]]” by [[extracting]] [[sequences of labeled tokens]] that [[fit]] a particular [[pattern]], such as [[gene-allele reference association]]s.
On [[chains of reasoning]] in [[WordNet]] [[we]] [[reduce error]] in [[mean quantile]] by 84% versus previous [[state-of-the-art]] .
On each of 21 popular [[dataset]]s from the [[UCI repository]], the [[KDD Cup 09]], [[variant]]s of the [[MNIST dataset]] and [[CIFAR-10]], [[we]] show [[classification performance]] often much better than using [[standard selection]] and [[hyperparameter optimization method]]s.
One approach to [[diversification]] is to [[maximize]] the [[average dissimilarity]] among [[document]]s.
[[One]] assumes that the [[data]] are [[generated by]] a given [[stochastic data model]] .
On [[eBay]], [[product listing title]]s cannot exceed 55 [[character]]s in length.
One characteristic of a [[production function]] is that it defines an [[elasticity of substitution between capital and labor]]: that is, it measures [[how easy it is]] to [[substitute]] [[capital]] for [[labor]], or [[labor]] for [[capital]], to [[produce]] required [[goods and service]]s.
One common [[factor]] links the [[healthiest]] and [[happiest societi]]es: the [[degree of equality]] among their [[member]]s.
One essential [[issue]] of [[document clustering]] is to [[estimate]] the appropriate [[number of clusters]] for a [[document collection]] to which [[documents]] should be partitioned.
[[One exception]] is the <i>[[mixed Kronecker Product Graph Model (mKPGM)]]</i>, a [[generalization]] of the [[Kronecker Product Graph Model]], which uses <i>[[parameter tying]]</i> to capture [[variance]] in the underlying [[distribution]] [10].
One important [[approach]] for [[knowledge discovery]] and [[data mining]] is to [[estimate unobserved variables]] because [[latent variables]] can indicate [[hidden specific properties]] of [[observed data]] .
One important [[business model]] in [[online display advertising]] is [[Ad Exchange marketplace]], also called [[non-guaranteed delivery (NGD)]], in which [[advertiser]]s buy [[targeted page view]]s and [[audience]]s on a [[spot market]] through [[real-time auction]] .
One important [[problem]], known as [[community detection]], is to [[detect]] and [[extract]] the [[community structure]] of [[network]]s.
One important [[task]] in this regard is to employ [[data mining technique]]s to [[rank]] [[patents]] in terms of their potential to earn [[money]] through [[licensing]] .
One is a [[spatial classifier]] based on an [[artificial neural network (ANN)]], which takes [[spatially-related feature]]s (e.g., the [[density of POI]]s and [[length of highway]]s) as [[input]] to model the [[spatial correlation]] between [[air qualiti]]es of different [[location]]s.
One is [[Freebase]], which is [[collaboratively]] collected [[knowledge]] about [[entiti]]es and their [[organization]]s.
One is "[[sensitive content detection]]" where the [[advertiser]] wants to avoid [[content]] relating to [[war]], [[violence]], [[pornography]], etc. even if they occur only in a small part of a [[page]] .
One issue in this [[domain]] is the [[automated]] [[discovery]]/[[retrieval]] of the [[specifications]] for each [[sector]] .
One key [[prerequisite]] of [[harnessing]] the "[[crowd wisdom]] " is the [[independency of individuals' opinion]]s, yet in [[real settings collective opinion]]s are rarely simple [[aggregation]]s of [[independent mind]]s.
One key [[problem of software maintenance]] is the [[difficulty in understanding]] the [[evolving]] [[software systems]] .
One [[long-term]] [[goal]] of [[machine learning research]] is to produce [[method]]s that are applicable to [[reasoning]] and [[natural language]], in particular building an [[intelligent dialogue agent]] .
One needs [[CTR estimate]]s un[[bias]]ed by [[positional effect]] in order for [[ad ranking]], [[allocation]], and [[pricing]] to be based upon [[ad relevance]] or [[quality]] in terms of [[click propensity]] .
One [[non-existent capability]] in the [[EOSDIS]] is the retrieval of [[satellite sensor data]] based on [[weather events]] (such as [[tropical cyclones]]) [[similarity]] [[query output]] .
One of [[key application]]s of [[our work]] is [[social media]] monitoring that can provide [[compani]]es with [[temporal summari]]es of highly [[overlap]]ped or [[discriminative topic]]s with their major [[competitor]]s.
One of [[our finding]]s is that some [[offline metric]]s like [[AUC (the Area Under the Receiver Operating Characteristic Curve)]] and [[RIG (Relative Information Gain)]] that [[summarize]] the [[model performance]] on the entire [[spectrum]] of [[operating point]]s could be quite misleading sometimes and result in significant [[discrepancy]] in [[offline]] and [[online metric]]s.
One of the biggest [[challenge]]s is how to capture all the [[information of a social network]] in a [[unified manner]], such as [[link]]s, [[communiti]]es, [[user attribute]]s, [[role]]s and [[behavior]]s.
One of the critical [[issues]] in the use of the [[Nesterov's method]] is the [[estimation]] of the [[step size]] at each of the [[optimization]] [[iterations]] .
One of the [[data set]]s is obtained from [[Wikipedia]], which comprises approximately [[two million]] [[document]]s.
One of the first [[practical guide]]s to [[mining business data]], it describes [[techniques for detecting]] [[customer behavior pattern]]s useful in formulating [[marketing]], [[sales]], and [[customer support strategi]]es.
One of the [[key obstacle]]s in making [[learning protocol]]s [[realistic]] in [[application]]s is the need to [[supervise them]], a costly [[process]] that often requires hiring [[domain expert]]s.
One of the main current [[challenges]] in [[itemset mining]] is to [[discover]] a [[small set]] of [[high-quality itemsets]] .
One of the most [[popular approach]]es for the [[formal verification of hardware]] and [[software]] relies on [[general-purpose SAT solver]]s and [[SAT encoding]]s, typically with [[hundred]]s of [[thousand]]s of [[variable]]s.
One of the [[salient feature]]s of [[ICU]] is the [[diversity of patient]]s: [[clinician]]s are faced by [[patient]]s with a [[wide variety of disease]]s.
One particular [[incarnation]] of this architecture, [[GoogLeNet]], a [[22]] [[layers deep network]], was used to assess its [[quality]] in the context of [[object detection]] and [[classification]] .
One particularly interesting [[observation]] is that the very weak [[relationship]]s between [[user]]s defined by [[online replies]] have similar [[diffusion]] [[curve]]s as those of [[real friendships]] or [[co-authorships]] .
One possible [[reason]] is that these [[text embedding method]]s [[learn]] the [[representation]] of text in a [[fully unsupervised way]], without leveraging the [[labeled information]] available for the [[task]] .
One [[promising approach]] is to utilize the given [[tree structure]], which describes the [[hierarchical relation]]s among [[task]]s, to [[learn model parameter]]s under the [[regularization framework]] .
One specific [[challenge]] is the [[sharing]] or [[public release]] of [[anonymized data]] without accidentally leaking [[personally identifiable information (PII)]] .
One such [[algorithm]] is [[Gibbs sampling]], a simple [[Monte Carlo algorithm]] that is appropriate for [[inference]] in any [[factored probabilistic model]], including [[sequence model]]s and [[probabilistic context free grammar]]s ([[Geman and Geman, 1984]]).
One such [[problem]] is that [[spatial relationships]] are [[embedded in space]], unknown [[a priori]], and it is part of the [[algorithm's]] [[task]] to determine which [[relationships]] are important and what [[properties]] to consider.
One to many [[recursive relationship]]s may be "[[tree]]s" or "[[forest]]s ".
One type of [[connectivity]], called [[effective connectivity]], defined as the [[directional relationship]] between [[brain region]]s, is essential to [[brain function]] .
One widely used class of [[similarity measures]] is based on [[random walks on graphs]], e.g., [[personalized pagerank]], [[hitting]] and [[commute times]], and [[simrank]] .
On [[large graph]]s, [[our experiment]]s indicate that the [[communiti]]es produced by this [[method]] have better [[conductance]] than those produced by [[PageRank]], although they take slightly longer to [[compute]] .
[[Online advertising]] is becoming more and more [[performance oriented]] where the decision to show an [[advertisement]] to a [[user]] is made based on the [[user's propensity]] to respond to the [[ad]] in a [[positive manner]], (e.g., [[purchasing a product]], [[subscribing]] to an [[email list]]).
[[On-line Advertising]], [[Predictive Modeling]], [[Social Network]]s, [[User-generated content]], [[Privacy]]
[[Online controlled experiment]]s are often utilized to make [[data-driven decision]]s at [[Amazon]], [[Microsoft]], [[eBay]], [[Facebook]], [[Google]], [[Yahoo]], [[Zynga]], and at many other [[compani]]es.
[[Online controlled experiment]]s are widely used to improve the [[performance]] of [[website]]s by comparison of [[user behavior]] related to different [[variation]]s of the given [[website]] .
[[Online experiment]] [[result]]s show that [[Wide & Deep]] significantly increased [[app acquisition]]s compared with [[wide-only]] and [[deep-only model]]s.
[[Online]] [[experiments]] allow for [[technique]]s like [[gradual ramp-up]] of [[treatments]] to avoid the possibility of exposing many [[customers]] to a bad (e.g., buggy) [[Treatment]] .
[[Online forum]]s represent one type of [[social media]] that is particularly rich for [[studying]] [[human behavior]] in [[information seeking]] and [[diffusing]] .
[[Online health communiti]]es are a valuable [[source of information]] for [[patient]]s and [[physician]]s.
[[Online mining]] when such [[data stream]]s [[evolve over time]], that is when [[concepts drift]] or [[change]] completely, is becoming one of the core [[issues]] .
[[Online platform]]s, such as [[Meetup]] and [[Plancast]], have recently become popular for [[planning gathering]]s and [[event organization]] .
[[Online professional network]]s like [[LinkedIn]] have taken these [[resume databank]]s to a [[dynamic]], [[constantly updated]] and [[massive scale]] [[professional profile dataset spanning career record]]s from hundreds of [[industri]]es, millions of [[compani]]es and hundreds of millions of [[people worldwide]] .
[[Online recruiting system]]s have gained immense attention in the wake of more and more [[job seeker]]s [[searching job]]s and [[enterprise]]s finding [[candidate]]s on the [[Internet]] .
[[Online review]]s capture the [[testimonial]]s of [[" real " people]] and help shape the [[decision]]s of other [[consumer]]s.
[[Online review]]s provide [[consumer]]s with valuable [[information]] that guides their [[decision]]s on a variety of fronts: from [[entertainment]] and [[shopping]] to [[medical service]]s.
[[Online Services Division (OSD) within Microsoft]] is a leader in the [[consumer cloud space]] today with a strong [[portfolio]] of a set of 3 [[mutually reinforcing]] [[businesses]]: [[Search]], [[Portal]], [[Advertising]] .
[[Online social network]] information promises to increase [[recommendation]] [[accuracy]] beyond the [[capabiliti]]es of purely [[rating]] / [[feedback-driven [[recommender system]]s (RS)]] .
[[Online social network]]s have become important [[channel]]s for [[user]]s to [[share content]] with their [[connection]]s and [[diffuse information]] .
[[Online social network]]s offering various [[service]]s have become [[ubiquitous]] in our daily [[life]] .
[[Online user review]]s play a central role in the [[decision-making process]] of [[user]]s for a variety of [[task]]s, ranging from [[entertainment]] and shopping to [[medical service]]s.
Only few [[approach]]es for [[clustering of object]]s with [[mixed-type attribute]]s exist and those few [[approach]]es do not consider [[cluster-specific dependenci]]es between [[numerical]] and [[categorical attribute]]s.
On multiple [[real-world dataset]]s, [[our model]] offers excellent [[prediction]] [[accuracy]] and it is [[very compact]], since [[we]] need not [[learn latent state]] but rather just the [[state transition function]] .
[[ONM]] uses [[maximum likelihood estimation]], to represent how the [[information]] contained in a [[ticket]] is used by [[human experts]] to make [[ticket]] [[routing decisions]] .
On one hand, [[E-tree]]s treat [[ensemble]]s as [[spatial database]]s and employ an <i>[[R-tree]]</i> like [[height-balanced structure]] to [[reduce]] the expected [[prediction time]] from [[linear]] to [[sub-linear complexity]] .
On [[production data]] at [[Twitter]], [[we]] demonstrate that [[our proposed technique]]s significantly beat the [[MF]] [[baseline]] and also [[outperform]] [[production model]]s for [[Tweet recommendation]] .
[[On-street parking]], just as any [[publicly owned utility]], is used [[inefficientl]]y if [[access]] is [[free]] or [[priced]] very far from [[market rate]]s.
On [[successful]] [[identification]], the [[travelling pattern]]s of [[tourist]]s are then [[revealed]] and thus allow further analyses to be carried out such as on their [[favorite destination]]s, [[region of stay]], etc. [[Technically]], we model the [[tourists identification]] as a [[classification problem]], and design an [[iterative learning algorithm]] to perform [[inference]] with [[limited prior knowledge]] and [[labeled data]] .
On the basis of a multiple [[abstraction level]]s [[specification process]], [[we]] developed a [[representational model]] for environmental [[robotic knowledge]] through the [[definition]] of a [[set of ontologi]]es using a [[multi perspective approach]] .
On the [[basis of stereotypes' intergroup function]]s, the [[stereotype content model]] hypothesizes that (1) [[2 primary dimension]]s are [[competence]] and [[warmth]], ([[2) frequent mixed cluster]]s combine [[high warmth]] with [[low competence]] ([[paternalistic]]) or [[high competence]] with [[low warmth]] ([[envious]]), and (3) distinct [[emotion]]s ([[pity]], [[envy]], [[admiration]], [[contempt]]) differentiate the 4 [[competence-warmth combination]]s.
On the basis of the whole [[database]], developed [[model]], and [[experimental result]]s, it is easy for us to find some new [[feature]]s or [[population mobility pattern]]s after the recent severe [[earthquake]], [[tsunami]] and [[release of radioactivity]] in [[Japan]], which are likely to play a vital role in future [[disaster]] [[relief]] and [[management worldwide]] .
On the [[cultural]] and [[organizational front]], the larger [[organization]] needs to [[learn the reason]]s for running [[controlled experiment]]s and the [[tradeoff]]s between [[controlled experiment]]s and other [[method]]s of [[evaluating idea]]s.
On the [[engineering]] side, we architected a highly [[scalable system]], able to [[handle data]] at [[massive scale]]: hundreds of [[concurrent experiment]]s, each containing [[million]]s of [[user]]s.
On the first [[set]] of over [[50 million]] [[search instance]]s of [[1.1 million]] distinct [[queries]], [[BBM]] [[outperform]]s the [[state-of-the-art competitor]] by [[29.2%]] in [[log-likelihood]] while being [[57]] [[times]] faster.
On the grammar-based side, [[Bourigault (1992)]] describes a [[system for extracting]] "[[terminological noun phrases]]" from [[French]] [[text]] .
On the [[Internet]], [[human's behavior]]s are [[uncertain]], the [[interaction]]s and [[influence]] among people are also [[uncertain]] .
On the [[MNIST2M dataset]] ([[2M point]]s in 784 [[dimension]]s) for a [[Gaussian kernel]] with [[bandwidth]] <i>h=1 </i>, the [[Nystrom methods' error]] is over 90% whereas our [[treecode]] delivers [[error]]1%.
On the one hand, the [[mapping]] of the particular [[mental model]] of the [[specialist]]s to the provided [[knowledge representation]] and [[interface]]s, respectively, often turned out to be [[difficult]] and [[time-consuming]] .
On the other hand, [[online shop]]s are increasing adopting [[semantic markup language]]s such as [[Microformat]]s, [[RDFa]] and [[Microdata]], to [[annotate]] their [[content]], making [[large amounts]] of [[product description data]] [[publicly available]] .
On the other hand, [[SAME]] dramatically increases the [[parallelism]] in the [[sampling schedule]], and is an excellent [[match]] for [[modern]] ([[SIMD]]) [[hardware]] .
On the other hand, when we do [[classification]], [[data point]]s lying around the [[decision boundary]] ([[boundary point]]s) are [[noisy for learning the correct classifier]] and [[deteriorate]] the [[classification performance]] .
On the [[Penn Treebank dataset]], [[our model]] can compose a novel [[recurrent cell]] that [[outperform]]s the widely-used [[LSTM cell]], and other [[state-of-the-art baseline]]s.
On the second [[click-log set]], [[spanning]] a [[quarter]] of [[petabyte]] [[data]], [[we]] showcase the [[scalability]] of [[BBM]] : [[we]] [[implemented]] it on a [[commercial]] [[MapReduce cluster]], and it took only 3 hours to [[compute]] the [[relevance]] for [[1.15 billion]] distinct [[query-URL pair]]s.
On the selected [[benchmark]]s, [[ours achieve]] up to 11.35% [[relative]] [[accuracy]] improvement compared to the [[state-of-the-art]] [[model]]s.
On these [[website]]s, [[user]]s have created and [[shared]] with each other various kinds of [[resource]]s, such as [[photo]]s, [[video]], and travel [[blog]]s.
On the [[trustworthiness front]], we have a high [[occurrence]] of [[false positive]]s that we address, and we [[alert experimenter]]s to [[statistical interaction]]s between [[experiment]]s.
[[OntoDM-core]] defines the most essential [[data mining entiti]]es in a [[three-layered]] [[ontological structure]] comprising of a [[specification]], an [[implementation]] and an [[application layer]] .
[[OntoLearn]] is a [[system]] for [[automated [[ontology learning]] from domain texts]] that uses the [[WordNet lexical database]] .
[[Ontologi]]es allow a [[fine-tuned]] [[semantic analysis]], as opposed to [[Open Information Extraction]] [5] or [[general-purpose approaches exploiting terminology extraction]] [[on the fly]] [11, 3, 12, 17].
[[Ontologi]]es are used to [[annotate]] [[experimental data]], to assist [[information retrieval]], to enable [[integration of heterogeneous data]], to drive [[literature mining]], and to build [[knowledge base]]s ([[Soldatova et al. 2010]]).
[[Ontologi]]es are used to specify [[controlled vocabulari]]es, which can be used to [[exchange data]] among different [[system]]s, provide services for [[answering queri]]es, [[publish reusable knowledge base]]s, and [[offer service]]s to facilitate [[interoperability]] across multiple, [[heterogeneous system]]s and [[databases (Gruber 2009]]).
[[Ontologi]]es are widely used for [[capturing]] and [[organizing knowledge]] of a particular [[domain of interest]] .
[[Ontologi]]es as [[data]] and [[knowledge models offer logically defined]], [[flexible]] and [[interoperable repre]] - [[sentation]]s of [[principal domain entiti]]es to [[empower information system]]s.
[[Ontologi]]es, as widely used [[model]]s in [[semantic technologi]]es, have much in common with the [[lexicon]] .
[[Ontologi]]es play a major role in [[life science]]s, enabling a number of [[application]]s, from new [[data integration]] to [[knowledge verification]] .
[[Ontologi]]es represent an essential [[component]] to developing [[the Web of Data]] (such as [[Linked Open Data]]1) and [[Semantic Web application]]s.
[[Ontology Authoring]], [[Controlled Natural Language Interface]]s, [[Evaluation of Ontology Building Tools]], [[Geographical Ontologies]]
[[Ontology based annotation]] refers to the [[process]] of [[creating]] [[metadata]] using [[ontologies]] as their [[vocabularies]] .
[[Ontology learning]] is the process of [[acquiring]] ([[constructing]] or [[integrating]]) an [[ontology]] [[(semi-) automatically]] .
[[Ontology Learning]], [[Semantic Relation Learning]], [[Glossary Formalization]] .
[[Ontology learning]] - [[Text mining]] - [[Term clustering]] - [[Concept discovery]] - [[Cluster analysis]] - [[Featureless similarity measure]]s
[[Ontology mapping]] is required for combining [[distributed]] and [[heterogeneous]] [[ontologi]]es.
[[Ontology merging]], [[integration]], and [[alignment]] can be considered as an [[ontology reuse process]] .
:: [[Open Calais]] also [[map]]s your [[metadata-tag]]s to [[Thomson Reuters]] unique [[ID]]s.
[[Open education]]; [[concept hierarchy]]; [[textbook]]s; [[Web knowledge]];
[[Operating budget]]: A budget making [[appropriations]] for the [[ordinary maintenance]] or [[administration]] of [[activiti]]es for the [[fiscal year]], i.e., it does not include [[capital outlay]] [[authorization]]s.
[[Opinion and sentiment analysis]], [[review mining]], [[latent rating analysis]]
[[Opinionated]] [[social media]] such as [[product review]]s are now widely used by [[individual]]s and [[organization]]s for their [[decision making]] .
[[Opinion expressions]] are [[identified]] and [[opinion]] orientations for each recognized [[product entity]] are classified as [[positive]] or [[negative]] .
[[Opinion word]]s are [[word]]s that express [[desirable]] (e.g., great, amazing, etc.) or [[undesirable]] (e.g., bad, poor, etc) [[state]]s.
[[Optimization]] and [[probability]] review, [[games against nature]], [[Bayesian classification]], [[zero-sum game]]s, [[nonzero-sum game]]s, [[Nash equilibria]], [[utility theory]], criticisms of [[decision theory]] .
[[Optimizing]] the [[trade-off]] between [[goodness-of-fit]] and [[model complexity]], [[ROCAT]] automatically determines a [[meaningful number]] of [[cluster]]s to represent the [[data]] .
[[OptRank]] can be characterized as follows: (1) [[Edge]]s and [[node]]s are [[represented by]] [[feature]]s.
[[OQA]] achieves [[robustness]] by decomposing the full [[Open QA problem]] into smaller [[sub-problem]]s including [[question paraphrasing]] and [[query reformulation]] .
OQA solves these [[sub-problem]]s by [[mining million]]s of [[rule]]s from an [[unlabeled question corpus]] and across multiple [[KB]]s.
[[OQA]] then learns to integrate these [[rule]]s by performing [[discriminative training]] on [[question-answer pair]]s using a [[latent-variable structured perceptron algorithm]] .
[[Ordinal classification]] [29] studies the [[problem]] where a [[natural ordering]] exists among all the [[class label]]s.
[[Organizational Change]], [[Data Mining]], [[Customer Lifetime Value]], [[Successful Data Mining]] .
Originating within [[theoretical computer science]], [[this work]] was [[subsequently extended]] and applied in important ways by [[researcher]]s from [[numerical linear algebra]], [[statistics]], [[applied mathematics]], [[data analysis]], and [[machine learning]], as well as [[domain scientist]]s.
Or [[indirectly]], allowing [[agent]]s to [[denounce]] the occurrence of a [[violation]] and then verifying their [[claim]] .
::Other differences have to do with [[nomenclature]], [[notation]], and [[computation]]s.
Other [[learning approaches]] can [[extract]] [[logical forms]], but require [[supervision]] and do not [[scale]] .
Other [[metric]]s such as [[node in-degree]] or even standard [[Pagerank]] focus only in the [[static topology]] of the [[network]] .
Other notions of [[distance]]s such as the [[information theoretic Kullback-Leibler divergence]] and [[statistical]] [[Ï2 distance]], account only for the [[correspondence]] between [[bin]]s with the same [[index]], and do not use [[information]] across [[bin]]s, and are sensitive to [[bin size]] .
Other proteins are important in [[cell signaling]], [[immune response]]s, [[cell adhesion]], and the [[cell cycle]] .
Other reasons deal with [[privacy]] and [[secrecy consideration]]s where [[agent]]s may not be [[comfortable sharing]] their [[data with remote fusion center]]s.
… Other [[researcher]]s have posited a specific [[functional form]], [[hyper-bolic discounting]], to account for [[observed tendenci]]es for [[immediate gratification]] [ see [[Shin-Ho Chung and Richard We]] ...] We have contrived the [[term]] <i> [[resent-biased preference]] </i> as a more [[descriptive term]] for the underlying [[human characteristic]] that [[hyperbolic discounting]] represents.</ref>
Others [[design graph]]s with the [[item]]s as [[node]]s and choose diverse [[item]]s based on [[visit rate]]s ([[PageRank]]).
Others draw attention to significant [[contribution]]s from de [[Bruijn]]'s [[Automath]] and [[Martin-Löf]]'s [[Type Theory]] in the [[1970s]] .
Other [[t–test]]s include the [[one-sample t–test]], which compares a [[sample mean]] to a [[theoretical mean]], and the [[paired t–test]] .
Other types of speech varieties include [[jargon]]s, which are characterized by differences in [[lexicon]] ([[vocabulary]]); [[slang]]; [[patois]]; [[pidgin]]s; and [[argot]]s.
Other [[unsupervised approach]]es, including [[autoencoder]]s and [[recursive autoencoder]]s, also fall out of [[scope]] .
[[Our]] [[advertiser]]s are thus given more valuable [[targeting opportunities]] and better [[ROI]], which in turn, provide better [[economics]], [[usability data]], and allows for a [[higher quality]] [[service]]s for our [[advertiser]]s and [[experience for our users]] .
[[Our algorithm]] applies to both [[pairwise comparison]] and [[rating data]] .
[[Our algorithm]] calculates all possible [[routes]] to [[potential resolvers]] and makes globally [[optimal recommendations]], in contrast to existing [[classification methods]] that make [[static]] and [[locally optimal]] [[recommendations]] .
[[Our algorithm]] can more accurately [[predict distribution]]s over such [[label]]s compared to several [[competitive baseline]]s.
[[Our algorithm]] has a simple [[tunable parameter]] for [[users]] to control the [[balance]] between the [[running time]] and the [[influence spread]] of [[the algorithm]] .
[[Our algorithm]] is a simple [[iterative scheme]] with [[guaranteed convergence]], and is modular; by [[changing]] the [[internals]] of a single [[subroutine]] in the [[algorithm]], [[we]] can switch [[cost functions]] and [[target space]]s easily.
[[Our algorithm]] is based on a [[recursive application]] of [[fingerprinting via shingles]], and is extremely [[efficient]], capable of [[handling graph]]s with tens of [[billion]]s of [[edge]]s on a [[single machine]] with modest [[resource]]s.
[[Our algorithm]] is based on [[intelligent caching]] and [[reuse]] of [[computation]]s, and the [[admissible pruning]] of the [[search space]] .
[[Our algorithm]] is called [[Active Vote]], and it works by [[actively selecting]] [[instance]]s that force several [[perturbed copi]]es of an [[on-line algorithm]] to make [[mistake]]s.
[[Our algorithm]] is [[design]]ed to use "[[clickthrough]]" [[information]] as it is provided by the [[user]]s to improve future [[ranking]] [[decisions]] .
[[Our algorithm]] is [[distributed]] and operates as a [[vertex program]] over the [[GraphLab PowerGraph framework]] .
[[Our algorithm]] is easily [[implement]]able in [[computational model]]s such as [[MapReduce]] and [[streaming]], and runs in a small number of [[round]]s.
[[Our algorithm]] is formally a [[relaxation method]] for solving a [[linear system]] to [[estimate]] the [[matrix exponential]] in a [[degree-weighted norm]] .
[[Our algorithm]], referred to as [[AdaRank]], repeatedly constructs '[[weak rankers]]' on the basis of [[reweighted training data]] and finally [[linearly combines]] the [[weak rankers]] for making [[ranking predictions]] .
[[Our algorithm]] requires [[minimal modification]]s to incorporate most [[loss function]]s in a variety of [[supervised task]]s, and we observe in [[our experiment]]s an [[order of magnitude]] [[speedup]] over the current [[state-of-the-art]] [[implementation]], while achieving [[similar]] [[prediction performance]]s.
[[Our algorithm]]s and [[analysis]] require novel [[technique]]s as they involve [[online computation]] of multiple [[dual variable]]s per [[ad]] .
[[Our algorithm]]s handle this by returning [["noisy" lists]] of [[patterns]] that are close to the [[actual list]] of [[<i>k</i> most frequent patterns]] in the [[data]] .
[[Our algorithm]]s [[satisfy]] <i>[[differential privacy]]</i>, a recently [[introduced definition]] that provides meaningful [[privacy guarantees]] in the presence of [[arbitrary external information]] .
[[Our algorithm]]s use [[sampling]] to [[decrease]] the [[data size]] and they run a [[time consuming]] [[clustering algorithm]] such as [[local search]] or [[Lloyd's algorithm]] on the resulting [[data set]] .
[[Our algorithm]] uncovers [[operationally]] [[significant]] [[event]]s in [[high dimensional data stream]]s in the [[aviation industry]] which are not [[detectable]] using [[state of the art methods]] .
[[Our algorithm]] uses as a [[black-box]] any [[active learning module]] that [[minimize]]s 0-1 [[loss]] .
[[Our algorithm]] uses the novel [[concept]] of [[3-profile sparsifier]]s: [[sparse graph]]s that can be used to [[approximate]] the full [[3-profile]] [[count]]s for a given [[large graph]] .
[[Our analysis]] addresses both their [[individual properti]]es (including [[temporal dynamics]]) and their [[inter-relationship]]s.
[[Our analysis]] consistently shows that (1) [[users]]' [[posting]] [[behavior]] in these [[networks]] exhibits strong [[daily]] and [[weekly]] [[patterns]], but the [[user]] [[active time]] in these [[OSNs]] does not follow [[exponential distributions]]; (2) the [[user]] [[posting]] [[behavior]] in these [[OSNs]] follows [[stretched]] [[exponential distributions]] instead of [[power-law distributions]], indicating the influence of a [[small number]] of [[core]] [[users]] cannot dominate the [[network]]; (3) the [[distributions]] of [[user contributions]] on [[high-quality]] and [[effort-consuming]] [[contents]] in these [[OSNs]] have smaller [[stretch]] [[factors]] for the [[stretched]] [[exponential distribution]] .
[[Our]] [[approach]] allows to [[predict drug-target interaction]]s by the two [[low-rank matrices collaboratively]] and to [[detect similariti]]es which are important for [[predicting drug-target interaction]]s.
[[Our approach]] analyzes a [[sequence]] of [[graph]]s and [[discover]]s [[rule]]s that capture the changes that occur between [[pairs]] of [[graph]]s in the [[sequence]] .
[[Our approach]] assigns [[importance score]]s to [[entiti]]es based on both the [[number]] and the [[quality]] of [[set-cover solution]]s that they participate.
[[Our approach]], called [[C3]], consists of [[Coupled Collective Classifier]]s that are [[iteratively]] applied to [[propagate information]] among [[solution]]s to the problems.
[[Our approach]] [[clusters]] [[similar]] [[hierarchies]] using their [[structure]] and [[tag statistics]], then incrementally weaves them into a [[deeper]], [[bushier tree]] .
[[Our approach]] comprises an [[algorithm]] that evenly [[distributes the workload]] to [[core]]s, and an [[RDF]] [[indexing data structure]] that supports [[efficient]], 'mostly' [[lock-free parallel update]]s.
[[Our approach]] enables the [[assessement]] of the [[structural dissimilarity]] among the [[output]] of multiple [[community detection algorithm]]s and between the [[output]] of [[algorithm]]s and [[communiti]]es that arise in practice.
[[Our approach]] enjoys several attractive properties, including [[ease of training]], [[fast performance]] at [[test time]], and the ability to [[robustly tolerate]] [[corrupted training data]] using a novel [[latent variable approach]] .
[[Our approach]] for [[discovering]] [[anomalous windows]] along [[linear paths]] comprises of the following distinct [[steps]] : (a) [[Cross Path Discovery]] : where we identify a [[subset]] of [[intersecting]] [[paths]] to be considered, (b) [[Anomalous Window Discovery]] : where we outline three [[order]] [[invariant]] [[algorithms]], namely [[SSLIP]], [[Brute Force-SSLIP]] and [[Central Brute Force-SSLIP]], for the [[traversal]] of the [[cross paths]] to identify varying size [[directional windows]] along the [[paths]] .
[[Our]] [[approach]] has shown to greatly alleviate the curse of [[cardinality]] in challenging [[task]]s of [[sequential pattern mining]] and [[clustering]] .
[[Our approach]] [[identifies]] not just [[leaking candidates]] and their [[structure]], but also provides [[aggregate information]] about the [[access path]] to the [[leaks]] .
[[Our approach]] in [[this paper]] is to [[extract feature]]s that [[reveal]] when [[Web page]]s [[linked]] to the same [[affiliate program]] share a similar underlying [[structure]] .
[[Our approach]] is (a) capable of [[capturing]] [[cyberphysical interaction]]s and [[automatically learning]] them from [[data]]; (b) [[computationally]] and [[physically scalable]] to [[data center scale]]s; (c) able to provide [[online prediction]] with [[real-time]] [[sensor measurement]]s.
[[Our approach]] is based on a [[block-coordinate descent framework]] and is capable of utilizing only the most [[representative]], thus [[meaningful]], [[keyword]]s in each [[topic]] through a novel [[pseudo-deflation approach]] .
[[Our approach]] is based on a [[linear multi-task learning formulation]], in which the [[sparse]] and [[low-rank patterns]] are induced by a [[cardinality regularization term]] and a [[low-rank]] [[constraint]], respectively.
[[Our approach]] is based on a [[model]] that [[predicts response]] as a [[multiplicative function]] of [[row]] and [[column latent factors]] that are [[estimated]] through separate [[regressions]] on known [[row]] and [[column features]] .
[[Our approach]] is based on extending [[information]] [[regularization framework]] to [[boosting]], bearing [[loss functions]] that combine [[log loss]] on [[labeled data]] with the [[information-theoretic measures]] to encode [[unlabeled data]] .
[[Our approach]] is based on [[mining time-ordered text stream]]s, [[representing]] [[retail transaction]]s, formed from a combination of [[visually detected checkout]] related [[activiti]]es called [[primitive]]s and [[barcode]]s from [[TLog data]] .
[[Our approach]] is based on simple [[information theoretic principle]]s, and achieves improved [[performance]] across a variety of [[language]]s without requiring extensive [[pre-processing]] or [[feature selection]] .
[[Our approach]] is based on the [[Functional Influence Model]] that [[quantifies]] the [[influence]] of a [[biological component]] on another.
[[Our approach]] is based on the insight that we can define [[badge]]s using [[high precision]], [[low recall rule]]s (e.g.," [[Twitter profile]] contains the [[phrase]] ' [[Apple fanboy]]' ), and with enough [[data]], generalize to other [[user]]s by observing [[shared behavior]] .
[[Our approach]] is capable of [[discovering]] three important [[aspect]]s of [[event-timeseries correlation]] in the context of [[incident diagnosis]]: existence of [[correlation]], [[temporal order]], and [[monotonic effect]] .
[[Our approach]] is distinguished from [[earlier work]] in that it [[cleanly]] and [[efficiently]] handles [[sparse dataset]]s through the use of a novel [[clustering technique]] .
[[Our approach]] is highly [[scalable]] as it relies on [[FastFood]], a [[randomized]] [[explicit feature representation]] for [[kernel embedding]]s.
[[Our approach]] is to introduce a [[random graph model]] for a version of the [[de-anonymization problem]], which is [[parameterized]] by the [[expected node degree]] and a [[similarity parameter]] that controls the [[correlation]] between two [[graph]]s over the same [[vertex set]] .
[[Our approach]] not only wins out in all [[scenario]]s, but on the [[average]] achieves 9.90% [[AUC]] and 12.59% [[NDCG]] improvement over the best [[competitor]]s.
[[Our approach]] offers [[robust improvement]]s in [[mean average precision]] compared to the [[standard binary relevance approach]] across all 12 [[dataset]]s involved in [[our experiment]]s.
[[Our approach]] outperforms several other [[state-of-the-art]] [[image retrieval algorithm]]s for [[large scale]] [[image data]] .
[[Our approach]] outperforms standard [[cluster analysis]] and several previous [[method]]s.
[[Our approach]] relies on storing [[user-specific information]] entirely within the [[user's control]] (in a [[browser cookie]] or [[browser local storage]]), thus allowing the [[user]] to [[view]], [[edit]] or [[purge]] it at any [[time]] (e.g., via a dedicated [[webpage]]).
[[Our approach]] runs in 0.25 seconds and [[we]] additionally demonstrate a near [[real-time]] [[variant]] with only [[minor loss]] in [[accuracy]] .
[[Our approach]] seamlessly integrates [[heterogenous data type]]s [[measured]] at different [[scale]]s, most importantly [[continuous numerical]] and [[discrete categorical data]] .
[[Our approach]] simultaneously performs both [[constituent selection]] and [[labelling]], by defining an [[undirected random field]] over the [[parse tree]] .
[[Our approach]] to find the [[statistical significance]] takes into account the [[autocorrelation]], the [[seasonality]] and the [[trend]] in the [[time series]] over a [[period of time]] .
[[Our approach]] to giving [[formal semantics]] to such [[speech act]]s uses [[operational semantics]] and builds upon existing work that provides [[computationally grounded semantics]] for [[agent mental attitude]]s such as [[belief]]s and [[goal]]s.
[[Our approach]] utilizes a [[k-nearest-neighbor]] [[network construction method]] to capture the [[topology]] [[embedded]] in [[high dimensional data]], and applies a [[modularity-based algorithm]] to identify the [[optimal]] [[community structure]] .
[[Our assumption]] is that each [[user]] has an underlying [[topic interest distribution]] over various [[named entiti]]es.
[[Our assumption]] is that [[entitie]]s mentioned in a [[Web list]] can be any [[collection]] of [[entiti]]es that have the same [[conceptual type]] that [[people]] have in mind.
[[Our assumption]] is that [[syntax]] is less [[dependent]] than [[learned pattern]]s from the [[length]] and the [[complexity]] of [[textual expression]]s.
[[Our basic approach]] to [[uncovering]] [[pathological structure]] within the [[ECG]] focuses on [[characterizing]] [[beat-to-beat time-warped shape deformation]]s of the [[ECG]] using a modified [[dynamic time-warping (DTW)]] and [[Lomb-Scargle periodogram-based algorithm]] .
[[Our]] basic [[quality measure]] is the [[expected percentile ranking]] of a [[watching unit]] in the [[test period]], which is:
Our best guess is that "human-level" [[TEPS performance]] will cost$100/hour in [[seven]] to [[fourteen]] [[year]]s.
[[Our classifier]] uses [[hidden Markov models (HMMs)]] and [[decision tree]]s to [[classify]] [[name]]s into 13 [[cultural]]/[[ethnic group]]s with [[individual]] [[group]] [[accuracy]] comparable [[accuracy]] to earlier [[binary]] (e.g., [[Spanish]]/non-Spanish) [[classifiers]] .
Our comprehensive [[experimental studi]]es demonstrate that [[our proposed strategy]] drives the [[detection cost]]s [[100-fold]] down into practical realm for [[application]]s producing [[high volume]] [[trajectory stream]]s to utilize.
Our comprehensive [[results]] show [[the proposed method]] [[outperform]]s several [[state-of-the-art approaches]] for all the [[tasks]], with a [[significant gain]] for most [[tasks]] .
[[Our contribution]] is a number of [[randomized approximation algorithm]]s, [[categorized]] according to the available [[space]] ([[superlinear]], [[linear]], and [[sublinear]] in the number of [[nodes]] <i>n </i>) and according to different [[model]]s ([[landmark]] and [[sliding window]]).
[[Our contribution]] is the use of a [[bi-relational directed graph]] that [[capture]]s [[relationship]]s between [[pair]]s of [[protein]]s, between [[pairs of function]]s, and between [[protein]]s and [[function]]s.
[[Our]] [[contributions]] here are twofold: (a) we present an [[efficient]] [[deterministic algorithm]] to [[find the <math>k</math> closest neighbors]] (in terms of [[personalized pagerank]]) of any [[query node]] in such a [[clustered graph]], and (b) [[we]] develop a [[clustering algorithm]] ([[RWDISK]]) that uses only [[sequential sweeps]] over [[data files]] .
[[Our contribution]]s include an [[architecture]] for [[extracting]] [[action-outcome relationship]]s from [[social media data]], [[techniques for identifying]] [[experiential social media message]]s and converting them to [[event timeline]]s, and an [[analysis]] and [[evaluation]] of [[action-outcome extraction]] in [[case studi]]es.
[[Our contribution]]s involve [[Pythia]], a [[framework]] and [[algorithm]]s for providing the answers to the [[above question]]s and for engaging the appropriate [[subset]] of [[cohort]]s per [[MV imputation request]] .
Our [[core ideas]] allow useful [[extensions]] of [[our algorithm]] to deal with [[arbitrary]] [[data rates]] and [[discovering multidimensional motifs]] .
[[Our corpus]] contains 4.200.000 words from 11.000 [[paper]]s ([[abstracts and introduction]]s) in the [[ACL Anthology Corpu]]s, [[pre-processed]] by [[E]] .
[[Our decoding algorithm]] mainly requires one [[linear scan]] of the [[coordinate]]s, followed by a few [[iteration]]s on a [[small number of coordinate]]s which are "[[undetermined]] " in the previous [[iteration]] .
[[Our dynamic graph-based relational mining approach]] has been developed to [[learn]] [[structural]] [[pattern]]s in [[biological network]]s as they [[change over time]] .
[[Our]] [[easy-to-use]] [[library]] is [[orders of magnitude]] faster then existing [[CPU librari]]es, and several times faster than prior [[GPU approach]]es.
[[Our empirical evaluation]]s on [[synthetic]] and [[real-world data]] show that both of the proposed [[algorithm]]s achieve higher [[accuracy]] than existing [[convex formulation]]s.
Our [[empirical evaluation]]s show that, both [[SFA]] and [[EFLA]] significantly [[outperform]] [[existing]] [[solvers]] .
[[Our empirical experiments]] on various [[data sets]] demonstrate that [[our combined classifier]] produces "[[effective]]" [[results]] when [[compared]] with a [[single classifier]] .
[[Our empirical result]]s on a [[large-scale]] [[news article]] [[recommendation dataset]] collected from [[Yahoo! Front Page]] conform well with our [[theoretical result]]s.
[[Our empirical result]]s on several [[challenging]], [[real-world dataset]]s from multiple [[domain]]s, corroborate the potential of [[the proposed framework]] for [[real-world hierarchical classification application]]s.
[[Our empirical result]]s, over a [[corpus]] of over 30 million [[microblog post]]s, show that [[TM-LDA]] significantly outperforms [[state-of-the-art]] [[static LDA model]]s for [[estimating]] the [[topic distribution]] of new [[document]]s over [[time]] .
[[Our empirical studi]]es on several [[real-world]] [[benchmark dataset]]s show that [[the proposed algorithm]] [[outperform]]s the [[state-of-the-art]] [[semi-supervised]] [[clustering algorithm]]s with visible [[performance gain]] and significantly reduced [[running time]] .
[[Our empirical studi]]es on [[UCI dataset]]s show that incorporation of [[uncertainty]] information improves [[performance]] at later [[iteration]]s while our [[studi]]es on 20 [[Newsgroups dataset]] show that [[transfer learning]] improves the [[performance]] of the [[classifier]] during [[initial iteration]]s.
[[Our empirical study]] on a [[real-world data set]] demonstrates the [[effectiveness]] of [[our method]] .
[[Our empirical study]] on the [[benchmark dataset]]s demonstrates that the [[LOGM algorithm]] yields higher [[classification accuraci]]es than the [[MCE]], [[generalized learning vector quantization (GLVQ)]], [[soft nearest prototype classifier (SNPC)]] and the robust [[soft learning vector quantization (RSLVQ)]], and moreover, the [[LOGM]] with [[prototype-dependent weighting]] achieves comparable [[accuraci]]es to the [[support vector machine (SVM) classifier]] .
Our [[empirical study]] shows the [[efficiency]] of the [[proposed algorithm]] based on the [[maximum]] [[flow]] .
[[Our estimator]]s are admissible ([[Pareto optimal]] in terms of [[variance]]) and have compelling [[properti]]es.
[[Our evaluation]] indicates that [[our proposed online-update method]]s are accurate in [[approximating a full retrain]] of a [[RKMF model]] while the [[runtime]] of [[online-updating]] is in the range of [[millisecond]]s even for [[huge dataset]]s like [[Netflix]] .
[[Our evaluation]] on three [[product categori]]es related to [[electronic]]s show [[promising result]]s in terms of [[enriching]] [[product ad]]s with useful [[product data]] .
[[Our evaluation]] reveals the existence of [[bandwidth value]]s that should be examined in [[cross-validation]] but whose corresponding [[kernel matrice]]s cannot be [[approximated]] well by [[Nystrom method]]s.
[[Our evaluation]] shows that [[our proposed weighting method]] can [[accurately sample]] given [[log]]s, and be [[compatible]] only with [[previous algorithm]]s for [[real time recommendation]]s.
[[Our experimental assessment]] on [[real-world data confirm]]s the [[accuracy]] of [[WTFW]] in the [[link prediction]] and the [[quality]] of the associated [[explanation]]s.
[[Our experimental assessment]], through [[user studi]]es, demonstrates that [[NaLIR]] is good enough to be [[usable in practice]]: even [[naive user]]s are able to specify quite [[complex]] [[ad-hoc queri]]es.
[[Our experimental evaluation]] confirms the [[practicality]] of [[our approach]] on [[real dataset]]s, [[outperforming approach]]es based on [[one-shot static sampling]] .
[[Our experimental evaluation]]s on three [[real dataset]]s demonstrate that [[our method]] (1) outperforms its [[competitor]]s in two common [[data mining task]]s ([[imputation]] and [[prediction)]]; and (2) enjoys a [[linear scalability]] w.r.t. the [[length]] of [[time series]] .
[[Our]] [[experimental results]] confirm the [[equivalence relationship]] established in [[this paper]] .
[[Our experimental result]]s demonstrate that [[our approach]] is capable of producing [[output]] of high [[usability]] and [[our algorithm]]s achieve superior [[efficiency]] over classic [[MCE algorithm]]s.
Our [[experimental result]]s demonstrate that [[the proposed algorithm]] is promising in revealing the [[brain region connectivity]] [[difference]]s among these [[group]]s.
[[Our experimental result]]s demonstrate the efficacy and the [[efficiency]] of [[our method]]s in [[multiple]] [[real-world dataset]]s obtained from different [[application domain]]s.
Our [[experimental results]] demonstrate their [[effectiveness]] and [[efficiency]] on different [[problems]], including the [[Netflix Prize]] [[data]] .
[[Our experimental result]]s indicate that the [[mixture-KDE method]] provides a useful and [[accurate methodology]] for [[capturing]] and [[predicting individual-level spatial pattern]]s in the presence of [[noisy]] and [[sparse data]] .
Our [[experimental results]] on a [[real world dataset]] indicate that [[changes]] in [[temperature]] are not solely accounted for by [[solar radiance]], but [[attributed]] more significantly to [[CO2]] and other [[greenhouse gases]] .
[[Our experimental result]]s on [[benchmark data set]]s demonstrate the [[effectiveness]] and [[efficiency]] of [[the proposed algorithm]] .
[[Our experimental result]]s on four different types of [[large social network]]s, i.e., [[Flickr]], [[Gowalla]], [[Weibo and Co-Author]], verify the existence of the [[conformity phenomena]] .
[[Our experimental result]]s on [[simulation data set]]s and two [[real data set]]s demonstrate the [[effectiveness]] of the [[algorithm]] .
[[Our experimental result]]s on two [[real-world dataset]]s show that [[the proposed approach]] outperforms the [[state-of-the-art]] [[POI recommendation method]]s substantially.
[[Our experimental result]]s show interesting [[characteristic]]s of different [[formulation]]s and our findings may provide valuable [[guidance]] to the [[design]] of [[recommendation engine]]s for [[web portal]]s.
Our [[experimental results]] show that : (1) [[<i>l</i><sub>1</sub>-M<sup>3</sup>N]] can effectively [[select]] significant [[features]]; (2) [[<i>l</i><sub>1</sub>-M<sup>3</sup>N]] can perform as well as the [[pseudo]]-[[primal sparse]] [[Laplace]] [[M<sup>3</sup>N]] in [[prediction]] [[accuracy]], while consistently outperforms other competing [[methods]] that enjoy either [[primal]] or [[dual sparsity]]; and (3) the [[EM-algorithm]] is more robust than the other two in prediction [[accuracy]] and [[time]] efficiency.
[[Our experimental result]]s validate the [[effectiveness]] and [[efficiency]] of [[the proposed model]] .
[[Our experimental result]]s with [[artificial]] and [[real-world sponsored search data]] show the soundness of the [[underlying model assumption]], which in turn yields superior [[prediction]] [[accuracy]] .
[[Our experimental studi]]es demonstrate the effectiveness of [[the proposed algorithm]] for [[capturing]] the [[progression trend]] and the [[cross-sectional]] [[group difference]]s of [[AD]] severity.
[[Our]] [[experimental studi]]es demonstrate the [[utility]] of [[the proposed method]]s.
[[Our experimental study]] with large, [[high-length time-series data]] confirms the advantage of [[our approach]] over both the current [[state-of-the-art method]], [[iSAX]], and [[classical]] [[index-based method]]s.
[[Our experiment result]]s show that [[MinRPset]] and [[FlexRPset]] produce fewer [[representative pattern]]s than [[RPlocal]] --- an [[efficient algorithm]] that is developed for solving the same [[problem]] .
[[Our experiment]]s also suggest that different [[weighting scheme]]s result in [[varied]] [[false]] and [[true positive performance]]s on the [[task]] of [[fake scan detection]] .
[[Our experiment]]s corroborate [[the algorithm's scalability]] and improved [[convergence rate]] .
[[Our experiment]]s demonstrate that [[modeling]] [[trust propagation]] leads to a substantial increase in [[recommendation accuracy]], in particular for [[cold start user]]s.
[[Our experiment]]s on both [[real]] and [[synthetic data set]]s show that [[Assember]] is [[effective]], [[efficient]], and [[scalable]] .
[[Our experiment]]s on both [[synthetic]] and [[real-world dataset]]s demonstrate that the [[APM algorithm]] accelerates [[P-RFP mining]] dramatically, [[orders of magnitude]]s faster than an [[exact solution]] .
[[Our experiments]] on [[DBLP data]] show that [[NetClus]] generates more [[accurate]] [[clustering]] [[results]] than the [[baseline]] [[topic model algorithm]] [[PLSA]] and the recently proposed [[algorithm]], [[RankClus]] .
[[Our experiment]]s on [[large text corpora]] demonstrate the [[quality]] and [[efficiency]] of [[the new method]] .
[[Our experiment]]s on large [[undirected real-world graph]]s show that [[the algorithm]] based on [[two-pass breadth-first searche]]s works surprisingly well, [[outperforming]] the other [[algorithm]]s in terms of [[running time]] and/or [[accuracy]] by up to [[orders of magnitude]] .
[[Our experiment]]s on multiple [[genre]]s --- [[news]], [[Yelp review]]s and [[tweet]]s --- demonstrate the [[effectiveness]] and [[robustness of ClusType]], with an [[average]] of 37% improvement in [[F1 score]] over [[the best compared method]] .
[[Our experiment]]s on [[real data]] from different [[entity domain]]s demonstrate the superior quality of [[our synonym]]s as well as the [[efficiency]] of [[our algorithm]]s.
[[Our experiment]]s on [[real-life data set]]s show that [[the proposed algorithm]] finds [[diverse]] and [[interesting]] [[trajectory pattern]]s and [[identifi]]es the [[semantic region]]s in a [[finer granularity]] than the traditional [[geographical clustering method]]s.
[[Our experiment]]s on [[real-world network dataset]]s show the [[effectiveness]] and [[scalability]] of [[the proposed algorithm]] as compared to the [[state-of-the-art embedding method]]s.
[[Our experiment]]s on several [[benchmark dataset]]s show that [[Deep Graph Kernel]]s achieve significant [[improvement]]s in [[classification accuracy]] over [[state-of-the-art]] [[graph kernel]]s.
[[Our experiment]]s on [[synthetic]] and [[real data]] demonstrate that [[the proposed algorithm]] produces [[encouraging performance]] while keeping comparable [[computational efficiency]] to [[convex relaxation model]]s.
[[Our experiment]]s on the [[English]]-[[Spanish]] pair in the [[domains of Computer Science]], [[Science]], and [[Sport]]s show that our [[in-domain translator]] performs significantly better than a [[generic one]] when translating [[in-domain Wikipedia article]]s.
[[Our experiments]] ran on [[M45]], one of the top 50 [[supercomputers]] in the [[world]] .
[[Our experiment]]s showed a [[consistent improvement]] over other [[competing approach]]es and achieved [[state-of-the-art]] [[result]]s on [[benchmark dataset]]s.
[[Our experiment]]s show improvements in [[prediction accuracy]] over [[classical approach]]es for [[recommendation task]]s.
[[Our experiment]]s show [[state-of-the-art performance]]s on both [[task]]s on 6 different [[dataset]]s, including a [[multilingual setting]] .
[[Our experiment]]s show that [[Confluence]] significantly improves the [[prediction]] [[accuracy]] by up to 5-10% compared with several [[alternative method]]s.
[[Our experiment]]s show that [[ItemRank]] performs better than the other [[algorithms]] [[we]] compared to and, at the same time, it is [[less complex]] than [[other proposed algorithm]]s with respect to [[memory usage]] and [[computational cost]] too.
[[Our experiment]]s show that [[our algorithm]] can analyse [[real trace]]s of up to two [[hour]]s of [[video]] .
[[Our experiment]]s show that [[our approach]] is up to seven [[orders of magnitude]] [[faster]] than the existing [[alternative]]s.
[[Our experiment]]s show that [[our solution]] is more [[effective]] than traditional [[IM method]]s on the [[partial information]] .
[[Our experiment]]s show that [[shallow bagged tree]]s with [[gradient boosting]] distinguish itself as the best [[method]] on [[low -to medium-dimensional dataset]]s.
[[Our experiment]]s show that [[ShoppingAdvisor]] produces good quality [[interpretable recommendation]]s, while requiring less [[input]] from [[user]]s and being able to handle the [[cold-start problem]] .
[[Our experiment]]s show that the algorithm learns useful [[high-level visual feature]]s, such as [[object part]]s, from [[unlabeled]] [[object images data]] and [[natural scene]]s.
Our [[experiments]] show that the [[model]] is able to capture both the newly observed and [[previously studied]] [[network]] [[properties]] .
[[Our experiment]]s show that the proposed [[WhoAmI method]] significantly improves the [[prediction]] [[accuracy]] by up to 10% [[compared]] with several [[alternative method]]s.
[[Our experiment]]s show that using the new [[LSH]] in [[nearest-neighbor application]]s can improve their [[running time]]s by [[significant amount]]s.
[[Our experiment]]s suggest that [[item-based algorithm]]s provide dramatically better [[performance]] than [[user-based algorithm]]s, while at [[the same time providing]] better [[quality]] than the best available [[user-based algorithm]]s.
[[Our experiment]]s with [[artificial data involve local]], [[distributed]], [[real-valued]], and [[noisy pattern representation]]s.
[[Our experiment]]s with [[generated data coming]] from different [[distribution]]s demonstrate that [[our algorithm]] is significantly better than the [[current strategi]]es in vogue for dividing students in a [[class]] into [[section]]s.
[[Our experiment]]s with [[real review corpora]] on different types of [[product]]s demonstrate the utility of [[our method]]s, and our [[user studi]]es indicate that [[our method]]s provide a better [[summary]] than [[prior approach]]es.
[[Our experiment]]s with several [[clustering task]]s in two [[real network]]s demonstrate the power of [[the algorithm]] in [[comparison]] with the [[baseline]]s.
Our extensive [[empirical study]] with multiple [[real-world data set]]s verifies the efficacy of [[the proposed approach]] in [[learning]] the [[exploration]] vs. [[exploitation tradeoff parameter]] .
Our extensive [[experiments]] show that [[social network]]s and the [[Web]] [[graph]] exhibit vastly different [[compressibility]] [[characteristics]] .
Our [[Factorization Machine implementation]] ( [[<i>fast</i>FM]]) provides easy access to many [[solver]]s and supports [[regression]], [[classification]] and [[ranking task]]s.
[[Our feature-based algorithm]] combines [[knowledge about <i>content</i>]] using a [[text-based algorithm]] as a [[feature]] and about <i>form</i> using [[linguistic]] and [[acoustic cue]]s about [[topic shift]]s [[extracted from speech]] .
[[Our finding]]s also confirm that [[inaccurate conclusion]]s can be [[reached]] when not controlling for [[content]] .
Our findings also underscored the importance of good [[information retrieval]] and of the ability to [[disambiguate]] between [[genes]], [[proteins]], [[RNA]], and a variety of other [[referents]] for performing [[entity identification]] with high [[precision]] .
[[Our finding]]s indicate that [[information overload]] and [[use]]s and [[gratification]]s are two major [[theori]]es for explaining [[user satisfaction]] with [[personalized service]]s.
[[Our]] [[findings]] suggest that the [[expert]] can, and should, provide more [[information]] than [[instance labels]] alone.
Our focus is on the [[operations research]] and [[management science literature]], but we also discuss relevant contributions from [[marketing]], [[economic]]s, [[econometric]]s, and [[computer science]] .
[[Our framework]] can be used for the situations such as [[ERP 2]], i.e., [[data provider]]s would like to [[control their]] own [[privacy polici]]es and/or the [[workload]] of [[DSMS server]] needs to be [[reduced]] .
[[Our framework]] can handle the [[semi-structured text]] in alerts [[generated by]] [[IT infrastructure component]]s such as [[storage device]]s, [[network device]]s, [[servers etc.]], as well as the [[unstructured text]] in [[incident ticket]]s created manually by [[operations support personnel]] .
[[Our framework]] can mine common as well as individual <i>[[shift-invariant temporal pattern]]s</i> from [[heterogeneous event]]s over different [[patient group]]s, and [[handle]] [[sparsity]] as well as [[scalability problem]]s well.
[[Our framework]] extends to [[embedding]] [[high-dimensional points]] lying on a [[sphere]] to [[points]] on a [[lower dimensional]] [[sphere]], preserving [[geodesic distance]]s.
[[Our framework]] further [[learn]]s [[discriminant malware distance metric]]s that evaluate the [[similarity]] between the [[attributed function call graph]]s of two [[malware program]]s.
[[Our framework]] leverages the [[dependency information]] between [[sub-structure]]s by <i>[[learning]]</i> their [[latent representation]]s.
[[Our goal]] in [[this paper]] is to [[learn]] the [[semantic]]s of [[substitute]]s and [[complement]]s from the [[text of online review]]s.
[[Our]] goal is to combine the rich [[multistep inference]] of [[symbolic logical reasoning]] with the [[generalization capabiliti]]es of [[neural networks]] .
[[Our]] [[goal]] is to construct the [[classification model]] for the [[target class]] by leveraging the above [[data and information]] .
[[Our goal]] is to [[discover contention]] and [[agreement indicator expression]]s, and [[contention point]]s or [[topic]]s both at the [[discussion collection level]] and also at each individual [[post level]] .
[[Our goal]] is to [[leave reader]]s with an [[appreciation]] of the various ways in which to [[measure user engagement]], and their associated [[strength]]s and [[weaknesse]]s.
[[Our goal]] is to [[model this interaction]] between [[belief]]s, [[intention]]s, and [[utterance]]s.
[[Our graph model]], derivable from [[protein sequence]] and [[structure]] only, is competitive with [[vector model]]s that require additional [[protein information]], such as the [[size]] of [[surface pocket]]s.
[[Our]] [[high-level methodology]] consists of examining a [[collection]] of 6,000 [[software project]]s and [[measuring]] the degree to which each [[project]] can be 'assembled' solely from portions of this [[corpus]], thus providing a [[precise]] [[measure of `uniqueness']] that we call <i>[[syntactic redundancy]] </i>.
[[Our]] implementation further employs an [[approximation algorithm]] that combines [[iterative deepening]] with [[binary decision diagram]]s.
[[Our implementation]] of [[MLGF-MF]] is available at http://dm.postech.ac.kr/MLGF-MF as executable [[file]]s.
Our initial [[application]] is for the [[manufacture]] of [[microprocessor]]s, and we [[predict]] [[final microprocessor speed]] .
Our [[interactive prioritization component]] is built on top of a [[batch classifier]] that has been [[trained]] to [[detect payment error]]s in [[health insurance claim]]s and [[optimize]]s the [[interaction]] between the [[classifier]] and the [[domain expert]]s who are consuming the results of [[this system]] .
[[Our interest]] is in the [[temporal history]] of [[event code]]s [[embedded]] in [[patients' record]]s, specifically investigating [[frequently occurring sequence]]s of [[event code]]s across [[patient]]s.
[[Our intuition]]s are embodied by a [[rich set of content]], [[social]] and [[user feature]]s capturing the [[aforementioned aspect]]s.
[[Our]] investigation considers the [[dynamics of the community activity]] that shapes the [[set of answer]]s, both how answers and [[voters arrive]] over [[time]] and how this [[influence]]s the eventual [[outcome]] .
Our key [[observation]] is that we can localize [[anomali]]es by identifying a [[sparse low dimensional space]] that [[capture]]s the [[abnormal event]]s in [[data stream]]s.
Our [[knowledge]] acquired in the [[segmentation]] [[results]] also provides assistance to [[editors]] in [[content management]] and [[user targeting]] .
[[Our]] [[largest]] [[graph]] has ~3M [[vertice]]s and ~32M [[edges]], spanning 4.5 [[days]] .
[[Our learned algorithm]]s, implemented by [[LSTM]]s, outperform [[generic]], [[hand-designed competitor]]s on the tasks for which they are [[trained]], and also generalize well to new [[task]]s with similar [[structure]] .
Our main [[assumption]] is that the [[observed movement]] is generated from multiple [[interleaved]] [[periodic behaviors]] associated with certain [[reference locations]] .
[[Our main contribution]] is a [[sampling scheme]] that gives <i>[[densest subgraph sparsifier]] </i>, yielding a [[randomized algorithm]] that produces [[high-quality approximation]]s while providing [[significant speedup]]s and improved [[space complexity]] .
[[Our]] main contribution is a [[semi-supervised hierarchical model]] called [[Wikipedia-based Pachinko Allocation Model (WPAM)]] that exploits: (1) [[All word]]s in the [[Wikipedia corpus]] to [[learn]] [[word-entity association]]s (unlike existing [[approach]]es that only use [[word]]s in a [[small fixed window]] around [[annotated entity reference]]s in [[Wikipedia page]]s), (2) [[Wikipedia annotation]]s to appropriately [[bias]] the [[assignment of entity label]]s to [[annotated]] (and [[co-occurring unannotated]]) [[word]]s during [[model learning]], and (3) [[Wikipedia]]'s [[category hierarchy]] to capture [[co-occurrence pattern]]s among [[entiti]]es.
[[Our main contribution]] is the " [[Experimentation Evolution Model]] " in which we detail three [[phases of evolution]]: [[technical]], [[organizational]] and [[business evolution]] .
[[Our main contribution]] is the "[[Experimentation Evolution Model]]" in which we detail three phases of [[evolution]]: [[technical]], [[organizational]] and [[business evolution]] .
Our main [[contribution]]s are: (1) [[We]] propose [[BoW]] and carefully derive its [[cost function]]s, which [[dynamically choose]] the [[best strategy]]; (2) [[We]] show that [[BoW]] has numerous desirable [[features]]: it can work with most [[serial clustering method]]s as a [[plugged-in clustering subroutine]], it balances the [[cost]] for [[disk access]]es and [[network access]]es, achieving a [[very good tradeoff]] between the two, it uses no [[user-defined parameter]]s (thanks to our [[reasonable default]]s), it [[match]]es the [[clustering quality]] of the [[serial algorithm]], and it has [[near-linear scale-up]]; and finally, (3) [[We]] report [[experiment]]s on [[real]] and [[synthetic data]] with billions of [[point]]s, using up to 1, 024 [[cores in parallel]] .
[[Our main contribution]]s are (a) <i>[[formulation]] </i>: [[we]] show how to [[formalize]] [[this problem]] as [[minimizing]] the [[encoding cost]] in a [[data compression paradigm]], (b) <i>[[algorithm]] </i>: [[we]] propose [[TIMECRUNCH]], an [[effective]], [[scalable]] and [[parameter-free method]] for finding [[coherent]], [[temporal pattern]]s in [[dynamic graph]]s and (c) <i>[[practicality]] </i>: we apply [[our method]] to several [[large]], [[diverse]] [[real-world dataset]]s with up to <i>36 million</i> [[edge]]s and <i>[[6.3 million]]</i> [[node]]s.
[[Our main result]] is an [[active learning algorithm]] that approximately [[maximize]]s [[recall]] of the [[classifier]] while respecting a [[precision constraint]] with [[provably]] [[sub-linear label complexity]] (under certain [[distributional assumption]]s).
[[Our]] main [[result]] is that on an [[English to French translation task]] from the [[WMT-14 dataset]], the [[translation]]s produced by the [[LSTM]] achieve a [[BLEU score]] of 34.8 on the entire [[test set]], where the [[LSTM]]'s [[BLEU score]] was penalized on [[out-of-vocabulary word]]s.
Our main [[thesis]] is that a simple [["one-vs-all" scheme]] is as [[accurate]] as any other [[approach]], assuming that the underlying [[binary classifiers]] are [[well-tuned]] [[regularized classifiers]] such as [[support vector machines]] .
Our major [[contribution]]s are threefold: (1) [[the proposed model]]s handle both [[feature-level]] and [[source-level analysis]] in a [[unified formulation]] and include several existing [[feature learning approach]]es as [[special case]]s; (2) [[the model]] for [[incomplete data]] avoids direct [[imputation]] of the [[missing element]]s and thus provides superior [[performance]]s.
[[Our method]] automatically determines the [[interval]]s where [[anomalies]] occur, and it captures the [[temporal effect]]s of the [[general behavior]] by using a [[state space model]] on the [[natural parameter]]s of the [[categorical distribution]]s.
[[Our method]] can be applied to [[clinical prediction]] of other [[diseases]], where [[missing data]] are common and [[risk factors]] are [[not well understood]] .
[[Our method]] can not only [[perform]] [[prediction]] but also [[discover]] the [[relationship]]s among those [[risk]]s.
[[Our method]] combines [[fictitious self-play]] with [[deep reinforcement learning]] .
[[Our method]] corrects for [[multiple hypothesis testing]] and [[correlation]]s between [[pattern]]s via the [[Westfall-Young permutation procedure]], which [[empirically estimate]]s the [[null distribution]] of [[pattern frequenci]]es in each [[class]] via [[permutation]]s.
[[Our method]] discovers the [[schools of thought]], shows [[reasonable estimation]] of [[worker reliability]] and [[task clarity]], and is robust to [[hyperparameter change]]s.
[[Our method]] effectively integrates [[medical domain knowledge]] relating to the [[similarity]] among [[disease]]s and the [[similarity]] among [[Electronic Health Records (EHRs)]] into a [[data-driven approach]] by incorporating [[graph Laplacian]]s into the [[regularization term]] to [[encode]] these [[similariti]]es.
[[Our method]] exploits [[correlations]] in [[aggregates]] observed at multiple [[resolutions]] when working with multiple [[hierarchies]]; [[stable estimates]] at [[coarser resolution]] provide informative [[prior information to improve estimates]] at [[finer resolutions]] .
[[Our method]] first constructs a [[hypergraph]] that [[encode]]s the [[betweenness centrality]], and then computes the [[adaptive betweenness centrality]] by examining [[this graph]] .
[[Our method]] has the following [[properti]]es: (a) [[Sense-making]]: it detects important [[patterns of epidemic]]s, such as [[periodiciti]]es, the [[appearance of vaccine]]s, [[external shock event]]s, and more; (b) [[Parameter-free]]: our [[modeling framework]] frees the [[user]] from providing [[parameter value]]s; (c) [[Scalable]]: [[FUNNELFIT]] is carefully designed to be [[linear]] on the [[input size]]; (d) [[General]]: [[our model]] is general and [[practical]], which can be applied to various [[types of epidemic]]s, including [[computer-virus propagation]], as well as [[human disease]]s.
[[Our method]] has thoroughly considered the [[uncertainti]]es and [[noise]]s in [[periodic behavior]]s and is provably robust to [[incomplete]] [[observation]]s.
[[Our method]] is able to integrate two usually separate [[tasks]], i.e. [[inferring]] [[underlying state]]s and [[learning]] [[temporal graph]]s, in one [[unified model]] .
[[Our]] [[method]] is based on [[posing metric learning]] as a [[convex optimization problem]], which allows us to give efficient, [[local-optima-free algorithm]]s.
[[Our method]] is based on the [[two-stage procedure]]; [[multiple worker]]s are first requested to work on the same [[task]]s in the [[creation stage]], and then another [[set]] of [[workers review]] and [[grade]] each [[artifact]] in the [[review stage]] .
[[Our method]] [[learns vector space representation]]s for [[multi-word phrase]]s.
[[Our method]] models the [[domain similarity]] as a [[network]], which can be utilized to [[regularize]] the [[clustering structure]]s in different [[network]]s.
[[Our method]] models [[topical interest]]s of a [[user dynamically]] where both the [[user]] association with the [[topic]]s and the [[topic]]s themselves are allowed to vary over [[time]], thus ensuring that the [[profile]]s remain current.
[[Our method]] offers several [[practical advantages]]: it can [[encode]] the [[degree of belief (weight)]] in [[Must-Link]] and [[Cannot-Link constraints]]; it [[guarantees]] to [[lower-bound]] how well the given [[constraints]] are satisfied using a [[user-specified threshold]]; and it can be [[solved deterministically]] in [[polynomial time]] through [[generalized eigendecomposition]] .
[[Our methodology]] is evaluated against strong [[baseline]]s via a [[user study]] and [[experiment]]s on multiple [[dataset]]s from different [[domain]]s.
[[Our method]] [[outperforms alternative]]s based on [[robust statistics]] both in [[tracking]] the [[true price level]]s and the [[true price trend]]s.
[[Our method]]s are illustrated on a [[large scale]] [[computational advertising dataset]] consisting of billions of [[observation]]s and [[hundreds of million]]s of [[attribute combination]]s ([[cell]]s).
[[Our method]] showed [[consistent improvement]] in accurately [[identifying]] [[informal settlement]]s.
[[Our]] [[methods]] [[outperform]] existing [[AL strategies]] on three [[real-world]] [[systematic review datasets]] .
[[Our method]] uses a [[multilayered]] [[Long Short-Term Memory (LSTM)]] to map the [[input sequence]] to a [[vector of a fixed dimensionality]], and then another [[deep LSTM]] to decode the [[target sequence]] from the [[vector]] .
[[Our method]], which is based on [[model reduction]], [[outperform]]s existing [[method]]s by nearly <i>five [[orders of magnitude]] </i>.
[[Our mining framework]] is based on a [[phrase-centric]] view for [[clustering]], [[extracting]], and [[ranking topical phrase]]s.
[[Our model]] aims to [[find vector representation]]s for [[variable-sized phrase]]s in either [[unsupervised]] or [[semi-supervised training regime]]s.
[[Our model]] can be [[learned]] in minutes on [[dataset]]s with millions of [[sample]]s, for which most existing [[nonlinear method]]s will be prohibitively expensive in [[space]] and [[time]] .
[[Our model]] [[converse]]s by [[predicting the next sentence]] given the [[previous]] [[sentence]] or [[sentence]]s in a [[conversation]] .
[[Our model]] gives only 15% [[relative error]] in [[predicting]] [[final size]] of an [[average information cascade]] after [[observing]] it for just one [[hour]] .
[[Our model]] has the following advantages: (a) [[unification power]]: it generalizes and explains earlier [[theoretical model]]s and [[empirical observation]]s; (b) [[practicality]]: it matches the [[observed behavior]] of diverse [[sets of real data]]; (c) [[parsimony]]: it requires only a handful of [[parameter]]s; and (d) [[usefulness]]: it enables further [[analytics task]]s such as [[forecasting]], [[spotting anomali]]es, and interpretation by [[reverse-engineering]] the [[system parameter]]s of interest (e.g.
[[Our model]] is [[efficient]] and thus facilitates [[real-time]] [[adaptive routing]] in the face of [[uncertainty]] .
[[Our model]] is intuitive and retains the desired [[properties]] of [[monotonicity]] and [[submodularity]] .
[[Our]] [[model]] is optimized with [[stochastic gradient descent]] and scales to [[large graph]]s.
[[Our model]] learns [[low-dimensional embeddings]] of [[words]] and [[knowledge base constituent]]s; these [[representation]]s are used to score [[natural language question]]s against candidate [[answer]]s.
[[Our model]] of the [[scientific domain]] contains [[scientific article]]s linked to [[typed relation]]s whose arguments are [[mapped]] to existing [[ontologi]]es (see [[Figure 1]]).
[[Our model]] outperforms representative [[relational learning]] [[methods]] based on [[collective inference]], especially when few [[labeled data]] are available.
Our [[model]] posits that the position-specific [[multinomial parameters]] for [[monomer]] [[distribution]] are distributed as a [[latent Dirichlet-mixture]] [[random variable]], and the position-specific [[Dirichlet]] [[component]] is determined by a [[hidden Markov process]] .
[[Our model]] relies on a [[typology]] of [[explicit semantic relation]]s.
[[Our model]] requires no [[train]]ing or [[expensive feature engineering]] .
[[Our]] [[model]] significantly improves [[predictive accuracy]], especially in [[cold-start scenario]]s.
[[Our model]] uses [[maximum]] a [[posteriori estimation]] to [[learn]] the [[latent coordinate]]s of [[node]]s that best explain the [[observed cascade data]] .
Our most powerful 21st-century [[technologi]]es - [[robotics]], [[genetic engineering]], and [[nanotech]] - are threatening to make [[humans]] an [[endangered species]] .
[[Our new method]] effectively [[prunes record]]s based on [[redundant]] [[assignments to blocks]], providing better [[fault-tolerance]] and [[maintaining result quality]] while [[scaling linearly]] with respect to the [[dataset size]] .
[[Our new model]] [[learn]]s the [[data similarity matrix]] by assigning the [[adaptive]] and [[optimal neighbor]]s for each [[data point]] based on the [[local distance]]s.
[[Our new theory]] reveals the [[learning mechanism]] of [[low-rank regression]], and shows that the [[low-rank structures exacted]] from [[class]]es / [[task]]s are connected to the [[LDA projection result]]s.
[[Our]] novel [[algorithm]] <i>[[SCMiner]]</i> (for [[Summarization-Compression Miner]]) reduces a [[large bipartite input graph]] to a [[highly compact representation]] which is very useful for different [[data mining task]]s: 1) [[Clustering]]: The compact [[summary graph]] contains the [[truly relevant cluster]]s of both types of [[nodes of a bipartite graph]] .
[[Our novel approach]] to [[distribution regression]] exploits the connection between [[Gaussian process regression]] and [[kernel ridge regression]], giving us a [[coherent]], [[Bayesian approach]] to [[learning]] and [[inference]] and a convenient way to include [[prior information]] in the form of a [[spatial covariance function]] .
[[Our novel classifier]] shows [[statistically significant]] better [[accuracy]] when compared to [[well known]] [[Ensemble methods]] -- [[Bagging]] and [[AdaBoost]] .
[[Our novel method]] uses machine learning and is based on a [[high-recall]] [[concept ranking]] and a [[high-precision]] [[concept selection]] [[step]] .
[[Our]] novel [[structural cluster kernel (SCK)]] incorporates [[similarities]] induced by a [[structural clustering algorithm]] to improve [[state-of-the-art]] [[graph kernel]]s.
[[Our numerical experiment]]s show that [[this approach]] is [[computationally efficient]] and also capable of [[producing solution]]s of [[high quality]] .
[[Our objective]], in [[this paper]], is to develop an extreme [[multi-label classifier]] that is faster to [[train]] and more [[accurate]] at [[prediction]] than the [[state-of-the-art]] [[Multi-label Random Forest (MLRF) algorithm]] [2] and the [[Label Partitioning]] for [[Sub-linear Ranking (LPSR) algorithm]] [35].
Our [[objective]] is to explore [[semantic meaning]] beyond [[surface proposition]]s.
Our [[optimization criterion]] for [[TF]] is motivated by a detailed [[analysis]] of the [[problem]] and of [[interpretation]] [[scheme]]s for the [[observed data]] in [[tagging systems]] .
Our particular example is one of [[infer]]ring an [[underlying network]] for how [[text]] is reused in the [[Internet]], although the [[general approach]] is applicable to other [[inference method]]s and [[information source]]s.
Our [[pointer sentinel]] - [[LSTM model]] achieves [[state of the art]] [[language modeling performance]] on the [[Penn Treebank]] (70.9 [[perplexity]]) while using far [[fewer parameter]]s than a [[standard softmax LSTM]] .
[[Our prior work]] has demonstrated that the [[efficiency]] of [[sharing]] and [[managing information]] plays an important role in [[business recovery]] efforts after [[disaster event]] .
[[Our procedure]] is based on the [[classic probabilistic]] [[result]], <i>the [[birthday paradox]] </i>.
[[Our program, AlphaGo Zero]], differs from [[AlphaGo Fan]] and [[AlphaGo Lee]] (12) in several important [[aspect]]s.
[[Our proposal]] uses a broader term definition of [[product feature]] that encompasses their [[size]] and [[color]] [[label]]s (and is therefore applicable to other [[industri]]es, but loses the [[granular information]]).
[[Our proposed algorithm]], [[INCONCO]], successfully finds [[cluster]]s in [[mixed type data set]]s, identifies the relevant [[attribute dependenci]]es, and explains them using [[linear model]]s and [[case-by-case analysis]] .
[[Our proposed approach]] involves [[implicitly building]] [[ontological user profile]]s by assigning [[interest score]]s to existing [[concepts in a domain ontology]] .
Our proposed [[FastXML algorithm]] achieves significantly [[higher accuraci]]es by directly [[optimizing]] an [[nDCG]] based [[ranking loss function]] .
[[Our proposed method]] assumes a common [[prior]] for the [[set]] of all [[class]]es, known and missing.
[[Our]] proposed method for [[detecting]] [[unlinkable entiti]]es achieves 24% greater [[accuracy]] than a [[Named Entity Recognition baseline]], and our [[method for fine-grained typing]] is able to propagate over 1,000 [[type]]s from [[linked Wikipedia entiti]]es to [[unlinkable entiti]]es.
[[Our proposed model]]s compare [[favorably]] against existing [[approach]]es.
[[Our proposed model]] shares [[knowledge]] across [[video shot]]s via [[user]]s to enrich the [[short comment]]s, and peels off [[user interaction]] and [[user bias]] to solve the [[noisy-comment problem]] .
[[Our proposed solution]], by [[combining]] <i>[[wisdom of crowd]]s</i> and <i>[[wisdom of data]]</i>, achieves [[robustness]] and [[efficiency]] over existing approaches.
[[Our proposed technique]]s create a [[collaborative solution]] on a [[mobile platform]] using advanced [[data mining]] and [[information retrieval technique]]s for [[disaster preparedness]] and [[recovery]] that helps [[impacted communiti]]es better understand the current [[disaster situation]] and how the [[community]] is [[recovering]] .
[[Our publication method]] also puts [[data]] in a [[generalized form]], but does <i>not</i> require that [[published record]]s form [[disjoint group]]s and does not assume a [[hierarchy]] either; instead, it employs <i>[[generalized bitmap]]s</i> and [[recast]]s [[data value]]s in a <i>[[nonreciprocal]]</i> manner; [[formally]], the [[bipartite graph]] from original to [[anonymized record]]s does <i>not</i> have to be composed of [[disjoint complete subgraph]]s.
[[Our]] [[qualitative]] and [[quantitative evaluation]]s demonstrate consistently better [[precision]], recall and [[accuracy]] compared to three other existing [[ad-hoc measure]]s.
Our [[quantification]]-based [[methods]] estimate the [[class]] [[distribution]] of the [[unlabeled data]] from the changed [[distribution]] and adjust the original [[classifier]] accordingly, while our [[semi-supervised]] [[methods]] build a new [[classifier]] using the [[examples]] from the new ([[unlabeled]]) [[distribution]] which are supplemented with [[predicted class]] values.
[[Our]] [[research goal]] is to provide a better [[understanding]] of how [[users engage]] with [[online service]]s, and how to [[measure this engagement]] .
[[Our research]] is directed towards the [[real-time]] [[detection]] of [[food pathogen]]s using [[optical-scattering technology]] .
[[Our results]] clarify several common [[misconception]]s about [[data utility]] and provide [[data publishers]] useful [[guideline]]s on choosing the right [[tradeoff]] between [[privacy]] and [[utility]] .
[[Our]] [[result]]s demonstrate that [[the proposed algorithm]] achieves clearly better [[performance]] than several [[alternative method]]s.
[[Our result]]s indicate that a [[user]] with [[exposed friend]]s is more likely to submit [[queri]]es relevant to the [[campaign]], as compared to a [[user]] without [[exposed friend]]s.
[[Our]] [[results on synthetic data]] and a [[real-world]] [[tumor detection task]] show the superiority of [[SVRFs]] over both [[SVMs]] and [[DRFs]] .
Our results on two [[practical]] [[social network datasets]] shows that most of the well-known [[classification algorithm]]s ([[decision tree]], [[k-NN]], [[multilayer perceptron]], [[SVM]], [[RBF network]]) can [[predict links]] with comparable [[performance]]s, but [[SVM]] outperforms all of them with narrow margin in all [[performance measures]] .
[[Our result]]s reveal the [[relation]] between the [[budget]] that an [[advertiser invest]]s and [[marketing strategi]]es, and indicate that the [[mixing parameter]], a [[benchmark evaluating network community structure]]s, plays a critical role for [[information diffusion]] .
[[Our result]]s show that [[conditional topical coding]] can achieve [[state-of-the-art]] [[prediction performance]] and is much more [[efficient]] in [[training]] ([[one order of magnitude]] [[faster]]) and [[testing]] (two [[orders of magnitude]] [[faster]]) than [[probabilistic conditional topic model]]s.
[[Our result]]s show that [[DeepWalk]] outperforms challenging [[baseline]]s which are allowed a [[global view]] of the [[network]], especially in the presence of [[missing information]] .
[[Our result]]s show that [[Marble]] derived [[phenotype]]s provide at least a 42.8% [[reduction]] in the number of [[non-zero element]] and also retains [[predictive power]] for [[classification]] purposes.
[[Our result]]s show that, on [[knowledge base]]s with large [[ABox]]es but with simple [[TBox]]es, [[our technique]] indeed shows [[good performance]]; in contrast, on [[knowledge base]]s with large and [[complex TBox]]es, existing techniques still perform better.
[[Our result]]s show that [[our algorithm]] is [[scalable]] and [[outperform]]s the [[state-of-the-art]] [[local triangle estimation algorithm]] .
[[Our result]]s show that [[our model]] contributes towards improved [[behavioral targeting]] of [[display advertising]] relative to [[baseline model]]s that do not incorporate [[topical]] and/or [[temporal dependenci]]es.
[[Our result]]s show that [[Rubik]] can [[discover]] more [[meaningful]] and distinct [[phenotype]]s than the [[baseline]]s.
[[Our result]]s show that the [[structural cluster similarity information]] can indeed leverage the [[prediction performance]] of the [[base kernel]], particularly when the [[dataset]] is [[structurally sparse]] and [[consequently structurally diverse]] .
[[Our]] [[result]]s suggest that [[community-based method]]s can offer an important additional [[dimension]] for [[intrusion detection system]]s.
[[Our sample]] and [[hold framework]] facilitates the [[accurate estimation]] of [[subgraph pattern]]s by enabling the [[dependence]] of the [[sampling process]] to vary based on previous history.
[[Our scheme]] is [[differential private]], and hence, provides [[provable privacy guarantee]] to each [[individual]] in the [[dataset]] .
Our [[scientist]]s have produced [[landmark studi]]es on the [[dangers]] of [[DDT]], [[tobacco smoke]], [[acid rain]], and [[global warming]] .
[[ Our]] [[shared subspace learning framework]] is applicable to a range of [[problem]]s where one needs to [[exploit]] the [[strengths]] existing among [[multiple]] and [[heterogeneous datasets]] .
[[Our software]] [[implementation]] of the [[model]]s investigated in [[this paper]] is [[publicly available]] at http://www.dii.unisi.it/~freno/.
[[Our solution]] first [[learn]]s the [[utility]] of individual [[message]]s in [[isolation]] and then produces a [[diverse]] [[selection of interesting message]]s by [[maximizing]] the defined [[objective function]] .
[[Our solution]] thus hinges upon the idea to separate the two steps in [[prominent streak discovery]]' [[candidate]] [[streak generation]] and [[skyline operation]] over [[candidate]] [[streak]]s.
[[Our SS approach]] achieves a 90.47% [[F1 score]] and 93.44% [[recall rate]] by using only eight [[sentence]]s [[labeled]] by a [[human expert]] .
Our [[structuring]] and [[modeling methods]] can also deal with [[data sets]] which include both [[types]] of [[time series]] .
[[Our studi]]es are based on a [[large corpus]] of [[crowdsourced]] [[cartoon caption]]s that were submitted to a [[contest]] [[hosted]] by the [[New Yorker]] .
Our study focused on three [[large]] [[datasets]], each of which is a different type of [[communication service]], with over [[one million]] [[records]], and [[spans]] several [[months]] of [[activity]] .
[[Our study]] includes existing [[spline]] and [[tree-based method]]s for [[shape function]]s and [[penalized]] [[least square]]s, [[gradient boosting]], and [[backfitting]] for [[learning]] [[GAM]]s.
[[Our study]] is based on a [[real-world large mobile network]] of more than 7,000,000 [[user]]s and over 1,000,000,000 [[communication record]]s ([[CALL]] and [[SMS]]).
[[Our study]] provides a detailed [[phylogenetic]] and [[historical context]] against which to [[compare level]]s of [[lethal violence]] observed throughout [[our history]] .
Our [[study]] provides insights into [[user activity]] [[patterns]] and lays out an [[analytical foundation]] for further understanding various [[properties]] of these [[OSNs]] .
[[Our study]] reveals that the [[effective connectivity]] of [[AD]] is different from that of [[NC]] in many ways, including the [[global-scale]] [[effective connectivity]], [[intra-lobe]], [[inter-lobe]], and [[inter-hemispheric]] [[effective connectivity]] [[distribution]]s, as well as the [[effective connectivity]] associated with specific [[brain region]]s.
[[Our suite of experiments]] together consumed approximately four [[month]]s of [[CPU time]], providing [[quantitative answer]]s to the following [[question]]s: at <i> what </i>[[levels of granularity]] is [[software unique]], and at a <i>given</i> level of [[granularity]], <i>how</i> [[unique]] is [[software]]?
[[Our]] [[survey]] complements these publications by aiming at a [[larger scope]], and, although our main focus is on the [[OR/MS literature]], we also address relevant contributions from [[computer science]], [[marketing]], [[economic]]s and [[econometric]]s.
[[Our system]] combines [[structural]] and [[semantic information]] from a [[real corporate database]] of [[monitored activity]] on their [[users' computer]]s to [[detect independently]] developed red team inserts of [[malicious insider activiti]]es.
[[Our system]] exploits the [[message]]s shared in [[real-time]] on [[Twitter]], one of the most [[popular]] [[social network]]s in the [[world]] .
[[Our system]] is able to (a) [[discover]] [[inconsistenci]]es in [[review]]s; (b) [[identify]] reasons why [[user]]s [[like]] or [[dislike]] a given [[app]], and provide an [[interactive]], [[zoomable view]] of how [[users' reviews evolve over time]]; and (c) provide valuable insights into the entire [[app market]], [[identify]]ing [[user]]s' major concerns and [[preference]]s of different types of [[app]]s.
[[Our system]] is able to (a) [[mine]] [[large]], bursty, [[real-world monitoring data]], (b) find significant [[trend]]s and [[anomali]]es in the [[data]], (c) [[compress]] the [[raw data]] effectively, and (d) [[estimate]] [[trend]]s to make [[forecast]]s.
[[Our system]] is developed from an extensive [[crawl]] of [[black-market Web site]]s that deal in [[illegal pharmaceutical]]s, [[replica luxury good]]s, and [[counterfeit software]] .
[[Our system]] reduces the [[money]] being wasted by [[provider]]s and [[insurer]]s dealing with [[incorrectly processed claim]]s and makes the [[healthcare system]] more [[efficient]] .
[[Our task]]s measure [[understanding]] in several ways: whether a [[system]] is able to [[answer question]]s via [[chaining facts]], [[simple induction]], [[deduction]] and many [[more]] .
[[Our technique]] first [[mines]] a [[set]] of [[discriminative features]] [[capturing]] [[repetitive series]] of [[events]] from [[program execution]] [[trace]]s.
[[Our technique]] is [[information theory]] based and aims to ensure [[alternative clustering]] [[quality]] by [[maximizing]] the [[mutual information]] between [[clustering labels]] and [[data observations]], whilst at the same time ensuring [[alternative clustering]] [[distinctiveness]] by [[minimizing]] the [[information sharing]] between the two [[clusterings]] .
[[Our technique]] is simple, [[efficient]] and [[practical]] for [[clustering distribution]]s.
[[Our template-independent]] [[wrapper]] can [[extract]] [[news pages]] from different [[sites]] regardless of [[templates]] .
[[Our theoretical analysis]] and [[empirical study]] on the [[Beijing Taxi]] and [[GMTI (Ground Moving Target Indicator) dataset]]s demonstrate its [[effectiveness]] in [[capturing]] [[abnormal moving object]]s.
[[Our theoretical analysis]] and [[experiment]]s show that [[the new method]] can [[efficiently]] fit [[the model]]s and effectively simplify [[over-complicated model]]s.
Our thorough [[experimental evaluation]] on [[large]] [[social graph]] demonstrates that the [[empirical performance]] of [[our algorithm]]s agrees with that [[predicted]] by the [[theory]] .
[[Our transductive learning result]]s reveal that incorporating [[social-network information]] can indeed lead to [[statistically significant]] [[sentiment classification]] [[improvement]]s over the [[performance]] of an [[approach]] based on [[Support Vector Machine]]s having access only to [[textual feature]]s.
[[Our understanding]] of how individual [[mobility pattern]]s shape and impact the [[social network]] is [[limited]], but is essential for a [[deeper understanding]] of [[network dynamics]] and [[evolution]] .
Our [[user studies]] demonstrate the [[algorithm's effectiveness]] in [[helping users]] [[understanding]] the [[news]] .
[[Our user study]] and [[experimental result]]s reveal that [[SITINA]] [[outperform]]s [[manual selection]] and the [[baseline approach]] in solution quality [[efficiently]] .
[[Our user study]] with [[real data confirm]]s the effectiveness of [[ONION]] in recognizing "[[true " outlier]]s.
[[Our USP system]] starts by [[clustering]] [[tokens]] of the same [[type]], and then [[recursively clusters]] [[expressions]] whose [[subexpressions]] belong to the same [[clusters]] .
[[Our Wikipedia Vandal Behavior (WVB) approach]] uses a novel [[set]] of [[user editing pattern]]s as features to [[classify]] some [[user]]s as [[vandal]]s.
[[Our work]] also highlights several broad challenges that arise in the [[large-scale]], [[empirical study]] of [[malicious activity]] on [[the Web]] .
[[Our work]] builds upon [[prior research]] on [[recommender system]]s, looking at [[properties of recommendation list]]s as [[entiti]]es in their own right rather than specifically focusing on the [[accuracy]] of [[individual recommendation]]s.
[[Our work]] can benefit [[trip planning]], [[traffic management]], and [[animal movement studi]]es.
Our [[workflow mining approach]] is a [[three-step]] [[coarse-to-fine algorithm]] .
[[Our work]] focuses on the [[dynamic]] ([[time-varying) setting]] because [[model]]s from this [[regime]] could facilitate an [[on-going]] severity [[stratification system]] that helps [[direct care-staff resource]]s and [[inform treatment strategi]]es.
[[Our work]] formulates the problem and examines the [[performance]] of several [[learning model]]s on [[data]] from several thousands of [[patient]]s.
[[Our work]] has three key contributions : (1) [[metagraph]], a novel [[relational hypergraph]] [[representation]] for [[model]]ing [[multi-relational]] and [[multi-dimensional data]]; (2) an efficient [[factorization method]] for [[community extraction]] on a given [[metagraph]]; (3) an [[on-line]] [[method]] to handle [[time-varying]] [[relation]]s through [[incremental]] [[metagraph]] [[factorization]] .
[[Our work]] highlights important considerations for [[crowdsourcing task]]s requiring various [[type]]s of [[expertise]]
[[Outlier Analysis]] is a [[comprehensive exposition]], as understood by [[data mining expert]]s, [[statistician]]s and [[computer scientist]]s.
[[Outlier detection]] and [[ensemble learning]] are well established [[research direction]]s in [[data mining]] yet the [[application]] of [[ensemble technique]]s to [[outlier detection]] has been rarely [[studied]] .
[[Outlier Detection]], [[Coding Cost]]s, [[Minimum Description Length]], [[Data Compression]]
[[Outlier detection]] has been used for centuries to [[detect]] and, where appropriate, [[remove]] [[anomalous observation]]s from [[data]] .
[[Outlier detection]] in [[information networks]] can reveal important [[anomalous]] and [[interesting behaviors]] that are [[not obvious]] if [[community information]] is ignored.
[[Outlier]]s are also referred to as [[abnormaliti]]es, [[discordant]]s, [[deviant]]s, or [[anomali]]es in the [[data mining]] and [[statistics literature]] .
[[Outlier]]s arise due to [[mechanical fault]]s, [[changes in system behaviour]], [[fraudulent behaviour]], [[human error]], [[instrument error]] or simply through natural deviations in [[population]]s.
[[Outlier]]s may cause a [[negative effect]] on [[data analyse]]s, such as [[ANOVA]] and [[regression]], based on [[distribution assumption]]s, or may provide useful [[information]] about [[data]] when we look into an [[unusual response]] to a given [[study]] .
Out of a similar [[motivation]], to incorporate the [[consistenci]]es between different [[information source]]s into [[MIL]], [[we]] propose a novel [[research framework]] - [[Multi-Instance Learning from Multiple Information Sources (MI2LS)]] .
[[Outsourcing]] the [[training]] of [[support vector machines (SVM)]] to [[external service providers]] benefits the [[data owner]] who is [[not familiar]] with [[the]] [[technique]]s of the [[SVM]] or has limited [[computing resource]]s.
Over 3600 [[term]]s are [[defined]], covering [[medical]], [[survey]], [[theoretical]], and [[applied statistics]], including [[computational aspect]]s.
Over 40% of [[column]]s in [[hundreds of million]]s of [[Web table]]s contain [[numeric quantiti]]es.
Overall, [[CSTM]] is a [[joint directed probabilistic model]] of [[user-item score]]s ([[rating]]s), and the [[textual side information]] in the [[user librari]]es and the [[item]]s.
[[Overall]], [[event recommendation]] in [[EBSN]]s inevitably faces the [[cold-start]] [[problem]] .
Overall, [[our approach]] [[outperform]]s the current [[state-of-the-art]] in both [[metrics]] of [[AUC]] and [[concordance index]] .
Overall [[performance]] of the [[data mining]] [[process]] depends not just on the [[value]] of the induced [[knowledge]] but also on various [[costs]] of the [[process]] itself such as the [[cost]] of acquiring and [[pre-processing]] [[training examples]], the [[CPU]] [[cost]] of [[model]] [[induction]], and the [[cost]] of committed [[error]]s.
[[Overall]], [[the proposed method]] outperforms both [[types of method]] in terms of [[accuracy]] ([[consistently ranked first]]) and is an [[order of magnitude]] [[faster]] than [[state of the art]] [[multi-metric method]]s (16x faster in the [[worst case]]).
Overall, they [[rated]] their own [[chance]]s to be significantly [[above average]] for [[positive event]]s and [[below average]] for [[negative event]]s.
Overall, [[we]] provide [[actionable insight]]s with major [[implication]]s for [[firm]]s and [[social media platform]]s and [[contribute]] to [[the related literature]] as we discover new rich [[finding]]s [[enable]]d by the [[employ]]ed [[causal estimation framework]] .
[[Overdispersion]], [[Underdispersion]], [[Variance]], [[Standard deviation]], [[Median Absolute Deviation]], [[Average absolute deviation]]
Overly [[complex decision-making process]]es and [[systems of service delivery]] that [[citizen]]s and [[taxpayer]]s cannot understand thwart [[transparency]] and [[damage accountability]] .
over the intersection of the [[cone]] of [[positive semidefinite]] [[matrices]] with an [[affine space]], i.e., a [[spectrahedron]] .
Over the past 10+ years, [[online compani]]es [[large]] and [[small]] have adopted widespread [[A/B testing]] as a [[robust]] [[data-based method]] for [[evaluating potential product improvement]]s.
Over the years, [[people]] have often held the [[hypothesis]] that [[pattern-based methods]] should perform better than [[term-based ones]] in describing [[user preference]]s, but many [[experiment]]s do not support [[this hypothesis]] .
p.25 [[American]]s [[mythologize]] [[competition]] and credit it with saving us from [[socialist]] [[bread line]]s.
[[PageRank]] models web surfing as a [[random walk]] where the [[surfer]] [[randomly selects]] and follows [[links]] and occasionally [[jumps]] to a new [[web page]] to start another [[traversal]] of the [[link structure]] .
[[PAM]] is a family of [[generative model]]s in which [[word]]s are [[generated]] by a [[directed acyclic graph (DAG)]] consisting of [[distribution]]s over [[word]]s and [[distribution]]s over other [[node]]s.
[[Pandora]], [[Spotify]]) have fundamentally changed how [[music]] is [[consumed]] .
[[Parallel data mining]], [[Association mining]], [[top-down approach]], [[Candidate distribution]] .
[[Parameter]]s are [[estimated]] by [[optimising]] the [[fit]], as [[measure]]d by the chosen [[score function]], and [[model choice]] is guided by the [[size]] of the [[score]]s for the different [[model]]s.
[[PAR]]s are [[association rule]]s where the [[consequence]] of the [[rule]] is a [[class label]] .
[[Partially observable Markov decision processes (POMDPs)]] provide an [[elegant]] [[mathematical framework]] for [[modeling]] [[complex decision]] and [[planning problem]]s in [[stochastic domain]]s in which [[states of the system]] are [[observable only indirectly]], via a [[set of imperfect]] or [[noisy]] [[observation]]s.
[[Participant]]s were asked to implement a [[web service]] for their [[system]] to [[minimize]] [[human involvement]] during [[evaluation]] and to enable measuring the [[processing time]]s.
Particularly, we analyze [[vast amount]]s of [[information collected]] from multiple [[review platform]]s (multiple [[source]]s) in which people can [[rate]] and [[review]] the [[item]]s they have [[purchased]] .
Particularly, we augment [[user]]s' and [[POI]]s' [[latent factor]]s in the [[factorization model]] with [[activity area vector]]s of [[user]]s and [[influence area vector]]s of [[POI]]s, respectively.
Particularly, when multiple [[SUGAR]]s are [[stack]]ed, the [[performance]] is significantly [[boosted]] .
[[Partitioning large graph]]s is difficult, especially when performed in the [[limited model]]s of [[computation]] afforded to modern [[large scale]] [[computing system]]s.
Part [[memoir]], part [[science]], part [[inspiration]], and part [[practical instruction]], [[Into the Magic Shop]] shows us how we can fundamentally change our [[live]]s by first changing our [[brain]]s and our [[heart]]s.
[[part-of-speech tag]], [[eBay]], [[opinion mining]], [[Bajari]], [[Conditional Random Fields]], [[ACIDplanet]], [[Data Mining]]
[[Pasca et. al]]’s [[system]] derives [[relation-specific extraction patterns]] from a starting set of [[seed facts]], acquires [[candidate]] [[facts]] using the [[patterns]], adds [[high-scoring]] [[facts]] to the [[seeds]], and [[iterates]] until some [[convergence criterion]] .
[[Past exposure]] builds [[brand-awareness]] and [[familiarity]] with the [[user]], which in turn leads to a higher [[propensity of the user]] to [[buy]] / [[convert]] on the [[ad impression]] .
[[Past papers]] have assumed that when the [[CTR]] of a [[result]] varies based on the [[pattern of clicks]] in [[prior positions]], this [[variation]] is solely [[due]] to [[changes]] in the [[probability of examination]] .
[[Past work]] on [[this problem]] has typically focused on [[learning classifiers]] to [[make prediction]]s at the [[group level]] .
[[Path segmentation based metrics]] to capture [[local feature]] [[difference]]s and to combine [[similarity]] and [[dissimilarity]] together are then presented.
Patients who have [[atrial fibrillation (AF)]] have an [[increased risk]] of [[stroke]], but their [[absolute rate]] of [[stroke]] depends on [[age]] and [[comorbid conditions]] .
[[Pattern-based representation]]s are compared to [[word embedding]]s in [[unsupervised clustering]] [[experiment]]s, according to their potential to discover new types of [[semantic relation]]s and recognize their [[instance]]s.
[[Paul Thagard]] explains, "the [[decision]]s that [[scientist]]s and others need to make about what [[projects to pursue]], what [[theories to accept]], and what [[applications to enact]] will [[unavoidably]] have an [[emotional]], [[value-laden aspect]]," and concludes, "the best [[course]] is not to eliminate [[value]]s and [[emotion]]s, but to try to ensure that the best [[value]]s are used in the most [[effective]] ways."9
[[PCA compute]]s the directions of [[maximum covariance]] between elements in a [[single matrix]], whereas [[CCA]] computes the directions of [[maximal correlation]] between a [[pair of matrice]]s.
[[People]] achieve a [[level of competence]] on these tests in [[middle age]], which [[holds steady]] until about [[age seventy-five]] .
[[People]] are [[impatient]] - they like to [[experience rewards]] [[soon]] and to [[delay]] [[cost]]s until later.
[[People]] are now compelled to [[absorb]] and [[process]] a much more [[complicated array]] of [[information stream]]s.
[[People around the world]] are becoming [[healthi]]er, [[wealthi]]er, better [[educated]], more [[peaceful]], and increasingly [[connected]], and they are [[livi]]ng longer.
[[People]] assume they should [[attend to the present]]; their [[future self]] can [[handle the future]] .
[[People]] decide what [[action]]s to take based on their [[personal experience]], [[knowledge]] and [[gut instinct]] .
[[People interpret]] accessible [[time metric]]s in two ways: If [[preparation for the future]] is [[under way]] (Studies 1 and 2), [[people interpret]] [[metric]]s as implying when a [[future event]] will [[occur]] .
[[People]] often resort to [[unethical mean]]s to [[win]] (e.g., the recent [[Volkswagen scandal]]).
[[People's interest]]s are [[dynamically evolving]], often affected by [[external factor]]s such as [[trend]]s promoted by the [[media]] or [[adopted]] by their [[friend]]s.
[[People]] tend to hold overly [[favorable view]]s of their [[abiliti]]es in many [[social]] and [[intellectual domain]]s.
[[People]] who hold [[strong opinion]]s on [[complex social issue]]s are likely to [[examine]] relevant [[empirical evidence]] in a [[biased manner]] .
[[Perception]] of the [[emotional state]] of another [[automatically activate]]s shared representations causing a matching [[emotional state]] in the [[observer]] .
[[Performance-based budgeting]]: A [[budgeting technique]] which defines [[units of service]], calculates a [[cost-per-unit]], and makes [[recommendation]]s based on the [[desired level of service]] .
Performed a meta-analysis of [[100]] [[studi]]es (published 1963-1988) of [[gender difference]]s in [[mathematics performance]] .
[[Periodicity analysis]] from the [[recorded data]] is an important [[data mining task]] which provides useful insights into the [[physical event]]s and enables us to [[report]] [[outlier]]s and [[predict future behavior]]s.
[[Periodicity]] is a frequently happening [[phenomenon]] for [[moving objects]] .
[[Personalization]] is a [[ubiquitous phenomenon]] in our [[daily]] [[online experience]] .
[[Personalization]] is [[ubiquitous]] in [[modern online application]]s as it provides significant [[improvement]]s in [[user experience]] by adapting it to [[infer]]red [[user preference]]s.
[[Personalized PageRank]] is a [[standard tool]] for finding [[vertice]]s in a [[graph]] that are most relevant to a [[query]] or [[user]] .
[[Personalized search system]]s tailor [[search result]]s to the [[current user intent]] using [[historic search interaction]]s.
[[Personalized web service]]s strive to adapt their [[service]]s ([[advertisement]]s, [[news article]]s, etc.) to individual [[user]]s by making use of both [[content]] and [[user information]] .
per [[State]], per [[Hospital Referral Region]], etc.) to [[protect privacy]] .
[[Petabytes of data]] about [[human movement]]s, [[transaction]]s and [[communication pattern]]s are being [[generated by]] everyday [[technologies]] such as [[mobile phone]]s & [[credit card]]s.
[[PET]], [[Popular Events Tracking]], [[Social Communities]], [[Topic Modeling]]
[[Phase-space obstacle]]s, [[nonholonomic planning]], [[kinodynamic planning]], [[trajectory planning]], [[reachability analysis]], [[motion primitive]]s, [[sampling-based planning]], [[Barraquand-Latombe nonholonomic planner]], [[RRT]]s, [[feedback planning]], [[plan-and-transform method]], [[path-constrained trajectory planning]], [[gradient-based trajectory optimization]] .
[[Physical properti]]es include [[shape]], [[weight]], [[strength]], [[texture]] and [[appearance of things]], and ways to [[handle]] them.
Pilot [[user studi]]es demonstrate that our [[method]] helps [[researcher]]s acquire new [[knowledge]] [[efficiently]]: [[map user]]s achieved better [[precision]] and [[recall score]]s and found more [[seminal paper]]s while performing fewer [[search]]es.
Pilot [[user studi]]es over [[real-world dataset]]s demonstrate that [[our method]] helps [[user]]s comprehend [[complex stories]] better than [[prior work]] .
Pity targets the [[warm]] but not [[competent subordinate]]s; [[envy target]]s the [[competent]] but not [[warm competitor]]s; contempt is reserved for [[out-groups deemed]] neither [[warm]] nor [[competent]] .
[[Pixel]]s appear on the [[screen]]; [[lifts move]]; [[airline tickets are ordered]]; [[lists are sorted]]; [[e-mail]]s are delivered.
Plans are afoot to [[improve]], [[extend]] and [[miniaturize]] [[our technique]]s so that they can be used in other [[application]]s.
[[PMI-IR]] uses [[Pointwise Mutual Information (PMI)]] and [[Information Retrieval (IR)]] to measure the [[similarity]] of [[pairs]] of [[words]] or [[phrases]] .
[[POEM]] is evaluated on several [[multi-label classification problem]]s showing substantially improved [[robustness]] and [[generalization performance]] compared to the [[state-of-the-art]] .
[[Point]]s that are su±ciently far away from the [[region]]s, e.g., [[point]]s o1 and [[o2]], and [[point]]s in [[region O3]], are [[anomali]]es.
[[Point vector]]s are typically compared by [[dot product]]s, [[cosine-distance]] or [[Euclean distance]], none of which provide for [[asymmetric comparison]]s between objects (as is necessary to [[represent]] [[inclusion]] or [[entailment]]).
[[Pollack]] circumvented [[this problem]] by having [[vocabulari]]es with only a [[handful of]] [[word]]s and by [[manually defining]] a [[threshold]] to [[binarize]] the [[resulting vector]]s.
[[Polygonal]], [[polyhedral]], and [[semi-algebraic model]]s, [[Rigid-body transformation]]s, [[3D rotation]]s, [[kinematic chain]]s, [[Denavit-Hartenberg parameter]]s, [[kinematic tree]]s, [[nonrigid transformation]]s.
[[Polynomial]]s were developed to simulate five archetypal [[non-normal distribution]]s for [[baseline]] and [[post-treatment score]]s in a [[randomized trial]] .
Poor [[academic performance]] in [[K-12]] is often a [[precursor]] to [[unsatisfactory]] [[educational outcome]]s such as [[dropout]], which are associated with significant [[personal]] and [[social cost]]s.
Popularly applied in the public to [[cultural question]]s ([[election result]]s, [[box-office return]]s), they have recently been applied by [[corporation]]s to leverage employee [[knowledge]] and [[forecast answer]]s to [[business question]]s ([[sales volume]]s, [[product]]s and [[feature]]s, [[release timing]]).
[[Populating a database with unstructured information]] is a [[long-standing problem]] in [[industry]] and [[research]] that encompasses [[problems of extraction]], [[cleaning]], and [[integration]] .
[[Positive Punishment]] describes a situation where the [[presentation or addition]] of a [[stimulus as a consequence]] of an [[instance of behaviour]] makes that [[behaviour]] [[less likely to]] occur in that [[context]] in [[future]] .
Potential [[applications]] include [[computer-assisted hypothesis testing]], [[automated scientific discovery]], and [[automated]] construction]] of [[probabilistic expert systems]] .
[[Potential monitoring condition]]s are built on a [[set of predictive rule]]s which are [[automatically generated]] by a [[rule-based learning algorithm]] with [[coverage]], [[confidence]] and [[rule complexity criteria]] .
Potential [[performance]] [[improvement]]s could be gained by [[systematically]] posing [[MI-based feature selection]] as a [[global optimization problem]] .
[[Poverty]] has become more concentrated in [[high-poverty]] and [[disadvantaged neighborhood]]s.
[[Precision]] and [[recall]] are calculated in terms of [[pairs of items correctly]] or [[falsely assigned]] to the same [[cluster]] or to different [[cluster]]s.
[[Precision]], [[recall]], and the [[F measure]] are [[set-based measure]]s.
[[Predicting ad]] [[click-through rates (CTR)]] is a [[massive-scale learning problem]] that is central to the [[multi-billion dollar]] [[online advertising industry]] .
[[Prediction]], as we understand it in [[this book]], is concerned with [[guessing]] the [[short-term evolution]] of certain [[phenomena]] .
[[Predictive analytics]] include [[empirical method]]s ([[statistical]] and other) that generate [[data prediction]]s as well as [[methods for assessing predictive power]] .
[[Predictive text embedding]] utilizes both [[labeled]] and [[unlabeled data]] to [[learn]] the [[embedding of text]] .
Preferably, [[social networks]] should be [[compressed]] in a way that they still can be [[queried]] [[efficiently]] without [[decompression]] .
[[preference ranking]]s over different [[option]]s) from [[limited]] [[pairwise comparison]]s.
[[Preliminary result]]s show a promising [[precision]] in [[unsupervised relationship classification]] .
[[Previous approaches]] either applies the [[constant]] [[step size]] which assumes that the [[Lipschitz gradient]] is known in advance, or requires a [[sequence]] of [[decreasing]] [[step size]] which leads to slow [[convergence]] in practice.
Previous approaches to [[location-awareness with RF signal]]s have been severely hampered by [[non-Gaussian signal]]s, [[noise]], and [[complex correlation]]s due to [[multi-path effect]]s, [[interference]] and [[absorption]] .
[[Previous efforts]] either found [[approximate motifs]] or considered relatively [[small datasets]] residing in [[main memory]] .
[[Previous efforts]] on [[this problem]] either simplify the general [[problem]] by adding [[constraints]], or provide [[approximate solutions]] without any [[error]] [[guarantees]] .
Previous [[empirical results]] show that [[associative classification]] could provide better [[classification accuracy]] over many [[datasets]] .
[[Previous method]]s focus on [[linking entiti]]es in [[Web document]]s, and largely rely on the [[context]] around the [[entity mention]] and the [[topical coherence]] between [[entiti]]es in the [[document]] .
Previous [[research]] has focused on [[summarizing]] [[newswire articles]] or [[clusters]] of [[newswire documents]], [[scientific articles]], [[books]], and [[extracting]] [[opinion (sentiment) sentence]]s from [[reviews]] .
[[Previous research]] has tackled [[this problem]] by making use of various [[textual]] and [[structural feature]]s from a [[knowledge base]] .
[[Previous research]] has typically approached [[this]] as an [[unsupervised problem]] .
[[Previous research]] is limited to treating each [[query]] within a [[search session]] in [[isolation]], without paying attention to their [[dynamic interaction]]s with other [[queri]]es in a [[search session]] .
Previous [[research work]] mainly focuses on using [[categorical]] and [[textual information]] to [[predict]] the [[attributes of user]]s.
Previous studies usually took advantage of two major kinds of [[information]] for [[click prediction]], i.e., [[relevance information]] representing the [[similarity]] between [[ad]]s and [[queri]]es and [[historical click-through information]] representing [[users' previous preference]]s on the [[ad]]s.
[[Previous technique]]s are unable to handle [[this task]] well as they consider [[video semantic]]s [[independently]], which may [[overfit]] the [[sparse comment]]s in each [[shot]] and thus fail to provide [[accurate modeling]] .
Previous work on [[information diffusion]] has focused on [[modeling]] the [[diffusion dynamics]] and [[selecting node]]s to [[maximize]] / [[minimize influence]] .
Previous works solve this [[veracity problem]] by [[estimating]] both the [[user ability]] and [[question difficulty]] based on the [[knowledge]] in each [[task individually]] .
[[Price index]]es, [[nonfarm business sector]], [[first]] [[quarter]] [[1947]]–[[third]] [[quarter]] [[2010]]
[[Primarily]], the [[GAN]]s have difficulties in dealing with [[discrete data]] (e.g., [[text sequence]]s [ 3 ]).
[[Prior solutions]], such as the [[greedy algorithm]] of [[Kempe et al. (2003)]] and its [[improvements]] are slow and [[not scalable]], while other [[heuristic algorithm]]s do not provide consistently [[good performance]] on [[influence spreads]] .
[[Prior technique]]s suffer from several [[limitation]]s like [[click log sparsity]] and [[inability]] to distinguish between [[entiti]]es of different [[concept class]]es.
[[Prior work]] on [[computing semantic relatedness]] of [[word]]s focused on representing their [[meaning]] <i>in isolation </i>, effectively [[disregarding]] [[inter-word affiniti]]es.
[[Prior work]] on [[MIML]] has focused on [[predicting label set]]s for previously unseen [[bag]]s.
[[Prior work]] transforms the generalized [[eigenvalue problem]] into an equivalent [[least squares formulation]], which can then be [[solved]] [[efficiently]] .
[[Privacy]], [[Distributed]], [[Collusion]], [[Data Mining]], [[Secure Multiparty Computation]] .
[[Privacy-preserving]] [[data publishing address]]es the [[problem of disclosing sensitive data]] when [[mining]] for useful [[information]] .
[[Probabilistic frequent itemset mining]] in [[uncertain]] [[transaction databases]] [[semantically]] and [[computationally]] differs from [[traditional technique]]s applied to [[standard]] [[certain]] [[transaction databases]] .
[[probabilistic graphical models]], [[empirical Bayes]], [[variational methods]], [[automatic image annotation]], [[image retrieval]]
[[Probabilistic inference]] is an attractive approach to [[uncertain reasoning]] and [[empirical learning]] in [[artificial intelligence]] .
[[Probabilistic model]]s of [[network growth]] have been extensively studied as [[idealized representation]]s of [[network evolution]] .
[[probabilistic networks]], [[Bayesian belief network]]s, [[machine learning]], [[induction]]
[[Probabilistic soft logic (PSL)]] is a [[framework]] for [[collective]], [[probabilistic reasoning]] in [[relational domain]]s.
[[Probability theory]] provides a [[consistent framework]] for the [[quantification]] and [[manipulation]] of [[uncertainty]] and forms a one of the central foundsations for [[pattern recogtnition]] .
[[Problems of ordinal regression]] arise in many [[field]]s, e.g., in [[information retrieval]] ([[Herbrich et al. 1998]]), in [[econometric model]]s ([[Tangian and Gruber 1995]]), and in [[classical statistics]] ([[McCullagh 1980]]; [[Anderson 1984]]).
[[Processing large volumes of streaming data]] in [[near-real-time]] is becoming increasingly important as the [[Internet]], [[sensor network]]s and [[network traffic]] grow.
[[Product ad]]s are a popular form of [[search advertizing]] offered by major [[search engine]]s, including [[Yahoo]], [[Google]] and [[Bing]] .
[[Productivity]] and [[compensation measure]]s yield [[information]] on the extent to which [[the employed]] [[benefit]] from [[economic growth]] .
[[Productivity growth]] provides the basis for rising [[living standard]]s; [[real hourly compensation]] is a [[measure]] of [[workers’ purchasing power]] .
[[Product manager]]s talk to [[user]]s to help figure out [[what to build]], [[define requirements]], and [[write functional specifications]] .
[[Product matching]] is a challenging variation of [[entity resolution]] to [[identify]] [[representation]]s and [[offer]]s referring to the same [[product]] .
[[Product matching]] is highly difficult due to the broad spectrum of [[product]]s, many similar but different [[product]]s, frequently [[missing]] or [[wrong value]]s, and the [[textual nature]] of [[product title]]s and [[description]]s.
[[Product perception]] and [[popularity]] are constantly changing as new [[selection]] emerges.
[[Product]] [[perception]] and [[popularity]] are constantly changing as new selection emerges.
[[Product recommender system]]s are often deployed by [[e-commerce website]]s to improve [[user experience]] and increase [[sale]]s.
[[Professional Certificate]], [[Academic Degree]], [[Birth Certificate]], [[Digital Certificate]], [[Accreditation]], [[Audit]] .
[[Programming languages exist]] in many [[famili]]es and [[style]]s, rather like [[human language]]s.
:## [[Program]]s for which no current [[service]]s or [[performance]] are required such as [[annuities]], [[insurance claim]]s, or other [[benefit payment]]s.
[[Program workflows]] can help [[system operators]] and [[administrators]] to [[understand]] [[system behaviors]] and [[verify]] [[system executions]] so as to greatly [[facilitate]] [[system maintenance]] .
Progress in [[unifying]] [[logical]] and [[probabilistic inference]] has been slower.
Promising [[experimental result]]s demonstrate that the [[accuracy]] and [[efficiency]] of our [[Valkyrie system]] outperform other popular [[anti-malware software tool]]s such as [[Kaspersky AntiVirus]] and [[McAfee VirusScan]], as well as other [[alternative]] [[data mining]] based [[detection system]]s.
[[Promising result]]s are obtained in all three [[situation]]s for [[synthetic]] as well as [[real data-set]]s.
[[Properti]]es of interest often involve [[variable]]s that are not explicitly [[observed]] in [[real world diffusion]]s.
[[Properti]]es of the [[concept]] are defined by [[explicit semantic annotation]]s within [[the article]], where the [[annotation]]s often link to other [[articles and concept]]s, respectively.
[[Prototype system]]s, such as [[FaCT]] [21], [[Flora-2]] [31, 30], and [[TRIPLE]] [28], are now available to drive the [[knowledge representation]] and [[reasoning]] component of the [[Semantic Web]] .
Provided that these [[task]]s are [[imperfect substitute]]s, [[our model]] implies [[measurable change]]s in the [[composition]] of [[job task]]s, which we explore using representative data on [[task input]] for [[1960]] to [[1998]] .
Providing [[climatological]] and [[meteorological condition]]s covering a [[large region]] is potentially useful in many [[application]]s, such as [[smart grid]] .
Providing insight into the [[issue]] is important [[in gaining understanding]] of how [[unethical behavior]] may cascade from [[exposure]] to [[competitive setting]]s.
Providing such [[information]] on [[government activiti]]es is an important [[function]] of the [[accounting]] and [[reporting system]] which should capture, [[classify]], [[record]], and [[communicate relevant]], reliable, and comparable [[financial information]] for at least the following purposes: [[budgetary accounting]] and reporting, including reporting of [[actual against approved budget estimate]]s; general purpose [[financial reporting]]; [[management information]]; and [[statistical reporting]] .
[[PSL]] uses [[first order logic rule]]s as a [[template language]] for [[graphical model]]s over [[random variable]]s with [[soft truth value]]s from the [[interval]] [0,1].
[[PSORTb]] has remained the most precise [[bacteria]]l [[protein subcellular localization (SCL)]] [[predictor]] since it was first made available in [[2003]] .
[[Psychological factor]]s, applied to [[human]]s and [[robot]]s alike, include [[goal]]s, [[belief]]s, [[feeling]]s and [[preference]]s.
[[Published thesauri]], such as [[Roget’s]] and [[Macquarie]], divide the [[English vocabulary]] into around a thousand [[categories]] .
Pure [[functional programming]] is [[programming]] with [[mathematical function]]s.
Pure [[graph-based methods]] which may have significant [[constraints]] (such as every [[user]], every [[item]] and every [[tag]] has to occur in at least <math>p</math> [[posts]]), cannot make a [[prediction]] in most of [["real world" cases]] while [[our]] [[model]] [[improves]] the [[F-measure]] by over 30% compared to a [[leading algorithm]], in our [[real-world use case]] .
Put another way, [[written]] [[Chinese]] simply lacks [[orthographic words]] .
[[Pythia functionality rest]]s on two pillars: (i) [[dataset (partition) signature]]s, one [[per]] [[cohort]], and (ii) [[similarity notion]]s and [[algorithm]]s, which can identify the appropriate [[subset]] of [[cohort]]s to [[engage]] .
[[Python for Data Analysis]] is concerned with the nuts and bolts of [[manipulating]], [[processing]], [[cleaning]], and [[crunching data]] in [[Python]] .
[[QE]] initially increases the [[amount]] of [[bank deposit]]s those companies hold (in place of the [[asset]]s they [[sell]]).
[[Qualitatively]], [[we]] show that [[the proposed model]] learns a [[semantically]] and [[syntactically meaningful representation]] of [[linguistic phrase]]s.
Quantitatively, [[experimental results]] show that [[the proposed method]] [[outperform]]s several [[baseline methods]] for [[action prediction]] .
[[Quantitatively]], [[this decomposition]] suggests that [[ICT]] has large effects on the [[distribution of income within labor]], but only moderate effects on the [[distribution of income]] between [[capital]] and [[labor]] .
[[Quantitative notion]]s of [[diversity]] have been explored across a variety of [[discipline]]s ranging from [[conservation biology]] to [[economic]]s.
[[Query]] result [[clustering]] has recently attracted a lot of attention to provide [[user]]s with a [[succinct overview]] of [[relevant result]]s.
[[Question]]s trigger an [[iterative attention process]] which allows [[the model]] to condition its attention on the [[result]] of [[previous]] [[iteration]]s.
[[Random forests]] are a combination of [[tree predictor]]s such that each [[tree]] depends on the values of a [[random vector]] [[sampled independently]] and with the same [[distribution]] for all [[trees]] in the [[forest]] .
[[Random forests (RF)]] has become a popular [[technique for classification]], [[prediction]], [[studying variable importance]], [[variable selection]], and [[outlier detection]] .
[[Randomization approaches]] based on [[null model]]s provide, at least in principle, a general [[approach]] that can be used to obtain [[empirical]] [[p-value]]s for various types of [[data mining]] [[approach]]es.
[[Randomization]] offers new benefits for [[large-scale]] [[linear algebra computation]]s.
[[Randomization procedure]]s for [[clinical trial]]s are intended to [[create]] [[groups of patient]]s that are similar with regard to [[baseline characteristic]]s that influence [[prognosis]] (both [[known]] and [[unknown]]) other than the [[treatment being considered]] .
[[Randomized field trials (RFTs)]] provide a powerful means of [[testing]] a [[defined intervention]] under [[realistic condition]]s.
[[Ranking method]]s assign a [[score]] to every [[entity]] in the [[population]], and then use the [[assigned score]]s to create a [[ranked list]] .
[[Rank minimization]] is [[nonconvex]] and generally [[NP-hard]], imposing one major [[challenge]] .
Rather than a [[string of keyword]]s, [[we]] define a [[query]] as a [[small set]] of [[paper]]s deemed relevant to the [[research task]] at hand.
Rather than building a [[general-purpose model]], [[we]] propose a new [[type of topic model]] that incorporates the [[volume]] of [[term]]s into the [[temporal dynamics of topic]]s and [[optimizes estimates]] of [[term volume]]s.
Rather than giving all the [[credit]] to the last [[ad]] a [[user see]]s, [[multi-touch attribution]] allows more than one [[ad]]s to get the [[credit]] based on their corresponding [[contribution]]s.
Rather than training the [[discriminator]] to [[learn]] and [[assign]] [[absolute binary predicate]] for individual [[data sample]], the proposed [[RankGAN]] is able to [[analyze]] and [[rank]] a [[collection]] of [[human-written]] and [[machine-written sentence]]s by giving a [[reference group]] .
[[Rating data]] is ubiquitous on [[website]]s such as [[Amazon]], [[TripAdvisor]], or [[Yelp]] .
[[RDD]]s are motivated by two types of [[application]]s that current [[computing framework]]s handle [[inefficiently]]: [[iterative algorithm]]s and [[interactive data mining tool]]s.
[[RDF]] and [[RDF Schema]] are two [[W3C standard]]s aimed at enriching [[the Web]] with [[machine-processable semantic data]] .
[[Reaction label]]s are [[you rock (expressing approvement)]], [[tehee (amusement)]], [[I understand]], [[Sorry]], [[hugs]] and [[Wow, just wow (displaying shock)]] .
; [[Reaction norm]]: “If [[racial data]] are used in the [[allocation process]], then the [[hospital]] has to be [[fined accordingly.”]]
[[Reactive Search]] [[integrates sub-symbolic machine learning technique]]s into [[search heuristic]]s for solving [[complex optimization problem]]s.
Readers of [[medical literature]] need to consider two types of [[validity]], [[internal]] and [[external]] .
[[Readibility]], [[Level]], [[Reading Comprehension Level]], [[Readability of Text]] .
Read [[the book]] and learn about [[oracle]]s, [[geni]]es, [[singleton]]s; about [[boxing method]]s, [[tripwire]]s, and [[mind crime]]; about [[humanity's cosmic endowment]] and [[differential technological development]]; [[indirect normativity]], [[instrumental convergence]], whole [[brain emulation and technology coupling]]s; [[Malthusian economics and dystopian evolution]]; [[artificial intelligence]], and [[biological cognitive enhancement]], and [[collective intelligence]] .
[[Real]], and [[in-person]] [[social network]]s have many more [[triangle]]s than [[chance]] would dictate.
[[Realizing]] that [[heterogeneous entiti]]es and [[relationship]]s are also ubiquitous in the [[real world application]]s, there is an emerging need to [[retrieve]] and [[search similar]] or relevant [[data entiti]]es from multiple [[heterogeneous domain]]s, e.g., [[recommending]] relevant [[post]]s and [[image]]s to a certain <b> [[Facebook]] </b> [[user]] .
[[Real-scale]] [[Semantic Web application]]s, such as [[Knowledge Portal]]s and [[E-Marketplace]]s, require the management of [[large volumes of metadata]], i.e., information describing the available [[Web content]] and [[service]]s.
[[Real-Time Bidding(RTB)]] mimics [[stock spot exchange]]s and [[utilise]]s [[computer]]s to [[algorithm]]ically buy [[display ads]] [[per]] [[impression]] via a [[real-time auction]] .
[[Real-time]] [[search]] demands that they be [[indexed]] and [[searchable]] immediately, which leads to a number of [[implementation]] challenges.
[[Real-valued data]], [[biclustering]], [[association analysis]], [[range support]], [[microarray data]], [[functional module]]s
[[Real-world]], [[multiple-typed object]]s are often [[interconnected]], forming [[heterogeneous]] [[information network]]s.
[[Real-world network]]s, such as the [[World Wide Web]] and [[online social network]]s, are <i>[[very large]]</i> and are <i>[[evolving rapidly]] </i>.
[[Reasoning about time]], [[Event definition]], [[Event detection]], [[Event monitoring]] .
Recent [[advance]]s in [[linear classification]] have shown that for [[applications]] such as [[document classification]], the [[training]] can be extremely [[efficient]] .
Recent advances in [[search users' click modeling]] consider both [[users' search queri]]es and [[click]] / [[skip behavior]] on [[document]]s to infer the [[user's perceived relevance]] .
Recent [[experimental studi]]es [[document]] that disclosing [[prior collective opinion]]s distorts [[individuals' decision making]] as well as their [[perceptions of quality]] and [[value]], highlighting a fundamental disconnect from current modeling efforts: [[How to model [[social influence]] and its impact on [[system]]s that are [[constantly evolving]]?
Recent [[graphical Granger modeling]] can select [[features]] in [[lagged time point]]s but ignores the [[temporal correlation]]s within an [[individual's repeated measurement]]s.
Recent [[innovations]] have resulted in a [[plethora]] of [[social applications]] on [[the Web]], such as [[blogs]], [[social network]]s, and [[community photo]] and [[video sharing applications]] .
Recently, a large number of [[knowledge graph]]s have been created, including [[YAGO]] [4], [[DBpedia]] [5], [[NELL]] [6], [[Freebase]] [ 7], and the [[Google Knowledge Graph]] [8].
Recently, [[algorithm]]s have been extended to [[graph]]s with [[attribute]]s as often observed in the [[real-world]] .
Recently a [[method]] was suggested for [[combining]] [[active learning]] with a [[semi-supervised learning algorithm]] that uses [[Gaussian field]]s and [[harmonic function]]s.
Recently, [[graph-based methods]] have been proposed to [[rank]] [[sentence]]s or [[passages]] .
Recently, many [[research results]] demonstrate that one promising approach is creating [[compact]] and [[efficient]] [[hash codes]] that preserve [[data similarity]] .
Recently, many [[studies]] have been conducted on [[uncertain data]], where [[fields]] of [[uncertain attributes]] no longer have certain [[values]] .
Recently, more and more [[data-driven application]]s are being [[developed]] in [[search engine]]s based on [[search log]]s, such as [[query suggestion]], [[keyword bidding]], and [[dissatisfactory]] [[query analysis]] .
Recently, [[ranking data]] with respect to the [[intrinsic geometric structure]] ([[manifold ranking]]) has received considerable attentions, with encouraging [[performance]] in many [[application]]s in [[pattern recognition]], [[information retrieval]] and [[recommendation system]]s.
Recently several [[methods]] have been [[developed]] to [[automatically]] [[identify communities]] from [[complex networks]] by [[optimizing]] the [[modularity function]] .
Recently, several [[progressive sampling]] [[strategies]] for [[maximizing]] the overall [[data mining]] utility have been proposed.
Recently, [[simultaneous selection]] of [[feature]]s and [[feature group]]s (a.k.a [[bi-level selection]]) becomes increasingly popular since it not only reduces the number of [[feature]]s but also unveils the underlying [[grouping effect]] in the [[data]], which is a valuable [[functionality]] in many [[application]]s such as [[bioinformatics]] and [[web data mining]] .
Recently, there has been significant interest in a [[class]] of [[ranking algorithm]]s based on the [[assumption]] that [[data]] is [[sampled]] from a [[low dimensional manifold]] [[embedded]] in a [[higher dimensional Euclidean space]] .
Recently there have been many [[collection]]s of [[relational data]] in diverse areas such as the [[internet]], [[social network]]s, [[customer shopping record]]s, [[bioinformatics]], etc. The main goal of the [[relational data analysis]] is to discover [[latent structure]] from the [[data]] .
Recent [[mathematical result]]s on the [[stochastic properti]]es of [[MSP score]]s allow an analysis of the performance of [[this method]] as well as the [[statistical significance]] of [[alignment]]s it [[generate]]s.
Recent [[neural network sequence model]]s with [[softmax classifier]]s have achieved their best [[language modeling performance]] only with [[very large]] [[hidden state]]s and [[large vocabulari]]es.
Recent [[probabilistic approaches]] to [[citation matching]] include both [[directed]] (usually [[generative]]) [5, 27] and [[undirected]] (typically [[discriminative]]) [[graphical models]] [28, 29, 13] which [[we]] describe in more detail below.
Recent [[research]] on [[differential privacy]] has shown that the [[accuracy]] of many [[data queri]]es can be improved by [[post-processing]] the [[perturbed data]] to ensure [[consistency constraint]]s that are known to hold for the [[original data]] .
Recent [[studi]]es have demonstrated that [[AD]] is closely related to the [[structure]] [[change]] of the [[brain network]], i.e., the [[connectivity]] among different [[brain region]]s.
Recent [[studi]]es have suggested using [[relative distance]] [[comparison]]s as [[constraint]]s to represent [[domain knowledge]] .
Recent [[studies]] on [[social network]] [[evolution]] propose [[generative models]] which capture the [[statistical properties]] of [[real-world]] [[networks]] related only to [[node-to-node]] [[link]] [[formation]] .
Recent [[studi]]es suggest that [[human mobility]] is highly [[regular]] and [[predictable]] .
Recent [[work]] demonstrates that [[anonymizing node identiti]]es may not be sufficient to keep the [[network private]]: the availability of [[node]] and [[link data]] from another [[domain]], which is [[correlate]]d with the [[anonymized network]], has been used to [[re-identify]] the [[anonymized node]]s.
Recent [[work]] has developed [[methods for tracking]] [[topic shifts]] over long [[time scales]], as well as abrupt [[spikes]] in the appearance of particular [[named entities]] .
Recent [[work]] has examined the [[correlation]]s in different [[domain]]s and [[designed model]]s that exploit [[user preference]]s on a [[source domain]] to [[predict]] [[user preference]]s on a [[target domain]] .
Recent years have witnessed a [[proliferation]] of [[large-scale knowledge base]]s, including [[Wikipedia]], [[Freebase]], [[YAGO]], [[Microsoft's Satori]], and [[Google's Knowledge Graph]] .
Recent years have witnessed the wide [[proliferation]] of [[geo-sensory application]]s wherein a [[bundle]] of [[sensor]]s are [[deploy]]ed at different [[location]]s to [[cooperative]]ly monitor the [[target condition]] .
[[Reciprocity]], defined for a digraph as the [[percentage of]] [[edge]]s with a [[reciprocal edge]], is a [[key metric]] that has been used in the [[literature]] to compare different [[directed network]]s and provide "[[hint]]s" about their [[structural properti]]es: for example, are [[reciprocal edge]]s [[generated randomly]] by chance or are there other [[process]]es driving their [[generation]]?
Recognition of [[this problem]] has led to [[research]] on [[task management systems]], which can help by allowing fast [[task switching]], fast [[task resumption]], and [[automatic]] [[task identification]] .
Recognizing the role of [[repeated exposure]] to a [[message]], in [[this paper we]] propose a novel [[framework]] for the [[effective placement of content]]: Given the [[navigational pattern]]s of [[user]]s in a [[network]], <i>e.g. </i>, [[web graph]], [[hyperlinked corpus]], or [[road network]], and given a [[model]] of the [[relationship]] between [[content-adoption]] and [[frequency]] of [[exposition]], [[we]] define the <i>[[repetition-aware content-placement (RACP)]]</i> [[problem]] as that of [[identifying the set]] of <i>B</i> [[node]]s on which [[content]] should be placed so that the [[expected number of user]]s adopting that [[content]] is [[maximized]] .
[[Recommendation]] and [[review site]]s offer a wealth of [[information beyond rating]]s.
[[Recommender problem]]s with [[large]] and [[dynamic]] [[item pools]] are ubiquitous in [[web applications]] like [[content optimization]], [[online advertising]] and [[web search]] .
[[Recommender system]]s apply [[knowledge discovery technique]]s to the [[problem]] of making [[personalized recommendation]]s for [[information]], [[products or service]]s during a [[live interaction]] .
[[Recommender system]]s are becoming [[tools of choice]] to [[select]] the [[online information]] relevant to a given [[user]] .
[[Recommender system]]s are being used by an ever-increasing number of [[E-commerce sites]] to help [[consumers]] find [[products]] to [[purchase]] .
[[Recommender systems]]; [[Collaborative filtering]]; [[Similarity measure]]s; [[Evaluation metric]]s; [[Prediction]]; [[Recommendation]]; [[Hybrid;]] [[Social]]; [[Internet of things]]; [[Cold-start]]
[[Recommender system]]s have become very important for many [[online activiti]]es, such as [[watching movi]]es, [[shopping]] for [[product]]s, and connecting with [[friend]]s on [[social network]]s.
[[Recommender System]]s, [[Netflix]], [[Supervised Learning]], [[Ensemble Learning]]
[[Recommender system]]s traditionally assume that [[user profile]]s and [[movie attribute]]s are [[static]] .
[[Recording]] and [[reporting financial information]] requires keeping a [[chronological log of transaction]]s and [[events measured]] in monetary terms and [[classified]] and [[summarized]] in a [[useful format]] based on the [[business need]]s of the [[organization]] .
[[Recurrent model]]s have been shown to produce very [[strong result]]s for [[language modeling]], including ([[Mikolov, Karafiat, Burget, Cernocky, & Khudanpur, 2010]]; [[Mikolov, Kombrink, Lukas Burget, Cernocky, & Khudanpur, 2011]]; [[Mikolov, 2012]]; [[Duh, Neubig, Sudoh, & Tsukada, 2013]]; [[Adel, Vu, & Schultz, 2013]]; [[Auli, Galley, Quirk, & Zweig, 2013]]; [[Auli & Gao, 2014]]); as well as for [[sequence tagging (Irsoy & Cardie, 2014]]; [[Xu]], [[Auli, & Clark, 2015]]; [[Ling]], [[Dyer]], [[Black]], [[Trancoso]], [[Fermandez]], [[Amir]], [[Marujo, & Luis, 2015b]]), [[machine translation]] ([[Sundermeyer, Alkhouli, Wuebker, & Ney, 2014]]; [[Tamura, Watanabe, & Sumita, 2014]]; [[Sutskever, Vinyals, & Le, 2014]]; [[Cho, van Merrienboer, Gulcehre, Bahdanau]], Bougares, Schwenk, & Bengio, 2014b]]), [[dependency parsing]] ([[Dyer et al., 2015]]; [[Watanabe & Sumita, 2015]]), [[sentiment analysis]] ([[Wang, Liu, Sun, Wang, & Wang, 2015b]]), [[noisy text normalization]] ([[Chrupala, 2014]]), [[dialog state tracking]] (Mrksic, O Seaghdha, Thomson, Gasic, Su, Vandyke, Wen, & Young, 2015]]), [[response generation]] ([[Sordoni, Galley, Auli, Brockett, Ji, Mitchell, Nie, Gao, & Dolan, 2015]]), and [[modeling the relation]] between [[character sequence]]s and [[part-of-speech tag]]s ([[Ling et al., 2015b]]).
[[Recurrent network]]s ([[Elman, 1990]]) are designed to [[model sequence]]s, while [[recursive networks]] ([[Goller & Kuchler, 1996]]) are [[generalization]]s of [[recurrent networks]] that can handle [[tree]]s.
[[Recurrent neural networks (RNNs)]] are a [[superset]] of [[feedforward neural network]]s, augmented with the ability to [[pass information]] across [[time step]]s.
[[Recurrent]] (Section 10) and [[recursive]] (Section 12) [[architecture]]s, on the other hand, allow us to [[work]] with [[sequence]]s and [[tree]]s while preserving a lot of the [[structural information]] .
[[Recursive model]]s were shown to produce [[state-of-the-art]] or [[near state-of-the-art result]]s for [[constituency]] ([[Socher, Bauer, Manning, & Andrew Y., 2013]]) and [[dependency]] ([[Le & Zuidema, 2014]]; [[Zhu, Qiu, Chen, & Huang, 2015a]]) [[parse re-ranking]], [[discourse parsing]] ([[Li, Li, & Hovy, 2014]]), [[semantic relation classification]] ([[Hashimoto, Miwa, Tsuruoka, & Chikayama, 2013]]; [[Liu, Wei, Li, Ji, Zhou, & Wang, 2015]]), [[political ideology detection]] based on [[parse tree]]s ([[Iyyer, Enns, Boyd-Graber, & Resnik, 2014b]]), [[sentiment classification]] ([[Socher, Perelygin, Wu, Chuang, Manning, Ng, & Potts, 2013]]; [[Hermann & Blunsom, 2013]]), [[target-dependent sentiment classification]] ([[Dong, Wei, Tan, Tang, Zhou, & Xu, 2014]]) and [[question answering]] ([[Iyyer, Boyd-Graber, Claudino, Socher, & Daume III, 2014a]]).
[[Reduced rank regression]], [[recommender system]]s, [[latent factor]], [[factorization]], [[dyadic data]]
[[Referring]] to the [[minimum change principle]] in [[data repairing]], the [[objective]] is to find a [[minimum modification]] of [[inaccurate point]]s such that the [[large amount]] of [[dirty data]] can enhance the [[clustering]] .
Regarding [[analysis]], [[we]] summarize developments in marginal and conditional models, the sandwich estimator, model-based estimators, binary data, [[survival analysis]], [[randomization tests]], [[survey methods]], [[latent variable methods]] and [[nonlinear mixed models]], [[time series methods]], [[global tests for multiple endpoints]], [[mediation effects]], [[missing data]], [[trial reporting]], and [[software]] .
[[Regression analysis]] is the [[statistical methodology]] for [[predicting values]] of one or more [[<i>response</i> (dependent) variable]]s from a [[collection]] of [[<i>predictor</i> (independent) variable value]]s.
[[regularization]] - [[Radial Basis Function]]s - [[Support Vector Machines]] - [[Reproducing Kernel Hilbert Space]] - [[Structural Risk Minimization]]
[[Reisinger and Mooney (2010b]]) found [[pruning]] the [[low-value tf-idf feature]]s helps [[performance]] .
[[Reject null hypothesis]] [[Type I error rate]] [[Power=1-type II error rate]]
Related work in [[probabilistic matrix factorization]] ([[Mnih & Salakhutdinov, 2007]]) [[embed]]s [[row]]s and [[column]]s as [[Gaussians]], and some forms of this do provide each [[row]] and [[column]] with its own [[variance]] ([[Salakhutdinov & Mnih, 2008]]).
[[Relational databases]] exhibit rich [[graphical structure]] and are naturally [[modeled]] as [[graphs]] whose [[nodes]] represent [[entities]] and whose [[typed-edges]] represent [[relations]] between them.
[[Relational database]]s work by allowing the [[creation]] of arbitrary [[table]]s, which [[organize data]] into a [[collection of column]]s.
[[Relational similarity]] is [[correspondence]] between [[relation]]s, in contrast with [[attributional similarity]], which is [[correspondence]] between [[attribute]]s.
[[Relation classification]] uses the [[entity-annotated text]] as [[input]] and aims to [[identify the relation]]s between two [[entiti]]es based on a combination of two [[information source]]s: the [[text sequence]] between the two [[entiti]]es ([[extracted from the corpus]]), and the [[semantic type]] of the [[entities (extracted from the ontology]]).
[[Relation extraction]] is the [[task]] of discovering [[semantic connections]] between [[entities]] .
[[Relation extraction]] is the [[task]] of [[finding relationships]] between two [[entiti]]es from [[text content]]s.
[[Relation extraction]] is the [[task]] of [[finding]] [[semantic relations]] between [[entities from text]] .
[[Relationship mining]], [[Time-constrained probabilistic factor graph]], [[Co-author network]], [[Advisor-advisee prediction]] .
[[Releasing]] such [[network data]] could seriously jeopardize individual [[privacy]] .
[[Relevant sources of information]] on [[rural poverty]] include [[classic community]] and [[regional analyse]]s, studies of [[rural-urban migration]], [[regional development]] and [[underdevelopment]], [[economic restructuring]], and [[labor market analysis]] .
Relying on the [[scalability]] of [[our method]], we [[experimentally]] demonstrate that strategies based on [[adaptive betweenness centrality]] are effective in important [[applications studied]] in the [[network science]] and [[database communiti]]es.
Remarkably, the same individuals surface repeatedly - some of the same figures who have claimed that the [[science]] of [[global warming]] is "not settled" [[denied]] the [[truth]] of [[studi]]es [[link]]ing [[smoking]] to [[lung cancer]], [[coal smoke]] to [[acid rain]], and [[CFC]]s to the [[ozone hole]] .
Remarkably, training such a purely [[lexical model]] to [[maximize likelihood]] will [[induce word representations]] with striking [[syntactic]] and [[semantic properti]]es.
Renowned [[researcher]]s [[Richard Wilkinson]] and [[Kate Pickett]] lay bare the [[contradiction]]s between [[material success]] and [[social failure]] in the [[developed world]] .
[[Representing]] the [[product]] as a [[set of attribute-value pairs]] (<i>[[Brand]]: Tropicana, [[Pulp]]: Low, [[Fortified with]]: Vitamin-D, [[Size]]: 1 liter, [[Bottle Type]]: Plastic</i>) would enable the [[retailer]] to use [[data]] from other [[product]]s having [[similar]] [[attribute]]s and [[forecast more accurately]] .
Requiring [[heterogeneous information system]]s to [[cooperate]] and [[communicate]] has now become crucial, especially in application areas like [[e-business]], [[Web-based mash-up]]s and the [[life science]]s.
[[ResearchCyc]]<sup>TM</sup> is a version of [[Cycorp Inc.]]’s [[Cyc Knowledge Base]] - the world’s largest general [[common-sense ontology]] and [[reasoning engine]] .
Researchers from various [[discipline]]s such as [[statistics]] and [[AI]] considered the use of [[ensemble methodology]] .
[[Researcher]]s have access to [[large online archive]]s of [[scientific article]]s.
[[Researchers]] have [[analyzed]] different [[properties]] of such [[OSNs]], mainly focusing on the [[formation]] and [[evolution]] of the [[networks]] as well as the [[information propagation]] over the [[networks]] .
[[Research]] in [[relational data mining]] has two major [[directions]] : [[finding]] [[global models]] of a [[relational database]] and the [[discovery]] of [[local]] [[relational patterns]] within a [[database]] .
[[Research]] on [[time series forecasting]] is mostly focused on [[point prediction]]s - [[model]]s are obtained to [[estimate]] the [[expected value]] of the [[target variable]] for a certain [[point in future]] .
[[Resolve]]: A [[document promulgated]] by the [[Legislature]], [[expressing its intent]] .
[[Resting]] on the analysis of [[file content]]s extracted from the [[file sample]]s, like [[Application Programming Interface (API)]] calls, [[instruction sequence]]s, and [[binary string]]s, [[data mining method]]s such as [[Naive Baye]]s and [[Support Vector Machine]]s have been used for [[malware detection]] .
[[Results]] also demonstrate the [[scalability]] of [[proposed two-stage approach]] .
[[Result]]s also show that most [[marker]]s selected by [[the proposed algorithm]] are consistent with findings from [[existing]] [[cross-sectional studi]]es.
[[Result]]s from a [[user study]] evaluating the [[entertainment value]] of [[adaptive stori]]es created by [[our system]] as well as two [[fixed]], [[pre-authored stories indicate]] that [[automatically adapting a story]] based on [[learned player preference]]s can increase the [[enjoyment of playing a computer role-playing game]] for certain [[types of player]]s.
[[Result]]s from a [[user study]] show that our simple [[coverage algorithm]] does as well as most popular [[blog]] [[aggregation site]]s, including [[Google Blog Search]], [[Yahoo! Buzz]], and [[Digg]] .
[[results]] from extensive [[simulations]] on several [[real-world]] and [[synthetic networks]] demonstrate that [[our algorithm]] is currently the best [[scalable solution]] to the [[influence maximization problem]]: (a) [[our algorithm]] [[scales]] beyond [[million-sized]] [[graphs]] where the [[greedy algorithm]] becomes [[infeasible]], and (b) in all [[size ranges]], [[our algorithm]] performs consistently well in [[influence spread]] -- it is always among the [[best algorithms]], and in most [[cases]] it significantly [[outperform]]s all other [[scalable heuristics]] to as much as 100% -- 260% [[increase]] in [[influence spread]] .
[[Result]]s indicated that [[language-based assessment]]s can constitute valid [[personality measure]]s: they agreed with [[self-report]]s and [[informant report]]s of [[personality]], added [[incremental validity]] over [[informant report]]s, [[adequately discriminated]] between [[trait]]s, [[exhibited pattern]]s of [[correlation]]s with [[external criteria]] similar to those found with [[self-reported personality]], and were stable over [[6-month interval]]s.
[[Result]]s on [[real]] and [[synthetic dataset]]s show that <i>[[TKU]]</i> has excellent [[performance]] and [[scalability]] .
[[Result]]s on several [[dataset]]s show the efficacy of our [[approach]] .
[[Result]]s showed a 12.5% [[click lift]] compared to a standard [[context-free bandit algorithm]], and the [[advantage]] becomes even [[greater when data]] gets more [[scarce]] .
[[Results]] show that [[CRR]] often achieves [[performance]] equivalent to the best of both [[ranking-only]] and [[regression]]-only [[approaches]] .
[[Result]]s show that [[our formulation]] consistently [[outperforms method]]s without the [[temporal smoothness constraint]]s.
[[Result]]s show that our [[method]] is able to improve the [[accuracy of similarity estimation]] in comparison to single [[ontology approach]]es and against [[state of the art]] related [[work]]s in [[multi-ontology]] [[similarity assessment]] .
[[Result]]s show that [[our proposed algorithm]] is much more [[efficient]] and [[effective]] than its [[convex relaxation]] .
[[Result]]s show that [[the proposed model]] effectively captures [[salient semantic information]] in [[queri]]es and [[document]]s for the [[task]] while significantly [[outperforming]] previous [[state-of-the-art semantic model]]s.
[[Result]]s show that [[the system]] has a great ability to [[detect event]]s of a [[magnitude]] in the [[region]] of 3.5, with relatively [[low occurrence]]s of [[false positive]]s.
[[Result]]s show the [[benefit]] of each of [[the proposed technique]]s, and the final [[integrated approach]] significantly improves the [[prediction]] [[quality]] .
[[Result]]s using [[our technique]]s are reported on a 32[[GB]] [[dataset]] consisting of over 13 million [[user review]]s of 171, 493 [[Android app]]s in the [[Google Play Store]] .
[[Retained revenue receipt]]s, [[federal grant]]s, and [[trust fund]]s are [[automatically fully allotted]] .
[[Retrospective analysis]] demonstrates the [[accuracy]] of [[this approach]] and ongoing [[laboratory experiment]]s suggest that [[kinases]] identified by our [[system]] may indeed [[phosphorylate p53]] .
[[Retrospective model]]s that used a [[combination]] of [[latent topic feature]]s and [[structured feature]]s achieved [[AUC]]s of 0.96, 0.82, and 0.81 for [[in-hospital]], 30 day, and 1-year [[mortality prediction]] .
[[Reviewing]] the [[technologi]]es that enable [[robot musician]]s to [[jam]] .
[[reviews]], [[sentiment classification]], [[summarization]], [[text mining]] .
[[Review summarization]] (generating [[statistical description]]s of [[review set]]s) sacrifices the [[immediacy]] and [[narrative structure]] of [[review]]s.
[[Rich lexica]] such as [[WordNet]] are valuable [[resource]]s for [[information extraction from unstructured text]] .
[[Rifkin]] uncovers a paradox at the heart of [[capitalism]] that has propelled it to greatness but is now taking it to its death - the inherent [[entrepreneurial]] [[dynamism]] of [[competitive market]]s that drives [[productivity]] up and [[marginal cost]]s [[down]], enabling [[business]]es to [[reduce]] the [[price]] of their [[goods and service]]s in order to win over [[consumers]] and market share.
[[Risk model]]s need to be developed for specific [[patient population]]s, specific [[institutions]], specific [[procedure]]s, and specific [[outcome]]s.
[[Road surface]] [[skid resistance]] has been shown to have a [[strong relationship]] to [[road crash risk]], however, applying [[the current method]] of using [[investigatory level]]s to [[identify]] [[crash prone road]]s is problematic as they may fail in [[identifying]] [[risky road]]s outside of the norm.
[[Robot]]s may always have [[perceptual]], [[cognitive]] and [[actuation]] [[limitation]]s.
[[Robust Principal Component Analysis (RPCA)]] is a general [[framework]] to [[extract]] such [[structure]]s.
[[ROCAT]] is especially designed to [[detect]] [[subspace cluster]]s on [[categorical data]] which may [[overlap]] in [[object]]s and/or [[attribute]]s; i.e.
[[ROCAT]] naturally avoids undesired [[redundancy]] in [[cluster]]s and [[subspace]]s by allowing [[overlap]] only if it improves the [[compression rate]] .
[[Role discovery]] in [[graph]]s is an [[emerging area]] that allows [[analysis]] of [[complex graph]]s in an [[intuitive way]] .
[[Role]]s should be automatically determined from the [[data]], and could be, for example, "[[clique-member]]s", "[[periphery-node]]s", etc. [[Role]]s enable numerous novel and useful [[network-mining task]]s, such as [[sense-making]], [[searching]] for [[similar node]]s, and [[node classification]] .
[[ROO]] guides [[user]]s through the [[ontology construction process]] by following a [[methodology]] geared towards [[domain expert]]s’ involvement in [[ontology authoring]], and exploiting [[intelligent user interfaces technique]]s.
[[RSC]] was also able to [[detect bot]]s with a [[precision]] higher than [[94%]] .
[[RTB]] allows [[advertiser]]s to bid on a [[display ad impression]] in [[real time]] when it is being [[generated]] .
[[RTF]] handles [[missing values]] and learns from [[pairwise]] [[ranking]] [[constraint]]s.
[[Rubik]] incorporates 1) [[guidance constraint]]s to align with existing [[medical knowledge]], and 2) [[pairwise constraint]]s for obtaining distinct, [[non-overlapping phenotype]]s.
[[Running experiment]]s at [[large scale]] requires addressing multiple challenges in three areas: [[cultural]] / [[organizational]], [[engineering]], and [[trustworthiness]] .
[[Saban]], previously a [[head coach]] of [[NFL's Miami]], is now coaching [[Crimson Tide]] .
[[SALDO]] ([[Borin et al., 2013]]) is the largest [[freely available]] [[lexical resource for Swedish]] .
[[Sales representative]]s must have access to [[meaningful]] and [[actionable intelligence]] about potential [[customer]]s to be [[effective]] in their [[role]]s.
[[Sample Space]] (S): defined as the [[set]] of all possible [[outcomes]] from a [[random experiment]] .
[[Sampling]] and [[column (variable) selection]] are commonly used to gain insights that improve [[data mining]] [[performance]] on [[very large database]]s.
[[Sampling]] is a [[standard approach]] in [[big-graph analytic]]s; the goal is to [[efficiently estimate]] the [[graph properti]]es by consulting a [[sample]] of the whole [[population]] .
::[[Samsung]], [[Sony]], [[HTC]], [[LG]], [[Motorola Mobility]] and [[Apple]] are just a few examples of the many manufacturers that produce these types of devices.
[[Sato]] received the [[test-of-time award]] from the [[International Conference on Logic Programming]] in [[2015]] for [[his seminal 1995 paper]] on [[PRISM]] .
[[Scalable]] [[similarity search]] is the core of many [[large scale]] [[learning]] or [[data mining applications]] .
[[Scalar]]s are [[represented by]] [[lowercase italic letter]]s, <math>c</math>.
[[Scala]] unifies traditionally disparate [[programming-language philosophi]]es to develop new [[component]]s and [[component system]]s.
[[Scaling web application]]s like [[recommendation system]]s, [[search]] and [[computational advertising]] is challenging.
[[Scarcity]] is considered a [[ubiquitous feature]] of the [[human condition]] .
[[Scenic cluster]]s by [[arranging object]]s and [[attribute]]s in a [[cluster-specific]] [[low-dimensional space]] .
[[Scholarly data]] can be used to analyse a huge amount of [[research element]]s such as [[paper]]s, [[author]]s, [[affiliation]]s, [[venue]]s, [[topic]] and [[communiti]]es [19].
[[Schuetze (1998)]] proposed a [[method]] for dividing occurrences of a [[word]] into [[classes]], each of which consists of [[contextually]] [[similar]] [[occurrence]]s.
[[Scientific literature]] with [[rich metadata]] can be [[represented as]] a [[labeled directed graph]] .
[[Scientific publication]]s are [[documentary representation]]s of [[defeasible argument]]s, supported by [[data]] and [[repeatable method]]s.
... [[Scientists]] have always [[read strategically]], working with many [[articles]] simultaneously to [[search]], [[filter]], [[compare]], [[arrange]], [[link]], [[annotate]], and [[analyze]] [[fragments of content]] .
[[Scientists]] have always [[read strategically]], working with many [[articles]] simultaneously to [[search]], [[filter]], [[scan]], [[link]], [[annotate]], and [[analyze]] [[fragments of content]] .
[[Scientist]]s have [[segmented sleep]] into several [[stage]]s and stage two is characterized by two [[pattern]]s (or [[anomali]]es) in the [[EEG time series recorded]] on [[sleep subject]]s.
[[Search federation]] deals with problems of the selection of [[search engine]]s to [[query]] and [[merging]] of their [[result]]s into a [[single result set]] .
[[Search log]]s, which contain [[rich]] and [[up-to-date]] [[information]] about [[user]]s' [[needs]] and [[preference]]s, have become a critical [[data source]] for [[search engine]]s.
[[Search]] plays an important [[role]] in [[online social network]]s as it provides an essential [[mechanism]] for [[discovering member]]s and [[content on the network]] .
[[Search queries]] are typically very [[short]], which means they are often [[underspecified]] or have [[senses]] that the [[user]] did not [[think of]] .
Second, beyond mere [[classification accuracy]], [[we]] often wish to gain some [[insight]] into the [[data]] .
Second, [[confidence estimate]]s are essential for [[interactive information extraction]], in which [[user]]s may [[correct]] [[incorrectly extracted field]]s.
Second, in all the four [[cases]] our [[system]] managed to significantly improve [[clustering]] quality as well, achieving the [[state-of-the-art]] [[result]]s.
Second, [[it]] includes a highly extensible [[optimizer]], [[Catalyst]], built using features of the [[Scala programming language]], that makes it easy to [[add composable rule]]s, [[control code generation]], and define extension [[point]]s.
Second, [[it]] only uses the [[black]] and [[white stone]]s from the board as [[input feature]]s.
Secondly, by using [[hash table]]s in a [[heavy-hitters type algorithm]] for establishing a [[noise]] [[baseline]], [[we]] show how to [[track]] even all [[keyword pair]]s using only a [[fixed amount]] of [[memory]] .
Secondly, [[unlike prior work]]s, [[our proposed algorithm]] decouples the [[supervision information]] and [[intrinsic data structure]] .
Secondly, [[we]] propose [[DUST]], a novel [[distance measure]] that accommodates [[uncertainty]] and [[degenerates]] to the [[Euclidean distance]] when the [[distance]] is large compared to the [[error]] .
... Second, since many of the [[methods]] we discuss rely on [[Gibbs sampling]] to infer [[topic distributions]], [[we]] also present a simple method, [[SparseLDA]], for [[efficient]] [[Gibbs sampling]] in [[topic models]] along with a [[data structure]] that results in very fast [[sampling]] [[performance]] with a small [[memory footprint]] .
Second, the [[ads blindness]] / [[sighted-ness]] results indicate that a focus on [[user satisfaction]] could help to reduce the [[ad load]] on the [[internet]] at large with [[long-term neutral]], or even [[positive]], [[business impact]] .
Second, the [[set]] of [[client machine]]s used for [[training]] and their [[class label]]s may also change over [[time]] .
Second, to alleviate the impact of [[irrelevant]] [[content]] [[attributes]], [[we]] develop a [[discriminative model]] for [[content analysis]] .
Second, to exploit [[post-result]] [[user behavior]] for better [[ranking]] of [[sponsored results]], [[we]] focus on [[identifying]] [[patterns]] in [[user behavior]] and <i>[[predict]]</i> [[expected]] [[on-site actions]] in future [[instance]]s.
[[Second]], we compute the [[combined [[social influence]] based similarity]] between each [[pair of vertice]]s by unifying the [[self-similarity]] and multiple [[co-influence similarity score]]s through a [[weight function]] with an [[iterative update method]] .
Second, [[we]] describe a [[mechanism]] for effectively combining several [[signal]]s in building a [[unified dataset]] for [[related search recommendation]]s.
Second, [[we]] describe a [[scalable procedure]] for training a [[collection]] of [[neural network]]s of different [[size]]s but with [[partially shared architecture]]s.
Second, [[we]] formalize these [[properti]]es with the [[combination]] of [[community feature extraction]] into a [[graph ranking formulation]] based on [[constraint quadratic programming]] .
Second, [[we]] implement this methodology to [[estimate the probability]] of [[computerisation]] for 702 [[detailed occupation]]s, and examine expected impacts of future [[computerisation]] on [[US labour market]] [[outcome]]s.
Second, [[we]] [[model sequence]]s as [[graph]]s and propose two [[algorithm]]s to solve the problem by operating on the [[graph]]s.
Second, [[we]] present a number of new [[approximation method]]s and novel [[refinement]]s of existing [[technique]]s.
Second, [[we]] present several [[evaluation metric]]s to [[evaluate]] a given [[activity predictor]], and discuss their [[pros and cons]] in the [[context]] of [[real-world application]]s.
Second, [[we]] propose a [[clustering-based multigraph model]] to capture the [[fine-grained]] [[clustering-based relationship]]s between [[pairwise vertice]]s and between [[pairwise path edge]]s.
Second, [[we]] show how to employ a [[learning-to-rank approach]] in order to [[learn]], for each [[node]] of the [[tree]], a [[ranking of product]]s appropriate to the [[user]]s who reach that [[node]] .
Section 3 explores a variety of [[pitfall]]s and [[challenge]]s, [[spanning]] the [[evidential]], the [[methodological]], and the more [[properly conceptual]] .
[[Sedation]] with the [[SEDASYS System (SED)]] and sedation with each site's current standard of care ([[CSC]]; [[benzodiazepine]] / [[opioid]] [[combination]]).
See also [[BINOMIAL DISTRIBUTION]]; [[CHI-SQUARED DISTRIBUTION]]; [[POISSON DISTRIBUTION]]; [[t-DISTRIBUTION]] .
See also [[CRITICAL PATH ANALYSIS]]; [[NETWORK FLOW PROBLEMS]]; [[RELIABILITY THEORY]] .
See also [[incremental budgeting]], [[performance-based budgeting]] and [[program budgeting]] .
See also [[incremental budgeting]], [[performance-based budgeting]] and [[zero-based budgeting]] .
See also [[incremental budgeting]], [[program budgeting]] and [[zero-based budgeting]] .
See also [[performance-based budgeting]], [[program budgeting]] and [[zero-based budgeting]] .
Seeking a [[practical]], [[scalable approach]] to [[sparsification]], [[we]] devise [[Spine]], a [[greedy]], [[efficient algorithm]] with practically little [[compromise]] in [[quality]] .
Selecting an [[informative subset]] of [[feature]]s has important [[application]]s in many [[data mining task]]s especially for [[high-dimensional data]] .
Selecting small [[subset]]s of [[sensor]]s, also referred to as <i>[[active]]</i> [[sensor]]s, often leads to lower [[operational cost]]s, and it reduces the [[noise]] and [[information overload]] for [[prediction]] .
[[Selective hashing]] is also easy to [[build and update]], and [[outperform]]s all the [[state-of-the-art algorithm]]s such as [[DSH]] and [[IsoHash]] .
[[Selective sampling]] is an [[active variant]] of [[online learning]] in which the [[learner]] is allowed to [[adaptively query]] the [[label]] of an [[observed example]] .
[[Semantically Annotated Corpus]], [[Semantic Annotation]], [[Domain-Specific Ontology]], [[kddo1 Ontology]], [[kdd09cma1 Corpus]], [[Linguistically Grounded Ontology]], [[Data Mining Terminology]], [[Research Paper Abstract]] .
[[Semantic annotation]]s are to [[tag]] [[ontology class instance data]] and [[map]] it into [[ontology classes]] .
[[Semantic annotation]], [[semantic search]], [[highly accurate closed domains]], [[multi-ontology]] .
[[Semantic heterogeneity]], [[semantic technologi]]es, [[ontology matching]], [[ontology alignment]], [[schema matching]] .
[[Semantic parsing]] is a [[rich fusion]] of the [[logical]] and the [[statistical world]]s.
[[Semantic parsing]] maps [[text]] to [[formal meaning representations]] .
[[Semantic Relation Extraction Algorithm]], [[Protein-Protein Interaction]], [[Log Odds Ratio]], [[PPLRE Project]]
[[Semantic Web document]]s contain [[metadata]] to express the [[meaning]] of their [[content]] .
[[SEMG signal]]s are predominantly different in [[conditional probability distribution]] across [[subject]]s.
[[Semi-CRFs]] are a [[conditionally trained]] version of [[semi-Markov chain]]s.
[[Semi-supervised classification method]]s aim to exploit [[labelled]] and [[unlabelled examples]] to train a [[predictive model]] .
[[Semi-supervised clustering]] leverages [[side information]] such as [[pairwise constraint]]s to [[guide clustering procedure]]s.
[[Semi-Supervised Learning Algorithm]], [[Self-Training Algorithm]], [[Semi-Supervised Generative Model Algorithm]], [[Semi-Supervised S3VM Algorithm]], [[Semi-Supervised Graph-based Algorithm]], [[Semi-Supervised Multiview Algorithm]] .
[[Semi-Supervised Learning]], [[Feature Selection]], [[Graph Classification]], [[Data Mining]]
[[Semi-supervised Learning]], [[Quantification]], [[Classification]], [[Concept Drift]], [[Class Distribution]]
[[Semi-supervised learning]] reduces [[data sparsity]] and gives improved [[generalization accuraci]]es in [[high dimensional domain]]s like [[NLP]] .
[[Semi-supervised learning (SSL)]] is halfway between [[supervised]] and [[unsupervised learning]] .
[[Sense labeling]] [[assigns]] a [[sense]] to each [[class]], and, in combination with [[sense discrimination]], to each [[occurrence of the ambiguous word]] .
[[Senses]] are interpreted as [[groups]] (or [[clusters]]) of [[similar]] [[contexts]] of the [[ambiguous word]] .
[[Sensor node]]s have limited [[local storage]], [[computational power]], and [[battery life]], as a result of which it is desirable to [[minimize]] the [[storage]], [[processing]] and [[communication]] from these [[node]]s during [[data collection]] .
[[Sentence Similarity]] is the process of [[computing a similarity score]] between [[two]] [[sentence]]s.
[[Sentence]]s that share [[semantic]] and [[syntactic properti]]es are thus mapped to [[similar vector representation]]s.
[[Sentence]]s written in this [[language]] unambiguously map into a number of [[knowledge representation format]]s including [[OWL]] and [[RDF-S]] to allow [[round-trip ontology management]] .
[[Sentiment Analysis]] focuses on this [[task]] of [[automatically]] [[identifying]] whether a [[piece of text]] expresses a [[positive]] or [[negative opinion]] about the [[subject matter]] .
[[Sentiment analysis]] is the [[computational study]] of [[people]]'s [[opinion]]s, [[sentiment]]s, [[emotion]]s, and [[attitude]]s.
[[Separately]], [[researcher]]s have looked into using [[social network information]] to improve [[recommendation]] .
[[Sequence analysis tasks]] in [[language]] and [[biology]] are often described as mappings from [[input]] [[sequence]]s to [[sequences of labels]] [[encoding]] [[the analysis]] .
[[Sequence data]] is found in a wide variety of [[application domain]]s such as [[intrusion detection]], [[bio-informatics]], [[weather prediction]], [[system health management]], etc.
[[Sequence dataset]]s are encountered in a [[plethora]] of [[applications spanning]] from web usage [[analysis]] to [[healthcare studi]]es and [[ubiquitous computing]] .
[[Sequence]]s can be of different types, such as [[binary]], [[discrete]], and [[continuous]], depending on the [[type]] of [[event]]s that form the [[sequence]]s.
[[Sequential decision-making with multiple agent]]s and [[imperfect information]] is [[commonly modeled]] as an extensive [[game]] .
[[Sequential games against nature]], [[value iteration]], [[policy iteration]], [[infinite-horizon planning]], [[discounted cost]], [[average cost]], [[reinforcement learning]], [[sequential game]]s.
[[Sequential pattern]] [[discovery]] is a [[well-studied field]] in [[data mining]] .
:## [[Service]]s performed by [[employee]]s, [[contractor]]s, [[subrecipient]]s, and other [[payee]]s; and<br>
[[Setting]] appropriate [[minimum utility threshold]]s by [[trial and error]] is a [[tedious process]] for [[user]]s.
Several [[algorithm]]s have also been proposed to [[mine]] the most [[discriminative patterns]] directly without [[costly]] [[feature selection]] .
Several [[algorithm]]s have been proposed to solve the [[classification problem]] on [[uncertain data]] recently, for [[example]] by extending traditional [[rule-based classifier]] and [[decision tree]] to work on [[uncertain data]] .
Several examples are given, including the ability to describe [[physical phenomena involving gravity]], [[spring]]s, [[velocity]], [[acceleration]], etc. using ordinary [[differential equation]]s.
Several [[extensions]] are proposed, such as [[learning]] [[words]]' [[class boundary]] [[distributions]] .
[[Several factor]] influence the choice of an [[outlier model]], including the [[data type]], [[data size]], availability of relevant [[outlier example]]s, and the need for [[interpretability]] in a [[model]] .
Several [[feature]]s of our proposed [[summary]] are: (1) each [[profile]] in the [[summary]] can be analyzed independently, (2) it provides [[error guarantee]] ([[&#949;-adequate]]), and (3) it produces no [[false positive]]s or [[false negative]]s.
Several [[military]], [[intelligence]], and [[law-enforcement organization]]s are working with us to further [[test]] and [[field software]] for [[this emerging application]] .
Several [[model]]s were [[subsequently]] proposed in the [[statistics literature]], including [[autoregressive moving average (ARMA)]], [[autoregressive integrated moving average (ARIMA)]], [[vector autoregression (VARMA)]], [[CUmulative SUM Statistics (CUSUM)]], [[exponentially weighted moving average]], etc.
Several [[OBIE system]]s have been [[implemented previously]] but all of them use a [[single]] [[ontology]] although multiple [[ontologies]] have been designed for many [[domains]] .
Several [[parallel]] and [[sequential algorithm]]s have been proposed in the [[literature]] to solve [[this problem]] .
Several proposed techniques, such as [[workflow mining]], are aimed at [[automatically]] [[deriving]] the underlying [[workflow]] [[model]]s.
Several recent [[studies]] [1, 2, 3, 13, 25] have used [[post-result]] [[browsing]] [[behavior]] including the [[sites]] [[visited]], the [[number]] of [[clicks]], and the [[dwell time]] on [[site]] in order to improve the [[ranking]] of [[search results]] .
Several [[studi]]es have demonstrated improvements in [[physician]]s' adherence to [[guideline]]s when such [[guideline]]s are provided in a [[computerized format]] .
Several [[technique]]s deal with [[this problem]] by [[loading]] [[blocks of data]] from [[disk]] one at a [[time]], but usually take a considerable number of [[iteration]]s to [[converge]] to a reasonable [[model]] .
Several US studies show that [[fluctuation]]s in [[relative]] [[labor supply]] play an important role in [[explaining trends]] in the [[skill premium]] (e.g. [[Katz and Murphy (1992)]], [[Card and Lemieux (2001)]]).
[[Shallow]] and [[deep learner]]s are distinguished by the depth of their [[credit assignment path]]s, which are [[chain]]s of [[possibly learnable]], [[causal link]]s between [[action]]s and [[effect]]s.
[[Shallow parsing]] identifies the [[non-recursive cores]] of various [[phrase]] [[types]] in [[text]], possibly as a [[precursor]] to [[full parsing]] or [[information extraction]] (Abney, 1991).
[[shallow semantic parsing]]; [[automated terminology extraction]]; [[composite CRF ensemble]]s; [[product offer titles]]; [[hyperlink insertion]]
[[Shapelet]]s are [[discriminative sub-sequence]]s of [[time series]] that best [[predict]] the [[target variable]] .
[[Shark]] is a new [[data analysis system]] that marries [[query processing]] with [[complex analytics]] on [[large cluster]]s.
[[Shifted PPMI]] is far better at [[optimizing]] [[SGNS’s objective]], and [[performs slightly better]] than [[word2vec derived vector]]s on several [[linguistic task]]s.
Shifting to a new [[domain]] requires the [[user]] to [[name]] the [[target]] [[relations]] and to [[manually create]] new [[extraction rules]] or [[hand-tag]] new [[training examples]] .
[[Shilling attacker]]s apply [[biased rating profile]]s to [[recommender system]]s for manipulating [[online product recommendation]]s.
[[ShoppingAdvisor]] leverages both [[user preference]]s and [[technical product attribute]]s in order to generate its [[suggestion]]s.
[[Short-ranged travel]] is [[periodic]] both [[spatially]] and [[temporally]] and not effected by the [[social network structure]], while [[long-distance travel]] is more [[influence]]d by [[social network]] ties.
[[Short text clustering]] has become an increasingly important [[task]] with the popularity of [[social media]] like [[Twitter]], [[Google +]], and [[Facebook]] .
• Should there be [[zero]] (e.g., [[Go]]), [[one]] (e.g., [[Smalltalk]], [[Java]], [[JavaScript]], [[Objective-C]]), or many (e.g., [[C++]], [[Self]], [[Simula]]) [[superclass]]es or [[prototype]]s for an]]object]]?
[[SiGMa]] is an [[iterative propagation algorithm]] that [[leverage]]s both the [[structural information]] from the [[relationship graph]] and [[flexible similarity measure]]s between [[entity properti]]es in a [[greedy local search]], which makes it [[scalable]] .
[[Signed network]]s, in which the [[relationship]] between two [[node]]s can be either [[positive]] (indicating a [[relationship]] such as [[trust]]) or [[negative]] (indicating a [[relationship]] such as [[distrust]]), are becoming increasingly common.
[[Similarity between object]]s plays an important role in both [[human cognitive process]]es and [[artificial system]]s for [[recognition]] and [[categorization]] .
[[Similarity]]; [[Duplication]]; [[Resemblance]]; [[Web Search]]; [[Fingerprint]]s; [[Signature]]s
[[Similarity in Word Space]] is based on [[second-order co-occurrence]]: two [[tokens]] (or [[contexts]]) of the [[ambiguous word]] are [[assigned]] to the same [[sense cluster]] if the [[words]] they [[co-occur]] with in turn occur with [[similar words]] in a [[training corpus]] .
[[Similarity]] is measured by analyzing [[text]], [[links]], and [[mailing list]] .
[[Similarity of meaning]] is [[operationalized]] in terms of [[geometry]], by defining a [[distance metric]] .
[[Similarity search]] is achieved through [[consensus]] among the ([[similar]]) [[instance-level constraints]] based on [[ranking orders]] computed using the [[LCSS-based]] [[similarity measure]] .
Similarly, one [[type]] of [[flu]] may give [[partial immunity]] against some other similar [[disease]] .
Similarly, other [[technique]]s have been proposed for automatically [[discovering novel]] [[senses of words]] ([[Lau et al., 2012]]); however, these [[sense]]s were not [[re-integrated]] into the [[taxonomy]] .
(...) Similar terminology is used dealing with [[differential]],[[integral]] and [[functional equation]]s.
Similar to other [[social network]]s, [[co-offending network]]s also suffer from a [[highly skewed distribution]] of [[positive]] and [[negative pair]]s.
Simple [[correlation over time]] is typically not [[meaningful]], since [[users]] change their [[preference]]s due to different [[external events]] .
Simple models like [[label propagation]] and [[iterative classification]] can aggravate a [[misclassification]] by [[propagating]] [[mistakes]] in the [[network]], while more [[complex models]] that define and optimize a [[global objective function]], such as [[Markov random fields]] and [[graph mincuts]], can [[misclassify]] a [[set]] of [[nodes]] [[jointly]] .
Simple [[perplexity]]-based [[classifier]]s are then applied to filter the [[tweet stream]] for [[topics of interest]] .
Simply discarding a [[large number]] of [[inaccurate point]]s (as [[noise]]s) could greatly affect [[clustering result]]s.
[[SimRank]] is a [[simple]] and [[influential measure]] of this kind, based on a [[solid]] [[graph theoretical model]] .
[[Simulation result]]s show that [[our algorithm]] significantly [[outperform]]s several [[well-studied heuristic]]s including [[degree centrality]] and [[distance centrality]] in almost all of the [[scenario]]s.
[[Simulations]] based on [[realistic scenarios]] show that the [[resulting estimates]] are more [[robust]] to [[selection bias]] than [[traditional alternatives]], such as [[regression modeling]] or [[propensity scoring]] .
Simulation studies compared the power of [[Mann-Whitney]] and [[ANCOVA]] for analyzing each distribution, varying [[sample size]], [[correlation]] and [[type of treatment effect]] ([[ratio]] or [[shift]]).
Since [[2013]], [[PDP-Miner]] has been deployed as the [[data analysis platform]] of [[ChangHong COC]] .
Since after [[discharge]] a [[patient]] can be [[assigned or classified]] to several [[ICD-9 codes]], the [[coding problem]] can be seen as a [[multi-label classification problem]] .
Since a [[mixture summarize]]s the [[cluster]]s located in similar [[subspace projection]]s, each [[view]] highlights specific [[aspect]]s of the [[data]] .
Since [[ancient times]] [[human being]]s have constantly searched for new ways to [[express]] and [[encode]] their [[knowledge]] .
Since an [[institution]] is nothing more than a [[rule]] that [[persists over time]], [[human being]]s therefore have a [[natural tendency]] to [[institutionalize]] their [[behavior]] .
Since at least the [[late 1970s]], “[[flexible production]]” has commonly been considered as a [[positive]] and [[necessary]] [[innovation]] to ensure [[sustainable]] [[economic growth]] .
Since exact [[discovery]] may be [[computationally intractable]], [[we]] introduce a novel [[sampling scheme]] which enables [[approximate discovery]] of [[highly reliable subgraph]]s with high [[probability]] .
Since [[exact evaluation]] of a [[kernel matrix]] requires [[O (N2) work]], [[scalable learning algorithm]]s using [[kernel]]s must [[approximate]] the [[kernel matrix]] .
Since [[GAUC weight]]s each [[pairwise comparison equally]] and the [[calculation of GAUC]] requires [[quadratic time]], [[we]] derive two [[lower bound]]s of [[GAUC]] which can be computed in [[linear time]] and put more emphasis on [[ranking positive link]]s on the [[top]] and [[negative link]]s at the [[bottom]] of a [[ranking list]] .
Since [[healthcare]] [[data]] is diverse and rich and each [[source]] and [[feature-subset]] provides different [[insight]]s into a [[complex problem]], our [[DHC]] based [[prediction approach]] intelligently leverages each [[source]] and [[feature-subset]] to [[optimize]] different [[objective]]s (such as, [[Recall]] or [[AUC]]) for [[CHF]] [[risk-of-readmission]] .
Since in many [[clinical research]]es [[non-overlapping group]]s are preferred for better [[interpretability]], we further improve the [[formulation]] to [[generalize feature]]s using [[mutually exclusive feature group]]s.
Since [[innovator]]s exist in many [[communiti]]es, and their [[opinion]]s will spread and then [[stimulate]] their [[follower]]s to adopt the same [[behavior]], [[our approach]] is based on the [[hypothesis]] that a [[set]] of [[innovative consumer]]s is sufficient to represent the most [[representative opinion]]s in each [[community]] .
Since in [[real-world application]]s we usually have both abundant [[numerical feature]]s and [[categorical feature]]s with large [[cardinality]] (e.g. [[geolocation]]s, [[ID]]s, [[tag]]s etc.), [[we]] design a new [[model]], called <b>[[GB-CENT]]</b>, which leverages [[latent factor embedding]] and [[tree component]]s to achieve the merits of both while avoiding their [[demerit]]s.
Since most [[previous studi]]es only [[infer]] [[global authoritativeness]] of a [[user]], there is no way to [[differentiate]] the [[authoritativeness]] in different aspects of [[life (topics)]] .
Since [[our approach]] uses a [[weighted network]] as an [[input]], [[we]] also discuss [[supervised]] and [[unsupervised]] [[weighting schemes]] for [[unweighted]] [[biological networks]] .
Since our [[team formation problem]]s are proven to be [[NP-hard]], [[we]] design efficient [[approximate algorithm]]s for finding [[near optimum]] [[team]]s with provable [[guarantee]]s.
Since [[patterns]] are [[discovered]] directly from the [[input database]], [[feature selection]] usually taking a great amount of [[time]] could be [[avoided]] completely.
Since [[P]] is usually very [[sparse]], [[our algorithm]] is [[extremely efficient]] and can solve [[problem]]s with millions of [[user]]s and items.
Since [[rating]]s are not [[static]] but given at various [[point]]s in time, a [[temporal analysis]] of [[rating data]] provides deeper insights into the [[evolution]] of a [[product's quality]] .
Since [[retweet history]] reveals [[users' personal preference]] for [[tweet]]s, [[we]] study how to [[learn a predictive model]] to [[rank the tweet]]s according to their [[probability of being retweeted]] .
Since [[rule induction methods]] generate [[rules]] whose [[lengths]] are the shortest for [[discrimination between given classes]], they tend to generate [[rules]] too [[short]] for [[medical experts]] .
Since the [[1960s]], [[HCI]] has often been ascendant when setbacks in [[AI occurred]], with successes and [[failure]]s in the two [[fields redirecting mindshare]] and [[research]] funding.14 Although early figures such as [[Allen Newell]] and [[Herbert Simon]] made fundamental contributions to both [[field]]s, the [[competition]] and relative lack of dialogue between [[AI]] and [[HCI]] are curious.
Since the [[1990s]], [[computers]] have [[represented]] a [[larger]] [[share]] of [[nonfarm business output]] than of [[consumer purchase]]s.
Since the [[1990s]], the [[US]] has experienced continued [[growth in productivity]] and [[GDP]], but [[median income]] has [[stagnated]] and the [[employment to population ratio]] has fallen.
Since the data are [[interviews transcribed]] by [[nurse]]s during [[phone conversation]]s, they include a [[significant volume]] and [[variety]] of [[noise]] .
Since the [[design]] of [[machine]]s is one of these [[intellectual activiti]]es, an [[ultraintelligent machine]] could [[design]] even better [[machine]]s; there would then unquestionably be an ‘[[intelligence explosion]],’ and the [[intelligence]] of [[man]] would be left far behind.
Since the [[diversity metric]] is hard to [[optimize]] directly, we instead [[optimize]] its [[lower bound]] and prove that [[maximizing]] the [[lower bound]] with [[projected gradient ascent]] can increase this [[diversity metric]] .
Since [[Theil-Sen]] is a [[median-based estimator]], it is more robust against corrupted [[data]] aka [[outlier]]s.
Since the [[optimization problem]] is [[NP-hard]] to [[solve exactly]], [[we]] develop an [[efficient]] [[approximation algorithm]] that [[scales]] to [[large datasets]] and in [[practice]] gives [[provably]] [[near-optimal performance]] .
Since the [[ranking measure]] is a [[stepwise function]] of a [[single parameter]], [[we]] propose a novel [[line search algorithm]] that can locate the [[interval]] with the best [[ranking measure]] along this [[coordinate]] quite [[efficiently]] .
Since there are many possible [[subspace]]s, [[we]] propose a [[submodular optimization framework]] for [[subspace selection]] .
Since, there is no [[consolidated nomenclature]] for most [[biomedical NE]]s, any [[NER system]] relying on [[limited dictionari]]es or [[rule]]s does not seem to perform [[satisfactorily]] .
Since [[these method]]s do not rely on [[annotated data]], predefined [[typing schema]] or [[hand-crafted feature]]s, they can be quickly [[adapted to a new domain]], [[genre]] and [[language]] .
Since these models only include one - and [[two-dimensional component]]s, the components of [[GA<sup>2</sup>M-model]]s can be [[visualized]] and [[interpreted]] by [[user]]s.
Since the [[shape function]]s can be [[arbitrarily complex]], [[GAM]]s are more [[accurate]] than simple [[linear model]]s.
Since they can either [[promote]] or harm the [[reputation]] of a [[product]] or a [[service]], [[buying]] and selling [[fake review]]s becomes a [[profitable business]] and a big [[threat]] .
Since [[topic change]]s sometimes happen within a single [[document]], and our end [[task]] is [[sentence retrieval]], [[we]] also investigate the notion of [[word cooccurrence]] in a [[smaller segment of text]] such as a [[sentence]] .
[[Singular Value Decomposition (SVD)]] is [[computationally costly]] and therefore a [[naive implementation]] does [[not scale]] to the needs of [[scenario]]s where [[data evolves continuously]] .
[[Sisyphus]], in the [[ancient myth]], is [[condemned]] to an [[existence]] that is generally acknowledged to be [[awful]] .
[[SITA]] achieves both [[time and space efficiency]] by developing the [[data structure]] called [[interval-splitting tree]]s, which enables to efficiently [[prune]] the useless [[portion]]s of [[search space]], and by incorporating the ideas behind [[wavelet tree]], a [[succinct data structure]] to compactly [[represent tree]]s.
[[Sketch update]]s [[per]] [[row]] in <i>A</i> require amortized <i>O</i> (<i> ml </i>) [[operation]]s and [[the algorithm]] is perfectly [[parallelizable]] .
[[Slavery]] defined what it meant to be [[black]] (a [[slave]]), and [[Jim Crow]] defined what it meant to be [[black]] (a [[second-class citizen]]).
[[SLF]] exhibits [[robust]] and [[scalable]] behaviour and is easy to [[tune]] .
[[Slot]]s and [[slot value]]s can be again any [[entities]] in the [[domain of discourse]], including [[frame]]s.
[[SmartDispatch]] comes with two [[classification approach]]es - the [[well-known SVM method]], and a [[discriminative term-based approach]] that [[we]] designed to address some of the [[issue]]s in [[SVM classification]] that were empirically [[observed]] .
[[SO-CAL]] is applied to the [[polarity classification task]], the [[process of assigning]] a [[positive]] or [[negative label]] to a [[text]] that captures the [[text's opinion]] towards its main subject matter.
[[Social communiti]]es connect [[people]] of [[similar interest]]s together and play [[essential role]]s in [[social network application]]s.
[[Social media]] is a [[platform]] for [[people]] to [[share]] and [[vote content]] .
[[Social media response]]s to [[news]] have increasingly gained in importance as they can enhance a [[consumer]]'s [[news reading experience]], [[promote]] [[information sharing]] and aid [[journalist]]s in assessing their [[readership's response]] to a [[story]] .
[[Social media service]]s allow [[user]]s to constantly [[share]], [[follow]] and [[comment on posts]] from other [[user]]s.
[[Social media service]]s allow [[user]]s to make [[posting]]s, [[generating]] [[large dataset]]s of [[human activity]] [[time-stamp]]s.
[[Social media]] such as [[blog]]s, [[Facebook]], [[Flickr]], etc., presents [[data in a network format]] rather than classical [[IID distribution]] .
[[Social media]] such as [[Twitter]] or [[weblog]]s are a popular [[source]] for [[live textual data]] .
[[Social networking website]]s allow [[user]]s to [[create]] and [[share content]] .
[[Social network]]s continue to grow in [[size]] and the [[type]] of [[information hosted]] .
[[Social networks]] have become a major focus of [[research]] in recent years, initially directed towards [[static]] [[networks]] but increasingly, towards [[dynamic]] [[ones]] .
[[Social Network]]s, [[Online Forum]]s, [[Information Diffusion]], [[Social Selection Model]]
[[Social network]]s where the [[actor]]s occupy [[geospatial location]]s are prevalent in [[military]], [[intelligence]], and [[policing operation]]s such as [[counter-terrorism]], [[counter-insurgency]], and combating [[organized crime]] .
[[Social network system]]s, like [[last.fm]], play a significant [[role]] in [[Web 2.0]], containing [[large amount]]s of [[multimedia-enriched data]] that are enhanced both by explicit [[user-provided annotation]]s and [[implicit aggregated feedback]] describing the [[personal preference]]s of each [[user]] .
[[Social recommendation]], which aims to [[systematically]] leverage the [[social relationship]]s between [[user]]s as well as their [[past behavior]]s for [[automatic recommendation]], attract much attention recently.
[[Social tagging]] by nature is an [[incremental process]], meaning that once a [[user]] has saved a [[web page]] with [[tags]], the [[tagging system]] can provide more [[accurate predictions]] for the [[user]], based on [[user]]'s [[incremental]] [[behaviors]] .
[[Social tagging system]]s have become increasingly popular for [[sharing]] and [[organizing]] [[web resource]]s.
[[Social value]]s can play — and have played — an important part in the success of various forms of [[social organization]], including the [[market mechanism]], [[democratic politic]]s, [[elementary civil]] and [[political right]]s, provision of [[basic public good]]s, and [[institution]]s for [[public action]] and [[protest]] .
[[Sociologist]]s are well-positioned to [[explain]], [[offer insight]], and [[provide input]] into [[public policy]] about such changes and the [[state of]] [[contemporary]] [[employment relations]] .
So far, most [[machine learning approaches to text understanding]] consist in [[tokenizing]] a [[string of characters]] into [[structure]]s such as [[word]]s, [[phrase]]s, [[sentence]]s and [[paragraph]]s, and then apply some [[statistical classification algorithm]] onto the [[statistics of such structures]] ([[Soderland, 2001]]).
So far, most of the proposed techniques rely on [[lexico-syntactic pattern]]s, either [[manually]] or [[semi-automatically]] produced ([[Hovy et al., 2003]]; [[Zhang and Jiang, 2009]]; [[Westerhout, 2009]]).
So far, the [[RAE]] was completely [[unsupervised]] and [[induced]] [[general representation]]s that capture the [[semantics]] of [[multi-word phrase]]s.
So far, [[we]] either looked at [[estimating the conditional expectation]]s of [[continuous variable]]s (as in [[regression]]), or at [[estimating distribution]]s.
[[Software development compani]]es are increasingly aiming to become [[data-driven]] by trying to [[continuously experiment]] with the [[product]]s used by their [[customer]]s.
So it's needed a new [[information processing strategy]] that [[correlate]]d with the [[content of information]] by [[learning]] some [[mechanism]]s from [[human being]]s.
Some are [[ridiculed]], some are [[pariah]]s, and some are occasionally [[honored]] and [[revered]] .
Some [[dominant characteristic]]s of those [[cluster]]s which have been flagged are large [[beneficiary payment]], large [[interest payment]] [[amount]]s, and long [[lag]] between [[submission]] and [[payment]] .
Some examples are [[recovering the original signals]] after their [[transmission over noisy channels]], [[finding DNA subsequence]]s after possible [[mutations]], and [[text searching]] where there are [[typing]] or [[spelling errors]] .
Some familiar conditions are [[deadlock-freedom]], [[starvation-freedom]], [[lock-freedom]], and [[wait-freedom]] .
Some familiar conditions are [[serializability]], [[linearizability]], [[sequential consistency]], and [[quiescent consistency]] .
Some [[feature]]s are [[atomic]] and [[provided]], but since [[CRFs]] are [[log-linear model]]s, one will also want to gain expressive power by using some [[conjunctions]] .
Some of the apparent shortcomings of existing techniques are the [[ad-hoc combination]] of [[termhood]] [[evidence]], [[mathematically-unfounded]] [[derivation of scores]] and [[implicit assumption]]s concerning term [[characteristic]]s.
Some [[online resource]]s for [[multi-label learning]] are summarized in [[Table III]], including [[academic activiti]]es (tutorial, [[workshop]]s, [[special issue]]), [[publicly-available software]] and [[data set]]s.
Some people wrongly suggest that allocation can be based purely on [[scientific]] or [[clinical fact]]s, often using the term “[[medical need]]”.13,14 There are no [[value-free]] [[medical criteria]] for [[allocation]].15,16 Although [[biomedical fact]]s determine a [[person]]’s [[post-transplant prognosis]] or the [[dose of vaccine]] that would confer [[immunity]], responding to these [[fact]]s requires [[ethical, value-based judgment]]s.
Some [[researcher]]s ([[Nakata et al., 2002]]; [[Lagus and Kuusisto, 2002]]; [[Adams and Martell, 2008]]) attempted to solve [[this problem]] with [[text categorization approach]]es for the [[utterance]]s in a given turn.
Some [[stereotyped group]]s are [[disrespect]]ed as [[incapable]] and [[useless]] (e.g., [[elderly people]]), whereas others are respected for [[excessive]], [[threatening competence]] (e.g., [[Asian]]s).
Some [[stereotyped group]]s are liked as [[sweet]] and [[harmless]] (e.g., [[housewiv]]es), whereas others are disliked as [[cold and inhuman]] (e.g., [[rich people]]).
Some [[theoretical analysis]] on the [[optimality]] of the [[adapt]]ed [[SGD method]] and the [[generalized error bound]] of the [[formulation]] are given based on [[the proposed method]] .
Sometimes "[[PAC]] " is used as a [[verb]], and [[accounts]] are said to be "[[PAC'd]] ".
[[Sometime]]s, [[user]]s are interested primarily in the [[object]]s appearing in [[these claim]]s.
Sophisticated [[information technologies]] are needed for effective [[data acquisition]] and [[integration]] from a growing [[body]] of the [[biomedical literature]] .
[[Sophistication]] mitigates [[procrastination]], but exacerbates [[preproperation]] .
So, please sort [[input]] [[sequence]]s in [[descending order]] of [[length]]s before applying the [[function]] (...)
Sorting out the [[place]]/[[crime]] [[relationship]] requires [[analytical method]]s that are best suited to isolating the [[impact]]s of [[place]] on [[crime]] .
So, the [[project administrator]] is able to [[evaluate]] [[annotation consistency]] among [[annotator]]s and across [[rounds of annotation]], while [[annotator]]s are able to [[reject]] or [[amend]] subsets of [[annotation]]s made in previous [[round]]s.
So [[this approach]] can provide more [[candidates]] for [[<math>k</math>-support anonymity]] with limited [[fake item]]s as only the [[leaf nodes]], not the [[internal nodes]], of the [[taxonomy tree]] need to appear in the [[transactions]] .
[[Source code]] is provided for [[reproducing]] the [[experiment]]s and [[integration]] with [[existing]] and [[future algorithm]]s.
[[SP2Bench]] uses a [[scalable dataset]] that reflects the [[structure]] of the [[DBLP Computer Science Bibliography]] .
[[Space]] does not have to be [[artificially created]] in the [[human mind]] for the [[idea of justice]] or [[fairness]] — through [[moral bombardment]] or [[ethical haranguing]] .
[[Spam filtering]], [[fraud detection]], and [[user query-intent analysis]] has relied heavily on [[machine learned classifiers]], and resulted in [[improvements]] in [[robust]] [[classification accuracy]] .
[[Spanner]] is a [[globally-distributed]] [[data management system]] that backs hundreds of [[mission-critical service]]s at [[Google]] .
[[Spanner]] is built on ideas from both [[systems]] and [[database communiti]]es.
Spanning 0.2 [[TB]] of [[multi-dimensional data]], it took only 8 [[minute]]s to be [[clustered]], using 128 [[core]]s.
[[Sparse logistic regression]] embeds [[feature selection]] in the [[classification framework]] using the [[L1-norm regularization]], and is attractive in many [[applications]] involving [[high-dimensional]] [[data]] .
[[Sparse SVM]] has been successfully applied in a variety of [[data mining application]]s including [[text mining]], [[bioinformatics]], and [[image processing]] .
[[Spatial analysis]] of [[disease risk]], or [[disease mapping]], typically relies on [[information]] about the [[residence]] and [[health status]] of [[individual]]s from [[population]] under [[study]] .
[[Spatial classification]] is the [[task of learning models]] to [[predict class labels]] based on the [[features]] of [[entities]] as well as the [[spatial relationships]] to other [[entities]] and their [[features]] .
[[Spatial event forecasting]] from [[social media]] is an important [[problem]] but encounters [[critical challenge]]s, such as [[dynamic pattern]]s of [[feature]]s ([[keywords]]) and [[geographic heterogeneity]] (e.g., [[spatial correlation]]s, [[imbalanced sample]]s, and different [[population]]s in different [[location]]s).
Speakers of [[low-resource languages correlate]] with lower access to [[healthcare]], [[education]] and higher [[vulnerability]] to [[disaster]]s.
[[Speaker]]s of more than 5,000 [[language]]s have access to [[internet]] and [[communication technologi]]es.
[[Special-interest group]]s, such as the [[Small Unmanned Aerial Vehicles Coalition]] that is promoting a [[US market]] for [[commercial drone]]s, are pushing the [[interest]]s of major [[corporation]]s to [[regulator]]s.
Special [[properties]] of the [[decision surface]] ensures [[high generalization ability]] of the [[learning machine]] .
Specific [[algorithm]]s are presented for [[least-squares]], [[least absolute deviation]], and [[Huber-M loss functions]] for [[regression]], and [[multiclass logistic likelihood]] for [[classification]] .
Specifically, a [[Gibbs Random Field]] is defined to [[model]] the [[influence]] of [[historic status]] and the [[dependency relationships]] in the [[graph]]; thereafter a [[topic model]] generates the [[word]]s in [[text content]] of the [[event]], [[regularized]] by the [[Gibbs Random Field]] .
Specifically, a [[measure]] called [[<math>\varphi</math>-frequent probability]] is introduced to [[evaluate]] the [[degree]] of [[recurrence of subgraphs]] .
Specifically, an [[coreference relation]] denotes an [[identity]] of [[reference]] and holds between two [[textual elements]] known as [[markables]], which can be [[definite noun phrases]], [[demonstrative noun phrases]], [[proper names]], [[appositives]], [[sub-noun phrases]] that act as [[modifiers]], [[pronouns]], and so on.
Specifically, [[ecommerce]] [[activity]] that involves the [[end user]] is undergoing a significant [[revolution]] .
Specifically, for handling the [[text disjointing problem]], [[we]] use a [[cross-sampling process]] in [[our model]] to extract [[topic]]s with terms coming from all the [[source]]s, and leverage existing [[matching relation]]s through [[latent topic layer]]s instead of at [[text layer]]s.
Specifically, given a [[collection]] of [[location]]s (e.g., cities), [[we]] propose to build [[forecasting]] [[model]]s for all [[location]]s simultaneously by [[extracting]] and utilizing appropriate [[shared information]] that effectively increases the [[sample size]] for each [[location]], thus improving the [[forecasting performance]] .
Specifically, given a [[network]], [[we]] perform <i>R</i> [[random walk]]s, each [[starting]] from a [[randomly picked]] [[vertex]] and [[walking]] <i>T</i> [[step]]s.
Specifically, [[hierarchical summarization technique]]s are used to generate brief [[review]]s from a [[large collection]] of [[report]]s at different [[granulariti]]es; [[probabilistic model]]s are proposed to [[dynamically]] generate [[query form]]s based on [[user's feedback]]; and [[recommendation technique]]s are adapted to help [[user]]s identify potential [[contact]]s for [[report sharing]] and [[community organization]] .
Specifically, in every [[discrete period]], all [[individual]]s in the [[network]] [[concurrently]] try a [[new technology]] or [[product]] with certain respective [[probabiliti]]es.
Specifically, [[information extraction]] is used to integrate the [[input data]] from different [[source]]s; the [[content recommendation engine]] and the [[report summarization module]] provide users [[personalized]] and [[brief views]] of the [[disaster information]]; the [[community generation]] [[module]] develops [[spatial clustering technique]]s to help [[users]] build [[dynamic community]] in [[disasters]] .
Specifically, it characterizes each [[named entity]] by its associated [[queries]] and [[URL]]s in the [[click-through]] [[data]] .
Specifically [[it]] introduces two [[probability model]]s, respectively referred to as [[permutation probability]] and [[top <i>k</i> probability]], to define a [[listwise loss function]] for [[learning]] .
Specifically, [[LCARS]] consists of two [[component]]s: [[offline modeling]] and [[online recommendation]] .
Specifically, [[our experiment]]s establish that [[state-of-the-art]] [[convolutional networks for image classification]] trained with [[stochastic gradient method]]s easily fit a [[random labeling]] of the [[training data]] .
Specifically, our major contributions include : (1) A [[MapReduce]] [[statistical learning algorithm]] and [[implementation]] that achieve [[optimal]] [[data parallelism]], [[task parallelism]], and [[load balance]] in spite of the typically [[skewed distribution]] of [[domain]] [[data]] .
Specifically, [[our methods]] can adaptively balance the two aspects of [[EE]] by [[automatically learning]] the [[optimal tradeoff]] and incorporating [[confidence metrics]] of [[historical performance]] .
Specifically, the hidden [[temporal periodic behavior]]s could be [[oscillating]] and [[noisy]], and the [[observation]]s of the [[event]] could be [[incomplete]] .
Specifically, the [[model loss]] is approximated by the [[size]] of [[version space]], and the [[reduction rate]] of the [[size]] of [[version space]] is [[optimized]] with [[Support Vector Machines (SVM)]] .
Specifically [[TriMine]] discovers [[hidden topic]]s, [[groups of URL]]s, and [[groups of user]]s, simultaneously.
Specifically, two [[pattern growing strategies]], such as [[column-centric strategy]] and [[row-centric strategy]], are presented, which are effective to grow the [[seed]] [[OPSMs]] into significant [[ROPSMs]] .
Specifically, [[we]] address the [[question]] : "How does one [[discover]] [[frequent migration pattern]]s in the [[stock market]]?" [[We]] present a new [[trajectory mining]] [[algorithm]] to [[discover]] [[migration motif]]s in [[financial market]]s.
Specifically, [[we]] build a [[graph-based ranking model]] to [[iteratively]] compute the [[ranking distribution of the object]]s within each [[class]] .
Specifically, [[we]] consider a wide range of [[feature selection technique]]s (combining [[3 search]] and 8 [[evaluator method]]s) and all [[classification approach]]es implemented in [[WEKA's standard distribution]], spanning 2 [[ensemble method]]s, 10 [[meta-method]]s, 27 [[base classifier]]s, and [[hyperparameter setting]]s for each [[classifier]] .
Specifically, [[we]] consider the [[common problem]]s of [[data imputation]], [[feature selection]], and [[prediction]] in [[medical datasets]] .
Specifically, [[we]] consider the [[microblog]]ging site [[Twitter]], where users provide short [[description]]s of themselves in their [[profile]]s, as well as perform actions such as [[tweeting]] and [[retweeting]] .
Specifically, [[we]] consider the [[Netflix Prize]] [[data set]], and its leading [[algorithms]], adapted to the [[framework]] of [[differential privacy]] .
Specifically, [[we]] decompose the [[weight (model) matrix]] for all [[task]]s into two [[component]]s.
Specifically, [[we]] define a [[semi-supervised learning framework]] for [[ranking of node]]s on a [[very large graph]] and [[derive]] within [[our proposed framework]] an [[efficient algorithm]] called [[Semi-Supervised PageRank]] .
Specifically, [[we]] develop a novel [[method]] to [[infer]] [[causality]] from [[spatial]]-[[temporal data]], as well as an [[procedure]] to incorporate [[extreme value]] [[modeling]] into our [[method]] in order to address the attribution of [[extreme]] [[climate]] [[events]], such as [[heatwaves]] .
Specifically, [[we]] develop [[BGP-lens]], an automated [[BGP]] [[updates]] [[analysis tool]], that has three desirable [[properties]] : (a) It is [[effective]], able to [[identify]] [[phenomena]] that would otherwise go [[unnoticed]], such as a peculiar "[[clothesline]]" [[behavior]] or [[prolonged]] "[[spikes]]" that last as long as 8 [[hours]]; (b) It is [[scalable]], using [[algorithm]]s are all [[linear]] on the number of [[time-ticks]]; and (c) It is [[admin-friendly]], giving useful leads for [[phenomenon of interest]] .
Specifically, [[we]] first build [[investor profile]]s based on [[quantitative analysis]] of past [[performance]]s, [[risk preference]]s, and [[investment]] experiences of [[investor]]s.
Specifically, [[we]] first construct a unified [[SSL framework]] to combine the [[manifold assumption]] and the [[pairwise constraints assumption]] for [[classification task]]s.
Specifically, [[we]] first design a [[geographic]] [[function learning model]] to jointly capture the [[correlation]]s among [[estate neighborhood]]s, [[urban function]]s, [[temporal effect]]s, and [[user mobility pattern]]s.
Specifically, [[we]] first [[extract]] the [[geographic utility]] of [[estate]]s from [[geography data]], estimate the [[neighborhood popularity of estate]]s by [[mining]] [[taxicab trajectory data]], and [[model]] the [[influence]] of [[latent business area]]s via [[ClusRanking]] .
Specifically, [[we]] first introduce the importance of [[measure]] [[normalization]] in the [[evaluation]] of the [[clustering]] [[performance]] on [[data]] with [[imbalanced]] [[class]] [[distributions]] .
Specifically, [[we]] formulate a [[double sparsity optimization problem]] that [[identifies group]]s in the [[low-level feature]]s, generalizes [[higher level feature]]s using the [[group]]s and performs [[feature selection]] .
Specifically, [[we]] formulate a [[joint optimization problem]] for two [[task]]s, [[type propagation with relation phrase]]s and [[multi-view relation phrase clustering]] .
Specifically, [[we]] formulate an [[objective function]] for the [[overlapping decomposition problem]] and propose an [[approximate algorithm]] for [[it]] .
Specifically, [[we]] formulate the [[prediction problem]] as a [[multi-task regression problem]] by considering the [[prediction]] at each [[time point]] as a [[task]] .
Specifically, [[we]] formulate the [[problem]] as the [[L1-ball constrained]] [[smooth]] [[convex optimization]], and propose to solve the [[problem]] using the [[Nesterov's method]], an [[optimal]] [[first-order]] [[black-box method]] for [[smooth]] [[convex optimization]] .
Specifically, [[we]] [[identify]] and consider two new [[bias]]es in [[TCM]] as the [[basis]] for [[user modeling]] .
Specifically, we incorporate the [[hierarchical dependenci]]es between the [[class-label]]s into the [[regularization structure]] of the [[parameter]]s thereby encouraging [[class]]es nearby in the [[hierarchy]] to share [[similar]] [[model parameter]]s.
Specifically, we [[learn]] [[group memberships for object]]s and the [[significance]] of [[relevance feature]]s for each [[interest group]], while also [[propagating]] [[relative authority]] between [[object]]s, by solving a [[joint optimization problem]] .
Specifically, [[we]] leverage the [[knowledge]] on how previous [[user]]s [[clip]] similar [[Web page]]s, and this [[knowledge repository]] can be [[represented as]] a <i>[[transaction database]]</i> where each [[transaction]] contains the [[clip]]s selected by a [[user]] on a certain [[Web page]] .
Specifically, [[we]] [[meta-analytically reviewed]] [[norm data]] on basic interests from 52 [[sample]]s in 33 [[interest inventori]]es published between [[1964]] and [[2007]], with a [[total of]] 209, 810 [[male]] and 223,268 [[female]] [[respondent]]s.
Specifically, we model the relationship between [[risk factor]]s (such as [[age]] and [[smoke exposure]]) and [[manifestation]]s of [[disease severity]] using [[Gaussian Process]]es, which allow us to represent so-called "[[disease trajectori]]es ".
Specifically, we present a [[theoretical bound]] to [[measure]] how well our proposed [[rMTFL approximate]]s the [[true evaluation]], and provide bounds to [[measure]] the [[error]] between the [[estimated]] weights of [[rMTFL]] and the underlying [[true weight]]s.
Specifically, [[we]] propose a [[dynamic probabilistic model]] to characterize the [[topical evolution]] of these [[object]]s within the [[patent network]] .
Specifically, [[we]] propose a novel [[convex fused sparse group Lasso (cFSGL) formulation]] that allows the [[simultaneous selection]] of a common [[set]] of [[biomarker]]s for multiple [[time point]]s and specific [[set]]s of [[biomarker]]s for different [[time point]]s using the [[sparse group Lasso penalty]] and in the meantime incorporates the [[temporal smoothness]] using the [[fused Lasso penalty]] .
Specifically, [[we]] propose a novel [[formulation]] for the [[structure learning]] of [[BN]]s, which involves one [[L1-norm penalty term]] to impose [[sparsity]] and another [[penalty]] to ensure the [[learn]]ed [[BN]] to be a [[directed acyclic graph]] - a [[required property of BN]]s.
Specifically, [[we]] propose a novel <i>[[Relation-aware Heterogeneous Hashing (RaHH)]]</i>, which provides a [[general framework]] for [[generating]] [[hash code]]s of [[data entiti]]es sitting in multiple [[heterogeneous domain]]s.
Specifically, [[we]] propose a novel [[parallel execution model]], called <i>[[pin-and-slide]] </i>.
Specifically, [[we]] propose two [[class]]es of [[data driven model]]s in the [[Deterministic Fashion Recommenders (DFR)]] and [[Stochastic Fashion Recommenders (SFR)]] for solving [[this problem]] .
Specifically, [[we]] segment a city into [[disjointed region]]s according to [[major road]]s, such as [[highway]]s and [[urban express way]]s.
Specifically, [[we]] [[select]] those [[features]] such that the [[multi-cluster structure]] of the [[data]] can be best [[preserved]] .
Specifically, [[we]] view [[FMs]] as a [[sum of simple surface]]s - a [[hyperplane]] plus several [[squared hyperplane]]s - in the original [[feature space]] .
Specific aspects of [[cache memori]]es that are investigated include: the [[cache fetch algorithm]] ([[demand]] versus [[prefetch]]), the [[placement]] and [[replacement algorithm]]s, [[line size]], [[store-through]] versus [[copy-back updating of main memory]], [[cold-start]] versus [[warm-start]] [[miss ratio]]s, [[multicache consistency]], the effect of [[input/output]] through the [[cache]], the behavior of [[split data/instruction cache]]s, and [[cache size]] .
[[Speech recognition]], [[deep belief network]], [[context-dependent phone]], [[LVSR]], [[DNN-HMM]], [[ANN-HMM]]
[[Spending plan]]: A [[document]], submitted to [[Administration]] and [[Finance]] by all [[state agenci]]es, which contains a detailed [[estimate]] of [[projected spending]] and [[revenue for the current year]] .
[[Spontaneous devaluation]] in [[preference]]s is ubiquitous, where [[yesterday's hit]] is today's [[affliction]] .
[[Sprint's existing process]] is the best that [[Jeremy]] has seen in [[industry]], and shows a lot of [[best practice]]s in [[data structure]], [[documentation]], [[testing]], [[automation]], and [[customer interaction]], and [[modeling]] .
[[Stability]] is an important yet under-addressed issue in [[feature selection]] from [[high-dimensional]] and [[small sample]] [[data]] .
[[Standard generalized additive models (GAMs)]] usually [[model]] the [[dependent variable]] as a [[sum of univariate model]]s.
([[Standard]] [[jackknife theory]] gives an [[approximate]] [[mean]] and [[variance]] in the case <math>R(\mathbf{X},F)=\theta(\hat{F})-\theta(F), \theta</math> some [[parameter]] of interest.) A general [[method]], called the "[[bootstrap]]," is introduced, and shown to work satisfactorily on a variety of [[estimation problem]]s.
[[Starting tabula rasa]], our new [[program AlphaGo Zero]] achieved [[superhuman performance]], winning 100–0 against the [[previously published]], [[champion-defeating]] [[AlphaGo]] .
Starting with an [[assumption]] that an individual's [[disease risk]] is a [[weighted average]] of [[risk]]s at the [[location]]s which were visited, [[we]] show that [[disease mapping]] can be accomplished by [[spatially regularized logistic regression]] .
Starting with the idea analogous to [[spectral hashing]], novel [[formulations]] and [[solutions]] are proposed such that a [[kernel based hash function]] can be [[explicitly]] [[represented]] and [[optimized]], and directly applied to [[compute]] [[compact hash codes]] for [[new samples]] of [[general formats]] .
[[State administration]] does not consist of the [[ruler]]’s [[family]] and [[friends]]; rather, [[recruitment]] to [[administrative position]]s is based on [[impersonal criteria]] such as [[merit]], [[education]], or [[technical knowledge]] .
[[State agency personnel]] can [[extract data]] to construct [[analytical]] and [[management summary report]]s and [[spreadsheet]]s.
[[State-of-the-art]] [[approach]]es assume there exists a [[text corpus]] to accurately characterize the [[domain of interest]], and that a [[taxonomy]] can be derived from the [[text corpus]] using [[information extraction technique]]s.
[[State-of-the-art approach]]es rely on [[lexical]] and [[statistical feature]]s which are abundant for [[popular entiti]]es but sparse for [[unpopular one]]s, resulting in a clear bias towards [[popular entiti]]es and [[poor accuracy]] for less [[popular one]]s.
[[State-of-the-art]] [[link prediction]] utilizes combinations of [[complex feature]]s derived from [[network panel data]] .
[[State-of-the-art]] [[prediction algorithm]]s rely heavily on [[historical information]] collected for [[advertiser]]s, [[user]]s and [[publisher]]s.
Statistically speaking, [[multivariate analysis]] refers to [[statistical model]]s that have [[2 or more]] [[dependent or outcome variable]]s, and [[multivariable analysis]] refers to [[statistical model]]s in which there are [[multiple]] [[independent or response variable]]s.
Statistically, [[we]] show that if the [[regularization parameter]] is appropriately chosen, the [[one-step LLA]] [[estimate]]s enjoy the [[oracle properti]]es with good initial [[estimator]]s.
[[Statistical modeling]] of [[pronunciation variation]] is also incorporated into the [[language model]] based on the [[alignment]] of [[large-scale transcription]]s.
[[Stepped wedge randomised trial designs]] involve [[sequential roll-out of an intervention]] to [[participant]]s ([[individual]]s or [[cluster]]s) over a number of [[time period]]s.
[[Step rate]]: [[Salary rates for unionized state employee]]s are outlined in [[salary table]]s which contain [[grade]]s and [[step]]s.
[[Stereotype]] [[research]] emphasizes [[systematic process]]es over seemingly [[arbitrary content]]s, but [[content]] also may prove [[systematic]] .
Stereotypically, (4) [[status]] [[predict]]s [[high competence]], and [[competition]] predicts [[low warmth]] .
[[Stereotyping process]]es respond to [[systematic principle]]s that [[generalize]] across different specific [[instances of stereotype]]s, so the [[processes invite social–psychological investigation]], because they are presumably stable [[over time]], place, and [[out-group]] .
Still, can [[innovation and progress]] really [[hurt]] [[large numbers of workers]], maybe even [[workers in general]]?
Still, [[iSAX]] also encounters [[false hit]]s, which are again eliminated by [[accessing record]]s in full: once a [[false hit]] is [[generated by]] the [[index]], there is no second chance to [[prune]] it; thus, the [[pruning capacity]] [[iSAX]] provides is also [[one-off]] .
[[Stochastic Dual Coordinate Ascent (SDCA)]] has recently emerged as a [[state-of-the-art method]] for solving [[large-scale]] [[supervised learning problem]]s formulated as [[minimization]] of [[convex loss function]]s.
[[Stochastic gradient descent (SGD)]] is a [[popular technique]] for [[large-scale optimization problem]]s in [[machine learning]] .
[[Stochastic shortest-path problem]]s, [[consumption and investment of money]], [[allocation of resource]]s, [[production planning]], and [[harvesting problem]]s are a few examples of [[MDP]]s.
[[Stock gift]]s are also strategically timed to occur prior to [[unfavorable]] [[quarterly earning]]s [[announcement]]s and after [[positive]] [[earnings announcement]]s.
Stories of [[Bitcoin]]'s mysterious [[creator]], [[Satoshi Nakamoto]], and of [[illegal markets hidden]] in the [[darknet]] have added to the hype.
[[Straight-forward application]]s of a [[feed-forward network]] as a [[classifier]] replacement (usually coupled with the use of [[pre-trained]] [[word vector]]s) provide benefits also for [[CCG supertagging]] ([[Lewis & Steedman, 2014]]), [[dialog state tracking]] ([[Henderson, Thomson]], & [[Young, 2013]]), [[pre-ordering for statistical machine translation (de Gispert]], [[Iglesias, & Byrne, 2015]]) and [[language modeling]] ([[Bengio, Ducharme, Vincent, & Janvin, 2003]]; [[Vaswani, Zhao, Fossum, & Chiang, 2013]]).
[[Strategic planning]] and [[talent management]] in large [[enterprise]]s composed of [[knowledge worker]]s requires complete, [[accurate]], and [[up-to-date]] [[representation]] of the [[expertise]] of [[employee]]s in a form that integrates with [[business process]]es.
[[Stratified randomization]] is a [[two-stage procedure]] in which [[patient]]s who enter a [[clinical trial]] are first [[grouped]] into [[strata]] according to [[clinical feature]]s that may influence [[outcome risk]] .
[[Stratified randomization]]; [[stratification]]; [[efficiency]]; [[review]]; [[power]]; [[randomized clinical trials]] .
[[StreamSVM]] works by performing [[update]]s in the [[dual]], thus obviating the need to [[rebalance]] [[frequently visited example]]s.
Stress usually causes [[deformation]] of [[solid material]]s, or [[flow]] in [[fluid]]s.
[[Stroke]] is the third leading [[cause]] of [[death]] and the [[principal cause]] of serious [[long-term disability]] in [[the United States]] .
student]]s at the [[Massachusetts Institute of Technology]] - [[Jeremy Stribling]], [[Max Krohn]], and [[Dan Aguay]] - [[created]] a [[program to generate]] [[nonsensical]] [[computer science research paper]]s.
[[Student]]s listed the [[factor]]s that they thought [[influenced]] their own [[chance]]s of experiencing 8 [[future event]]s.
[[Student-t prior]]s are employed to [[model]] the [[sparsity]] of [[unknown signals' coefficient]]s, and the [[Approximated Variational Inference (AVI) method]] is provided for [[effective]] and fast [[learning]] .
[[sub-document classification]], [[contextual advertising]], [[sensitive content detection]], [[opinion mining]]
Subsequent [[foundational system]]s include the [[ramified type theory]] of [[Russell]] and [[Alfred North Whitehead]]'s [[Principia Mathematica]], published in three volumes from 1910 to 1913; [[Ernst Zermelo]]'s [[axiomatic set theory]] of [[1908]], later extended by [[Abraham Fraenkel]]; and [[Alonzo Church]]'s [[simple type theory]] of [[1940]] .
[[Subsequently]], we design <b> [[Order&Extend]]</b>, which is the [[first algorithm]] to [[unify]] a [[matrix-completion approach]] and a [[querying strategy]] into a [[single algorithm]] .
[[Subsequently]], [[we]] use [[template]]s [[generalized]] from [[paraphrasing]] to [[identify tweet]]s with [[deep insight]]s, which reveal [[reason]]s, express [[demand]]s or reflect [[viewpoint]]s.
[[Subspace clustering]] discards irrelevant [[dimension]]s and allows [[object]]s to belong to multiple, [[overlapping]] [[cluster]]s due to individual [[subspace projection]]s for each [[set of object]]s.
Substantial [[experiment]]s on both [[synthetic]] and [[real dataset]]s show that [[USpan]] efficiently [[identifi]]es [[high utility sequence]]s from [[large scale data]] with very [[low minimum utility]] .
Substantial [[experiments]] using [[this technique]] on [[Reuters Corpus Volume 1]] and [[TREC topics]] show that [[the proposed approach]] significantly [[outperform]]s both the [[state-of-the-art]] [[term-based methods]] underpinned by [[Okapi BM25]], [[Rocchio]] or [[Support Vector Machine]] and [[pattern based methods]] on [[precision]], [[recall]] and [[F measures]] .
; [[Substantive norm]]: “[[The National Transplant Organization]] is [[not allowed]] to use [[racial data]] for [[allocating organs]] to [[patient]]s”.
[[Sub-unit]] of an account, indicated by a [[two-character code]], [[AA]], [[BB]], [[CC]], etc. [[Appropriations]] are recorded in [[MMARS]] by [[object class]]es within an [[account]] .
Successful [[software maintenance]] is becoming increasingly critical due to the increasing [[dependence]] of our [[society]] and [[economy]] on [[software systems]] .
Successful [[user modeling]] should be aware of several critical [[issue]]s: who are the [[target user]]s' How should the [[solution]]s be [[update]]d when new [[data]] come in?
Success of [[manufacturing companies]] largely depends on [[reliability]] of their [[product]]s.
[[Such accounts]] offer a [[unifying model]] of [[perception]] and [[action]], illuminate the [[functional role]] of [[attention]], and may neatly capture the special contribution of [[cortical processing]] to [[adaptive success]] .
Such acts like [[donating organ]]s to strangers rather than [[relative]]s, taking in or adopting dozens of [[children]] and [[famili]]es, [[fighting for animal right]]s, the [[rights of chicken]]s, starting an [[adoption agency]], becoming a [[monk]], [[living a subsistence existence]], and starting a leper [[colony]] are addressed.
Such a [[large number]] of [[candidate itemsets]] degrades the [[mining performance]] in terms of [[execution time]] and [[space requirement]] .
Such [[algorithm]]s necessarily introduce [[uncertainty]] i.e., [[noise]] to [[computation]]s, trading [[accuracy]] for [[privacy]] .
Such a [[model]] implies that a [[personalized ranking list]] of [[latent link]]s should place [[positive link]]s on the [[top]], [[negative link]]s at the [[bottom]], and [[unknown status link]]s in between.
Such an [[approach]] also support [[online-store]]s that often offer more [[item]]s than [[traditional store]]s and need [[recommender system]]s to enable [[user]]s to find the not so [[popular item]]s as well.
Such a [[network comprise]]s of a [[series]] of [[network snapshot]]s with [[dynamic local state]]s at [[node]]s, and a [[global network state]] indicating the [[occurrence of an event]] .
Such [[anomali]]es are also referred to as [[collective anomali]]es, because they can only be [[inferred collectively]] from a [[set]] or [[sequence]] of [[data point]]s.
Such a [[one-dimensional scale]] does not accurately reflect the [[complexity]] of [[human emotion]]s and [[sentiment]]s.
Such [[applications]] can typically be [[represented as]] [[evolving interaction graphs]] with [[nodes]] denoting [[entities]] and [[edges]] representing their [[interactions]] .
Such applications include [[incremental learning]], [[data fusion]], [[feature selection]], [[learning with missing feature]]s, [[confidence estimation]], and [[error correcting output code]]s; all areas in which [[ensemble system]]s have shown great promise
Such a [[procedure]] is made of two [[main component]]s: a [[credit assignment mechanism]], that computes a [[reward]] for each [[operator]] at hand based on some [[characteristic]]s of the [[past offspring]]; and an [[adaptation rule]], that modifies the [[selection mechanism]] based on the [[reward]]s of the different [[operator]]s.
Such a [[synthesis]] is offered by the [[principle of Propositions as Types]], which [[link]]s [[logic]] to [[computation]] .
Such a [[system]] is beneficial for both [[advertiser]]s and [[publisher]]s.
Such cases abound, e.g., [[billboard]]s, [[TV commercial]]s and [[newspaper ad]]s are utilized extensively to boost the [[popularity]] and raise [[awareness]] .
Such [[changes]] include not only obvious [[user-visible changes]] such as [[modifications]] to a [[user interface]], but also more [[subtle changes]] such as different [[machine learning algorithm]]s that might affect [[ranking]] or [[content selection]] .
Such [[collective anomali]]es typically represent [[unusual event]]s, which need to be [[discovered]] from the [[data]] .
Such [[cooperating system]]s have to [[automatically]] and [[efficiently match]], [[exchange]], [[transform]] and [[integrate]] [[large data set]]s from different [[source]]s and of different [[structure]] in order to enable seamless [[data exchange]] and [[transformation]] .
Such [[data associate individual]]s to [[sets of value]]s (e.g., [[preference]]s, [[shopping item]]s, [[symptom]]s, [[query log]]s).
Such [[decision]]s, during [[training]], are done in a [[differentiable fashion]] so that the [[entire network]] can be [[trained jointly]] by [[gradient descent]] .
Such [[dipole]]s have proven important for [[understanding]] and explaining the [[variability]] in [[climate]] in many [[regions of the world]], e.g., the [[El Nino climate phenomenon]] is known to be responsible for [[precipitation]] and [[temperature anomali]]es over [[large part]]s of the [[world]] .
Such [[disambiguation]] can help enhance [[readability]] and [[add semantics]] to [[plain text]] .
Such [[event]]s may be [[adverse]], such as [[death]] or [[recurrence of a tumour]]; [[positive]], such as [[conception]] or [[discharge from hospital]]; or [[neutral]], such as [[cessation of breast feeding]] .
Such examples are typical for [[glossary]] or [[dictionary entri]]es, but we also found them in [[ordered]] and [[unordered list]]s of [[our]] [[corpus document]]s.
Such [[function]]s have many [[application]]s, for example, [[aggregating the preferences of multiple agents]], or [[merging rankings]] (of, say, [[webpage]]s) into a single [[ranking]] .
Such [[information]] from multiple [[context]]s is often available in the form of several [[incomplete matrices spanning]] a [[set of entiti]]es like [[user]]s, [[item]]s, [[feature]]s, and so on.
Such kind of [[side information]] is more [[flexible]] and greatly mitigates the [[workload]] of [[annotator]]s.
Such [[manipulation]]s usually result in much more complicated [[model structure]]s, [[sophisticated]] [[inference procedure]]s, and [[low generalizability]] to accommodate [[arbitrary type]]s or [[combinations of context]]s.
Such [[network]]s are notoriously difficult to [[mine]] because of the [[bewildering combination]] of [[heterogeneous content]]s and [[structure]]s.
Such [[paradigm]]s cannot effectively handle varied [[data]] with special [[properti]]es, e.g., the [[interleaved temporal dependenci]]es.
Such [[policy]] [[change]]s took place after [[the Great Depression]] during [[the New Deal]] and permanently reduced [[income concentration]] until the [[1970s]] (Figures 2, 3).
Such [[preference]]s are <i> [[time-consistent]] </i>: A [[person]]'s [[relative preference]] for [[well-being]] at an [[earlier date]] over a [[later date]] is the same no matter when she is asked.
Such problems arise in [[semantic]] [[scene]] and [[document classification]] and in [[medical diagnosis]] .
[[Such problems]] arise naturally in [[natural language processing]], [[search engines]], and [[bioinformatics]] .
Such [[problem]]s naturally exist in [[fMRI scan]]s of [[human subject]]s.
Such [[procedure]]s draw [[controlled amount]]s of suitably [[biased samples]] directly from the [[pattern space]] of a given [[dataset]] in [[polynomial time]] .
Such [[schemes]] look at [[annotating]] mostly [[abstracts of papers]] and less often [[full papers]], with the majority focussing on annotation at the [[token level]] for [[keywords]], ([[Korhonen et al., 2009]]; [[Thompson et al., 2009]]).
Such [[site-level]] [[knowledge]] is [[mined]] through [[reconstructing]] the [[linking structure]], called [[sitemap]], for a given [[forum site]] .
Such systems include native [[RDF]] stores as well as systems that [[rewrite]] [[SPARQL queri]]es to [[SQL queri]]es against [[non-RDF relational database]]s.
Such [[system]]s must solve [[information processing problem]]s that are often very similar to problems faced by [[computational system]]s, including [[coordinated decision making]], [[29]] [[leader election]], [[2]] [[routing]] and [[navigation]], [[52]] and more.[[42]]
[[Such systems]] require a [[formal language]] to describe the [[real world]]; and the [[real world]] has things in [[it]] .
Such [[transformation]] enables the development of an [[efficient]] [[two-stage approach]] which combines novel [[peeling technique]]s for [[maximal set discovery]] with [[depth-first search]] for further [[enumeration]] .
Such [[vulnerabilities]] include [[buffer overflow errors]], improperly [[validated inputs]], and other [[unanticipated attack modalities]] .
[[Sugato Basu]], [[Mikhail Bilenko]], Arindam Banerjee and [[Raymond Mooney]] 73
[[Super-Intelligent Machines]] combines [[neuroscience]] and [[computer science]] to analyze [[future]] [[intelligent machine]]s.
[[Supervised concept learning]], [[instance-based concept description]]s, [[incremental learning]] .
[[supervised learning]], [[social media]], [[blogs]], [[unigrams]], [[WordNet]], [[Epinions]], [[sentiment analysis]], [[n-gram]],
[[Supervised local pattern discovery]] aims to find [[subset]]s of a [[database]] with a [[high statistical unusualness]] in the [[distribution]] of a [[target attribute]] .
[[Support Vector Machines (SVMs)]] are a popular [[machine learning method]] for [[classification]], [[regression]], and other [[learning task]]s.
[[Support Vector Machines (SVMs)]], first introduced by [[Vapnik]] ([[Cortes and Vapnik, 1995]]; [[Vapnik, 1995]]), are relatively new [[learning approach]]es for solving [[two-class pattern recognition problem]]s.
Suppose we have a [[virus]] or one [[competing idea]] / [[product]] that [[propagate]]s over a [[multiple profile]] (e.g., [[social network]]).
Suppose we have two [[competing idea]]s / [[product]]s / [[virus]]es, that [[propagate]] over a [[social]] or other [[network]] .
[[Surveyor]] on the other hand focuses on [[generating]] [[survey article]]s that contain [[well defined]] [[subtopic]]s presented in a [[coherent order]] .
[[Susan Wolf]]'s topic in [[these essay]]s - formerly [[lecture]]s delivered at [[Princeton University]] in [[November]] [[2007]] - is familiar and [[inescapable]], and yet the [[topic]] has not received sustained [[philosophical attention]] .
[[Suspicious graph pattern]]s show up in many [[application]]s, from [[Twitter user]]s who buy [[fake follower]]s, manipulating the [[social network]], to [[botnet member]]s [[perform]]ing [[distributed denial of service attack]]s, [[disturbing the network traffic graph]] .
[[SVMs]] deliver [[state-of-the-art performance]] in [[real-world applications]] such as [[text categorisation]], [[hand-written character recognition]], [[image classification]], [[biosequences analysis]], etc., and are now established as one of the standard tools for [[machine learning]] and [[data mining]] .
[[SVRFs]] can be [[efficiently trained]], [[converge quickly]] during [[inference]], and can be trivially augmented with [[kernel function]]s.
[[Symbolists view learning]] as the inverse of [[deduction]] and take ideas from [[philosophy]], [[psychology]], and [[logic]] .
[[Synchronization]] is a powerful [[basic concept]] in [[nature]] regulating a large variety of [[complex processes]] ranging from the [[metabolism in the cell]] to [[social behavior]] in [[groups of individuals]] .
[[Synergies]] can be obtained by integrating [[personalization]] between [[several Internet properti]]es.
[[syntactic parsing]] ([[Täckström et al., 2012]]; [[Parikh et al., 2014]]), [[POS Tagging]] ([[Dhillon et al., 2012b]]; [[Huang et al., 2013]]), [[dependency parsing]] ([[Bansal et al., 2014]]; [[Koo et al., 2008]]; [[Dhillon et al., 2012a]]), [[sentiment analysis]]([[Dhillon et al., 2012b]]), [[chunking]] ([[Turian et al., 2010]]; [[Dhillon et al., 2011]]), [[Named Entity Recognition (NER)]] ([[Turian et al., 2010]]; [[Dhillon et al., 2011]]), word analogies ([[Mikolov et al., 2013a]], b) and [[word similarity]] ([[Huang et al., 2012]]) to name a few.
[[SyntActic Similarity]], [[Enterprise Information Management]], [[Performance Modeling]], [[Shingling algorithm]]s, [[Content-based chunking algorithm]]s.
[[Synthetic data]] [[sets]] and [[algorithm]] [[source code]] are available at http://www.cs.binghamton.edu/~lyu/KDD09/.
[[System 1]] is [[fast]], [[intuitive]], and [[emotional]]; [[System 2]] is slower, more deliberative, and more logical.
[[Systematic approach]]es for [[dipole detection]] generate a [[large number]] of [[candidate dipole]]s, but there exists no [[method to evaluate]] the [[significance]] of the [[candidate]] [[teleconnection]]s.
[[Systematic experiment]]s in our [[project]] OfCourse for [[online course]] [[search]] show that this [[model]] significantly improves the [[F1]] [[value]] for both of the two steps.
[[System monitoring]] provides an [[effective]] and [[reliable]] means for [[problem detection]] .
[[System properti]]es, [[stability]], [[Lyapunov function]]s, [[controllability]], [[STLC]], [[Hamilton-Jacobi-Bellman equation]], [[Pontryagin's maximum principle]], [[Dubins curve]]s, [[Reeds-Shepp curve]]s, [[Balkcom-Mason curve]]s, [[affine control system]]s, [[distribution]]s, [[Frobenius theorem]], [[Chow-Rashevskii theorem]], [[Lie bracket]]s, [[control Lie algebra]], [[P. Hall basis]], [[steering]] with [[piecewise constant input]]s, [[steering]] with [[sinusoid]]s.
[[System requirement]]s are expressed using [[formal specification]]s given as [[annotations inserted]] at various [[user]] selected places in [[program]]s.
[[Systems biology]] has made massive [[strides]] in recent years, with [[capabilities]] to [[model]] [[complex system]]s including [[cell division]], [[stress response]], [[energy metabolism]], and [[signaling pathways]] .
Systems such as [[CiteSeer]] and [[Google Scholar]] [[extract a database-like representation]], relating papers and [[researcher]]s by authorship and [[citation link]]s, from [[raw ASCII citation string]]s.
[[Systems tracking]] [[spontaneous devaluation]] in [[user preference]]s can allow [[prediction]] of the onset of [[boredom]] in [[user]]s potentially catering to their changed needs.
Table 2 provides [[coding example]]s for specific [[balance sheet]], [[revenue]], and [[expenditure transaction]]s.
Table 2 shows the [[distribution]] of [[title]]s and of term types by the different [[industry]] [[label]]s.
Table 4: Examples of [[extracted]] [[pairs]] for system run with [[co-EM]]
Table 5 compares different [[model]]s’ [[results]] on [[this dataset]] .
Table of [[Contents: Preface / Acknowledgment]]s / [[Introduction]] and [[Scope / Approaches Based on Self-Report Methods / Approaches Based]] on [[Physiological Measurements / Approaches Based]] on [[Web Analytics / Beyond Desktop]], [[Single Site]], and [[Single Task]] / [[Enhancing]] the [[Rigor of User Engagement Method]]s and [[Measures / Conclusion]]s and [[Future Research Directions / Bibliography]] / [[Authors' Biographi]]es / [[Index]]
[[Table]]s are a richer [[source]] of [[structured knowledge]] than [[free text]] .
[[Tagging strategi]]es employed include both [[unsupervised]] and [[supervised approach]]es based on [[machine learning]] and [[natural language processing]] .
[[Tag recommendation]] is the [[task]] of [[predicting a personalized list]] of [[tag]]s for a [[user]] given an item.
Taken together, [[our result]]s demonstrate that [[DNC]]s have the capacity to solve [[complex]], [[structured task]]s that are inaccessible to [[neural network]]s without [[external readâwrite memory]] .
Taking into account both the [[social annotation]] and [[friendships inherent]] in the [[social graph established]] among [[user]]s, [[item]]s and [[tag]]s, [[we]] created a [[collaborative recommendation system]] that effectively adapts to the [[personal information]] needs of each [[user]] .
[[Tapestry]] is intended to handle any incoming [[stream of electronic documents]] and serves both as a [[mail filter]] and [[repository]]; [[its]] components are the [[indexer]], [[document store]], [[annotation store]], [[filterer]], [[little box]], [[remailer]], [[appraiser]] and [[reader/browser]] .
[[Tapestry]] was designed to support both [[content-based filtering]] and [[collaborative filtering]], which entails people [[collaborating]] to help each other [[perform filtering]] by [[recording their reactions]] to [[document]]s they [[read]] .
[[Targeting]] [[social-network]] [[neighbors]] resonates well with [[advertisers]], and [[on-line browsing]] [[behavior]] [[data]] counterintuitively can allow the [[identification]] of good [[audience]]s [[anonymously]] .
[[Task 14]] provides an [[evaluation framework]] for automatic taxonomy enrichment techniques by measuring the placement of a new [[concept]] into an existing [[taxonomy]]: Given a new [[word]] and its [[definition]], [[system]]s were asked to [[attach or merge the concept]] into an [[existing]] [[WordNet concept]] .
:::* [[Task Output]]: Sample [[Correlation Coefficient]], Optional outputs: [[P-value]], [[Region of Acceptance]], [[Region of Rejection]], [[Decision Error]]s ([[Type I]] or [[Type II error]]s) and respective [[probabilities]] (<math>\alpha,\; \beta</math>).
:::* [[Task Output]]: [[t-test statistic]] value, [[P-value]], [[Region of Acceptance]], [[Region of Rejection]], [[Decision Error]]s.
[[Tax Exempt Lease Purchase]]: A [[program]] where [[state agenci]]es may [[lease-purchase equipment]], and which includes tax benefits for the [[vendor]] and [[price break]]s for the [[state]] .
[[taxonomy]], [[document categorization]], [[SVM]], [[hierarchical loss]], [[class relationship]], [[subspace optimization]]
[[TCM]] characterizes [[user behavior]] related to a [[task]] as a [[collective whole]] .
[[TD-Gammon]], a [[backgammon-playing program]] developed by [[IBM's Gerald Tesauro in 1992]], was a [[neural network]] that learned to [[play backgammon]] through [[reinforcement learning]] (the [[TD]] in the [[name stand]]s for [[Temporal-Difference learning]], still a [[dominant algorithm in reinforcement learning]]).
[[Team formation]] has been long recognized as a natural way to acquire a diverse [[pool]] of [[useful skill]]s, by combining [[expert]]s with [[complementary]] [[talent]]s.
[[TechMiner]] was evaluated on a [[manually annotated gold standard]] and [[the result]]s indicate that it significantly outperforms alternative [[NLP approach]]es and that its [[semantic feature]]s improve [[performance significantly]] with respect to both [[recall]] and [[precision]] .
[[Technically speaking]], [[anything]] that [[stores data]] for later [[retrieval]] is a [[database]] .
[[Techniques for analyzing]] such [[event]]s are of increasing [[interest]] .
[[Technological innovation]]s have produced [[robot]]s capable of [[job]]s that, until recently, only [[human]]s could perform.
::[[Temperature]] is an important [[quantity]] in [[thermodynamics]] and [[kinetic theory]], appearing explicitly for example in the [[ideal gas law]]
[[Temporal dataset]]s, in which [[data evolves continuously]], exist in a wide variety of [[application]]s, and [[identifying]] [[anomalous or outlying object]]s from [[temporal dataset]]s is an important and [[challenging task]] .
[[Temporal signature]]s, [[systems biology]], [[feature selection]], [[rank-order space]]s, [[biological network]]s.
[[Tensor decomposition]] is an important [[data mining tool]] with various [[application]]s including [[clustering]], [[trend detection]], and [[anomaly detection]] .
[[TensorFlow]] is a [[machine learning system]] that operates at [[large scale]] and in [[heterogeneous environment]]s.
[[TensorFlow]] supports a variety of [[application]]s, with a focus on [[training]] and [[inference]] on [[deep neural network]]s.
[[Tensor-Flow]] uses [[dataflow graph]]s to represent [[computation]], [[shared state]], and the [[operation]]s that mutate that [[state]] .
[[Term identification]]; [[Term recognition]]; [[Term classification]]; [[Term mapping]]; [[Acronym recognition]]; [[Biomedical literature]]
[[Terminology]]; [[Auditing method]]; [[Quality factor]]; [[Knowledge source]]; [[Manual]]; [[Automated]]; [[Systematic]]; [[Heuristic
[[Term recognition]], [[termhood]], [[term characteristic]], [[Bayes' theorem]], [[word distribution models]], [[term extraction]]
[[Term]]s (such as names of [[genes]], [[proteins]], [[gene products]], [[organisms]], [[drugs]], [[chemical compounds]], etc.) are the means of [[scientific communication]] as they are used to identify [[domain concepts]]: there is no possibility to understand an [[article]] without precise [[identification of term]]s that are used to communicate the [[knowledge]] .
[[Tested]] on a [[large scale dataset]], our [[model]] exhibits good [[performance]], clearly [[outperforming]] standard [[baseline]]s.
[[Text Analysis]]; [[Language Models]]; [[Information Extraction]]; [[Ontology-Guided Search]]
[[Text categorization]], [[classification]], [[support vector machines]], [[machine learning]], [[information management]] .
[[Text chunking]] involves [[dividing]] [[sentence]]s into [[non-overlapping segment]]s on the basis of fairly superficial analysis.
[[Text classification]] and [[clustering]] play an important role in many applications, e.g, [[document retrieval]], [[web search]], [[spam filtering]] .
[[text classification]], [[Expectation-Maximization]], [[integrating supervised and unsupervised learning]], [[combining labeled and unlabeled data]], [[Bayesian learning]] .
[[Text Classification]] [[Machine Learning]], [[Feature Weighting]], [[Feature Scaling]], [[SVM]]
[[Text classification method]]s for tasks like [[factoid question answering]] typically use [[manually defined]] [[string matching rule]]s or [[bag of words representation]]s.
[[Text clustering method]]s can be used to [[structure]] [[large sets of text]] or [[hypertext documents]] .
[[Text corpora]] with [[document]]s from a range of [[time epoch]]s are [[natural]] and [[ubiquitous]] in many [[field]]s, such as [[research paper]]s, [[newspaper article]]s and a variety of types of recently emerged [[social media]] .
[[Text documents]]; [[Clustering]]; [[Frequent word sequence]]s; [[Frequent word meaning sequence]]s; [[Web search]]; [[WordNet]]
[[Text Mining]] in [[biology]] and [[biomedicine]] requires a large amount of [[domain-specific knowledge]] .
[[Text Mining]]; [[NLP]]; [[Ontology Design]]; [[Ontology Population]]; [[Ontological NLP]]
[[Text mining result]]s are stored in a [[knowledge base]] through a flexible [[export process]] that provides for a [[dynamic mapping]] of [[semantic annotation]]s to [[LOD vocabulari]]es through [[rules stored]] in the [[knowledge base]] .
[[Text routing]], for example, involves [[sending relevant incoming data]] to [[individual]]s or [[group]]s.
[[Text simplification]] modifies [[syntax]] and [[lexicon]] to [[improve]] the [[understandability of language]] for an [[end user]] .
[[textual resource]]s include [[linguistic text collections]], ranging from [[large]] [[generic repositories]] such as [[the Web]] to [[specific domain texts]] such as [[collections of texts or books]] on specific [[subjects]] .
[[Text understanding]] consists in [[reading text]]s formed in [[natural language]]s, [[determining the explicit]] or [[implicit meaning]] of each [[element]]s such as [[word]]s, [[phrase]]s, [[sentence]]s and [[paragraph]]s, and [[making inference]]s about the [[implicit]] or [[explicit properti]]es of these texts ([[Norvig, 1987]]).
Thanks to [[Bering]]'s [[insight]] and [[wit]], [[The Belief Instinct]] will reward readers with an [[enlightened understanding]] of the [[universal]] [[human tendency]] to [[believe]] - and the [[tool]]s to [[break free]] .
[[Thanks to careful design]], [[CatchSync]] has the following [[desirable properti]]es: (a) it is [[scalable]] to [[large dataset]]s, being [[linear]] on the [[graph size]]; (b) it is [[parameter free]]; and (c) it is [[side-information-oblivious]]: it can operate using only the [[topology]], without needing [[labeled data]], nor [[timing information]], etc., while still capable of using [[side information]], if available.
Thanks to [[MASCOT]], we also discover [[interesting]] [[anomalous pattern]]s in [[real graph]]s, like <i>[[core-peripheri]]es</i> in the [[web]] and <i>[[ambiguous author name]]s</i> in [[DBLP]] .
That involves [[learning]] about the [[user]]s and their [[location]]s, their [[personal preference]]s, and [[predicting which deal]]s are likely to delight them, presenting [[diversity]], [[discovery]] and [[engaging UX]] to gather [[user preference]]s and [[semantic graph approach]]es for [[user-deal matching]] .
That is, given a [[data set]] and some [[statistics]] (e.g., [[cluster centers]] or [[co-occurrence]] [[counts]]) of the [[data]], the [[randomization methods]] sample [[data sets]] having similar [[values]] of the given [[statistics]] as the original [[data set]] .
That is, [[object]]s associated with [[similar target]]s should be [[mapped to]] [[nearby point]]s in the [[embedded space]] .
That is one [[class]] (the [[majority class]]) vastly outnumbers the other [[class]] (the [[minority class]]).
That is, [[they]] do not explicitly contain [[variables]] indicating the [[“canonical” values]] for each [[attribute]] of an [[entity]] (e.g., [[name]], [[venue]], [[title]], etc.). This [[canonicalization step]] is typically implemented as a [[post-processing routine]] to [[coreference resolution]] prior to [[adding]] the [[extracted entity]] to a [[database]] .
That of [[sophister]]s, [[economist]]s, and [[calculator]]s has succeeded; and the glory of [[Europe]] is extinguished forever.
That such [[algorithm]]s should give [[reliable]] or [[consistent answers]] is surely a [[desideratum]], and in [10], [[we]] analyzed when they can be expected to give [[stable]] [[rankings]] under small perturbations to the [[linkage patterns]] .
The ability to [[identify sentiment]]s about [[personal experience]]s, [[product]]s, [[movi]]es etc. is crucial to understand [[user generated content]] in [[social network]]s, [[blog]]s or [[product review]]s.
The ability to [[transform data]] into [[meaningful]], [[actionable insight]] is an important source of [[competitive advantage]] for [[OSD]] .
:* The [[absolute value]] of [[complex number]] is [[complex number modulus]]
:* The [[absolute value]] of [[vector]] is its [[distance]] from zero, i.e. [[vector norm]]
The [[abstract of scientific paper]]s has [[strong semantic structure]], which contains abundant [[meaningful information]], such as the [[background]], [[research problem]], [[solution]], and [[result]] .
::The [[acceleration]] <i>a</i> is the [[rate of change]] of the [[velocity]], and [[Newton’s Second Law]] says more than that the effect of a given [[force]] varies [[inversely]] as the [[mass]]; it says also that the [[direction]] of the change in the [[velocity]] and the [[direction]] of the [[force]] are the same.
The [[account number]]s and [[descriptions of account]]s used in the [[DCED]] [[form]] [[DCED-CLGS-30: Annual Audit and Financial Report]], are based on [[this Chart of Accounts]] .
The [[accuracy]] of [[CHIRP]] on widely-used [[benchmark dataset]]s exceeds the [[accuracy]] of [[competitor]]s.
The [[ACL Anthology]] is a [[digital archive]] of [[conference]] and [[journal paper]]s in [[natural language processing]] and [[computational linguistics]] .
The [[active sample selection problem]] and the [[feature selection problem]] are [[correlate]]d for [[graph data]] .
The [[adaptation]]s are [[non-trivial]], and involve both careful [[analysis]] of the [[per-record sensitivity]] of the [[algorithm]]s to [[calibrate]] [[noise]], as well as new [[post-process]]ing steps to mitigate the impact of this [[noise]] .
:::# The [[adjustment]] or [[interpretation]] of the [[terms and conditions]] of the [[Federal award]]; or<br>
:# The [[adjustment]] or [[interpretation]] of the [[terms and conditions]] of the [[Federal award]]; or<br>
The adoption of [[data-intensive machine-learning method]]s can be found throughout science, technology and commerce, leading to more [[evidence-based decision-making]] across many walks of life, including [[health care]], [[manufacturing]], [[education]], [[financial modeling]], [[policing]], and [[marketing]] .
The adoption of the [[Linked Data best practice]]s has lead to the extension of [[the Web]] with a [[global data space]] connecting [[data]] from diverse [[domains]] such as [[people]], [[companies]], [[books]], [[scientific publications]], [[films]], [[music]], [[television]] and [[radio programmes]], [[genes]], [[proteins]], [[drugs]] and [[clinical trials]], [[online communities]], [[statistical and scientific data]], and [[reviews]] .
The advantage of [[this scheme]] is that in the limit of very many [[sample]]s, it is [[guaranteed]] to [[converge]] to the [[correct]] [[statistics]] .
The advantages of [[discriminative learning algorithm]]s and [[kernel machine]]s are combined with [[generative modeling]] using a novel [[kernel]] between [[distribution]]s.
The [[advantages of the proposed model]] include: 1) [[It]] can leverage the [[relationship]] between [[risk]]s and [[domain]]s and achieve better [[risk prediction performance]]; 2) It provides a [[data-driven approach]] to understand [[relationship]] between risk]]s; 3) It leverages the [[information]] between [[risk prediction]] and [[risk association learning]] to [[regulate]] the improvement on both parts; 4) It provides flexibility to incorporate [[domain knowledge]] in [[learning risk association]]s.
The advantages of these [[direct method]]s are an almost [[optimal time]] [[complexity]] [[per]] [[pattern]] as well as an exactly [[controlled distribution]] of the [[produced pattern]]s.
The [[aggregation model]]s that [[we]] consider are equivalent to defining a "[[support instance]] " for each [[bag]], which allows [[efficient optimization]] of the [[rank-loss objective]] using [[primal sub-gradient descent]] .
The [[AI]] and [[robotics community]] needs [[thought leader]]s who can engage with [[prominent commentator]]s such as [[physicist]] [[Stephen Hawking]] and [[entrepreneur–inventor]] [[Elon Musk]] and set the agenda at [[international meeting]]s such as the [[World Economic Forum]] in [[Davos, Switzerland]] .
The [[AI]] and [[robotics science communiti]]es, [[represented by]] their [[professional societi]]es, are obliged to take a position, just as [[physicist]]s have done on the use of [[nuclear weapon]]s, [[chemist]]s on the use of [[chemical agent]]s and [[biologist]]s on the use of [[disease agent]]s in [[warfare]] .
The aim of [[data mining]] is to find [[novel]] and [[actionable insights]] in [[data]] .
The aim of [[this article]] is to explore the [[possible future]]s [[generated by]] the [[development of]] [[artificial intelligence]] .
[[The algorithm]] accelerated the extraction of [[experimentally validated]] [[subcellular localizations]] .
[[The algorithm]] does not depend on a [[minimal frequency]] [[threshold]] and is shown to outperform several alternative [[approaches]] by [[orders of magnitude]], both in [[runtime]] and in [[memory requirement]]s.
The [[algorithm]] employs [[progressive sampling]], with a [[stopping condition]] based on [[bound]]s to the [[empirical Rademacher average]], a [[key concept]] from [[statistical learning theory]] .
[[The algorithm]] first [[trains]] a [[classifier]] using the available [[labeled document]]s, and [[probabilistically labels]] the [[unlabeled document]]s.
[[The algorithm]] has a simple interpretation: it [[alternately]] performs a [[supervised]] and an [[unsupervised step]], where in the former [[step]] it learns [[task-specific function]]s and in the [[latter step]] it [[learns common-across-tasks sparse representations]] for these [[function]]s.
[[The algorithm]] is [[automatic]] and [[unsupervised]] in both [[training]] and [[application]]: [[senses]] are [[induced from a corpus]] [[without labeled training instance]]s or other [[external knowledge]] [[source]]s.
The [[algorithm]] is [[flexible]], [[scalable]], and surprisingly [[straight-forward]] to implement as it is based on a [[modification]] of [[Gradient Boosted Tree]]s.
The [[algorithm]] is significantly faster than [[standard]] [[language identification method]]s, while providing [[state-of-the-art]][[identification]] .
The [[algorithm]] iteratively picks a [[random edge]] as [[pivot]], builds a [[cluster]] around it, and removes the [[cluster]] from the [[graph]] .
[[The algorithm]] performs [[hierarchical clustering]] in the [[space of model parameter]]s from [[historical model]]s in order to collapse related [[feature]]s into a single [[dimension]] .
[[The algorithm]] reduces this setting to [[binary classification]], allowing one to [[reuse]] any existing, [[fully supervised]] [[binary classification algorithm]] in this [[partial information]] [[setting]] .
[[The algorithm]]s [[minimize]] [[I/O overhead]]s by [[scanning]] the [[local]] [[database portion]] only [[twice]] .
[[The algorithm]]'s [[time complexity]] is dependent on the number of [[column]]s in the final [[model]] and not the number of [[column]]s in the [[dataset]] .
The [[algorithm]] that was proposed for [[maintaining]] all [[frequent items]], however, [[scales]] poorly when the [[number of item]]s becomes [[large]] .
[[The algorithm]] uses a [[greedy top-down search]] to locate a [[probabilistic IF-THEN rule]] that will [[classify]] the given [[event]] .
The [[analysis of data [[represented as]] graphs]] is common having [[wide scale]] [[application]]s from [[social network]]s to [[medical imaging]] .
The [[analysis]] of [[dynamic network]]s is important not only to understand life at [[the system]]-level, but also to [[discover]] novel [[pattern]]s in other [[structural]] [[data]] .
The [[analysis]] of [[multimedia application trace]]s can reveal important [[information]] to enhance [[program execution comprehension]] .
The [[analysis of network connection]]s, [[diffusion process]]es and [[cascade]]s requires [[evaluating properties]] of the [[diffusion network]] .
[[The analysis presented]] is based on a [[dataset]] of over 58,000 [[volunteer]]s who provided their [[Facebook Like]]s, detailed [[demographic profile]]s, and the [[result]]s of several [[psychometric test]]s.
The [[analysis]] provides the first [[evidence]] of [[spontaneous devaluation]] in [[preferences of music listener]]s.
The [[annotation]] of [[GENETAG]] required intricate [[manual judgment]]s by [[annotator]]s which hindered [[tagging consistency]] .
The [[annotation]]s correspond to [[ontological concept]]s, and [[knowledge reuse]] is improved by [[semantic search]] and [[semantic navigation]] .
The answer of [[this paper]] is a [[technique]] integrating [[graph summarization]], [[graph clustering]], [[link prediction]] and the [[discovery of the hidden structure]] on the basis of [[data compression]] .
[[The approach]] considers each [[word selection]] in the [[sentence]] as an [[action]], and [[computes the reward of the sequence]] with the [[Monte Carlo (MC) search]] .
[[The approach]] enjoys fast [[training]] and [[time]], but may sometimes achieve [[accuracy]] close to that of using highly [[nonlinear kernel]]s.
The approach is based on [[Harris' distributional hypothesis]] and, based on the [[vector-space model]], it [[assign]]s a [[named entity]] to the [[contextually most similar]] [[concept]] from the [[ontology]] .
[[The approach]] is evaluated using three [[large review data set]]s from [[IMDB]], [[Yelp]], and [[Amazon]], and we demonstrate [[the proposed approach]] is both [[accurate]] and [[scalable]] compared to various [[alternative]]s.
The [[approach]], named [[SCMILO]], is [[validated]] against the ability to [[identify the concept mentions]] within the 139 [[KDD-2009 conference]] [[paper abstracts]], and to [[link these mentions]] to a [[domain-specific ontology]] for the [[field of data mining]] .
The [[approach]] of [[this paper]] overcomes [[legal weakness]]es and [[technical limitation]]s of existing [[proposal]]s.
The approach uses a [[text-graph representation]] of the entire [[document]] that is based on [[intrasentential edges]] derived from each [[sentence]]’s predicted [[syntactic parse trees]], and on [[intersentential edges]] based on either the [[linking]] of [[adjacent sentence]]s or the [[linking of coreferents]], if reliable [[coreference]] [[predictions]] are available.
The approach utilizes three types of [[citation attribute]]s: [[co-author]] [[name]]s, [[paper title]]s, and [[publication venue title]]s 2.
The approach works on [[off-the-shelf]] [[legacy application]]s and does not require [[formal specification]]s, [[program annotation]]s or special [[coding practice]]s.
The [[architecture]] consists of a combination of [[Convolutional layer]] and a [[Recurrent layer]] for predicting the [[connectome]] of [[neuron]]s based on their [[time-series]] of [[activation data]] .
The architecture includes [[domain ontology]], [[domain text]]s, [[language specific lexicon]]s, [[regular grammar]]s and [[disambiguation rule]]s.
The [[architecture]]s are based on [[PCFG]]s with [[latent variable]]s, [[graph-based dependency parsing]] and [[transition-based dependency parsing]], respectively.
[[The article]] analyses the results of [[interpretation]], [[population]] and [[enrichment]] obtained in evaluation experiments in terms of measures such as [[precision]] and [[recall]] .
[[The article]] discusses lessons learned on the [[architecture]] of a [[knowledge harvesting system]], and points out [[open challenge]]s and [[research opportuniti]]es.
The article discusses the design of the [[BSBM benchmark]] and presents the results of a [[benchmark experiment]] comparing the [[performance]] of four popular [[RDF store]]s (Sesame, [[Virtuoso]], [[Jena TDB]], and [[Jena SDB]]) with the [[performance]] of two [[SPARQL-to-SQL rewriter]]s ([[D2R Server]] and [[Virtuoso RDF View]]s) as well as the [[performance]] of two [[relational database management system]]s ([[MySQL]] and [[Virtuoso RDBMS]]).
[[The article]] said the "... first significant [[probabilistic programming language]] was [[Pfeffer]]'s [[IBAL]] .
The [[assessment]] of [[WSD system]]s is discussed in the context of the [[Senseval]]/[[Semeval campaign]]s, aiming at the [[objective evaluation]] of [[system]]s participating in several different [[disambiguation task]]s.
The assumption of a [[conventional model]], [[logarithm]] of [[purchase rate]] and [[dropout rate]] with [[linear regression]], is extended to include our [[assumption]] of the [[Dirichlet Process Mixture of regression]] .
The [[attentional state]], being [[dynamic]], [[records the object]]s, [[properti]]es, and [[relation]]s that are salient at each [[point of the discourse]] .
The [[AUC]]s of several [[baseline]]s were improved, including [[logistic regression]] without [[multi-task learning]] and several [[multi-task learning method]]s that do not incorporate the [[domain knowledge]] .
The [[auction parameter]]s may be set for each [[individual keyword]], but the [[optimization problem]] becomes [[intractable]] since the number of [[keyword]]s is in the [[million]]s.
[[The author]] explains several example [[applications of learning to rank]] including [[web search]], [[collaborative filtering]], [[definition search]], [[keyphrase extraction]], [[query dependent summarization]], and [[re-ranking in machine translation]] .
[[The author]] gives detailed explanations on [[learning for ranking creation]] and [[ranking aggregation]], including [[training]] and [[testing]], [[evaluation]], [[feature creation]], and major [[approach]]es.
[[The authors]] describe the [[history]] and [[current use of]] [[computerized system]]s for implementing [[treatment guideline]]s in general medicine as well as the [[development]], [[testing]], and [[early use of]] a [[computerized decision support system]] for [[depression treatment]] among "real-world" [[clinical setting]]s in [[Texas]] .
[[The authors]] describe the [[state of the art]] by discussing the latest achievements such as more effective [[methods for matching data]], [[mapping transformation verification]], [[adaptation to the context]] and [[size of the matching]] and [[mapping task]]s, [[mapping-driven schema evolution]] and [[merging]], and [[mapping evaluation]] and tuning.
[[The authors]] examine how susceptible [[job]]s are to [[computerisation]], by implementing a novel methodology to estimate the [[probability]] of [[computerisation]] for 702 detailed [[occupation]]s, using a [[Gaussian process classifier]] .
[[The authors]] explored [[personality dimension]]s of [[World of Warcraft (WoW)]] [[player]]s and examined the differences between the [[44-item]] [[personality measure]] [[Big Five Inventory (BFI)]] and [[WoW player]]s.
The [[author's finding]]s, based on [[multivariate statistical analysis (MANOVA)]], [[establish a connection]] between [[personality characteristic]]s of the [[BFI]] and the style of [[play of the participant]]s.
[[The author]]s focused on the specifics of [[participant]]'s primary [[play style]] ([[player versus player (PVP)]], [[player versus environment (PVE)]], or [[role-playing (RP)]]), [[specialization of the character]] ([[tank]], [[healer]], [[damage]]), [[character race]] (13 races), [[character class]] (11 [[classe]]s), and gender as it relates to the [[BFI personality element]]s.
[[The authors]] present an [[integrated]] [[enterprise]] [[knowledge management architecture]] developed within the [[Ontologging project]] dealing with several challenges related to [[applying]] [[ontologies]] in [[real-world environment]]s.
The [[author topic model]] belongs to a [[family of generative model]]s for [[text]] where [[word]]s are viewed as [[discrete random variable]]s, a [[document]] contains a [[fixed number]] of [[word]]s, and each [[word]] takes one value from a [[predefined vocabulary]] .
The [[automated targeting]] of [[online display ads]] at [[scale]] requires the simultaneous [[evaluation]] of a [[single prospect]] against many [[independent model]]s.
The automatic [[consolidation of database records]] from many [[heterogeneous]] [[source]]s into a single [[repository]] requires solving several [[information integration tasks]] .
The basic idea is that [[document]]s are [[represented]] as [[random mixtures]] over [[latent]] [[topics]], where each [[topic]] is characterized by a [[distribution]] over [[words]]
The basic [[idea]] is to view each [[data object]] as a [[phase oscillator]] and simulate the [[interaction behavior]] of the [[objects]] [[over time]] .
The basic idea of our [[technique]]s relates [[outlier detection]] to [[data compression]] : [[Outliers]] are [[objects]] which can not be effectively [[compressed]] given the [[data set]] .
The [[basic pattern]] was to first apply brands, then product lines, then several [[product feature]]s, then [[product]]s, [[black list]] and [[offer feature]]s within the [[record]]’s [[industry]] .
The basic [[questionnaire format]] that we selected was for the worker to provide [[feedback]] on whether a given term was of the given [[offering-term type]] ([[feature]], [[category]], etc.) within a given [[industry]] ([[electronics]], [[books]], etc.).
The [[basis function]]s are generally chosen to be [[orthogonal]] over the [[domain]] [[of interest]], and the [[observed data]] are used to [[estimate the coefficients]] in the [[series]] .
The [[batch]]-[[labeling]], however, raises serious [[issues]] because [[labeled]] [[groups]] may contain [[non-positive samples]], and [[users]] may change their [[labeling]] [[interests]] at any [[time]] .
The [[Bayesian framework]] has its origins in [[Helmholtz]]’s idea of [[perception]] as [[unconscious inference]] .
The [[benchmark]] focuses on the [[production of RDF graph]]s from [[relational database]]s and thus only [[test]]s [[SPARQL]] CONSTRUCT queries.
The [[benchmark result]]s verified the [[effect]]s of the [[factor]]s considered in the [[test-set design]] .
The benefits and [[flexibility]] of [[the proposed methodology]] have been validated by developing [[statistical dialog manager]]s for four [[spoken dialog systems of different complexity]], designed for different [[languages (English]], [[Italian]], and Spanish) and [[application domain]]s (from [[transactional]] to [[problem-solving task]]s).
The best performing [[method]], an [[asynchronous variant]] of [[actor-critic]], surpasses the current [[state-of-the-art]] on the [[Atari domain]] while training for [[half the time]] on a single [[multi-core CPU]] instead of a [[GPU]] .
The best [[results]] on this [[dataset]] are obtained by integrating [[Google]], [[WordNet]] and [[Wikipedia]] based [[measures]] .
The best [[subset selection procedure]] along with traditional [[model selection criteria]], such as [[AIC]] and [[BIC]], becomes infeasible for [[feature selection]] from [[high-dimensional data]] due to too expensive [[computational cost]] .
The [[BFI]] measures [[personality trait]]s based on the five broad domains of [[openness]], [[conscientiousness]], [[extraversion]], [[agreeableness]], and [[neuroticism]] .
The [[bias tensor]] represents the [[baseline characteristic]]s common amongst the overall [[population]] and the [[interaction tensor]] defines the [[phenotype]]s.
The [[Bibliographic Ontology]] describe [[bibliographic things]] on the [[semantic Web]] in [[RDF]] .
The "[[big data]]" era is characterized by an [[explosion]] of [[information]] in the form of [[digital data collection]]s, ranging from [[scientific knowledge]], to [[social media]], [[news]], and everyone's daily life.
The [[big data era]] is witnessing a [[prevalent shift]] of [[data]] from [[homogeneous]] to [[heterogeneous]], from [[isolated]] to [[linked]] .
[[The book]] deals with the [[supervised-learning problem]] for both [[regression]] and [[classification]], and includes [[detailed algorithm]]s.
[[The book]] edited by [[Bellahsene]], [[Bonifati]] and [[Rahm]] provides an overview of the ways in which the schema and [[ontology matching]] and [[[mapping tool]]s have addressed the above [[requirement]]s and points to the [[open technical challenge]]s.
[[The book]] ends by comparing and contrasting the new account with some major representatives of [[earlier]] [[alternative]] [[approach]]es, from the [[field]]s of [[formal epistemology]], [[artificial intelligence]] and [[mathematical logic]] .
The book is organized into six sections: [[Classical Approaches]], [[Corpus-based Approaches]], [[Exploiting Discourse Structure]], [[Knowledge-Rich Approaches]], [[Evaluation Method]]s, and [[New Summarization Problem Areas]] .
[[The book]] is suitable as a [[self-study guide]] for the [[professional]], [[student]] or [[researcher]] .
[[The book]] is suitable for courses on [[machine learning]], [[statistics]], [[computer science]], [[signal processing]], [[computer vision]], [[data mining]], and [[bioinformatics]] .
[[The book]] is written in an [[informal]], [[accessible style]], complete with [[pseudo-code]] for the most important [[algorithm]]s.
[[The book]] looks at how [[scarcity]] has emerged as a [[totalizing discourse]] in both the [[North]] and [[South]] .
[[The book]] mixes together [[algebra]], [[analysis]], [[complexity theory]] and [[numerical analysis]] .
The book presents new formalisms of [[pattern processing]]: [[orthogonal projectors]], [[optimal associative mappings]], [[novelty filter]]s, [[subspace method]]s, [[feature-sensitive units]], and [[self-organization of topological maps]], with all their computable [[algorithm]]s.
The [[bootstrapping framework]] starts with only a [[small number]] of [[seeds]] and [[iteratively]] finds [[new]] [[facts]] and [[biographies]] .
The [[bounce rate]] of a [[ad]] can be [[informally defined]] as the [[fraction]] of [[user]]s who [[click]] on the [[ad]] but almost immediately move on to other [[task]]s.
The broad idea is to determine the [[functional dependenci]]es between [[sensor stream]]s efficiently in [[real time]], and [[actively collect]] the [[data]] only from a [[minimal]] [[set of sensor]]s.
The [[Brown algorithm]] is a [[hierarchical clustering algorithm]] which [[clusters words]] to maximize the [[mutual information]] of [[bigram]]s ([[Brown et al., 1992]]).
The [[business model]] [[concept]] offers strategists a fresh way to consider their [[option]]s in [[uncertain]], [[fast-moving]] and [[unpredictable environment]]s.
The [[canonical technique]] for [[analyzing]] [[functional magnetic resonance imaging (fMRI)]] [[data]], [[statistical parametric map]]ping, produces [[map]]s of [[brain location]]s that are more [[active]] during [[performance]] of a [[task]] than during a [[control condition]] .
The [[case studies]] on [[large]] and [[real]] [[daily]] [[collection]] of the [[gray list]] illustrate that the [[detection]] [[ability]] and [[efficiency]] of our [[IFSS system]] outperforms other [[popular]] [[scanning tools]] such as [[NOD32]] and [[Kaspersky]] .
The [[case studies]] on [[large]] and [[real]] [[daily]] [[malware collection]] from [[Kingsoft Anti-Virus Lab]] demonstrate the [[effectiveness]] and [[efficiency]] of our [[AMCS system]] .
The [[case study]] [[presentation]] will present a [[fast-paced overview]] of the [[business]] and [[technology]] [[context]] for [[Rocket Fuel]] at [[inception]] and at [[present]], [[key]] [[learning]]s and [[decision]]s, and the [[road ahead]] .
The catastrophic collapse of companies such as [[Enron]], [[WorldCom]], [[ImClone]], and [[Tyco]] left angry [[investor]]s, [[employee]]s, [[reporter]]s, and [[government investigator]]s demanding to know how the [[CEO]]s deceived everyone into believing their [[compani]]es were spectacularly [[successful]] when in fact they were massively [[insolvent]] .
The [[categorization results]] of our [[AMCS system]] can be used to [[generate]] [[signatures]] for [[malware families]] that are useful for [[malware detection]] .
The central [[challenge]] in [[optimizing]] various [[measures]] of [[ranking loss]] is that the [[objectives]] tend to be [[non-convex]] and [[discontinuous]] .
The central challenge in [[temporal]] [[data analysis]] is to obtain [[knowledge]] about its [[underlying dynamic]]s.
The central [[idea]] of [[our approach]] is to use a [[large amount]] of [[historical data]] to [[initialize]] the [[online models]] based on [[offline features]] and [[learn linear projections]] that can effectively [[reduce the dimensionality]] .
The [[cFTM]] automatically [[infer]]s the number of [[topic]]s needed to [[represent]] the [[corpus]], the number of [[author]] and [[venue cluster]]s, and the [[probabilistic importance]] of the [[author]], [[venue]] and [[random-effect information]] on [[word assignment]] for a given [[document]] .
The [[cFTM]] [[infer]]s a [[sparse ("focused") set]] of [[topic]]s for each [[document]], while also leveraging [[contextual information]] about the [[author(s)]] and [[document venue]] .
[[The challenge]] has attracted a lot of interest (over [[100]] [[team]]s [[registered]], and 27 of those submitted [[final result]]s).
The [[challenge]] is that each [[service provider]] is often listed under multiple [[service]] [[categories]] in a [[business]] [[directory]], making it infeasible to utilize standard [[supervised learning technique]]s.
The [[challenge]] of [[capturing shoppers' style]]s becomes more difficult as the [[size]] and [[diversity]] of the [[marketplace increase]]s.
The challenge of the proposed [[application]] is that [[user]]s with [[bias interact]] with one another frequently and bring [[noise]] into [[the data]], while the comments are too [[sparse]] to [[compensate]] for the [[noise]] .
The challenges of [[poor neighborhood]]s — including [[worse]] [[health outcome]]s, [[higher]] [[crime rate]]s, [[failing]] [[school]]s, and [[fewer]] [[job opportuniti]]es — make it that much harder for [[individual]]s and [[famili]]es to escape [[poverty]] and often perpetuate and [[entrench]] [[poverty]] across [[generation]]s.2
The chapters of [[this book]] fall into one of three categories: [[Fundamental chapter]]s: [[Data mining]] has four main problems, which correspond to [[clustering]], [[classification]], [[association pattern mining]], and [[outlier analysis]] .
The [[chunker]] converts a [[stream of words]] into a [[stream of chunks]], and the [[attacher]] converts the [[stream of chunks]] into a [[stream of sentence]]s.
The [[Citation Typing ontology (CiTo)]] [1] presents a [[typology]] of [[citation]]s according to the [[relation]] between the [[research paper]]s they express.
The claim of [[this work]] is that [[statistics]] from a [[large]] [[corpus of parsed sentence]]s combined with [[information-theoretic classification]] and [[training algorithm]]s can produce an accurate [[natural language parser]] without the aid of a complicated [[knowledge base]] or [[grammar]] .
The [[class]]es are defined in [[term]]s of [[fragment]]s sharing a common [[backbone]] .
The [[classical]] [[computational paradigm]], which assumes a [[fixed data set]] as an [[input]] to an [[algorithm]] that [[terminate]]s, is inadequate for such [[setting]]s.
The [[classification performance]] on [[MNIST digit]]s and eight [[benchmark dataset]]s demonstrates that [[SUGAR]] can effectively improve the [[performance]] by using the [[auxiliary network]]s, on both [[shallow]] and [[deep architecture]]s.
The [[classification structure]] (see Box 1 for examples of [[classification]]s commonly used) should not only meet the [[legal]] and [[administrative requirement]]s for [[budget management]] and [[financial reporting]], but should also conform to certain [[international standard]]s on [[financial]] and [[statistical reporting]] ([[discussed below]]).
The [[Clerk]]s publish the [[Journal]]s, [[calendar]]s, and other [[document]]s, and print copies of all [[bills filed]] .
The [[click-through rate (CTR)]] of a [[result]] is the [[product]] of the [[probability of examination]] (will the [[user]] look at the [[result]]) [[times]] the [[perceived relevance]] of the [[result]] ([[probability]] of a [[click]] given [[examination]]).
The [[Cloud]] builds a [[model]] incorporating [[day of the week]], [[time of day]], [[weather condition]]s, and individual [[driving strategi]]es (both of the [[taxi driver]]s and of the [[end user]] for whom the [[route]] is being computed).
The [[COA]], although appears to be just concerned with [[classifying]] and [[recording financial transaction]]s, is critical for effective [[budget management]], including [[tracking]] and [[reporting]] on [[budget execution]] .
; The [[COA]] is a critical element of the [[PFM framework]] for [[classifying]], [[recording]] and reporting information on [[financial plan]]s, [[transaction]]s and [[event]]s in a [[systematic]] and consistent way.
The [[COA]] is an [[organized]] and [[coded listing]] of all the individual [[account]]s that are used to [[record transaction]]s and make up the [[ledger system]] .
The [[co-association matrix based method]], which redefines the [[ensemble clustering problem]] as a [[classical]] [[graph partition problem]], is a [[landmark method]] in this [[area]] .
The [[co-author graph]] represents the [[relationship]] between [[author]]s.
The [[collaboration]] between two [[classifier]]s includes [[exchanging]] their own [[training instance]]s and their [[dynamically changing labeling decision]]s.
The [[collection]] of [[maximal frequent itemset]]s is a [[subset]] of the [[collection]] of [[closed frequent itemset]]s, which is a [[subset]] of the [[collection of all frequent itemset]]s.
The [[combination rule]] is based on [[aggregating]] [[weighted responses]], where a [[weight]] of an [[individual classifier]] is inversely based on their respective [[variance]]s around their [[responses]] .
The [[combinations of segment]]s and the [[numbering sequence]] of the [[coding structure]] are used to [[record data]] in respect to [[budget related]] and other [[financial transaction]]s and to generate [[budget execution report]]s, [[financial statement]]s and [[internal management reporting information]] .
The combined efforts of [[human volunteers]] have recently extracted numerous [[facts]] from [[Wikipedia]], storing them as [[machine-harvestable]] [[object-attribute-value triples]] in [[Wikipedia infoboxes]] .
[[The Committee]] focused its attention on the [[fund structure]], [[cost allocation]] and [[indirect cost]]s, and the [[numbering system]] and account [[coding scheme]] .
The [[communiti]]es of a [[social network]] are [[sets of vertice]]s with more [[connection]]s inside the [[set]] than [[outside]] .
The [[company]] [[internal management structure]] can be outlined with the [[organizational chart]] visually, which is normally [[confidential]] to the [[public]] out of the [[privacy]] and [[security]] [[concern]]s.
The [[comparison]] between [[our proposed approach]] and [[state-of-the-art]] [[document clustering approaches]] indicates that [[our approach]] is [[robust]] and [[effective]] for [[document clustering]] .
The [[compensation–productivity gap]] is partly [[accounted for]] by the [[difference]] between the two [[price index]]es used to remove the [[effect of]] [[inflation]] .
The [[competitive business climate]] and the [[complexity]] of [[IT environment]]s dictate efficient and [[cost-effective service delivery]] and support of [[IT service]]s.
The complexity of [[calculating gradients]] in [[MRFs]] is typically [[exponential]] to the size of [[maximal clique]]s.
The [[compression problem]] now consists of choosing [[supernode]]s, [[superedge]]s, and [[superedge weight]]s so that the [[approximation error]] is [[minimized]] while the [[amount of compression]] is [[maximized]] .
The [[computational problem]] with [[ERM]] is that it relies on [[computing]] the [[risk]] for all possible [[instance]]s.
The [[computation]] of the [[bound]]s uses [[characteristic quantities]] that can be obtained [[efficiently]] with a single [[scan]] of the [[sample]] .
The [[concept of "agency"]] in [[game]]s and other [[playable media]] (also referred to as "[[intention]]") has been discussed as a [[player experience]] and a [[structural property]] of [[work]]s.
[[The Concise Encyclopedia of Statistic]]s presents the [[essential information]] about [[statistical test]]s, [[concept]]s, and [[analytical method]]s in language that is accessible to [[practitioner]]s and students of the [[vast community]] using [[statistic]]s in [[medicine]], [[engineering]], [[physical science]], [[life science]], [[social science]], and [[business / economic]]s.
The [[confidence output]] of [[Culotta and McCallum’s]] [[model]] could then be used to provide the [[precision]] <math>p_m</math> for the [[urn]] .
The [[connectivity pattern]]s will provide useful [[imaging-based biomarker]]s to distinguish [[Normal Controls (NC)]], [[patient]]s with [[Mild Cognitive Impairment (MCI)]], and [[patient]]s with [[AD]] .
The consideration of [[existential uncertainty]] of [[item(sets)]], indicating the [[probability]] that an [[item(set)]] occurs in a [[transaction]], makes [[traditional techniques]] inapplicable.
The [[Consumer Price Index]] and the [[implicit price deflator]] comprise different [[baskets of goods and service]]s; if [[consumer price]]s rise more quickly than [[output price]]s, [[purchasing power]] falls and the [[compensation–productivity gap]] [[grows]] .
The [[Consumer Price Index]] [[measure]]s [[price]] [[changes]] in the [[basket of goods and service]]s [[purchased]] by [[families and workers]]; [[it]] is used to [[calculate]] [[real hourly compensation]] .
The [[contributions]] are the following : (a) we discover surprising [[pattern]]s with the [[clique]]s, (b) [[we]] report [[power-law]]s of the [[weight]]s on the [[edge]]s of [[clique]]s, (c) our [[real networks]] follow these [[pattern]]s such that we can [[trust]] them to [[spot outliers]] and finally, (d) [[we]] propose the first [[utility-driven]] [[graph generator]] for [[weighted]] [[time-evolving networks]], which match the [[observed pattern]]s.
The contributions from [[leading expert]]s are structured into three parts: [[large-scale]] and [[knowledge-driven schema matching]], [[quality-driven schema mapping]] and [[evolution]], and [[evaluation]] and [[tuning of matching task]]s.
The [[contribution]]s of [[our work]] are as follows: (a) [[ReFeX]] is [[scalable]] and (b) it is [[effective]], capturing [[regional(" behavioral") information]] in [[large graph]]s.
The contributions of [[the current paper]] are (1) more [[accurate model]]s based on a [[much larger]] [[data set]]; (2) a [[mechanism for adapting]] [[level design parameter]]s to given [[player]]s and [[playing style]]; (3) evaluation of this [[adaptation mechanism]] using both [[algorithmic]] and [[human player]]s.
The controversial book linking [[intelligence]] to [[class]] and [[race]] in [[modern society]], and what [[public policy]] can do to mitigate [[socioeconomic difference]]s in [[IQ]], [[birth rate]], [[crime]], [[fertility]], [[welfare]], and [[poverty]] .
The [[convexity]] of [[margin-based loss]] in [[LOGM]] ensures that there exists a [[unique]] [[maximum]] [[margin]] .
The [[coreset]] <i>[[C]]</i> can be computed in [[parallel]] and using only [[one pass]] over a possibly [[unbounded stream]] of [row vector]]s.
[[The corpus]]contains [[the paper abstract]]s from within the proceedings of [[ACM]]'s [[SIGKDD conference]]s for the years [[2009]] through [[2015]] .
[[The corpus]] is available as a [[collection]] of 225 [[.xml files]], where each file corresponds to a separate [[paper]] whose [[sentence]]s have been [[annotated]] individually with [[core]] [[scientific concepts]] .
The [[corpus]] is based on the [[abstracts]] for the [[papers]] accepted into the [[KDD-2009 conference]] .
The [[correct choice]] for [[model]] and [[data partition]]ing and [[overall system provision]]ing is highly dependent on the [[DNN]] and [[distributed system]] [[hardware]] [[characteristic]]s.
The [[correction]]s also help the [[machine]], which can update [[its model]] to produce [[higher-quality suggestion]]s in [[future session]]s.
The coverage combines breadth and depth, offering necessary [[background material]] on such [[topic]]s as [[probability]], [[optimization]], and [[linear algebra]] as well as discussion of recent [[development]]s in the [[field]], including [[conditional random fields]], [[L1 regularization]], and [[deep learning]] .
The creation of [[net wealth]] is thus [[positive]], because [[capital growth]] surpasses even the [[increase]] in [[debt]] .
The [[cumulative risk]] for the [[statistician]] is the [[average]] [[total loss]] up to time n.
The [[current]] [[anonymity-preserving solutions]] for [[on-line]] [[data collection]] are unable to adequately resist such [[attacks]] in a [[scalable fashion]] .
[[The current paper]] deals with the [[modelling of]] a [[linguistic ontology]] imported from a [[computational lexicon]] into [[OWL]] .
The [[current practice]] is to solve the [[optimization problem]] [[offline]] at a [[tractable level]] of [[impression granularity]] (e.g., the [[page level]]), and to serve [[ad]]s [[online]] based on the [[precomputed static delivery scheme]] .
The [[current system]] can handle [[airborne infectious disease]]s such as [[influenza]], [[pertussis]], and [[smallpox]] .
The [[current trend]] is to [[automatically discover entity synonym]]s using [[statistical technique]]s on [[web data]] .
The [[dark data extraction]] or [[knowledge base construction (KBC) problem]] is to [[populate a relational database with information]] from [[unstructured data source]]s, such as [[email]]s, [[webpage]]s, and [[PDF]]s.
The [[database]] contains [[FY89]], [[FY90]] and [[FY91 financial data]] for all [[citi]]es that were awarded the [[GFOA's Certificate of Achievement]] for [[Excellence in Financial Reporting]] in those [[fiscal year]]s.
The [[database]] is also [[selectively replicated]] so that the [[portion]] of the [[database]] needed for the [[computation]] of [[association]]s is [[local]] to each [[processor]] .
The [[dataflow]] [[CF]] [[implementation]] first [[compresses]] the [[large]], [[sparse]] [[training dataset]] into [[co-cluster]]s.
The [[dataflow library]] [[we]] use facilitates the [[development]] of sophisticated [[parallel program]]s designed to fully utilize [[commodity]] [[multicore hardware]], while hiding traditional difficulties such as [[queuing]], [[threading]], [[memory management]], and [[deadlock]]s.
[[The data]] has two [[normal region]]s, [[N1]] and [[N2]], since most [[observation]]s lie in these two [[region]]s.
The [[data]] in the figures in this [[survey paper]] is adapted from the [[Wine data set]] ([[Blake + Merz]], [[1998]]).
The [[data miner]]'s [[state of mind]] is [[model]]ed as a [[probability distribution]], called the [[background distribution]], which represents the [[uncertainty]] and [[misconception]]s the [[data miner]] has about the [[data]] .
The [[data mining]] [[initiatives]] within [[the division]] continue to strive for excellence around the following goals: [[actionable insight]]s through [[deep data analysis]], [[data mining]] and [[data modeling at scale]] and with [[speed]], increased [[productivity]] from [[deploy]]ed [[large scale data system]]s and [[tool]]s, improved [[product and service development]] and [[decision making]] gained from effective measurement and [[experimentation]], and a [[mature]] [[data culture]] in [[product team]]s that made the above possible.
The [[data-mining]] [[literature]] is rich in [[problem]]s asking to assess the importance of [[entiti]]es in a given [[dataset]] .
The [[data point]]s are [[represented by]] the [[vertice]]s of an [[undirected graph]] with the [[similarity]] between them captured by the [[edge weight]]s.
The [[dataset]] and the [[source code]] for the [[ensemble blending]] are available [[online]] .
[[The dataset]] consists of [[personal]] [[user stories]] [[annotated]] with multiple [[label]]s which, when [[aggregated]], form a [[multinomial distribution]] that [[captures emotional reaction]]s.
[[The dataset]] consists of very [[personal confession]]s [[anonymously]] made by [[people]] on the [[experience project website]] www.experienceproject.com.
[[The dataset]] [[contains valid]], [[non-corrupted replay]]s only and its [[quality]] and [[diversity]] was ensured by a [[number of heuristic]]s.
The [[dataset]] enables the [[evaluation]] of [[supervised approaches]] to [[semantic annotation]] of [[documents]] that contain a large number of [[high-level concepts]] relative the number of [[named entity mentions]] .
The [[data set]] is [[publicly available]] at: https://github.com/jorro / [[smelloffear]] .
The [[data]] typically used in [[data mining]] is in the format of a single [[table]], with [[primitive datatype]]s as [[attribute]]s.
The [[debt-income mismatch]] can also occurs when [[income]]s are able but [[debt]]s rise to [[unpayable level]]s.
The [[degree of dominance]] of a [[sense of a word]] is the [[proportion of occurrence]]s of that [[sense]] in [[text]] .
The degree of [[women]]'s [[underrepresentation]] varies by [[STEM field]]s.
The delivered [[solution]] involves [[automation]] and [[centralization of information]] about [[relationships]] between [[Microsoft]] [[product offerings]] .
The [[demand for items]] is a combined effect of [[form utility]] and [[time utility]], i.e., a [[product]] must both be [[intrinsically appealing]] to a [[consumer]] and the [[time must be right]] for [[purchase]] .
The [[density]] is [[computed]] from a [[call-data-record (CDR) dataset]], provided by the [[French Telecom operator Orange]], containing the [[CDR]] of roughly 2 million [[user]]s over one [[week]] .
The design goal is to equip the [[recommender system]] with the [[functionality]] which allows to [[automatically detect]] and [[evaluate]] the [[security risk]] of [[mobile App]]s.
The detailed [[experimental study]] [[demonstrates]] the [[effectiveness]] and [[efficiency]] of [[our approach]] .
The [[detection]] of [[abnormal moving object]]s over [[high-volume]] [[trajectory stream]]s is critical for [[real time application]]s ranging from [[military surveillance]] to [[transportation management]] .
The [[detection of repeated subsequence]]s, [[time series motifs]], is a [[problem]] which has been shown to have great [[utility]] for several [[higher-level data mining algorithms]], including [[classification]], [[clustering]], [[segmentation]], [[forecasting]], and [[rule discovery]] .
The development of [[FrameNet]], a large [[database]] of [[semantically annotated]] [[sentence]]s, has primed [[research]] into [[statistical methods]] for [[semantic tagging]] .
The [[diffusion of information]], [[rumor]]s, and [[disease]]s are assumed to be [[probabilistic process]]es over some [[network structure]] .
The discovered [[graph-rewriting rule]]s show how [[biological network]]s [[change over time]], and the [[transformation]] [[rules]] show the repeated [[patterns]] in the [[structural]] changes.
The [[discovered pattern]]s are used to build [[model]]s for [[true]] and [[fake item scan]]s by retaining or discarding the anchoring [[barcode]]s in those [[pattern]]s respectively.
The [[distinction]] among these [[component]]s is essential to provide an [[adequate explanation]] of such [[discourse phenomena]] as [[cue phrase]]s, [[referring expression]]s, and [[interruption]]s.
The [[distribution]] of the [[workload]] over a number of [[specialist]]s would decrease [[this problem]], but would also increase the [[risk]] of reducing the [[overall quality]] of the [[formalized knowledge]] .
The [[DMN]] can be [[trained end-to-end]] and obtains [[state of the art results]] on several types of [[task]]s and [[dataset]]s: [[question answering]] ([[Facebook's bAbI dataset]]), [[sequence modeling for part of speech tagging]] ([[WSJ-PTB]]), and [[text classification for sentiment analysis ([[Stanford Sentiment Treebank]]).
The [[dynamic]] [[marketplace in online advertising]] calls for [[ranking system]]s that are optimized to consistently [[promote]] and [[capitalize]] [[better performing ads]] .
The [[Dynamic Multi-Armed Bandit method]] is found to [[outperform]] the other [[method]]s on a [[scenario]] from the [[literature]], while on another [[scenario]], the basic [[Multi-Armed Bandit]] performs best.
The [[early]] [[DBMS]]s are among the most [[influential software system]]s in [[computer science]], and the [[idea]]s and implementation issues pioneered for [[DBMS]]s are widely copied and [[reinvented]] .
The [[Earth Observing System Data and Information System (EOSDIS)]] is a [[comprehensive]] [[data]] and [[information system]] which [[archives]], [[manages]], and [[distributes]] [[Earth science data]] from the [[EOS]] [[spacecrafts]] .
The [[economically turbulent]] [[2000s]] have redrawn [[America]]’s [[geography of poverty]] in more ways than one.
; The [[economic impact]] of [[robotic advance]]s and [[AI]] — [[Self-driving car]]s, [[intelligent digital agent]]s that can [[act for you]], and [[robot]]s are [[advancing]] rapidly.
The [[effect]] is also [[economically significant]] as it corresponds to an increase of several thousand additional new [[follower]]s [[per day]] for an [[average size]] [[brand]] .
[[The effect]] is [[stronger]] for [[user]]s whose motivation is in [[searching]] for a specific [[target]] .
The [[effective analysis]] of [[social network]]s and [[graph-structured data]] is often limited by the [[privacy concerns of individual]]s whose [[data]] make up [[these network]]s.
The [[effectiveness]] and strength of [[our algorithm]]s are validated by [[experiment]]s on a [[very large volume]] of [[real taxi trajectori]]es in an [[urban road network]] .
The [[effectiveness]] of existing <i>[[top -N recommendation method]]s</i> decreases as the [[sparsity]] of the [[datasets increase]]s.
The [[effectiveness]] of the [[measure]] using these [[approximations]], in [[comparison]] to several other [[existing]] [[feature selection methods for SVR]], is [[evaluated]] on both [[artificial]] and [[real-world problems]] .
The [[efficiency]] of [[Gibbs sampling-based]] [[inference methods]] depends almost entirely on how fast we can evaluate the [[sampling distribution]] over [[topics]] for a given [[token]] .
The [[Electronic Road Pricing (ERP) system]] was implemented by the [[Land Transport Authority]] of [[Singapore]] to [[control traffic]] by [[road pricing]] since [[1998]] .
The [[element]]s of the [[suite]] are: (i) the [[data engine]], a [[hardware design pattern]] that balances [[storage]], [[CPU]] and [[GPU acceleration]] for typical [[data mining]] [[workload]]s, (ii) [[BIDMat]], an [[interactive matrix library]] that integrates [[CPU]] and [[GPU acceleration]] and novel [[computational kernel]]s (iii), [[BIDMach]], a [[machine learning system]] that includes very [[efficient model optimizer]]s, (iv) [[Butterfly mixing]], a [[communication strategy]] that hides the [[latency of frequent model update]]s needed by fast [[optimizer]]s and (v) [[Design pattern]]s to improve [[performance]] of [[iterative update algorithm]]s.
The [[elevator pitch]] behind this [[performance difference]] is straightforward: [[column-store]]s are more [[I/O efficient]] for [[read-only queri]]es since they only have to [[read from disk]] (or [[from memory]]) those attributes accessed by a [[query]] .
The [[embedding]] serves as a compact [[cluster model]] allowing to [[reconstruct]] the original [[heterogenous attribute]]s with [[high accuracy]] .
The emerging [[domain]] of [[Semantic Publishing]] aims at making [[scientific knowledge]] accessible to both [[human]]s and [[machine]]s, by adding [[semantic annotation]]s to [[content]], such as a [[publication’s contribution]]s, [[method]]s, or [[application domain]]s.
The [[empirical evaluation]] reveals that [[this approach]] of [[scoring]] [[phrase pair]]s with an [[RNN Encoder–Decoder]] improves the [[translation performance]] .
The [[empirical experiment]]s on [[synthetic]] and [[real world data set]]s demonstrate that [[our approach]] is [[efficient]] and [[scalable]] to very large [[high-dimensional data set]]s.
The [[empirical result]]s demonstrate that [[our approach]] is [[effective]] and [[robust]] in [[discovering latent group]]s and [[detecting]] [[group]] [[anomali]]es.
[[The empirical result]]s show that the [[proposed new method]] can effectively integrate different [[visual descriptor]]s, and consistently [[outperform]]s related [[method]]s using the [[concatenated visual descriptor]]s.
The [[encoder]] and decoder of [[the proposed model]] are [[jointly trained]] to [[maximize the conditional probability]] of a [[target sequence]] given a [[source sequence]] .
The [[end result]] is an [[off-the-shelf]] [[encoder]] that can produce [[highly generic]] [[sentence representation]]s that are [[robust]] and [[perform well in practice]] .
The [[entity synonym]]s produced by [[our system]] is in [[production]] in [[Bing Shopping]] and [[Video search]], with experiments showing the [[significance]] it brings in [[improving search experience]] .
::: "the [[entropy]] of a [[perfect crystal]] of any [[pure substance]] approaches zero as the [[temperature]] approaches [[absolute zero]] ."
The [[estimation]] of [[semantic similarity between word]]s is an important [[task]] in many [[language related application]]s.
The [[estimation technique]]s produce [[statistically sound estimate]]s of [[size]]s relying solely on [[passively mining aggregated application log data]], without [[probing machine]]s or [[deploying]] [[active content]] like [[Java applet]]s.
The [[evaluation]] of [[classifier]]s in [[data stream]]s is fundamental so that [[poorly-performing model]]s can be [[identified]], and either improved or replaced by [[better-performing model]]s.
The [[evaluation]] of the proposed [[method]] is presented based on [[processing]] the [[online]] [[product reviews]] from [[Amazon]] and other publicly available [[datasets]] .
The [[evaluation result]]s on [[synthetic]] and [[real data]] show that [[our framework]] [[outperform]]s two [[baseline method]]s in terms of both [[detection accuracy]] and [[computational cost]]s.
The [[evaluation]] shows that [[the proposed weighting]] can significantly improve [[the results]] of [[community detection method]]s on [[network]]s with [[assortative community structure]] .
The [[evaluation]]s on two [[real world dataset]]s demonstrate that [[our algorithm]] achieves [[state-of-the-art]] [[performance]] in both [[effectiveness]] and [[efficiency]] .
The [[evolution]] of [[hotspot]]s is highly affected by [[external stimulation]], the [[social network structure]], and individual [[user's activiti]]es.
The [[evolving]] [[community tree]] enables a smooth [[transition]] between the [[two]] [[community trees]] and well [[represents]] the [[evolution]] of [[organizational structure]] in the [[dynamic]] [[social network]] .
The exceptional simplicity of [[physics-based function]]s hinges on [[properti]]es such as symmetry, [[locality]], [[compositionality]] and [[polynomial log-probability]], and we explore how these [[properties translate]] into [[exceptionally simple neural network]]s [[approximating]] both natural phenomena such as images and [[abstract representations thereof]] such as [[drawing]]s.
The existing [[Grafting method]] can avoid doing [[inference]] on [[dense graphs]] in [[structure learning]] by [[incrementally selecting]] new [[features]] .
The existing [[method]]s tackle [[this problem]] by [[estimating pairwise similarity]] between [[user]]s in two [[network]]s.
[[The experimental evaluation]]s on [[real graph]]s demonstrate its [[effectiveness]] and [[efficiency]] .
[[The experimental result]]s also show that the [[expanded symptom]]s can serve as useful [[feature]]s for improving [[AUC]] [[measure]] for [[disease diagnosis prediction]], thus confirming the potential [[clinical value]] of [[our work]] .
[[The experimental result]]s also show that the [[relative performance]] gains compared to [[competing method]]s increase as the [[data]] gets [[sparser]] .
[[The experimental result]]s and [[validation]]s demonstrate the [[efficiency]] of [[our behavior model]], and suggest that [[human behavior]] and their [[movement]]s during [[disaster]]s may be significantly more [[predictable]] than previously thought.
[[The experimental result]]s clearly demonstrate that the [[RaHH]] [[outperform]]s several [[state-of-the-art]] [[hashing method]]s with significant [[performance gain]]s.
[[The experimental result]]s clearly [[validate]] the [[effectiveness]] of [[our approach]] .
[[The experimental result]]s confirm the [[effectiveness]] of [[our strategy]] for both types of [[pattern matching algorithm]]s.
[[The experimental result]]s demonstrate [[our algorithm]]'s [[effectiveness]] in the [[networked bandit setting]] .
[[The experimental result]]s demonstrate the enhanced [[performance]] of the [[proposed method]] for [[real estate appraisal]] .
[[The experimental result]]s have demonstrated the [[efficiency]] and [[effectiveness]] of [[the proposed algorithm]] .
[[The experimental result]]s have verified the [[feasibility]] and [[effectiveness]] of [[our system]] .
[[The experimental result]]s on a [[real dataset]] from a hospital [[corroborate]] the [[effectiveness]] of [[the proposed method]] .
[[The experimental result]]s on [[click prediction model]] for [[search advertising]] are highly [[promising]] .
[[The experimental result]]s on [[synthetic]] and [[benchmark]] [[time series classification task]]s confirm the [[efficiency]] of [[the proposed kernel]] in terms of both [[generalization accuracy]] and [[computational speed]] .
[[The experimental result]]s on various [[data set]]s show the [[effectiveness]] of [[the proposed approach]] .
[[The experimental result]]s show that our [[adaptive item sampler]] indeed can [[speed up]] [[our model]], and [[our model]] outperforms [[advanced method]]s in [[personalized ranking]] .
[[The experimental result]]s show that [[our approach]]es significantly [[outperform]] the [[baseline]]s.
[[The experimental result]]s show that the adaptively generated [[feature space]] can outperform a [[static]] one significantly in [[text mining task]]s, and [[WorkiNet]] dominates [[WordNet]] most of the [[time]] due to its [[high coverage]] .
[[The experimental result]]s show that the proposed [[OSLOR]] significantly and consistently [[outperform]] [[topological measure based method]] and other [[data driven method]]s in [[prediction performance]]s.
[[The experimental result]]s show that the proposed [[technique]]s clearly [[outperform]] the corresponding [[baseline method]]s.
[[The experimental result]]s show that [[TLC]] significantly improves the [[accuracy]] over several [[state-of-the-art]] [[non-transfer-learning approach]]es under very limited [[budget]] in various [[labeling task]]s.
The [[experimental results]] using the proposed [[clustering framework]] on three [[datasets]] ([[20-newsgroup]], [[TDT2]], and [[LA Times]]) show that [[clustering]] [[performance]] improves significantly by enriching [[document representation]] with [[Wikipedia concepts]] and [[categories]] .
[[The experimental result]]s validate that [[our model]] significantly [[outperform]]s [[state-of-the-art approach]]es for [[mobility prediction]] in terms of multiple [[metric]]s such as [[accuracy]] and [[percentile rank]] .
[[The experiment]] results of a [[system]] developed with [[our approach]] show that it can accurately forecast the [[bid distribution]]s for various [[campaign]]s [[running]] on the world's largest [[NGD advertising exchange system]], outperforming two [[baseline method]]s in term of [[forecasting error]]s.
[[The experiment]]s further show good [[accuracy]] in classifying the condition of an [[input image]], suggesting that the [[component]]s from [[the proposed decomposition]] indeed capture physically meaningful [[feature]]s of the [[input]] .
[[The experiment]]s indicate that the proposed [[adaptive sampler]] improves the [[state-of-the art learning algorithm]] [[largely in convergence]] without [[negative effect]]s on [[prediction quality]] or [[iteration runtime]] .
[[The experiment]]s on a broad range of [[dataset]]s validate the [[effectiveness]] of [[the proposed approach]] against other [[well-established method]]s, in terms of [[predictive accuracy]], [[pattern number]]s and [[running time]] .
[[The experiment]]s on several [[large data set]]s show that [[AMM]] is nearly as fast during [[training]] and [[prediction]] as the [[state-of-the-art]] [[linear SVM solver]] and that it can be [[orders of magnitude]] faster than [[kernel SVM]] .
[[The experiment]]s on three [[real world application]]s and a [[set]] of [[synthetic data]] demonstrate the advantages of [[the proposed method]] over [[state-of-the-art]] [[truth discovery method]]s.
The [[experiment]]s show that [[SLIM]] achieves significant [[improvement]]s both in [[run time performance]] and [[recommendation quality]] over the best existing [[method]]s.
The [[explanatory]] and [[predictive power]] of [[the proposed model]] has been demonstrated by the [[experiment]]s on [[large real data set]]s.
The [[exploration]] vs. [[exploitation tradeoff strategy]] is a well-known [[technique]] for [[online learning]] with [[incomplete feedback]] (i.e., [[bandit setup]]).
The explosion of [[user-generated content]] on [[the Web]] has led to new opportunities and significant [[challenges]] for [[companies]], that are increasingly concerned about [[monitoring]] the [[discussion]] around their [[product]]s.
The [[explosive growth]] in [[sharing]] and [[consumption]] of the [[video content]] on the web creates a unique opportunity for [[scientific advance]]s in [[video retrieval]], [[recommendation]] and [[discovery]] .
The explosive [[growth]] of [[heterogeneous]] [[web objects]], especially [[non-textual objects]] such as [[products]], [[pictures]], and [[videos]], has made the [[problem of web classification]] increasingly challenging.
The extensive [[experiments]] on [[real]] [[uncertain graph data]] verify that [[the algorithm]] is [[efficient]] and that the [[mining results]] have very [[high quality]] .
: the [[extracted]] local [[model]]s, [[T-pattern]]s, are [[combined]] in a [[prefix]] [[tree]] called [[T-pattern Tree]] .
The fact that [[core decomposition]] can be [[computed efficiently]] in [[deterministic graph]]s does not guarantee [[efficiency]] in [[uncertain graph]]s, where even the simplest [[graph operation]]s may become [[computationally intensive]] .
The [[fastest algorithm]]s turn out to be [[conjugate gradient ascent]] and [[quasi-Newton algorithm]]s, which far outstrip [[Iterative Scaling]] and [[its variant]]s.
The [[feature]]s extracted from [[EP]] are summarized from all [[check-in]]s at a specific [[place]] .
The [[feature]]s from [[IR]] are derived by building a novel [[network of related places (NRP)]] where [[similar place]]s are [[link]]ed by [[virtual edge]]s.
The [[feature]]s we [[mine]] are based on two general signals: [[geographic]], where features are formulated according to the [[type]]s and [[density]] of [[nearby place]]s, and [[user mobility]], which includes [[transition]]s between [[venue]]s or the [[incoming flow]] of [[mobile user]]s from [[distant area]]s.
The [[Financing Lead Triggers system]] is comprised of three [[core component]]s that perform [[information fusion]], [[knowledge discovery]] and [[information visualization]] .
The firehose of [[data]] [[generated by]] [[user]]s on [[social networking]] and [[microblogging site]]s such as [[Facebook]] and [[Twitter]] is enormous.
The first 50M [[click]]s [[tag]]ged by the [[filter]] had a significant [[recall]] of all [[tag]]ged [[click]]s, and their [[false positive rate]] was below 1.4%.
The first [[algorithm]][[pass]]es [[data block]]s from each [[machine]] around the [[ring]], [[incrementally updating]] the [[nearest neighbor]]s of the [[points passed]] .
The first [[algorithm]] uses [[mixed integer programming]] to optimize a [[weighted balance]] between [[positive]] and [[negative class accuraci]]es.
The first [[BioCreAtIvE assessment]] achieved a high level of [[international participation]] (27 [[group]]s from 10 [[countri]]es).
The first [[challenge]] is the [[measure]] - how to [[quantify]] the [[goodness]] of a given [[top-k ranking list]] that [[capture]]s both the [[relevance]] and the [[diversity]]?
The first [[combiner]] [[induces a similarity measure]] from the [[partitionings]] and then [[reclusters the objects]] .
The first dealt with [[extraction]] of [[gene]] or [[protein name]]s from [[text]], and their mapping into [[standardized]] [[gene identifier]]s for three [[model organism database]]s ([[fly]], [[mouse]], [[yeast]]).
... The first flaw, then, is the failure of [[lexicographer]]s to bring the [[etymology]] consistently back to the [[form]] which actually underlies the [[word]] rather than to some [[canonical reference form]], such as the [[infinitive]] of [[verb]]s or the [[nominative]] and [[genitive singular]] of [[noun]]s or [[adjective]]s.
[[The first]] is a [[recurring pattern]] by which [[sexually reproducing animal]]s behave [[altruistically]] toward one [[another]] in proportion to the number of [[gene]]s they [[share]]; that is, they practice [[nepotism]] and favor [[genetic relative]]s.
The first is the wide-scale use by [[scientists]] of digital indexing, retrieval, and navigation resources (such as [[PubMed]], [[Web of Science]], the [[ACM Digital Library]], [[NASA]]’s [[Astrophysics Data System]], [[CiteSeer]], [[Scopus]], and [[Google Scholar]]) to exploit large quantities of relevant [[information]] without reading individual [[articles]] .
The first [[method]] employs a [[convex function]] to penalize the [[pairwise]] <math>l_1</math> [[norm]] of [[connected regression]] / [[classification coefficient]]s, achieving simultaneous [[feature grouping]] and [[selection]] .
The [[first method]] is based on the [[posterior distribution]] of <math>\alpha</math>, and the second exploits the [[property]] of [[online learning]], namely [[adapting to change]], to [[adjust]] <math>\alpha</math> [[dynamically]] .
The first named entity set had 7 categories: [[names of persons]], [[organizations]], [[locations]], [[date]], [[time]], [[money]] and [[percent expressions]] [10].
The first one [[capture]]s the [[compactness of an event]] using the [[sum of distance]]s among all [[pair]]s of the [[event node]]s.
The [[first phase]] generates an [[implicit]] [[social network]] under the form of a [[probabilistic graph]] .
[[The first Spanner paper]] published at [[OSDI'12]] focused on [[the system]]s aspects such as [[scalability]], [[automatic sharding]], [[fault tolerance]], [[consistent replication]], [[external consistency]], and [[wide-area distribution]] .
The [[first stage]] creates a [[graph]] from [[pairs of entities]] that are likely to be related, and the [[second stage]] scores [[maximal clique]]s in that [[graph]] as potential [[complex relation instance]]s.
The [[FLA project]] will program [[tiny]] [[rotorcraft]] to [[manoeuvre unaided]] at [[high speed]] in [[urban area]]s and [[inside buildings]] .
The following [[paragraph]]s explain the process for identifying [[fund]]s, [[asset]]s, [[liabiliti]]es and [[expenditure]]s or [[expense]]s.
The force behind [[the interface]] is the combination of [[adaptive navigation functionality]] with the [[mastery-oriented aspect]]s of [[OLM]] and the [[performance-oriented aspect]]s of [[social comparison]] .
The formulation and development of [[MDP]]s started in the [[1950s]] with [[Shapley]], [[Bellman]], and, later, [[Howard]], [[Dubins]], [[Savage]], and [[Blackwell]] .
The [[Forward-Backward algorithm]] can be viewed as a [[generalization]] of the [[Viterbi algorithm]]: instead of choosing the [[optimal state sequence]], [[Forward-Backward]] evaluates [[all possible state sequence]]s given the [[observation sequence]] .
The [[Foundational Model of Anatomy (FMA)]], initially developed as an enhancement of the [[anatomical]] [[content]] of [[UMLS]], is a [[domain ontology]] of the [[concept]]s and [[relationship]]s that pertain to the [[structural organization]] of the [[human body]] .
[[The framework]] can be used to help in designing new [[data mining algorithm]]s that [[maximize]] the [[efficiency]] of the [[information exchange]] from [[the algorithm]] to the [[data miner]] .
[[The framework]] is a [[BSD-licensed]] [[C++ library]] with [[Python]] and [[MATLAB binding]]s for [[training]] and [[deploying]] [[general-purpose]] [[convolutional neural network]]s and other [[deep model]]s efficiently on [[commodity architecture]]s.
[[The framework]] requires only limited [[training]] but the [[quality of phrase]]s so [[generated is close]] to [[human judgment]] .
The fundamental [[idea]] is to envision the [[target network]] as an [[adaptive dynamical system]], where each [[node interact]]s with its [[neighbor]]s.
The further [[analysis]] of the [[model reveal]]s that the [[RNN Encoder–Decoder]] learns a [[continuous space representation of a phrase]] that preserves both the [[semantic]] and [[syntactic structure]] of the [[phrase]] .
The [[fused Lasso penalty]] enforces [[sparsity]] in both the [[coefficients]] and their [[successive difference]]s, which is desirable for [[applications]] with [[features ordered]] in some meaningful way.
[[The game]] can be modeled with [[utterance]]s as [[action]]s in [[Situation Calculus]] ([[McCarthy 1983]]), based on [[Austin’s theory of performatives]] ([[Austin 1975]]).
[[The game]] proceeds in alternating [[day]] and [[night]] [[stage]]s, overlooked by an [[impartial judge]] .
The [[game state data]] was recorded every 3 [[frame]]s which ensures suitability for a wide variety of [[machine learning task]]s such as [[strategy classification]], [[inverse reinforcement learning]], [[imitation learning]], [[forward modeling]], [[partial information extraction]], and others.
The [[gap]] between [[real hourly compensation]] and [[labor productivity]] will be referred to in [[this essay]] as the [[compensation–productivity gap]] .
The [[gauntlet]] is now on [[search engine]]s to [[test]] whether [[our finding]]s hold in their [[infrastructure]] for different [[social network]]s and whether enabling [[diversity]] has sufficient [[business imperative]] for them.
The [[gene mention]]s identified by these [[system]]s were filtered using a [[stop list]] of [[term]]s like [[antibody]], [[Ab]], [[antigen]], [[IgG]], etc. [[Shorthand Term]] [[gene name]]s were expanded to constituent terms as in [[Xnr1]], [[Xnr2…Xnr6]] for [[Xnr1-6]] .
The [[generalization error]] for [[forests]] [[converges]] as to a limit as the number of [[tree]]s in the [[forest]] becomes large.
The [[generalization error]], or [[probability of misclassification]], of [[ensemble classifiers]] has been shown to be [[bounded]] above by a [[function]] of the [[mean correlation]] between the [[constituent (i.e., base) classifiers]] and their [[average strength]] .
The [[generalization performance]] of [[LMEM]]s including [[data-driven random effects structure]]s strongly depends upon [[modeling criteria]] and [[sample size]], yielding reasonable results on [[moderately-sized sample]]s when [[conservative criteria]] are used, but with [[little]] or no power advantage over [[maximal model]]s.
The [[generalized task]] extends the [[optimization]] to preserve [[longer-range connectiviti]]es between [[node]]s, not just individual [[edge weight]]s.
The [[generative process]] begins by selecting the [[set of classes]] (instead of a [[single class]]) that will be the [[label]]s for this [[document]]; then producing a [[set of]] [[mixture weight]]s for those [[classes]]; finally, each [[word]] in the [[document]] is [[generated by]] first selecting a class according to these [[mixture weight]]s, then letting that [[class]] generate a [[single word]] .
The goal in [[correlation clustering]] is, given a [[graph]] with [[signed edge]]s, [[partition]] the [[node]]s into [[cluster]]s to [[minimize]] the number of [[disagreement]]s.
The goal in such [[unsupervised learning problem]]s may be to [[discover]] [[groups of similar examples]] within the [[data]], where it is called [[clustering]], or to determine the [[distribution of data]] within the [[input space]], known as [[density estimation]], or to [[project the data]] from a [[high-dimensional space]] down to [[two]] or [[three]] [[dimension]]s for the purpose of [[visualization]] .
The [[goal in topic discovery]] is to [[identify]] [[groups of keywords]] from [[large corpora]] so that the [[information]] in those [[corpora]] are [[summarized succinctly]] .
[[The goal]] is to [[annotate each document]] with [[concept]]s being too specific to be [[explicitly mentioned]] in [[text]]s.
The goal is to [[identify individual]]s who when [[influenced]] to leave [[gang]]s will [[propagate]] this [[action]] .
The goal is to [[predict]] the [[missing link]]s in the [[target]] [[network]] .
The [[goal]] of [[category detection]] is to bring to the [[user's attention]] a [[representative]] [[data point]] from each [[category]] in the [[data]] in as few [[queries]] as possible.
The goal of <i>[[energy-based learning]]</i> is to [[train the parameters]] of the [[energy function]] to score [[observed]] [[positive]] [[input-output pairs]] higher (or lower, depending on sign conventions) than [[negative]] [[pair]]s.
The [[goal]] of [[Information Extraction]] is to [[automatically]] [[generate]] [[structured pieces of information]] from the relevant [[information]] contained in [[text documents]] .
The goal of [[information extraction]] is to extract [[database records]] from [[text]] or [[semi-structured source]]s.
The goal of [[the algorithm]] is to [[embed]] the [[semantic network]] in a [[geometric space]]: that is, to associate each [[sense]] <math>s_{ij}</math> with a [[sense embedding]], a [[vector]] <math>E(s_{ij})</math> of [[real number]]s, in a way that reflects the [[topology]] of the [[semantic network]] but also that the [[vector]]s representing the [[lemma]]s are related to those corresponding to the [[sense]]s.
The goal of the [[gene normalization task]] is to [[link]] [[genes]] or [[gene products]] mentioned in the [[literature]] to [[biological databases]] .
The goal of these [[system]]s is not just to [[maximize]] the [[performance]] of the [[classifier]] but to make the [[expert]]s more [[efficient]] at [[performing]] their [[task]], thus [[maximizing]] the overall [[Return on Investment]] of [[the system]] .
The [[goal]] of [[this mobile recommendation system]] is to [[maximize the probability]] of [[business success]] .
The goal of [[this paper]] is to efficiently compute [[single node relevance]] and [[top-k]] / [[highly relevant node]]s without iteratively computing the [[relevance]]s of all [[node]]s.
The goal of [[this program]] is to develop [[advanced technologies]] to [[automatic]]ally [[detect]], [[diagnose]], [[predict]], and [[mitigate]] [[adverse event]]s during the flight of an [[aircraft]] .
The goal of [[Wikification]] is [[resolving ambiguiti]]es and [[variabiliti]]es of every [[mention]] in natural language by [[linking the expression]] to its [[relevant Wikipedia concept]] .
The goal, says [[Stribling]], now a [[software engineer]] in [[Palo Alto]], [[California]], was "to [[expose]] the [[lack]] of [[peer review]] at [[low-quality conference]]s that essentially scam [[researcher]]s with [[publication]] and [[conference fee]]s ."
The [[GPS technology]] and new forms of urban [[geography]] have changed the [[paradigm]] for [[mobile service]]s.
The [[graph]] is first [[successively]] [[coarsened]] to a [[manageable]] [[size]], and a small number of [[iterations]] of [[flow]] [[simulation]] is performed on the [[coarse]] [[graph]] .
The [[graph]] is then [[successively]] [[refined]], with [[flow]]s from the previous [[graph]] used as [[initializations]] for brief [[flow]] [[simulation]]s on each of the intermediate [[graph]]s.
The [[gray list]], containing unknown [[software program]]s which could be either [[normal]] or [[malicious]], is usually [[authenticated]] or [[rejected]] [[manually]] by [[virus]] [[analysts]] .
The [[GRF framework]] is applied to [[searching]] [[DBpedia]] with [[graph queri]]es derived from [[YAGO]] and [[Wikipedia]] .
[[The Handbook of Statistical Analysis and Data Mining Applications]] is a comprehensive professional [[reference book]] that guides [[business analyst]]s, [[scientist]]s, [[engineer]]s and [[researcher]]s (both [[academic]] and [[industrial]]) through all stages of [[data analysis]], [[model building]] and implementation.
The [[hazard based approach]] offers several benefits including the ability to [[work]] with [[censored data]], to [[model]] the [[dynamics]] in [[user return rate]]s, and to easily incorporate different types of [[covariates]] in [[the model]] .
The heart of [[our solution]] is to compute [[single node relevance]] accurately in [[non-iterative manner]] based on [[sparse matrix [[representation]], and to compute [[top-k]] / [[highly relevant node]]s exactly by pruning unnecessary [[relevance computation]]s based on [[upper]] / [[lower relevance estimation]]s.
The heart of [[our system]] is a [[data-driven component]] that learns the [[matching function]] [[off-line]], which is then applied at [[run-time]] for [[matching]] [[offer]]s to [[product]]s.
The [[heat kernel]] is a [[type]] of [[graph diffusion]] that, like the [[much-used]] [[personalized PageRank diffusion]], is useful in [[identifying]] a [[community]] nearby a [[starting seed node]] .
The [[heuristic]]s are [[scalable]] in the [[size of the graph]]s and the number of [[partition]]s.
The [[high accuracy]], [[efficiency]], and [[parallelism]] of [[our best implementation]] allows the [[fast generation]] of [[eccentricity estimate]]s for [[large graph]]s, which are useful in many [[application]]s arising in [[large-scale network analysis]] .
The high [[efficiency]] of [[our algorithm]] is attributed to two [[factors]]: (i) the [[transformation]] of the original [[data]] into a [[bipartite graph]] [[database]], and (ii) the [[mining]] of [[transpose closures]] from a [[wide]] [[transactional database]] .
The [[history]] of all hitherto existing [[society]] is the [[history]] of [[class struggle]]s.
The [[host]] sells [[viral marketing campaign]]s as a [[service]] to its [[customer]]s, keeping [[control of the selection of seed]]s.
The [[House]] and [[Senate]] [[Committees on Ways and Means]] employ budget staff, including [[analyst]]s.
The [[hub]] and [[authority scores]] are the [[steady-state distributions]] of the respective [[random processes]] .
The [[huge amount]] of [[data]] produced by the [[sensor]]s was not analyzable using the traditional [[method]]s employed by [[behavioral neuroscience]] [[researcher]]s.
The idea extends [[author]]s’ [[previous work]] on relating the [[ontology]] to the [[text term]]s in two [[domain]]s – IT and textile.
The [[idea]] is to [[artificially inject]] a [[significant change]] on the [[target entity]], and [[estimate]] its [[direct]] and [[indirect influence]] on the others, by following an [[inference rule]] under the [[Markovian assumption]] .
The [[idea]] is to [[discover]] hidden [[variables]] and [[learn]] their [[dynamics]], making [[our algorithm]] able to [[function]] even when there are [[missing values]] .
The [[idea]] is to [[maximize]] the [[dependency]] between [[subgraph feature]]s and [[graph label]]s using an [[active learning framework]] .
The idea of a [[Guaranteed Annual Income (GAI)]] is once again receiving attention from [[policy maker]]s and [[decision maker]]s at [[local]], [[provincial]], and [[national level]]s.
The idea of [[ensemble methodology]] is to [[build a predictive model]] by [[integrating]] multiple [[model]]s.
The ideas are illustrated with [[example]]s from [[bank loan]]s, [[fraud]], [[face recognition]], and [[health screening]] .
The idea was introduced by [[Brown et al. (1992)]] and used in different applications, including [[speech recognition]], [[named entity tagging]], [[machine translation]], [[query expansion]], [[text categorization]], and [[word sense disambiguation]] .
The <i>[[Drosophila]]</i> [[gene expression pattern]] [[image]]s [[document]] the [[spatial and temporal dynamics]] of [[gene expression]] and they are valuable tools for explicating the [[gene function]]s, [[interaction]], and [[networks]] during [[Drosophila]] [[embryogenesis]] .
The [[ImageNet Large Scale Visual Recognition Challenge]] is a [[benchmark]] in [[object category classification]] and [[detection]] on hundreds of [[object categori]]es and millions of [[image]]s.
The impact of the [[Asilomar meeting]], and important [[advances in AI]] that included [[AI algorithm]]s and [[technologi]]es starting to enter [[daily life]] around [[the globe]], spurred the idea of a [[long-term]] [[recurring study]] of [[AI]] and its influence on [[people]] and [[society]] .
The [[implementation]]s include (1) a [[simple algorithm]] based on [[executing two-pass breadth-first search]]es from a [[sample of vertice]]s, (2) [[algorithm]]s with [[sub-quadratic worst-case running time]] for [[sparse graph]]s and [[non-trivial approximation guarantee]]s that execute [[breadth-first search]]es from a carefully chosen [[set of vertice]]s, (3) [[algorithm]]s based on [[probabilistic counter]]s, and (4) a [[well-known 2-approximation algorithm]] that executes one [[breadth-first search]] [[per]] [[connected component]] .
The [[implicit price deflator]], used to remove the effect of [[inflation]] on [[output]], [[measure]]s [[price]] [[changes]] in the [[goods and service]]s [[produced]] in the [[nonfarm business sector]] .
The importance of [[contextual information]] has been recognized by [[researcher]]s and [[practitioner]]s in many [[discipline]]s, including [[e-commerce personalization]], [[information retrieval]], ubiquitous and [[mobile computing]], [[data mining]], [[marketing]], and [[management]] .
The importance of [[event logs]], as a [[source]] of [[information]] in [[system]]s and [[network management]] cannot be overemphasized.
The improvement of [[Crisis Management]] and [[Disaster Recovery technique]]s are [[national prioriti]]es in the wake of [[man-made]] and [[nature inflicted calamiti]]es of the last [[decade]] .
The increasing [[availability]] of [[electronic communication data]], such as that arising from [[e-mail exchange]], presents [[social]] and [[information scientists]] with new [[possibilities]] for [[characterizing]] [[individual behavior]] and, by extension, identifying [[latent structure]] in [[human populations]] .
The increasing availability of [[large-scale]] [[location trace]]s creates unprecedent [[opportunities]] to change the paradigm for [[knowledge discovery]] in [[transportation system]]s.
The increasing [[deployment]] of [[distributed system]]s to solve [[large data]] and [[computational problem]]s has not seen a [[concomitant increase]] in [[tools and techniques]] to [[test these system]]s.
The [[indexing scheme]] is based on [[sorting the data point]]s in order of increasing [[distance]] from a [[fixed]] [[reference point]] and then accessing those [[point]]s based on this [[sorted order]] .
The [[information overload problem]] remains serious for both [[consumer]]s and [[service]] / [[content provider]]s, leading to [[heightened demand]]s for [[personalized recommendation]]s.
The [[information overload theory]] implies that [[user satisfaction]] increases when the [[recommended content]] fits [[user interest]]s (i.e., the [[recommendation accuracy]] increases).
The [[infrastructure]] allows one to compose [[parallel ML-DM algorithm]]s using [[reusable]] [[(serial and parallel) building block]]s that can be [[efficiently executed]] using [[MR]] and other [[parallel programming model]]s; it currently runs on top of [[Hadoop]], which is an [[open-source]] [[MR implementation]] .
The [[INGV network]] detects [[shaking level]]s produced by the [[earthquake]], but can only [[model]] the [[damage scenario]] by using [[empirical relationship]]s.
The [[instance-based]] [[evaluation]] assigns a [[semantic label]] to each instance of a [[head noun]] .
The [[institutional “goal”]] of [[science]] is [[publishing]] [[result]]s.
The [[instrument]] is the [[grant agreement]], [[cooperative agreement]], other [[agreement]] for [[assistance]] covered in paragraph (b) of §200.40 [[Federal financial assistance]], or the [[cost-reimbursement contract]] awarded under the [[Federal Acquisition Regulation]]s.<br>
The [[integrative mining]] of [[heterogeneous data]] and the [[interpretability]] of the [[data mining]] result are two of the most important [[challenge]]s of today's [[data mining]] .
The intellectual [[BSC knowledge-based system]] facilities efficient [[automated]] [[strategic planning]] .
The [[interaction]] will change the [[distances among node]]s, while the [[distance]]s will affect the [[interaction]]s.
The [[interdependence]] between different [[entity assignment]]s in a [[Web list]] makes the [[optimization]] of this [[list linking problem]] [[NP-hard]] .
The [[intermediate data explosion problem]], associated with [[naive implementation]]s of [[tensor decomposition algorithm]]s, would require the [[materialization]] and the [[storage]] of a [[matrix]] whose largest [[dimension]] would be â7 x 10<sup>14</sup>; [[this amount]]s to ~10 [[Petabyte]]s, or equivalently a few [[data center]]s worth of [[storage]], thereby rendering the [[tensor analysis]] of [[this knowledge base]], in the naive way, [[practically impossible]] .
The [[internet]] has brought on a rapid [[evolution]] in [[advertising]] .
The [[interpretation]] (i.e. [[decompression]]) of a [[compressed]], [[weighted graph]] is that a [[pair of original nodes]] is connected by an [[edge]] if their [[supernode]]s are connected by one, and that the [[weight]] of an [[edge]] is [[approximated]] to be the [[weight of the superedge]] .
The [[intuition]]s behind [[our work]] are that an [[interesting selection of message]]s contains [[diverse]], [[informative]], [[opinionated]] and [[popular message]]s referring to the [[news article]], written mostly by [[user]]s that have [[authority]] on the [[topic]] .
The investigation of a wide range of successful nations, including the [[United State]]s, [[Japan]], [[Italy]], [[Hong Kong]], [[Singapore]], [[Chile]], and [[Costa Rica]], reveals wide and [[subtle cultural difference]]s associated with improving [[economic circumstance]]s that further belie a simple connection between [[culture]] and [[prosperity]] .
Their final [[decision]] is further [[consolidated]] by [[calculating]] each [[model's]] [[weight]] based on its [[degree]] of consistency with other [[models]] .
Their main [[difference]] lies in the kind of [[meaning inventori]]es that are used: [[EL]] uses [[encyclopedic knowledge]], while [[WSD]] uses [[lexicographic information]] .
[[Their]] main [[limitation]] is that [[memory requirement]]s and [[computational demand]]s grow as the [[square]] and [[cube]] respectively, of the [[number of training case]]s n, effectively limiting a [[direct implementation]] to [[problem]]s with at most a few thousand [[case]]s.
Their [[main property]] is to account for all [[ratings]] -- whether [[observed]] or [[missing]] in the [[data]] .
[[Their]] role is to map [[words]] onto a [[small set]] of [[patterns over character types]] .
The [[ISLE framework]] allows us to view the classic [[ensemble method]]s of [[Bagging]], [[Random Forest]], [[AdaBoost]], and [[Gradient Boosting]] as special cases of a single algorithm.
The <i>[[Wise Market]]</i> consists of a [[set]] of <i>[[investor]]s</i> each with an [[associated individual confidence]] in his / her [[prediction]], and after the [[investment]], only the ones whose [[choice]]s are the same as the whole [[market]] are [[granted reward]]s.
the [[Johnson-Lindenstrauss lemma]], the existing [[guarantee]]s on the [[dot product]] under [[random projection]] are [[loose]] and [[incomplete]] in the current [[data mining]] and [[machine learning literature]] .
The [[joint probability]] is the [[product]] of the [[local probabilities]] .
The [[joint representation]] also allows the [[information]] from each [[type of annotation]] to improve [[performance]] on the other, and, in [[experiments]] with the [[OntoNotes corpus]], [[we]] found improvements of up to 1.36% absolute [[F1]] for [[parsing]], and up to 9.0% [[F1]] for [[named entity recognition]] .
The [[junction tree approach]], with [[application]]s in [[artificial intelligence]], [[computer vision]], [[machine learning]], and [[statistics]], is often used for [[computing]] [[posterior distribution]]s in [[probabilistic graphical model]]s.
[[The kernel]] is straightforward to [[evaluate]] for all [[exponential family model]]s such as multinomials and [[Gaussian]]s and yields interesting [[nonlinear kernel]]s.
The key [[algorithmic problem]] in [[viral marketing]] is to identify a [[set]] of [[influential user]]s (called <i>[[seed]]s </i>) in a [[social network]], who, when convinced to adopt a [[product]], shall [[influence]] other [[user]]s in the [[network]], leading to a [[large number]] of [[adoption]]s.
The key argument is that the [[discriminative estimator]] converges to the [[conditional density]] that [[minimize]]s the [[negative log-likelihood]] [[classification loss]] against the true [[density]] <math>p(x,y)</math> [2].
The key benefit of [[first-order logic]] is its [[expressive power]], which leads to [[concise]] — and hence [[learnable]] — [[model]]s.
The [[key challenge]] for the [[Chief Marketing Officer]] is to tie [[investment]] in [[marketing]] to [[business result]]s.
The [[key contribution]] of [[this paper]] is a [[dynamically programmed layer]] that is critical in determining the [[alignment]] between the [[neuronal activation]]s of [[pair-wise combination]]s of [[neuron]]s.
The key [[developmental]] challenge is how to [[identify]] those [[focal group]]s in the [[training data]] .
The key difference between the [[approach [[we]] propose]] (called [[RelDC]]) and the [[traditional technique]]s is that [[RelDC]] analyzes not only [[object features]] but also [[inter-object relationships]] to improve the [[disambiguation quality]] .
The key factor complicating such tasks is the presence of numerous [[nuisance variable]]s, for instance, the [[unknown]] [[object position]], [[orientation]], and [[scale]] in [[object recognition]] or the unknown [[voice pronunciation]], [[pitch]], and [[speed]] in [[speech recognition]] .
The key feature of [[belief networks]] is their [[explicit representation]] of the [[conditional independence]] and [[dependence]] among [[events]] .
The key feature of [[the proposed framework]] is a novel [[weighting scheme]] that addresses the [[conditional probability distribution]] [[difference]]s across [[multiple domain]]s ([[subject]]s).
The key idea in [[DeepDive]] is to frame traditional [[extract-transform-load (ETL)]] style [[data management problem]]s as a [[single]] [[large statistical inference task]] that is [[declaratively defined]] by the [[user]] .
The key [[idea]] is that [[sample objects]] are employed as "[[sensors]]" to perceive the [[vehicle crowdedness]] in [[nearby areas]] using their instant [[mobility]], rather than the "[[object representatives]]".
The key idea is to use [[tensor factorization]] to address [[multi-aspect challenge]]s, and perform careful [[regularization]]s to attack both [[contextual]] and [[temporal]] [[challenge]]s.
The key [[ideas]] of the [[method]] are : (1)construction of [[quantities]] representing [[feature]] of a whole [[network]] and each [[node]] from the same input based on [[eigen equation compression]], and (2) [[incremental anomalousness scoring]] based on [[learning]] the [[probability distribution]] of the [[quantities]] .
The key insight is the introduction of [[intermediate]] [[aspect variables]] that [[encode]] [[properties]] of the [[labels]] .
The key is to associate each [[document]] with three [[representation]]s: a [[coordinate]] in the [[visualization space]], a [[multinomial distribution]] in the [[topic space]], and a [[directional vector]] in a [[high-dimensional unit hypersphere]] in the [[word space]] .
The key novel [[component]]s of [[our formulation]] are (1) a [[loss function]] for general [[leveraging]] and [[opposing]] [[allocation position]]s and (2) a [[penalty function]] which [[hedge]]s between [[structurally dependent]] [[allocation position]]s to [[control risk]] .
The [[KIM platform]] provides a novel [[Knowledge]] and [[Information Management infrastructure]] and [[service]]s for [[automatic semantic annotation]], [[indexing]], and [[retrieval]] of [[document]]s.
The [[KL divergence]] between [[Gaussian mixture densiti]]es is no [[longer analytically tractable]]; hence [[we]] derive a [[variational EM algorithm]] that [[minimize]]s an [[upper bound]] of the [[total within-cluster KL divergence]] .
The [[Kleinberg HITS]] and the [[Google PageRank algorithm]]s are [[eigenvector methods]] for [[identifying]] ``[[authoritative]]'' or ``[[influential]]'' [[articles]], given [[hyperlink]] or [[citation information]] .
The [[Klink-2 algorithm]] combines [[semantic technologi]]es, [[machine learning]] and [[external source]]s to generate a [[fully populated]] [[ontology of research areas]] .
The [[knowledge base]] is a [[semantic network]] presently consisting of over 1.6 million assertions of [[commonsense knowledge]] encompassing the [[spatial]], [[physical]], [[social]], [[temporal]], and [[psychological aspect]]s of everyday life.
[[The language system]] is contrasted with other work in [[applied natural language processing]], and [[the system]]'s [[limitation]]s are [[analyzed]] .
The [[large]] [[research community]] working on [[time series classification]] has typically used the [[UCR Archive]] to test their [[algorithm]]s.
The [[large-scale case]] involves the [[computational complexity]] of the underlying [[optimization algorithm]] in [[non-trivial]] ways.
The "[[lasso]]" [[minimizes]] the [[residual]] [[sum of squares]] subject to the [[sum]] of the [[absolute value]] of the [[coefficients]] being [[less than]] a [[constant]] .
[[The Lasso]] uses the [[sum of]] the [[absolute value of]] the [[coefficients in the model]] as the [[penalty function]] and had roots in work done by [[Breiman]] on a [[coefficient post-processing technique]] which he had termed [[Garotte]] ([[Breiman et al., 1993]]).
The last 5 [[column]]s ([[brown]]) have indicators for the last [[movie]] the [[user]] has rated before the [[active one]] .
The [[latency]] and [[duration]] of [[rapid movement sequence]]s: comparisons of [[speech]] and [[typewriting]] .
The [[latent coordinate]]s of [[node]]s in the [[visualization space]] can 1) enable [[the system]] to suggest [[network layout]]s most suitable for [[browsing]], and 2) lead to [[high accuracy]] in [[inferring]] the [[underlying network]] when analyzing the [[diffusion process]] of new or [[rare information]], [[rumor]]s, and [[disease]] .
([[The law]]s, or [[statute]]s, allow a certain [[percentage]] of [[prior year excess revenue]] to be counted in the balance.) [[See]] also [[Structural Balance]] .
The [[leading algorithm]]s for [[measuring semantic relatedness]] use [[VSMs]] ([[Pantel & Lin, 2002a]]; [[Rapp, 2003]]; [[Turney, Littman, Bigham, & Shnayder, 2003]]).
The [[learned HL-MRF]]s are as [[accurate]] as [[analogous]] [[discrete model]]s, but much more [[scalable]] .
The [[learning algorithm]]s [[minimize]] the [[hinge loss]] while assuming the [[adversary]] is modifying [[data]] to [[maximize]] the [[loss]] .
The [[learning process]] takes the [[characteristic]]s of the given [[document collection]] into consideration and the [[semantic concept]]s in the [[tailor-made collection]] can be used as new [[feature]]s for [[document representation]] .
The [[least squares problem]] is one of the most important [[regression problem]]s in [[statistics]], [[machine learning]] and [[data mining]] .
The [[legislative session]] runs by [[calendar year and Act]]s are [[numbered consecutively]] in each [[session]] .
The lesson for [[entrepreneur]]s is clear: <i>if you want to [[create]] and [[capture]] [[lasting value]], don’t build an [[undifferentiated commodity business]]</i>.
The [[linear]] [[document publication format]], dating from [[1665]], has survived transition to [[the Web]] .
The [[linguistic structure]] consists of [[segments of the discourse]] into which the [[utterance]]s [[naturally aggregate]] .
The literature on [[dynamic pricing]] and [[learning]] has grown fast in recent years, with [[contribution]]s from different [[scientific communiti]]es: [[operations research]] and [[management science (OR/MS)]], [[marketing]], [[computer science]], and [[economics / econometric]]s.
The [[local triangle counting]] which [[counts triangle]]s for each [[node in a graph]] is a very important [[problem]] with wide applications in [[social network analysis]], [[anomaly detection]], [[web mining]], etc.
The [[log-likelihood function]] is the [[log]] of the [[likelihood]]: <math>
The [[long-term impact]] (e.g., the [[size of the population]] a [[post]] [[benefit]]s) of a [[question]] / [[answer post]] is the key quantity to answer these [[question]]s.
The [[loss function]] is a [[binary logistic regression classifier]] that treats the score of a word and its [[observed]] [[context]] as the score of a [[positive example]], and the [[score]] of a [[word]] and a [[randomly sampled]] [[context]] as the [[score]] of a [[negative example]] .
The [[lower]] [[price]] of [[investment good]]s [[explains]] roughly [[half]] of the observed [[decline]] in the [[labor share]], even when we allow for other mechanisms influencing factor shares such as increasing profits, [[capital-augmenting technology]] [[growth]], and the changing [[skill]] [[composition]] of the [[labor force]] .
The [[low-rank regression model]] has been [[studied]] and applied to capture the underlying [[class]]es / [[task]]s [[correlation pattern]]s, such that the [[regression]] / [[classification result]]s can be enhanced.
The [[LP formulation]] is restricted to [[medium-size]] [[matrice]]s, due to the [[large number]] of [[variables]] involved for [[large]] [[matrice]]s.
The [[LSA approach]] makes three [[claims]]: that [[semantic information]] can be derived from a [[word-document co-occurrence matrix]]; that [[dimensionality reduction]] is an essential part of [[this derivation]]; and that [[words]] and [[documents]] can be [[represented as]] points in [[Euclidean space]] .
[[The LSTM]] also [[learned sensible phrase]] and [[sentence representation]]s that are sensitive to [[word order]] and are relatively [[invariant]] to the [[active]] and the [[passive voice]] .
[[The LSTM]] [[automatically infers a representation]] of [[dialog history]], which relieves [[the system developer]] of much of the [[manual feature engineering]] of [[dialog state]] .
[[The LSTM]] can be optimized using [[supervised learning (SL)]], where a [[domain expert]] provides [[example dialogs]] which [[the LSTM]] should imitate; or using [[reinforcement learning (RL)]], where [[the system]] improves by [[interacting directly with end user]]s.
The [[LUBM]] can be used to [[evaluate systems]] with different [[reasoning capabiliti]]es and [[storage mechanism]]s.
The [[LUBM]] features an [[ontology]] for the [[university domain]], [[synthetic]] [[OWL data]] scalable to an [[arbitrary size]], 14 [[extensional queri]]es representing a variety of [[properti]]es, and several [[performance metric]]s.
The main [[advantage]] of this [[methodology]] over [[existing]] [[approaches]] is that it does not require any [[knowledge]] of the [[dataset properties]] such as the [[total]] [[number]] of [[categories]] or the [[the prior probabilities]] of the [[categories]] .
The main aim of [[this paper]] is to design a [[co-ranking scheme]] for [[object]]s and [[relation]]s in [[multi-relational data]] .
The main contribution of [[this paper]] is to (i) propose a [[framework]] ([[MultiRank]]) to determine the importance of both [[object]]s and [[relation]]s simultaneously based on a [[probability distribution]] computed from [[multi-relational data]]; (ii) show the existence and [[uniqueness]] of such [[probability distribution]] so that it can be used for [[co-ranking]] for [[object]]s and [[relation]]s very effectively; and (iii) develop an [[efficient]] [[iterative algorithm]] to solve a [[set of tensor]] ([[multivariate polynomial]]) [[equation]]s to obtain such [[probability distribution]] .
The main [[contribution]]s of [[this paper]] include (1) [[large-scale]] [[analysis]] of [[impression data]] from [[LinkedIn]] and [[KDD Cup]]; (2) novel [[anti-noise regression technique]]s, and its application to learn four different [[impression discounting function]]s including [[linear decay]], [[inverse decay]], [[exponential decay]], and [[quadratic decay]]; (3) applying these [[impression discounting function]]s to [[LinkedIn]]'s "[[People You May Know " and " Endorsement]]s "[[recommender system]]s.
The main [[contributions]] of [[this paper]] include (1) [[we]] rigorously [[establish]] the [[equivalence relationship]] between the proposed [[two-stage approach]] and the original [[formulation]] without any [[assumption]]; and (2) [[we]] show that the [[equivalence relationship]] still holds in the [[regularization setting]] .
The main goal of the [[ERD challenge]] was to promote [[research]] in [[recognition]] and [[disambiguation of entiti]]es in [[unstructured text]] .
The main idea behind [[TANGENT]] is to envision the [[problem]] as [[node selection]] on a [[graph]], giving [[high score]]s to [[node]]s that are well [[connect]]ed to the older [[choice]]s, and at the same time well [[connect]]ed to [[unrelated choice]]s.
The [[main question]]s are (a) how to [[minimize]] the [[I/O cost]], taking into account the already existing [[data partition]] (e.g., on [[disk]]s), and (b) how to [[minimize]] the [[network cost]] among [[processing]] [[node]]s.
The main [[result]]s show that: (1) our proposed [[approach]] to [[active inference]] on [[stream]]s can indeed [[reduce error cost]]s substantially over [[alternative approach]]es, (2) more sophisticated [[online estimation]]s achieve larger [[reductions in error]] .
The major difference from [[filtering]] in [[this case]] is the [[static nature]] of the [[categori]]es, when compared to [[profile]]s.
The majority of [[phone]]s, [[tablet]]s and [[computer]]s now ship with [[language-enabled capabiliti]]es like [[speech-recognition]] and [[intelligent auto-correction]], and [[people]] increasingly [[interact with]] [[data-intensive cloud-based language technologi]]es like [[search-engine]]s and [[spam-filter]]s.
The majority of [[world]]'s new [[software]] is now [[connected software]], also [[resident]] in or using the [[cloud]] .
The major [[shortcoming]] of such [[approach]]es is that they ignore the [[redundancy]] between [[high-ranked entiti]]es, which may in fact be very [[similar]] or even [[identical]] .
The [[Massachusett]]s [[General Law]]s require that [[annual budget]]s are in [[balance]] .
The [[Master Algorithm]] is to machine learning what the [[Standard Model]] is to [[particle physic]]s or the [[Central Dogma]] to [[molecular biology]]: a [[unified theory]] that [[makes sense]] of [[everything we know to date]], and lays the [[foundation]] for decades or [[centuri]]es of [[future progress]] .
The [[mathematical]] [[properties]] of [[pSkip]] are further [[analyzed]] and [[compared]] with several objective [[metric]]s as well as the [[cumulated gain method]] that uses [[subjective judgment]]s.
The [[matrix row]]s ([[entity pair]]s) were clustered using [[CLUTO]]’s [9] [[divisive algorithm]] with [[repeated bisection]]s.
The maximal form is a [statement]] with its complete [[supporting argument]], consisting of all [[relevant evidence]], [[interpretation]]s, [[discussion]] and [[challenge]]s brought forward in [[support of or opposition]] to [[it]] .
The [[meaning]] of an [[unknown]] [[word]] can often be [[inferred]] from its [[context]] .
The [[measure]] employs [[sequence alignment]] between two [[career trajectori]]es to quantify [[professional similarity]] between [[career path]]s.
The [[measure]] is defined by [[connecting]] [[statistical models]] and [[collections of itemsets]] .
The [[mechanism]] whereby [[EIA]] occurs is thought to relate to the [[consequence]]s of [[heating]] and [[humidifying]] large volumes of [[air]] during [[exercise]] .
The [[message updating process]] enables [[Affinity Propagation]] to have higher [[clustering quality]] compared with other [[approach]]es.
[[The method]] [[computes the importance of a feature]] by [[aggregating]] the [[difference]], over the [[feature space]], of the [[conditional density functions]] of the [[SVR prediction]] with and without the [[feature]] .
The [[method]] efficiently [[mine]]s tree-shaped [[subgraph]] [[descriptor]]s under [[minimum]] [[frequency]] and [[significance constraint]]s, using [[class]]es of [[fragment]]s to reduce [[feature set]] size and [[running time]]s.
The [[method]] is able to [[optimize]] [[structural inter-feature entropy]] as opposed to [[occurrence]]s, which is [[characteristic]] for [[open]] or [[closed fragment]] [[mining]] .
The [[method]] is carefully [[design]]ed to be (a) [[parameter-free]] (b) [[effective]] and (c) [[fast]] .
The [[method]] is [[compared]] against existing [[methods]] by [[experimental evaluation]] on a [[real world]] [[crime]] [[dataset]] which demonstrate the importance of the [[neighbourhood definition]] and the advantages of [[parallelization]] .
The method is [[efficient]] to [[implement]] and can be applied to [[vertical]]s of different [[nature]] .
[[The method]] is fast, robust to [[noise]], does not [[overfit]] and offers possibilities for [[explanation]] and [[visualization]] of its [[output]] .
[[The method]] is solidly founded in [[Bayesian statistics]] and [[decreases monotonically]], allowing for [[efficient]] [[discovery]] of all [[interesting itemsets]] .
The [[methodology]], called [[soft cost-sensitive classification]], is [[established]] from a [[multicriteria optimization problem]] of the [[cost]] and the [[error rate]], and can be viewed as regularizing [[cost-sensitive classification]] with the [[error rate]] .
The method potentially solves the [[problem of missing]] [[skid resistance]] [[value]]s which occurs during [[network-wide]] [[crash analysis]], and allows [[risk assessment]] of the major [[proportion]] of [[road]]s without [[skid resistance]] [[value]]s.
The [[methods]]' [[detection performance]] on [[geostatistic data]] with [[linear]] or [[nonlinear trend]] is also not well [[studied]] .
[[The method]] was developed in the [[1970s]], with roots in the [[1950s]], and is equivalent or closely related to many other [[algorithm]]s, such as [[dual decomposition]], the [[method of multipliers]], [[Douglas-Rachford splitting]], [[Spingarn's method of partial inverse]]s, [[Dykstra's alternating projection]]s, [[Bregman iterative algorithm]]s for <math>l_1</math> problems, [[proximal method]]s, and others.
[[The method]], which can be applied to context-counting as well as [[context-predicting space]]s, works by [[decomposing]] [[word vector]]s as [[linear combination]]s of [[sense vector]]s, and by pushing the [[sense vector]]s towards their [[neighbor]]s in the [[semantic network]] .
The [[micropublications semantic model]] of [[scientific argument]] and [[evidence]] provides these [[feature]]s.
The [[mixture-KDE approach]] is evaluated on two [[large geolocation]] / [[check-in data set]]s, from [[Twitter]] and [[Gowalla]], with comparisons to [[non-KDE baseline]]s, using both [[log-likelihood]] and [[detection]] of [[simulated identity theft]] as [[evaluation metric]]s.
[[The model]] correctly discriminates between [[homosexual]] and [[heterosexual men]] in 88% of cases, [[African American]]s and [[Caucasian American]]s in 95% of [[case]]s, and between [[Democrat]] and [[Republican]] in 85% of cases.
The model does not account for [[fading]], [[frequency]] selectivity, [[interference]], [[nonlinearity]] or [[dispersion]] .
The [[model]] integrates the [[topic extraction]] and [[entity matching]], two [[core subtask]]s for dealing with [[the problem]], into a [[unified model]] .
The [[model]] is a [[Bayesian approach]], and integrates three [[factors]] - [[ego-centric effect]], [[environmental effects]] and [[web page content]] .
[[The model]] is simple and [[linear in nature]], while [[empirically justified]] by the [[advertising domain]] .
The [[model]] itself has no knowledge of [[syntax]] or [[morphology]] or [[semantics]] .
[[The model]] [[jointly learn]]s [[representations of word]]s, [[entiti]]es and [[MR]]s via a [[multi-task training process]] operating on these diverse [[sources of data]] .
[[The model]] [[outperform]]s multiple [[baseline]]s and, when combined with [[information retrieval method]]s, rivals the best [[human player]]s.
[[The model]] [[predict]]s that [[industri]]es and [[occupation]]s that are initially intensive in [[labor input]] of [[routine task]]s will make relatively [[larger investment]]s in [[computer capital]] as its [[price]] [[decline]]s.
The [[model]]s, [[analytic]]s and [[decision]]s they inform all stem from the assumption that [[observed data capture]]s the [[intent of user]]s.
The [[models]] are demonstrated by analyzing the [[OCR'ed]] [[archives]] of [[the journal Science]] from [[1880]] through ([[2000]]).
The [[model]]s they form attain high [[likelihood]]s, and [[inspection]] shows that they [[summarize]] the [[data]] well with increasingly specific, yet [[non-redundant itemset]]s.
The [[model]] that [[attributes]] all the [[CTR]] [[change]] to [[relevance]] yields substantially better [[predictors]] of [[CTR]] than [[models]] that attribute all the [[change]] to [[examination]], and does only slightly [[worse than]] the [[model]] that attributes [[CTR]] [[change]] to both [[relevance]] and [[examination]] .
[[The model]] was [[fitted]] to the [[oral health data]] of the [[Signal–Tandmobiel® study]] .
The [[moral]]: [[Performance]] improves with [[data size]], and [[getting]] more [[data]] will make more [[difference]] than [[fine-tuning algorithms]] .
The more than 500 entries include [[definition]]s, [[history]], [[mathematical detail]]s, [[limitation]]s, [[example]]s, [[reference]]s, and further [[reading]]s.
The most basic [[annotation primitive]] identifies a [[text span]] and [[assign]]s it a [[type]] (or [[tag or label]]), marking for e.g.
:* The most commom [[units of measurement]] of [[temperature]] are [[Celsius]] (°C), [[Fahrenheit]] (°F), [[Kelvin]] (K).
The most [[difficult obstacle]] with [[recursive relation]]s concerns the [[navigation]] in each [[mapping direction]] and with a [["rollup" process]] .
The most important [[challenge]] for [[this task]] is to [[estimate]] [[source reliability]] and [[select answers]] that are provided by [[high-quality]] [[source]]s.
The most popular form of [[entities]] is [[named entities]] like [[names of persons]], [[locations]], and [[companies]] as popularized in the [[MUC]] [57, 100], [[ACE]] [1, 159], and [[CoNLL [206]] competitions]] .
The most promising [[emerging technologi]]es likely lie in [[training cognitive abiliti]]es and [[augmenting]] or [[substituting for]] [[impaired abiliti]]es.
The motivation for studying [[this model]] stems from [[social network]]s, where the [[node]]s are the [[user]]s, the [[public graph]] is visible to everyone, and the [[private graph]] at each [[node]] is [[visible]] only to the user at the [[node]] .
The motivation is that [[user]]s have stable [[characteristic]]s across different [[crowdsourcing task]]s and thus [[data]] from different [[task]]s can be [[exploited collectively]] to [[estimate]] [[users' abiliti]]es in the [[target task]] .
The [[multivariate]] [[probit model]] is a popular choice for modelling [[correlated]] [[binary]] [[responses]] .
The [[National Environment Agency of Singapore (NEA)]] is the [[primary government agency]] responsible for [[monitoring]] and [[mitigating]] the [[food safety risk]]s.
The [[natural form]]s of [[sociability]], reliance on [[family]] and [[friends]], are still at work in [[patrimonial state]]s.
The [[natural language processing (NLP) community]] attempted to achieve this goal in the [[1970’s]] and [[1980’s]] by building [[system]]s for [[understanding]] and [[answering questions]] about [[simple stories]] [3, 13, 23, 6].
Then, based on [[investor profile]]s, [[we]] develop an [[investor composition analysis model]], which can be used to [[select]] [[valuable investment]]s and improve the [[investment decision]]s.
Then, based on the [[MDLprinciple]], the novel [[RDB-KRIMP algorithm]] selects the [[set]] of [[patterns]] that allows for the most [[succinct encoding]] of the [[database]] .
Then, based on the [[mined]] [[pair-wise temporal dependencies]], [[we]] construct a basic [[workflow model]] by a [[breadth-first]] [[path pruning algorithm]] .
The need for [[existing dataset]]s can be avoided by using a [[universal schema]]: the union of all involved [[schema]]s ([[surface form predicate]]s as in [[OpenIE]], and [[relation]]s in the [[schema]]s of preexisting [[database]]s).
The need to [[construct ontologies]] – ranging from [[small domain ontologi]]es to [[large ontolog]]ies linked to [[legacy dataset]]s – hinders the ability and willingness of [[organisation]]s to apply [[Semantic Web (SW) technologi]]es to [[large-scale]] [[data integration]] and [[sharing initiati]]ves [1,7,9].
The [[network inference problem]] consists of [[reconstructing]] the [[edge set]] of a [[network]] given [[trace]]s representing the [[chronology]] of [[infection time]]s as [[epidemics spread]] through the [[network]] .
Then it generates [[recommendation]]s by [[combining]] the [[average]] [[rating]]s of the [[co-cluster]]s with the [[bias]]es of the [[user]]s and [[movie]]s.
The [[nodes of the graph]] are [[author]]s, while the [[edge]]s represent [[mutual publication]]s.
The [[node]]s of the [[tree]] are [[region]]s frequently visited and the [[edge]]s represent [[travel]] among [[region]]s and are [[annotated]] with the typical [[travel time]] .
The [[non-linear LPS-based method (NLPS)]] considers a [[superset]] of [[LPS]]s as [[candidate]]s, and the [[linear LPS-based method (LLPS)]] further guarantees to consider only [[LPS]]s.
the [[noun]] ' [[pollutant']] and the [[noun-phrase]] ' [[greenhouse gases]]').
Then, [[our approach]] feeds the [[Graph rewrite rule]]s into a [[machine learning]] [[system]] that learns general [[transformation]] [[rules]] [[describing]] the [[types]] of changes that occur for a [[class]] of [[dynamic biological network]]s.
Then our [[co-clustering method]] is formulated as [[semi-nonnegative]] [[matrix tri-factorization]] with two [[graph regularizers]], requiring that the [[cluster labels]] of [[data points]] are smooth with respect to the [[data manifold]], while the [[cluster labels]] of [[features]] are smooth with respect to the [[feature manifold]] .
The novelty of [[our approach]] is to algorithmically generate various [[layer]]s and combine them to [[predict]] overall [[30-day]] [[risk-of-readmission]] .
Then, the [[extracted]] '[[thread-title, reply]]' [[pair]]s are [[ranked]] with a [[ranking SVM]] based on their [[content qualiti]]es.
Then the [[family]] of [[surface]]s of [[constant]] [[likelihood]], <math>\lambda</math>, appropriate for [[testing]] a [[simple hypothesis]] <math>H_0</math>, is defined by
Then the [[key challenge]]s are how to adapt the [[world knowledge]] to [[domain]]s and how to [[represent]] it for [[learning]] .
Then [[the system]] [[learn]]s the [[weight]]s for each [[meta-path]] that are consistent with the [[clustering result]] implied by the [[guidance]], and generates [[cluster]]s under the learned [[weight]]s of [[meta-path]]s.
Then the [[text documents]] are [[clustered]] based on a [[similarity metric]] which combines [[document content information]], [[concept]] [[information]] as well as [[category]] [[information]] .
Then, the [[trusted-anonymization server]] performs the [[anonymization]] using the [[location semantic information]] by [[cloaking]] with [[semantically heterogeneous location]]s.
The [[number of clusters]] within each [[group]] is [[unknown]] and is to be [[inferred]] .
The number of distinct [[active key]]s in the [[data]] can be [[very large]], [[making exact computation]] of [[queries costly]] .
The number of [[patent]]s filed each [[year]] has increased [[dramatically]] in recent years, raising concerns that [[patent]]s of [[questionable validity]] are restricting the [[issuance]] of truly [[innovative]] [[patent]]s.
The [[number of]] [[swapped record]]s is [[kept secret]] even though it is needed for [[data analysis]] and investigations into the [[confidentiality protection]] of [[individual record]]s.
The [[number of]] [[transaction]]s required for the [[itemset]] to [[satisfy]] [[minimum support]] is therefore referred to as the [[minimum support count]] .
Then we can use these [[relationship]]s to effectively infer the [[correlation]]s among different [[class label]]s in general, as well as the [[dependenci]]es among the [[label set]]s of [[data example]]s [[inter-connected]] in the [[network]] .
Then [[we]] compare different classes of [[supervised learning algorithm]]s in terms of their [[prediction]] [[performance]] using various [[performance metrics]], such as [[accuracy]], [[precision]]-[[recall]], [[F-values]], [[squared error]] etc. with a [[5-fold]] [[cross validation]] .
Then [[we]] comprehensively analyze [[FFMs]] and compare [[this approach]] with [[competing model]]s.
Then, we develop a [[graph representation]] of [[road network]]s by [[mining]] the [[historical taxi GPS trace]]s and provide a [[Brute-Force strategy]] to generate [[optimal driving route]] for [[recommendation]] .
Then, [[we]] discuss [[parameter estimation]] (Section 1.3.2) and [[inference]] (Section 1.3.3) in [[linear-chain CRFs]] .
Then, [[we]] formulate a new [[pattern mining problem]], <i>[[mining top-1 qualified pattern]] </i>, on [[transaction database]] for this [[recommendation]] .
Then, we learn the [[estate]] [[ranking indicator]] by simultaneously [[maximizing]] [[ranking consistency]] and [[functional diversity]], in a unified [[probabilistic optimization framework]] .
Then [[we]] propose a [[branch-and-bound algorithm]] to [[efficiently]] [[search]] for [[optimal]] [[subgraph features]] by judiciously [[pruning]] the [[subgraph search space]] .
Then [[we]] propose a [[clustering algorithm]] that can [[cluster]] multiple [[type]]s and incorporate the [[sub-type information]] as [[constraint]]s.
Then, [[we]] propose a [[flexible approach]] based on [[modern portfolio theory]] for [[recommending App]]s by striking a balance between the [[Apps' popularity]] and the [[users' security concern]]s, and build an [[App hash tree]] to [[efficiently]] [[recommend App]]s.
Then [[we]] propose a new [[Feature selection method]] called “[[Term Contribution (TC)]]” and perform a [[comparative study]] on a variety of [[feature selection methods]] for [[text clustering]], including [[Document Frequency (DF)]], [[Term Strength (TS)]], [[Entropy-based (En)]], [[Information Gain (IG)]] and [[N2 statistic (CHI)]] .
Then [[we]] propose to improve the [[model]] by [[sharing parameter]]s between different [[rating]]s.
Then [[we]] regard [[transfer learning]] as a [[special case]] of [[multi-task learning]] and adapt the formulation of [[multi-task metric learning]] to the [[transfer learning setting]] for [[our method]], called [[transfer metric learning (TML)]] .
Then [[we]] use the [[mixed Poisson process]] to model [[event cascade]] [[generated by]] different [[factors respectively]] and [[integrate]] different [[Poisson process]]es with [[shared parameter]]s.
Then, [[we]] utilize the [[local continuity]] [[characteristics]] of [[trajectories]] to build local [[cluster]]s upon [[trajectory stream]]s and [[monitor anomalies]] via efficient [[pruning strategies]] .
The [[Nystrom method]] generates a [[fast]] [[approximation]] to any [[large-scale symmetric positive semidefinete]] ([[SPSD) matrix]] using only a few [[column]]s of the [[SPSD matrix]] .
The [[objective]] in [[extreme multi-label classification]] is to [[learn a classifier]] that can [[automatically]] [[tag a data point]] with the most relevant [[subset of labels]] from a [[large label set]] .
The [[objective]] of [[LBR]] is to exploit the [[link structure]] of a [[graph]] to order or [[prioritize]] the [[set of objects]] within the [[graph]] .
The [[objective]] of [[reference disambiguation]] is to [[identify]] the [[unique]] [[entity]] to which each [[description]] [[corresponds]] .
The [[objective]] of [[reference disambiguation]] is to [[identify]] the unique [[entity]] to which each [[description]] corresponds.
The objective of [[this paper]] is to mitigate [[this problem]] by [[integrating]] [[multimedia feature]]s [[extract]]ed from display [[ad]]s into the [[click prediction model]]s.
The [[objective]] of [[this paper]] is two fold: <i>first </i>, [[we]] introduce the [[emerging domain]] of [["big" healthcare claims data]] to the [[KDD community]], and <i> second </i>, [[we]] describe the success and challenges that we encountered in analyzing [[this data]] using [[state of art]] [[analytics]] for [[massive data]] .
The objectives of [[feature selection]] include: [[building simpler]] and more [[comprehensible model]]s, improving [[data mining performance]], and [[preparing clean]], [[understandable data]] .
The [[OBO Foundry]] is coordinating the efforts on [[establishing]] a [[set of principles]] for [[ontology development]] in order to create a [[suite]] of [[orthogonal]] [[interoperable]] [[reference ontologi]]es in the [[biomedical domain]] for [[supporting data]] and [[knowledge sharing]] and to avoid [[duplication of effort]]s.
The [[obvious solution]] is to [[split the graph]] into [[clusters of nodes]] and store each [[cluster]] on a [[disk page]]; ideally [[random walks]] will rarely [[cross]] [[cluster boundaries]] and [[cause]] [[page-faults]] .
The [[occurrence frequency]] of an [[itemset]] is the [[number of]] [[transaction]]s that [[contain]] the [[itemset]] .
The [[offline modeling]] part, called [[LCA-LDA]], is designed to [[learn]] the [[interest of each individual user]] and the [[local preference]] of each [[individual city]] by [[capturing item co-occurrence pattern]]s and exploiting [[item content]]s.
The [[OLiA ontologi]]es represent a [[repository]] of [[annotation terminology]] for various [[linguistic phenomena]] on a great band-width of [[language]]s, they have been used to facilitate [[interoperability]] and [[information integration]] of [[linguistic annotation]]s in [[corpora]], [[NLP pipeline]]s, and [[lexical-semantic resource]]s.
[[The One Hundred Year Study]] was subsequently endowed at a [[university]] to enable extended [[deep thought]] and [[cross-disciplinary]] [[scholarly investigation]]s that could inspire [[innovation]] and provide [[intelligent advice]] to [[government agenci]]es and [[industry]] .
The [[online recommendation]] part [[automatically combine]]s the [[learnt interest]] of the [[querying user]] and the [[local preference]] of the [[querying city]] to [[produce]] the top - [<i> k</i> [[recommendation]]s.
The [[online regression]] for each [[item]] can be performed [[independently]] and hence [[the procedure]] is [[fast]], [[scalable]] and easily [[parallelizable]] .
The only viable solution appear to be [[system]]s that help [[author]]s create [[correct]], [[complete]], and [[annotated bibliographies]], thus enabling [[autonomous]] [[citation indexing]] to create correct and complete [[citation networks]] .
The [[ontology]] is based on a [[human generated]] and [[readable]] [[semantic wiki]] focused on [[concepts]] and [[relationships]] for the [[domain]] along with other related [[topics]], [[papers]] and [[researchers]] from [[information science]]s.
The open-source [[C++]] [[implementation]] of [[the proposed algorithm]] is available at https://github.com/xunzheng / [[light_medlda]] .
The [[operational goal]] of [[the system]] is to identify the [[affiliate program]]s of [[online merchant]]s behind these [[Web site]]s; [[the system]] itself is part of a [[larger effort]] to [[improve the tracking]] and [[targeting]] of these [[affiliate program]]s.
The [[operation]] of [[subtracting]] the initial from the final [[velocity]] must be done by [[vector addition]] since they are inherently [[vector]]s.
The [[OpinionMiner system]] designed in [[this work]] aims to [[mine]] [[customer reviews]] of a [[product]] and [[extract]] high detailed [[product entities]] on which [[reviewer]]s express their [[opinions]] .
[[The optimization method]] [[we]] develop is [[scalable]], with a small [[in-memory footprint]], and working in [[linear time per iteration]] .
The [[optimization problem]] for [[the proposed formulation]] is [[non-convex]], [[non-smooth]], and [[non-separable]] .
The [[optimization problem]] of [[selecting the most influential nodes]] is [[NP-hard]] here, and we provide the first [[provable]] [[approximation guarantees]] for [[efficient algorithms]] .
[[Theoretical]] and [[empirical chapter]]s by leading [[academic]]s and [[scholar-activist]]s grapple with these issues by questioning [[scarcity]]'s [[taken-for-granted nature]] .
[[Theoretical framework]]s, [[design]]s, [[procedure]]s, and [[software]] for such [[tests]], based on [[structural equation modeling]], have been developed and successfully used in the last three decades [13,21].
[[Theoretical issue]]s including [[learning curve]]s and the [[PAC-Bayesian framework]] are treated, and several [[approximation method]]s for [[learning]] with [[large dataset]]s are discussed.
[[Theoretically]], [[the algorithm]] [[guarantee]]s that the [[sampling size]] <i>[[R]]</i> = <i>O</i> (2Îµ<sup>-2</sup> log<sub>2</sub> <i>T </i>) depends on the [[error-bound]] Îµ, the [[confidence level]] (1 -- [[Î´]]), and the [[path length]] <i>T</i> of each [[random walk]] .
Theoretically, [[we]] show that our [[method]] [[reconstruct]]s the [[oracle estimator]] exactly from [[noisy data]] .
[[Theoretically]], [[we]] show that under a [[linear programming (LP)]] [[primal-dual formulation]], the simple [[real-time]] [[bidding algorithm]] is indeed an [[online solver]] to the original [[primal problem]] by taking the [[optimal solution]] to the [[dual problem]] as [[input]] .
[[Theories of normative design]] from [[economics]] may prove more relevant for [[artificial agent]]s than [[human agent]]s, with [[AI]]s that better respect [[idealized assumption]]s of [[rationality]] than [[people]], interacting through [[novel rule]]s and [[incentive system]]s quite distinct from those tailored for [[people]] .
The original [[courage scale]] developed by [[Woodard (2004)]] measured [[courage]] as the [[product]] of the [[willingness to take action]] and the [[fear experienced]] while [[taking the action]] .
The original [[term space]] of the [[target domain]] is [[projected]] to a [[concept space]] using [[LaSA model]] at first, then the [[address standardization]] [[model]] is [[active learned]] from [[LaSA features]] and [[informative samples]] .
The [[OSAD problem]] was introduced to design a [[residual system]], where all [[anomali]]es (known and [[unknown]]) are detected but [[the system]] only [[trigger]]s an alarm when [[non-SS anomali]]es appear.
The [[osmotic]], or [[airway-drying]], [[hypothesis developed]] from [[1982]]-[[1992]] because neither [[airway]] cooling nor rewarming appeared to be necessary for [[EIA]] to occur.
The other [[direction of dialogue topic tracking]] made use of [[external knowledge source]]s including [[domain model]]s ([[Roy and Subramaniam, 2006]]), [[heuristic]]s ([[Young et al., 2007]]), and agendas ([[Bohus and Rudnicky, 2003]]; [[Lee et al., 2008]]).
The other is a [[temporal classifier]] based on a [[linear-chain conditional random field (CRF)]], involving [[temporally-related feature]]s (e.g., [[traffic]] and [[meteorology]]) to model the [[temporal dependency]] of [[air quality]] in a [[location]] .
The other is [[YAGO2]], a [[knowledge base]] [[automatically extract]]ed from [[Wikipedia]] and [[maps knowledge]] to the [[linguistic knowledge base]], [[WordNet]] .
The other new feature is [[global feature]] based on the [[offering]]s [[industry]] (such as [[Electronics]], [[Books]], or [[Automotive]]).
The [[output]]s are then the [[mean]], [[median]], [[distribution]], or [[high-percentile value]]s for those [[peer]] [[group]]s on those [[metric]]s.
The [[output]] [[temporal graph]]s provide better understanding about [[complex system]]s, i.e. how their [[dependency]] [[graph]]s [[evolve over time]], and achieve more [[accurate predictions]] .
The [[Overall Evaluation Criteria (OEC)]] (also known as [[goal metric]]s or [[key metric]]s) of an [[online service]] are [[metrics defined]] to help [[the system]] move toward the [[North Star]] .
The overall [[system accuracy]] is [[maximized]] while explicitly [[controlling]] the [[expected cost]] for [[feature acquisition]] .
The [[overlapping]] [[vertice]]s shared between [[communities]] can also be easily [[identified]] by an additional simple [[postprocessing]] .
[[The paper]] also introduces [[TEXTRUNNER]], a [[fully implemented]], [[highly scalable]] [[OIE]] system where the [[tuples]] are assigned a [[probability]] and [[indexed]] to support [[efficient]] [[extraction]] and [[exploration]] via user [[queries]] .
The [[paper]] also shows how the two [[methods]] can be combined into a system able to [[automatically]] [[enrich a text]] with [[links]] to [[encyclopedic knowledge]] .
The [[paper]] demonstrates good performance of [[context-group discrimination]] for a [[sample]] of [[natural]] and [[artificial]] [[ambiguous words]] .
[[The paper]] describes the [[overall architecture]] of [[the system]], [[business needs]], and shares [[experience]] from successful [[large-scale]] [[commercial deployments]] .
[[The paper]] describes the results of a [[Dutch study]] into the [[consequence]]s of replacing [[home interview]]s by [[trained interviewer]]s with [[Internet-delivered interview]]s in a [[survey]] on [[fraud]] in the [[area of disability benefit]]s.
[[The paper]] examines [[inflation targeting]] in a [[small open economy]] with [[forward-looking]] [[aggregate supply]] and [[demand]] with [[microfoundation]]s, and with stylized realistic lags in the different [[monetary-policy transmission channel]]s.
[[The paper]] explores the [[co-reference chain]]s as a way for improving the [[density]] of [[concept annotation]] over [[domain text]]s.
The [[paper]] further describes our first efforts in [[operationalising]] the [[evaluation procedure]], including the creation of a [[semantically annotated]] [[corpus]] that will function as a [[test bed]] for the [[proposed]] [[evaluation mechanism]], and [[comparison]] of different [[evaluation metrics]] .
[[The paper]] introduces a new [[field of application]] for [[data mining]], where [[trace gas response]]s of [[people]] reacting [[on-line]] to [[film]]s shown in [[cinema]]s (or [[movie theater]]s) are related to the [[semantic content]] of the [[film]]s themselves.
[[The paper]] provides an [[explanation]] for the [[low impact]] of various [[individual application]]s of [[smart citi]]es and discusses ways of improving their [[effectiveness]] .
[[The paper]]s come from a broad range of application areas including [[Customer Relationship Management]], [[Medicine]], [[Taxation]], and [[Software Development]] .
The paper's main [[contribution]]s are: (i) [[We]] provide a [[systematic approach]] to integrate [[physical law]]s and [[sensor observation]]s in a [[data center]]; (ii) [[We]] provide an [[algorithm]] that uses [[sensor data]] to [[learn]] the [[parameter]]s of a [[data center]]'s [[cyber-physical system]] .
The [[paradigm]] [[we]] offer is creating a [[model]] [[tracking]] the [[time changing]] behavior throughout the [[life span]] of the [[data]] .
The [[parameters learned]] from [[this model]] include: (1) the [[topical distribution of each document]], (2) the [[perspective distribution]] of each [[user]], (3) the [[word distribution]] of each [[topic]], (4) the [[tag distribution]] of each [[topic]], (5) the [[tag distribution]] of each [[user perspective]], (6) and the [[probabilistic]] of each [[tag]] being generated from [[resource topics]] or [[user perspectives]] .
The [[parameter]]s of [[the prior]] are [[estimated]] from the [[samples]] of the known [[class]]es.
[[The participants]] represent diverse [[specialti]]es and [[geographic region]]s, [[gender]]s, and [[career stage]]s.
The [[participating system]]s were evaluated by matching their [[extracted keyphrase]]s against [[manually assigned]] [[one]]s.
The particular subjects covered include [[motion planning]], [[discrete planning]], [[planning under uncertainty]], [[sensor-based planning]], [[visibility]], [[decision-theoretic planning]], [[game theory]], [[information space]]s, [[reinforcement learning]], [[nonlinear system]]s, [[trajectory planning]], [[nonholonomic planning]], and [[kinodynamic planning]] .
The [[passive classifier]] employed in [[our algorithm]] builds a [[random-walk]] on the [[data graph]] based on a [[modified graph geometry]] that [[combine]]s the [[data distribution]] with [[current label hypothesis]]; while the [[query component]] uses the [[uncertainty]] of the [[evolving hypothesis]] .
The [[patient EHR]]s are generally [[longitudinal]] and naturally [[represented as]] [[medical event sequence]]s, where the [[event]]s include [[clinical note]]s, [[problem]]s, [[medication]]s, [[vital sign]]s, [[laboratory report]]s, etc. The [[longitudinal]] and [[heterogeneous properti]]es make [[EHR analysis]] an inherently difficult [[challenge]] .
The [[patterns of]] [[gender difference]]s in [[interest]]s and the actual [[gender composition]] in [[STEM field]]s were explained by the [[people-orientation]] and [[things-orientation]] of [[work environment]]s, and were not associated with the [[level of quantitative ability]] required.
The [[PCRW based method]] significantly [[outperformed]] [[unsupervised random walk based queries]], and [[models]] with [[learned edge weights]] .
The [[percentage of]] the [[U.S. workforce]] between the [[age]]s of [[55]] and [[64]], for example, is growing faster than any other [[age group]] .
The [[performance]] of [[UP-Growth]] was [[evaluated]] in comparison with the [[state-of-the-art algorithm]]s on different [[types]] of [[datasets]] .
The [[pervasiveness]] of [[mobile device]]s and [[location based service]]s is leading to an increasing [[volume]] of [[mobility data]] .
The [[pitfalls]] include a wide range of [[topic]]s, such as [[assuming]] that common [[statistical formulas]] used to [[calculate]] [[standard deviation]] and [[statistical power]] can be applied and ignoring [[robot]]s in [[analysis]] (a problem unique to [[online settings]]).
The [[pixel]]s correspond to [["visible" unit]]s of the [[RBM]] because their [[states are observed]]; the [[feature detector]]s correspond to [["hidden" unit]]s.
The [[pLSI approach]], which [[we]] describe in detail in Section 4.3, [[models]] each [[word]] in a [[document]] as a [[sample]] from a [[mixture model]], where the [[mixture components]] are [[multinomial random variables]] that can be viewed as representations of “[[topics]].” Thus each [[word]] is generated from a single [[topic]], and different [[words]] in a [[document]] may be generated from different [[topics]] .
The [[PMI-IR algorithm]] is employed to [[estimate]] the [[semantic orientation]] of a [[phrase]] ([[Turney, 2001]]).
The popularity of [[collaborative tagging site]]s has created new [[challenge]]s and opportunities for [[designer]]s of [[web item]]s, such as [[electronics product]]s, [[travel itinerari]]es, popular [[blog]]s, etc. An increasing number of [[people]] are turning to [[online review]]s and [[user-specified tag]]s to choose from among [[competing]] [[item]]s.
The [[popularity]] (or [[frequency]]) of [[topic]]s in [[text corpora]] usually follow a [[power-law distribution]] where a few [[dominant topic]]s occur very frequently while most [[topic]]s (in the [[long-tail region]]) have [[low probabiliti]]es.
The potential of [[this approach]] is illustrated by a [[cross-validation comparison]] of [[predictive performance]] with [[neural net]]s, [[MARS]], and [[conventional tree]]s on [[simulated]] and [[real data set]]s.
The power of [[deep neural network]]s has sparked renewed interest in [[reinforcement learning]], with [[application]]s to [[game]]s, [[robotic]]s, and [[beyond]] .
The [[PPLRE Automated Evaluation System]] is the system used within the [[PPLRE Project]] to [[evaluate]] the [[Performance]] of the [[PPLRE Relation Extraction Algorithm]]s, particularly with respect to [[Correctness Metric]]s of [[Precision]] and [[F-Score]] .
The [[PPLRE Manual Evaluation System]] is the [[System]] used in the [[PPLRE Project]] by [[Domain Experts]] to [[evaluate]] a predicted ''[[OPL]]()'' relation in a [[Passage]] .
The [[practical research]] on [[hot spots]] in [[smart city]] presents many unique features, such as highly [[mobile environments]], supremely [[limited size of sample objects]], and the [[non-uniform]], [[biased samples]] .
The [[prediction runtime]] of [[RTF]] is independent of the [[number of observations]] and only depends on the [[factorization]] [[dimension]]s.
The [[prediction]] uses previously extracted [[movement]] [[pattern]]s named <i>[[Trajectory Pattern]]s</i>, which are a [[concise]] [[representation]] of [[behavior]]s of [[moving object]]s as [[sequence]]s of [[regions frequently visited]] with a typical [[travel time]] .
The [[prequential error]] allows us to [[monitor]] the [[evolution]] of the [[performance]] of [[models]] that [[evolve over time]] .
[[The presentation]] adopts a useful [[statistical framework]] and [[derives performance result]]s that [[elucidate the mean-square stability]], [[convergence]], and [[steady-state behavior]] of the [[learning network]]s.
The presentation is largely self-contained and covers results that relate to the [[analysis]] and [[design of [[multi-agent network]]s for the [[distributed solution]] of [[optimization]], [[adaptation]], and [[learning problem]]s from [[streaming data]] through [[localized interaction]]s among [[agent]]s.
The presented [[algorithm]] also stands out in that it is [[deterministic]], simple to [[implement]] and [[elementary]] to [[prove]] .
The [[presented approach]] [[StrAP]] is applied to the [[stream]] of [[jobs]] submitted to the [[EGEE Grid]], providing an understandable [[description]] of the [[job]] [[flow]] and enabling the [[system administrator]] to [[spot]] [[online]] some [[source]]s of [[failures]] .
The [[presented model]] has been deployed at [[IBM]], [[targeting]] for aiding the [[intelligent management]] of [[smart grid]] .
[[The present paper]] addresses some of the many possible uses of [[citations]], including [[bookmark]], [[intellectual heritage]], [[impact tracker]], and [[self-serving purposes]] .
The [[present paper]] will leave the [[problem]] of the [[interpretation]] of [[norm codifications aside]] .
The present research explores the psychology of "[[botsourcing]]" — the [[replacement]] of [[human job]]s by [[robot]]s — while examining how understanding botsourcing can inform the [[psychology of]] [[outsourcing]] — the [[replacement of jobs]] in one [[country]] by [[human]]s from other [[countri]]es.
The primary [[reason]] is that [[on-field practitioner]]s are required to be both [[technically proficient]] and [[knowledgeable]] about the [[business]] .
The [[primitive]]s are typically [[classes (or set)s]], [[attributes (or properties)]], and [[relation]]s among [[class member]]s ([[Gruber 2009]]).
The [[principle of [[maximum likelihood estimation]] (MLE)]], originally developed by [[R.A.
The [[privacy mechanism]] has a [[profound effect]] on the [[performance]] of [[the]] [[methods]] chosen by the [[data miner]] .
[[The problem]] is [[nontrivial]], because [[signal]]s, which may help to identify a [[user's location]], are [[scarce and noisy]] .
The [[problem of bounding]] [[sequential pattern]]s is very challenging in [[theory]] due to the [[combinatorial complexity]] of [[sequence]]s, even given some [[inspiring result]]s on [[bounding itemset]]s in [[frequent itemset mining]] .
The [[problem]] of [[efficiently]] [[finding the best match]] for a [[query]] in a given [[set]] with respect to the [[Euclidean distance]] or the [[cosine]] [[similarity]] has been extensively [[studied]] .
The [[problem]] of [[ethnicity identification]] from [[names]] has a variety of important [[applications]], including [[biomedical research]], [[demographic]] [[studies]], and [[marketing]] .
The [[problem]] of finding an [[alternative]] to a given [[original]] [[clustering]] has received little attention in the [[literature]] .
The problem of [[large-scale]] [[online]] [[matrix completion]] is addressed via a [[Bayesian approach]] .
The [[problem]] of [[point of interest (POI)]] [[recommendation]] is to provide [[personalized recommendation]]s of [[places of interest]]s, such as [[restaurant]]s, for [[mobile user]]s.
The [[problem]] of [[time series classification (TSC)]], where [[we]] consider any [[real-valued ordered data]] a [[time series]], presents a specific [[machine learning]] [[challenge]] as the [[ordering of variable]]s is often crucial in finding the [[best discriminating feature]]s.
The [[problem spans]] entire [[sectors]], from [[scientists]] to [[intelligence]] [[analysts]] and [[web users]], all of whom are constantly struggling to keep up with the larger and [[larger amounts]] of [[content published]] every [[day]] .
The [[procedural nature]] of our [[formal representation]] allows a [[robot]] to [[interpret]] [[route instructions]] online while moving through a previously unknown [[environment]] .
[[The procedure]] computes a [[covering]], in terms of [[regular itemsets]], of the [[frequent itemsets]] in the [[class of equivalence]] of a [[closed one]] .
The [[process of identifying term]]s is [[analysed]] through three steps: [[term recognition]], [[term classification]], and [[term mapping]] .
The [[process]] of [[rank aggregation]] is [[intimately intertwined]] with the [[structure]] of [[skew symmetric matrice]]s.
The process used to implement [[the model]] consists of three [[sub-task]]s: [[entity annotation]], [[concept mapping]] and [[relation classification]] .
The program - dubbed [[SCIgen]] - soon found [[user]]s across [[the globe]], and before long its [[automatically generated creation]]s were being accepted by [[scientific conference]]s and [[published]] in purportedly [[peer-reviewed journal]]s.
[[The project]]'s key features are (a) a commitment to [[corpus evidence]] for [[semantic]] and [[syntactic generalization]]s, and (b) the [[representation]] of the [[valence]]s of its [[target word]]s (mostly [[noun]]s, [[adjective]]s, and [[verb]]s) in which the [[semantic portion]] makes use of [[frame semantics]] .
The [[proliferation]] of [[online social network]]s, and the [[concomitant accumulation]] of [[user data]], give rise to hotly [[debated issue]]s of [[privacy]], [[security]], and [[control]] .
The promising [[experiment result]]s show the [[effectiveness]] of [[the proposed framework]] and [[algorithm]]s.
The [[proportion]] of [[human deaths]] [[phylogenetically predicted]] to be caused by [[interpersonal violence]] stood at 2%.
[[The proposed algorithm]] can rapidly switch between different [[sets of active sensor]]s when the [[model detect]]s the ([[periodic]] or [[intermittent]]) change in [[the system state]] .
[[The proposed algorithm]] enables us to deal with [[clustering dynamics]] including [[merging]], [[splitting]], [[emergence]], [[disappearance of cluster]]s from a unifying view of the [[MDL principle]] .
The [[proposed algorithm]] encompasses two [[components]]: 1) an [[algorithm for detecting communities]] in a [[social network]] by taking into account [[information diffusion]]; and 2) a [[dynamic programming algorithm]] for [[selecting communities]] to find [[influential nodes]] .
[[The proposed algorithm]] generalizes several recently [[proposed method]]s for computation of [[NMF]] with [[Bregman divergence]]s and is [[computationally faster]] than existing [[alternative]]s.
The [[proposed algorithm]] mines two forms of [[knowledge]]: [[must-link]] (meaning that two [[word]]s should be in the same [[topic]]) and [[cannot-link]] (meaning that two [[word]]s should not be in the same [[topic]]).
[[The proposed algorithm]]s may be seen as [[parallel optimization procedure]]s for constructing [[sparse non-negative factorization]]s of [[large]], [[sparse matrice]]s.
[[The proposed approach]] is designed 1) to [[group documents]] into a [[set of clusters]] while the [[number of]] [[document clusters]] is determined by the [[Dirichlet process mixture model]] automatically; 2) to identify the [[discriminative words]] and separate them from [[irrelevant]] [[noise]] [[words]] via [[stochastic search variable selection technique]] .
The proposed [[approach]] is evaluated on a variety of [[benchmark datasets]] using the [[RapidMiner]] [[environment]] for [[machine learning]] and [[data mining]] processes.
[[The proposed approach]] [[learn]]s a [[topic model]] over a [[corpus]] of [[document]]s, and computes a [[distance matrix]] between [[pair]]s of [[topics]] using [[measure]]s such as [[topic co-occurrence]] .
The proposed [[clinical warning system]] is currently under [[integration]] with the [[electronic medical record system]] at [[B arnes-Jewish Hospital]] in [[preparation]] for a [[clinical trial]] .
[[The proposed collapsed algorithm]] is [[observed]] to [[outperform]] [[baseline approach]]es such as [[mixture of expert]]s.
The proposed [[data mining ontology}ontology]], named [[OntoDM]], is based on a recent proposal of a [[general framework]] for [[data mining]], and includes definitions of [[basic data mining entiti]]es, such as [[datatype]] and [[dataset]], [[data mining task]], [[data mining algorithm]] and [[component]]s thereof (e.g., [[distance function]]), etc. [[It]] also allows for the [[definition of]] more [[complex]] [[entiti]]es, e.g., constraints in [[constraint-based data mining]], sets of such constraints ([[inductive queri]]es) and [[data mining scenario]]s ([[sequences of inductive queries]]).
[[The proposed deep structured semantic models]] are [[discriminatively trained]] by [[maximizing the conditional likelihood]] of the [[clicked document]]s given a [[query]] using the [[clickthrough data]] .
[[The proposed formulation]] encourages [[smoothness]] between [[temporally adjacent block]]s by employing the [[fused Lasso type]] of [[regularization]] .
[[The proposed framework]] also has [[advantage]]s over existing [[approach]]es by reducing [[computational cost]]s with an [[efficient density estimator]], promoting the [[change-score calculation]] by introducing [[effective divergence metric]]s, and by [[minimizing]] the [[effort]]s required from [[user]]s on the [[threshold parameter setting]] by using the [[Page-Hinkley test]] .
[[The proposed framework]] can be motivated using [[concept]]s from [[information theory]] and [[game theory]] .
[[The proposed goodness measure]] intuitively captures both (a) the [[relevance]] between each individual [[node]] in the [[ranking list]] and the [[query]]; and (b) the [[diversity]] among different [[node]]s in the [[ranking list]] .
The proposed [[kernel]]s can naturally handle [[time series]] of different [[length]] without the need to specify a [[parametric]] [[model class]] for the [[time series]] .
[[The proposed method]] analyses a [[complex]] and formerly [[impenetrable]] [[volume of data]] from [[road]]s and [[crash]]es using [[data mining]] .
The proposed [[method]] can [[quantify]] [[causality]] even from a [[small number of samples]] .
The proposed [[method]] can [[represent]] the [[strength]] of [[causality]] as the [[number]] of [[bits]], whether each of two [[time series]] is [[symbolic]] or [[numerical]] .
The [[proposed method]] is [[collusion-resistant]] and [[guarantees]] that an [[attacker]] will be unable to [[breach]] a [[honest participant's]] [[Anonymity Measureanonymity]] unless she controls all [[N-1 participants]] .
The proposed [[method]] [[learn]]s a [[factor analysis (FA) model]] for [[large matrice]]s, based on a [[small number]] of [[observed matrix element]]s, and leverages the [[statistical model]] to [[actively select]] which new [[matrix entries]] / [[observation]]s would be most [[informative]] if they could be acquired, to improve the [[model]]; the model [[inference]] and [[active learning]] are performed in an [[online setting]] .
[[The proposed method]] provides not only good [[performance]] in [[application]]s, but also [[scalable]], [[automatic model selection]], which greatly reduces the [[intensive]] [[modeling cost]]s in [[data mining process]]es.
The [[proposed method]]s combine [[feature grouping]] and [[feature selection]] to enhance [[estimation accuracy]] .
The [[proposed model]] facilitates [[controlled experiments]] which study the [[effect]] of actors' [[behavior]] on the [[evolution]] of [[affiliation networks]], and it allows the [[generation]] of realistic [[synthetic datasets]] .
[[The proposed model]] significantly improves performance over a [[model]] that does not [[share parameter]]s among [[textual relation]]s with common [[sub-structure]] .
[[The proposed model]] uses [[dimensionality reduction]] for preprocessing the [[Likes data]], which are then entered into [[logistic/linear regression]] to [[predict individual [[psychodemographic profile]]s from [[Like]]s.
[[The proposed objective]] is an [[NP-hard]] integer [[programming optimization problem]] .
[[The proposed problem]] of [[coupled link prediction]] and the [[corresponding framework]] demonstrate both the [[scientific]] and [[business application]]s in [[biology]] and [[social network]]s.
[[The proposed RankGAN]] is [[optimized]] through the [[policy gradient technique]] .
The proposed [[recommendation scheme]] offers several advantages in addressing the [[grocery shopping problem]], namely : 1) a [[product]] [[similarity measure]] that suits a [[domain]] where no [[rating information]] is available; 2) a [[basket sensitive random walk]] [[model]] to [[approximate]] [[product]] [[similarities]] by exploiting incomplete [[neighborhood]] [[information]]; 3) [[online]] adaptation of the [[recommendation]] based on the current [[basket]] and 4) a new [[performance measure]] focusing on [[product]]s that [[customer]]s have not [[purchase]]d before or purchase infrequently.
[[The proposed revision method]]s are [[invariant]] across different [[global justificatory structure]]s ([[foundationalist]], [[coherentist]], etc.).
[[The proposed RNN Encoder–Decoder]] with a novel [[hidden unit]] is [[empirically evaluated]] on the task of [[translating]] from [[English]] to [[French]] .
[[The proposed sampler]] is [[context-dependent]] and [[oversample]]s [[informative pair]]s to [[speed up]] [[convergence]] .
The [[proposed scheme]] is [[stronger]] in [[security]] than existing [[technique]]s, and [[incurs]] very little [[redundant communication]] and [[computation cost]] .
The [[proposed solution]] first [[probabilistic]]ally generalizes the [[raw data]] and then adds [[noise]] to [[guarantee]] [[â-differential privacy]] .
The [[proposed system]] allows [[observing]] many [[space related aspect]]s of [[tweet message]]s including the [[average scope]] of <i>[[spatial attention]]</i> of [[social media]] [[user]]s and [[variance]]s in [[spatial interest]] over [[time]] .
The [[proposed technique]]s first [[identify]] those [[sub-topic]]s or [[salient concept]]s of the [[topic]], and then [[find]] and [[organize]] those [[informative]] [[page]]s, containing [[definition]]s and [[description]]s of the [[topic]] and [[sub-topic]]s, just like those in a [[book]] .
[[The proposed theory]] of [[just consequentialism]] emphasizes [[consequences of policies]] within the [[constraints of justice]] .
The [[protocol]] employs [[cryptographic]] and [[random shuffling technique]]s to preserve [[participants' anonymity]] .
The public includes [[taxpayer]]s, [[policy-maker]]s, [[investor]]s and those who could [[benefit]] from the [[technology]] .
The purpose of [[electronic institutions (e-Institutions)]] is to [[guarantee]] the [[overall behavior]] of an [[open]] [[multi-agent system (MAS)]] to exhibit [[desired properti]]es without [[compromising]] the [[agents’ autonomy]] .
The purpose of [[this chart]] is to provide a uniform format for [[local government financial reporting]] and [[accounting]], allowing [[state agenci]]es to collect more [[reliable]] and [[meaningful financial data]] and [[information]] from [[local government]]s in the [[state]] .
The purpose of [[this research]] is to design a [[document retrieval model]] which is a [[structural model]] based on [[case relations]] and to [[test]] how effectively a [[prototype]] of this [[model]] would perform [[retrieval]] on a [[test database]] .
The purpose of [[this study]] is to leverage [[modern technology]] ([[mobile]] or [[web app]]s) to enrich [[epidemiology data]] and [[infer the transmission of disease]] .
The [[PY topic model]] captures two [[properties of a document]]; a [[power-law word distribution]] and the presence of [[multiple]] [[topic]]s.
The [[qualitative analysis]] shows that the [[RNN Encoder–Decoder]] is better at [[capturing]] the [[linguistic regulariti]]es in the [[phrase table]], indirectly explaining the [[quantitative improvement]]s in the overall [[translation performance]] .
The question is a simple one: if in the future [[robot]]s take most [[people]]'s [[job]]s, how will [[human being]]s [[eat]]?
The question [[this dissertation]] attempts to answer is: what kind of [[semantic information]] does the [[word-space model]] [[acquire]] and [[represent]]?
The [[RAE]] tries to [[lower reconstruction error]] of not only the [[bigram]]s but also of [[node]]s higher in the [[tree]] .
The [[randomization methods]] can be used in [[iterative]] [[data mining]] .
The [[random variable]]s <math>X</math> and <math>Y</math> are [[jointly distributed]], but in a [[discriminative framework]] [[we]] construct a [[conditional model]] <math>p(Y\vert X)</math> from [[paired]] [[observation]] and [[label sequence]]s, and do not [[explicitly model]] the [[marginal]] <math>p(X)</math>.
The [[ranking algorithm]] uses the [[latent factor]] in [[LAA]] to [[rank target document]]s by the strength of their [[semantic association]]s with the [[source document]] .
The [[rank of a given web page]] in this context is the [[fraction]] of [[time]] that the [[random]] [[web surfer]] would spend at the [[page]] if the [[random process]] were [[iterated]] ad infinitum.
The rapidly increasing availability of [[electronic health records (EHRs)]] from multiple [[heterogeneous source]]s has spearheaded the adoption of [[data-driven approach]]es for improved [[clinical research]], [[decision making]], [[prognosis]], and [[patient management]] .
[[Thereafter]], we can [[code]] any new [[document]] as a [[sparse non-negative linear combination]] of [[user label]]s, where we encourage [[correlated label]]s to appear together in the [[output]] via a [[structured sparsity penalty]] .
There are currently three broad classes of [[VSMs]], based on [[term-document]], [[word-context]], and [[pair-pattern matrice]]s, yielding three classes of [[applications]] .
There are defined [[standard class]]es, [[facet]]s, and [[slot]]s with specified names and [[semantic]]s expressing frequently used [[entities]] .
There are few [[concerted effort]]s to [[promote]] [[robotics]] and [[AI research]] in the [[public sphere]] .
There are fundamentally three [[categori]]es of [[data model]]s: [[relational]], [[key value]], and [[hierarchical]] .
There are many [[approaches to the simplification task]], including: [[lexical]], [[syntactic]], [[statistical machine translation]] and [[hybrid technique]]s.
There are many [[method]]s for [[item recommendation]] from [[implicit feedback]] like [[matrix factorization (MF)]] or [[adaptive k-nearest-neighbor (kNN)]] .
There are nevertheless ways [[democracy]] can regain control over [[capitalism]] and ensure that the [[general interest]] takes precedence over [[private interest]]s, while preserving [[economic openness]] and avoiding [[protectionist]] and [[nationalist reaction]]s.
There are no <i>[[golden standard]]s</i> for assessing [[performance]] in [[non-stationary environment]]s.
There are now 100s of [[Ripple-Down-Rule knowledge]] bases in use and [[this paper]] presents some [[observation]]s from [[log files tracking]] how [[people]] build these [[system]]s, and also [[outline]]s some recent [[research]] on how such [[technique]]s can be used to add greater [[specificity]] to the [[simpler model]]s developed by [[automated technique]]s.
There are [[public domain librari]]es that are frequently used for [[machine learning]], among them [[WEKA]] [11] for [[general machine learning]] and [[MALLET]][http://mallet.cs.umass.edu/index.php/Main_Page]for [[CRFs]] .
There are striking differences in [[economic prosperity]] among [[state]]s and [[region]]s within virtually every [[nation]], and some of the reasons may be related to [[attitude]]s, [[value]]s, and [[belief]]s.
There are three [[challenges]] for the [[task]] : finding suitable [[data source]], coping with the [[ambiguities]] of [[named entity classes]], and incorporating necessary [[human supervision]] into the [[mining process]] .
There are three [[collection]] types: [[set]], [[bag]] (unordered, multiple occurrences permitted), and [[list]] (ordered bag).
There are two major issues for [[this approach]]: (1) the [[coverage]] of the [[ontology]] is limited, even for [[WordNet]] or [[Mesh]], (2) using [[ontology]] terms as replacement or additional [[features]] may cause [[information loss]], or introduce [[noise]] .
There are two ways to [[measure]] [[occupational growth]] or [[decline]]: [[numeric change]] ([[projected-year employment]] minus [[base-year employment]]) and [[percent change]] ([[numeric change]] divided by [[base-year employment]]).
Thereby [[embedding]] reveals the major [[cluster-specific]] [[mixed-type attribute dependenci]]es.
Thereby, [[MLGF-MF]] [[overlap]]s the [[CPU]] and [[I/O processing]], which eventually [[offset]]s the [[I/O cost]] and [[maximizes the CPU utility]] .
Thereby, [[MLGF-MF]] produces [[reliable result]]s efficiently even on [[skewed matrice]]s.
The recent abundance of [[unstructured]] [[business data]] has thrown up new opportunities for [[analytics]], but has also multiplied the [[deployment]] challenge, since [[interpretation]] of [[concepts]] derived from [[textual]] [[source]]s require a deep [[understanding]] of the [[business]] .
The recent explosion in the adoption of [[search engine]]s and [[new media]] such as [[blog]]s and [[Twitter]] have facilitated faster [[propagation]] of [[news]] and [[rumor]]s.
The recent success of [[distributed]] and [[dynamic infrastructure]]s for [[knowledge sharing]] has raised the need for [[semiautomatic/automatic]] [[ontology evolution strategi]] es.
The [[re-finement algorithm]] tries to [[find out]] a [[workflow]] that can interpret all [[event trace]]s with minimal [[state transitions]] and [[threads]] .
[[Therefore]], an [[outlier]] often contains useful [[information about abnormal characteristic]]s of [[the system]]s and [[entiti]]es, which impact the [[data generation process]] .
Therefore, [[determining]] and [[localizing]] [[user-impacting availability]] and [[performance issue]]s in the [[network]] in near [[real time]] is crucial.
Therefore, [[effective]] [[team selection]] requires the team [[member]]s to be <i>[[socially close]]</i> as well as a [[division]] of the [[task]] among [[team member]]s so that no [[user]] is overloaded by the [[assignment]] .
Therefore, [[evaluating]] the [[stopping condition]] is [[fast]], and does not require an [[expensive mining]] of [[each sample]] .
Therefore, [[FaitCrowd]] demonstrates better ability to obtain [[true answer]]s for the [[question]]s compared with existing [[approach]]es.
Therefore, in [[this paper]] [[we]] propose, instead of [[reporting all frequent items]], to only [[mine]] the [[top-<math>k</math> most frequent ones]] .
Therefore, it is critical to consider the [[relative cost]] of [[search]] versus [[labeling]], and [[we]] demonstrate the [[tradeoffs]] for different [[relative costs]] .
Therefore, it is important to [[consolidate]] different [[concepts]] for [[intelligent]] [[decision making]] .
Therefore, it is important to deal with [[symbolic]] and [[numerical]] [[time series]] seamlessly when attempting to [[detect]] [[causality]] .
Therefore it is valuable to [[study]] the [[collective behavior]] of [[individual]]s and [[detect group anomali]]es.
Therefore, [[PLSA]] finds a [[topic distribution]] for each [[concept definition]] that [[maximize]]s the [[log likelihood]] of the corpus X ([[LDA]] has a similar form):
Therefore these [[technique]]s benefit from taking into account many [[syntactic]] and [[semantic features]], especially [[parse trees]] [[generated by]] [[automatic sentence parsers]] .
Therefore, [[this paper]] proposes [[Socio-Spatial Group Query (SSGQ)]] to [[select]] a [[group of nearby attendee]]s with [[tight social relation]] .
Therefore, [[we]] are forced to resort to [[approximate]] and [[heuristic technique]]s to get practical [[solution]]s.
Therefore, [[we]] need (1) a [[set]] of robust [[detector]]s for detecting [[semantically meaningful feature]]s such as [[boundari]]es, [[shape]]s, etc. in [[image]]s, [[word]]s, [[sentence]]s, etc. in [[text]], and (2) a [[set]] of [[method]]s that can effectively [[analyze]] and [[exploit]] the [[information structure]]s that [[encode]] the [[content of information]] .
Therefore, [[we]] propose a novel [[ranking algorithm]] on the [[data manifold]]s, called [[Parallel Field Ranking]] .
Therefore, [[we]] propose a novel [[two-step procedure]] to [[calculate]] [[information consistency degree]]s for a [[set of item]]s which are rated by multiple [[set]]s of [[user]]s on different [[platform]]s.
Therefore, [[we]] propose a [[real-time]] [[bidding algorithm]] that enables [[fine-grained impression valuation]] (e.g., [[targeting user]]s with [[real-time]] [[conversion data]]), and djusts [[value-based]] [[bid]]s according to [[real-time]] [[constraint snapshot]]s (e.g., [[budget consumption level]]s).
The [[regularization]] [[encourage]]s the [[graphical model]] to exhibit a [[brain-like structure]] .
The [[regularizer consist]]s of two [[component]]s including an [[L2]], [[1-norm penalty]] on the [[regression weight vector]]s, which ensures that a small [[subset]] of [[feature]]s will be selected for the [[regression model]]s at all [[time point]]s, and a [[temporal smoothness term]] which ensures a [[small deviation]] between two [[regression model]]s at successive time [[point]]s.
There has been considerable [[work]] on [[user browsing models]] for [[search engine results]], both [[organic]] and [[sponsored]] .
There has recently been a great deal of work focused on developing [[statistical model]]s of [[graph structure]] --- with the goal of [[modeli]]ng [[probability distribution]]s over [[graph]]s from which new, <i>[[similar graph]]s</i> can be [[generated by]] [[sampling]] from the [[estimated distribution]]s.
There is an increasing amount of work on [[model selection]] and [[model combination]] in the [[machine learning]] and [[data mining literature]]: for instance, [[model selection]] based on [[ROC space]] ([[Provost and Fawcett, 2001]]), model combination by means of [[bagging]] ([[Breiman, 1996]]), [[boosting]] ([[Freund and Schapire, 1996]]), [[arcing]] ([[Breiman, 1998]]), the [[mixture of experts method]] ([[Jacobs et al., 1991]]), to name just a few.
There is a palpable increase in [[machine intelligence]] across the [[touch point]]s of our lives, driven by the [[proliferation]] of [[data feeding]] into [[intelligent algorithm]]s capable of [[learning]] useful [[pattern]]s and [[acting]] on them.
There is often not enough [[time]] for someone to [[write]] a [[book]] on such [[topic]]s.
The [[relevancy]] and [[interestingness]] are scored by [[machine learned model]]s.
The remaining [[sensor]]s collect the [[data]] passively at low [[sampling rate]]s in order to [[detect]] any changing [[trend]]s in the underlying [[data]] .
The [[reported evaluation]] was carried out on a [[set of]] [[700]] [[entity pair]]s, [[manually classified]] to one or more of the [[semantic relation]]s we defined.
The research area of [[metabolomics]] is growing fast due to an enormous improvement of [[analytical technology]] as [[LCMS]], [[GCMS]] and [[NMR]] ([[Bollard et al. 2005]]; [[Van Der Greef and Smilde 2005]]).
The [[Resource Description Framework (RDF)]] enables the [[creation]] and [[exchange]] of [[metadata]] as [[normal Web data]] .
The result, called [[SCRIBE]], is a [[large-scale semantic model]] for [[Smarter Citi]]es based on [[data gathered]] from [[cities worldwide]] .
The resulting [[classifiers]] obtain [[95-99%]] [[accuracy]], [[detecting]] [[large]] [[numbers]] of [[malicious Web sites]] from their [[URLs]], with only [[modest]] [[false positives]] .
[[The resulting database]] will contain (a) [[descriptions of]] the [[semantic frame]]s underlying the [[meaning]]s of the [[word]]s [[described]], and (b) the [[valence representation]] ([[semantic]] and [[syntactic]]) of several thousand [[word]]s and [[phrase]]s, each accompanied by (c) a representative [[collection of annotated corpus attestation]]s, which jointly exemplify the [[observed linking]]s between "[[frame elements]]" and their [[syntactic realization]]s (e.g. [[grammatical function]], [[phrase type]], and other [[syntactic trait]]s).
The resulting [[data flows]] can be used to [[document]], [[deploy]], and [[reuse]] the resulting [[data generators]] .
The resulting [[knowledge base]] can support a [[number of task]]s, such as: richer [[semantic search]], which can exploit the [[technology dimension]] to support better [[retrieval of publication]]s; richer [[expert search]]; monitoring the [[emergence]] and [[impact]] of [[new technologi]]es, both [[within and across]] [[scientific field]]s; studying the [[scholarly dynamic]]s associated with the [[emergence of]] [[new technologi]]es; and others.
The resulting [[link detector]] and [[disambiguator]] performs very well, with [[recall]] and [[precision]] of almost 75%.
The [[resulting model]] has the advantage that [[its parameter]]s, including the [[mixture]] of [[topic]]s of each [[document]] and the resulting [[overlapping communiti]]es, can be inferred with a [[simple]] and [[scalable]] [[expectation-maximization algorithm]] .
The resulting [[model]]s achieve significantly [[higher prediction accuracy]] than [[PMF models trained]] using [[MAP estimation]] .
The resulting [[models]] cater for (1) [[multiple]] [[sequence]]s from a [[group of people]], (2) [[interactions]] among [[them]], (3) [[sequence item]] [[properties]], and (4) significant [[change]] among [[coupled]] [[sequence]]s.
The resulting [[model]]s, which we call [[GA<sup>2</sup>{M}$-model]]s, for <i>[[Generalized Additive Models plus Interaction]]s </i>, consist of [[univariate term]]s and a [[small number]] of [[pairwise interaction term]]s.
The [[resulting model]] tells which [[instance]]s to [[label]] so that by the end of each [[budget period]], the [[budget]] is best spent (in [[expectation]]).
The resulting [[operator]] can be [[parallelized]] with a [[small number]] of [[core]]s, leading to a [[parallel implementation]] that does not suffer any of the [[overhead]]s of existing [[parallel solution]]s when [[querying]] [[the results]] and offers significantly [[higher throughput]] .
[[The resulting system]] achieves 93% [[precision]] at substantial overall [[coverage]] .
The resulting [[system]], [[SDOI]], is [[tested]] on a novel [[corpus]] and [[ontology]] from the [[data mining field]] on [[intrinsic measures]] such as [[accuracy]], and [[extrinsic measures]] such time saved by the [[annotator]] in the [[annotation process]] .
The [[result]] is [[a system]] that matches the [[speedups reported]] for [[MPP analytic database]]s over [[MapReduce]], while offering [[fault tolerance properti]]es and [[complex analytics capabiliti]]es that they lack.
The result is both a [[reduction]] in time it takes for the [[auditor]]s to [[review]] and [[label claim]]s as well as improving the [[precision]] of [[the system]] in [[finding payment error]]s.
The [[result]]s also suggest that the [[predictability]] of [[human mobility]] is [[time-varying]], e.g., the [[overall predictability]] is higher on [[workday]]s than [[holiday]]s while predicting [[users' unvisited location]]s is more challenging for [[workday]]s than [[holiday]]s.
The [[results]] are [[evaluated]] against [[alerts]] [[generated by]] the [[exchange]]'s [[surveillance system]] from both [[technical]] and [[computational perspectives]] .
The [[result]]s clearly demonstrate that the proposed [[MuCH method]] significantly [[outperform]]s the [[state-of-art]] [[hashing method]]s especially on search [[efficiency]] .
The [[result]]s demonstrate the advantages of [[our method]] over [[baseline]]s, [[validating]] the [[contribution]] of its [[component]]s and finding [[interesting discoveries for the benefit]] of [[society]] .
The [[result]]s demonstrate the [[effectiveness]], [[efficiency]] and [[scalability]] of our [[method]] beyond [[baseline approach]]es.
The [[results]] demonstrate the [[scalability]] and [[capability]] of [[MetricForensics]] in [[analyzing]] [[volatile graphs]]; and highlight four novel [[phenomena in such graphs]]: [[elbows]], [[broken correlations]], [[prolonged spikes]], and [[lightweight stars]] .
The [[result]]s demonstrate the suitability of [[our approach]] and show the [[strength]] of taking both [[EP]] and [[IR]] into account in [[feature extraction]] .
The results for [[the advanced task]] ([[functional annotation]] from [[free text]]) were significantly lower, demonstrating the current [[limitation]]s of [[text-mining approach]]es where [[knowledge extrapolation]] and [[interpretation]] are required.
[[The results]] from a [[real-life NBA database]] highlight and reveal several [[expected]] and [[unexpected aspect]]s of the [[database]] .
The results [[generated by]] [[our framework]] can benefit a variety of [[application]]s, including [[urban planning]], [[location choosing]] for a [[business]], and [[social recommendation]]s.
The results indicate that for [[large corpora]] [[PMI]] works best on [[word similarity test]]s, and [[GLSA]] on [[synonymy test]]s.
The results indicate that the [[accuracy]] of [[disease mapping]] remains [[satisfactory]] when [[learning]] from [[movement data]] [[sanitized]] in this way.
[[The results]] indicate that the [[adaptation mechanism]] effectively [[optimizes]] [[level design parameter]]s for particular [[player]]s.
The [[results]] obtained on the [[ACE Relation Mention Detection task]] [[outperform]] in terms of [[F1 score]] by 5 [[points]] the [[state of the art]] of [[unsupervised technique]]s for this [[evaluation framework]], in addition to being [[simpler]] and more [[flexible]] .
The [[results]] of both [[simulation data]] and [[real]] [[program data]] show that [[our algorithm]] is highly [[effective]] .
The results of [[experiment]]s using multiple [[real dataset]]s verified the [[effectiveness]] of [[the proposed method]]s and showed [[orders of magnitude]] [[performance improvement]] against the [[baseline method]] .
The results of [[our model]] allow us to [[predict the probability of infection]] for each [[person]]s on each [[day]], and also to [[infer]] [[personal]] [[physical vulnerability]] and the relevant association with [[covariate]]s.
The [[result]]s on the [[real dataset]]s show that [[our approach]] [[outperform]]s the [[state-of-the-art]] [[tensor factorization method]]s.
[[The result]]s show how [[Mastery Grid]]s interacts with different [[factor]]s, like [[gender]] and [[achievement-goal orientation]], and ultimately, its impact on [[student engagement]], [[performance]], and [[motivation]] .
The [[result]]s show our [[performance model]]s estimate [[DNN training time]] with high [[estimation]] [[accuracy]] and our [[scalability optimizer]] correctly chooses the best [[configuration]]s, [[minimizing]] the [[training time]] of [[distributed DNN]]s.
The results show [[our proposed technique]]s perform well on the [[measure]]s of [[pairs completeness]] and [[reduction ratio]] compared to the [[baseline]] [[approach]]es, while also satisfying the [[block size]] [[restriction]]s.
[[The results]] show [[statistical presence]] of [[patterns relevant]] for both [[approach]]es.
The results show that [[multifocal learning]] can significantly boost the [[learning]] [[accuraci]]es of existing [[learning algorithm]]s, such as [[Support Vector Machines (SVMs)]], for [[classifying]] [[custormer]] [[problem]] .
The [[result]]s show that [[our method]]s are able to [[detect]] [[meaningful]] [[event]]s.
The [[result]]s show that [[RICK]] is both [[effective]] and [[efficient]] .
The results show that the [[learning]] and [[run-time]] [[curve]]s [[projected]] from only several [[data points]] can lead to a cheaper [[data mining]] [[process]] than the common [[progressive sampling method]]s.
The [[result]]s show that [[the proposed framework]] can not only [[model]] the [[workflow patterns effectively]], but also have [[managerial application]]s in [[workflow monitoring]], [[auditing]], and [[inspection]] of [[workflow compliance]], which are critical in the [[healthcare]] [[industry]] .
The [[result]]s show the advantages of our [[method]] over four [[categories of baseline]]s, including [[linear Gaussian interpolation]]s, [[classical]] [[dispersion model]]s, well-known [[classification model]]s like [[decision tree]] and [[CRF]], and [[ANN]] .
The [[result]]s show the superiority of [[LCARS]] in [[recommending spatial item]]s for [[user]]s, especially when traveling to new [[citi]]es, in terms of both [[effectiveness]] and [[efficiency]] .
The [[result]]s were presented in a workshop held in [[Granada, Spain]] [[March 28-31]], [[2004]] .
There were no [[gender difference]]s in [[problem solving]] in [[elementary]] or [[middle school]]; differences favoring men emerged in [[high school]] and [[college]] .
The [[RICK]] comprises two [[component]]s: <i>[[routable graph construction]]</i> and <i>[[route inference]] </i>.
The [[RNN]] is trained with [[backpropagation]] to maximize the [[data log-likelihood]] under the [[model]] .
The [[robustness]] and [[generalizability]] of [[SEC]] are then investigated to prove the superiority of [[SEC]] in [[theory]] .
The [[Roomba]], however, is a simple beast that [[moves randomly]], [[sense]]s only its [[immediate obstacle]]s and can get [[trapped]] in [[clutter]] .
The [[root cause]]s behind these [[puzzling result]]s are not [[isolated incident]]s; these [[issue]]s generalized to multiple [[experiment]]s.
[[The ROPSM model]] supports [[mining]] more reasonable [[noise-corrupted]] [[OPSM patterns]] than another [[well-known model]] called [[AOPC (approximate order-preserving cluster)]] .
The [[RP algorithm]] receives as [[input]] a [[set]] of <math>n</math> [[data point]]s, <math>D_t=\{\langle \mathbf{x_i},y_i\rangle\}^{n_t}_{i=1}</math>, and if certain [[termination criteria]] are not met it generates a [[test node]] <math>t</math>, whose [[branches]] are obtained by applying the same [[algorithm]] with two [[subset]]s of the [[input]] [[data point]]s.
The [[running time]] of [[our algorithm]] is [[data-dependent]], but large [[experiment]]s over real [[genomic data]] suggest that [[it scales empirically]] as n<sup>3/2</sup>.
The same [[website]] also [[host]]s the [[dataset]]s used in [[this work]] that are not available elsewhere in the same [[preprocessing]] used for [[our experiment]]s.
The [[scalability]] of [[influence maximization]] is a [[key factor]] for enabling prevalent [[viral marketing]] in [[large-scale]] [[online social networks]] .
[[The scale]] and [[complexity of data]] is beyond the [[analytic power]] of traditional [[IT infrastructure]]s.
The [['scare' of scarcity]] has led to [[scarcity]] emerging as a [[political strategy]] for powerful [[group]]s.
These [[account number]]s and [[description]]s also are used in the [[Department of Transportation (DOT)]] [[Form MS-903]], [[Treasurer Account Book]], [[Form MS-965]], [[Actual Use Report of State Fund]]s, and [[Form MS-991]], [[County Liquid Fuels Tax Fund Report]] .
These [[activities]] occupy the majority of [[employees]]' [[time in work]], and thus, provide a [[high quality]] [[approximation]] to the real [[social connections]] of [[employees]] in the [[workplace context]] .
These [[agent]]s are considered to be [[autonomous entiti]]es such as [[software program]]s or [[robot]]s.
These [[algorithm]]s can be implemented in [[distributed computation]] [[framework]]s such as [[MapReduce]] .
[[These algorithms]] typically require the [[text input]] to be [[represented]] as a [[fixed-length vector]] .
These [[alternative name]]s include [[synonym]]s, [[abbreviation]]s, and [[acronym]]s for the [[attribute]] .
These and [[our approach]] differ in how to define the [[word sense]], i.e., a [[set of synonym]]s in the same [[language]] versus a set of [[translation]] [[equivalents]] in another [[language]] .
These [[approaches]] are based on [[model]]ing the [[data]] by a [[collection]] of localized [[models]] learnt while simultaneously [[partitioning]] (co-[[clustering]]) the [[data]] .
The [[search engine]] receives millions of [[sales offer]]s from [[thousand]]s of [[independent merchant]]s that must be [[matched]] to the right [[product]]s.
These are aimed at [[extracting]] [[sequential pattern]]s where each [[transition]] between two [[events]] is [[annotated]] with a typical [[transition time]] that emerges from [[input data]] .
These areas include [[text processing]] of [[internet documents]], [[gene expression array analysis]], and [[combinatorial chemistry]] .
These are [[either-or situation]]s, not choices of [[degree]] or [[quantity]] .
These are, for instance, [[folksonomi]]es, [[database schema]]s, [[UML model]]s, directories, thesauri, [[XML schema]]s and [[formal ontologies (axiomatised theories)]] .
These are important problems with [[application]]s to [[influence analysis]], [[epidemiology]] and [[viral marketing application]]s.
These are largely achieved by [[automating]] [[routine maintenance procedure]]s, including [[problem detection]], [[determination]] and [[resolution]] .
These are names of persons: [[Alan Turing]], [[Alonzo Church]], [[Charles Kay Ogden]], [[Christopher Alexander]], [[Eric Raymond]], [[Fred Brooks]], [[John Horton Conway]], [[James Gosling]], [[Bill Joy]], and [[Dick Gabriel]] .
These are [[principle]]s that we [[believe]] have [[broad applicability]] in [[web optimization]] and [[analytic]]s outside of [[controlled experiment]]s, yet they are not [[provably correct]], and in some [[cases exception]]s are known.
These are the [[contribution]]s of [[this paper]] (a) the [[problem definition]], which is novel even in [[epidemiology literature]] (b) the [[phase-transition]] result and (c) [[experiment]]s on [[real data]], illustrating the suitability of our [[result]]s.
These [[characteristic]]s cause severe [[data sparsity]] and [[computational complexity problem]], which pose great challenge to [[human behavioral analysis]] and [[prediction]] .
These [[classifier]]s - the [[best-known example]] of which is the [[support vector machine (SVM)]] ([[Boser et al., 1992]]) - have proved extremely successful at [[binary classification task]]s ([[Vapnik, 1998]], [[Evgeniou et al., 2000]], [[Rifkin, 2002]]).
These concerns are driven by a host of [[new technologi]]es that [[automate physical tasks]] ([[robotics]]), [[intellectual tasks]] ([[cognitive computing]]), and [[customer service tasks]] (everything from [[self-help kiosk]]s to [[grocery store scanner]]s).
The second challenge is to tackle the [[combinatoric]]s of [[personalized]], [[targeted]], [[combination therapy]] in [[cancer]] .
The second challenge lies in the [[algorithmic aspect]] - how to find an [[optimal]], or [[near-optimal]], [[top-k ranking list]] that [[maximize]]s the [[measure]] we defined in a [[scalable]] way?
The second [[component]], “[[labor share]],” is the [[share]] of [[output]] accounted for by [[employees’ compensation]] .
[[The second experiment]] is a [[collaborative-filtering task]] for making movie [[recommendation]]s.
The second [[formulation]] captures [[compactness]] using a [[minimum-distance tree]] .
These conditions and [[delay time]]s are [[propagate]]d as [[configuration]]s into [[run-time monitoring system]]s.
::The [[second law]] states that if the [[physical process is irreversible]], the combined [[entropy]] of [[the system]] and the [[environment]] must [[increase]] .
The second [[problem]] involves [[opinion mining]] from [[review sites]] - the [[advertiser]] wants to [[detect]] and [[avoid]] [[negative opinion]] about their [[product]] when [[positive]], [[negative]] and [[neutral sentiments]] co-exist on a [[page]] .
The second [[problem]] is that of [[computing similarities]] at [[query time]] when [[graph]] is too [[large]] to be [[memory-resident]] .
The second stage takes the output from the first and [[label]]s all the [[edge]]s in the [[dependency graph]] with appropriate [[syntactic categori]]es using a [[globally trained]] [[sequence classifier]] over [[components]] of the [[graph]] .
These [[context]]s can be either [[positive]] ([[observed]]) or [[negative]] (often [[randomly sampled]]).
The [[SEDASYS System]] is an investigational [[computer-assisted]]] [[personalized sedation system]] integrating [[propofol]] delivery with [[patient monitoring]] to enable [[endoscopist]] / [[nurse]] [[team]]s to safely administer propofol.
[[These definition]]s take into account that [[negotiation dialogue]]s involve not only [[agents' belief]]s (of various [[strengths)]], but also [[their goal]]s (having maybe different [[prioriti]]es), as well as the [[belief]]s about the goals of other [[agent]]s.
These enhancements require [[extension]]s of the standard [[Semantic Wiki architecture]] by a [[task ontology]] for [[problem-solving]] and an [[adapted]] [[reasoning process]] .
These existing works mainly focused on [[interpreting]] [[ad click]]s in terms of what [[users seek]] (i.e., [[relevance information]]) and how [[user]]s choose to [[click]] ([[historically clicked-through information]]).
[[These experiments]] show that [[GP-BUCB]] and [[GP-AUCB]] are competitive with [[state-of-the-art]] [[heuristic]]s.
These [[factor]]s affect not only the [[resident]]s and [[communiti]]es touched by [[concentrated]] [[disadvantage]], but also the [[region]]s they inhabit and the ability of those [[metro area]]s to grow in [[inclusive]] and [[sustainable way]]s.
These [[factors]] can seriously affect the [[precision]] of a [[social]] [[tag-based web retrieval system]] .
These [[feature]]s are then used to [[learn a model]] to [[grade the program]]s, which are built [[against evaluation]]s done by [[expert]]s.
These [[features]] are then used to [[train]] a [[classifier]] to [[detect]] [[failures]] .
These features may be different from standard, possibly many, [[product attribute]]s (e.g., [[size]], [[color]], [[price]]) considered [[a priori]], much like [[feature]]s used for [[perceptual map]]s, a [[technique for visualizing peoples' perception of products]] [1].
These [[feature]]s, which we call [[badge]]s, represent fundamental [[traits of user]]s (e.g., "[[vegetarian]]" or "[[Apple fanboy]]") [[infer]]red by [[modeling]] the [[interplay]] between a [[user's behavior]] and [[self-reported identity]] .
These findings are consistent with known [[pathology]] and [[clinical progression]] of [[AD]], and will contribute to [[AD]] [[knowledge discovery]] .
[[These findings]] suggest potential [[intervention]]s targeting [[interest]]s in [[STEM education]] to facilitate [[individuals' ability]] and [[career development]] and strategies to reform [[work environment]]s to better attract and [[retain]] [[women]] in [[STEM occupation]]s.
[[These function]]s are critical in [[key data analyse]]s in [[astronomy]] and [[materials science]], among other [[field]]s, for example to test whether two [[point set]]s come from the same [[distribution]] and to validate [[physical model]]s and [[theori]]es.
These [[generative models]] have some shortcomings in that they [[failed]] to consider additional [[factors]] that could [[affect]] the [[community]] [[memberships]] and [[isolate]] the [[contents]] that are [[irrelevant]] to [[community]] [[memberships]] .
These [[hidden system state]]s are [[automatically learned]] by [[our model]] in a [[data-driven manner]] .
These [[hidden variable]]s therefore need to be [[estimated]] for these [[properti]]es to be [[evaluated]] .
The [[SEI]] intended the [[maturity questionnaire]] to provide a [[simple tool]] for [[identifying area]]s where an [[organization's software process]] needed improvement.
These include [[boundary erosion]], [[entanglement]], [[hidden feedback loop]]s, [[undeclared consumer]]s, [[data dependenci]]es, [[configuration issue]]s, [[changes in the external world]], and a variety of [[system-level anti-pattern]]s.
These include [[non-topical tweet detection]], [[automatic labeled data acquisition]], [[evaluation]] with [[human computation]], [[diagnostic]] and [[corrective learning]] and, most importantly, [[high-precision topic inference]] .
These include not only the [[actual judgment]]s, [[standard]]s, and [[rules]] to be found in the [[moral code]]s of [[existing societi]]es, but also what may be called [[ideal judgment]]s, [[standard]]s, and [[rule]]s: those which can be justified on [[rational ground]]s.
These include [[PRank]], [[OC SVM]], [[Ranking SVM]], [[IR SVM]], [[GBRank]], [[RankNet]], [[LambdaRank]], [[ListNet]] & [[ListMLE]], [[AdaRank]], [[SVM MAP]], [[SoftRank]], [[Borda Count]], [[Markov Chain]], and [[CRanking]] .
These include [[secretari]]es, [[bookkeeping]] and [[filing clerk]]s, [[mail sorter]]s, and [[bank teller]]s.
These include the use of [[near infrared spectroscopy]] coupled with [[data mining]] as an alternate [[laboratory technique]] for predicting [[compound concentration]]s in [[soil]] and [[plant sample]]s, and the [[analysis]] of [[gas chromatography mass spectrometry (GCMS) data]], a [[technique]] used to determine in [[environmental application]]s, for example, the [[petroleum content]] in [[soil]] and [[water sample]]s.
These include the vast [[literature]] of [[language modeling]] and [[acoustic modeling]], the use of [[neural network]]s for [[machine translation]], and [[multi-modal applications combining language]] and other [[signal]]s such as [[image]]s and [[video]]s (e.g. [[caption generation]]).
These include [[useful trick]]s for [[memory saving]]s, [[method]]s for assessing and [[visualizing performance]], [[practical method]]s for providing [[confidence estimate]]s for [[predicted probabiliti]]es, [[calibration method]]s, and [[method]]s for [[automated management of feature]]s.
These [[instance]]s can often be [[solve]]d in [[seconds]], even though the same [[solver]]s can be stymied by [[handcrafted]] [[instance]]s involving only [[hundreds of variable]]s.
These [[knowledge source]]s include [[capitalization features]], [[lexical features]], [[features indicating the current section of text]] (i.e. [[headline]] or [[main body]]), and [[dictionaries]] of [[single]] or [[multi-word terms]] .
These [[latent representation]]s [[encode]] [[social relation]]s in a [[continuous vector space]], which is easily exploited by [[statistical model]]s.
The selected [[subspace prediction]]s are then [[transform]]ed to new [[feature]]s on which a [[linear model]] can be [[learned]] .
The [[select statement]] is retained with [[inner join]]s, [[outer join]]s, and [[aggregation allowed]] .
The [[Semantic Feature]]s derived by this [[method gave good correlation]] with [[human ranking]]s in a [[web search]] [[query completion application]] .
The [[semantic lexicon]]s produced by [[Basilisk]] have higher [[precision]] than those produced by previous [[technique]]s, with several [[categori]]es showing substantial [[improvement]] .
The [[Semantic Orientation CALculator (SO-CAL]]) uses [[dictionari]]es of [[words annotated]] with their [[semantic orientation]] ([[polarity and strength]]), and incorporates [[intensification]] and [[negation]] .
The [[semantic similariti]]es between [[entiti]]es are often [[non-transitive]] since they could [[share]] different [[latent similarity component]]s.
The [[semantic]]s of [[ProbLog]] is then defined by the [[success probability]] of a [[query]], which corresponds to the [[probability]] that the [[query succeed]]s in a [[randomly sampled program]] .
The [[Semantic Web]] also requires facilities for the [[storage]] of [[annotation]]s and [[ontologies]], [[user interface]]s, [[access APIs]], and other [[features]] to fully support [[annotation]] [[usage]] .
These [[measure]]s usually assume that the [[variable]]s are both [[continuous]] or both [[categorical]] .
These [[methods]] allow the [[scientist]] to [[explore]] and [[describe]] [[data]] and draw [[valid statistical inference]]s without the usual concerns for [[mathematical tractability]] .
These [[methods]] are able to learn [[highly]] [[predictive models]] by [[extracting]] and automatically [[analyzing]] [[tens of thousands]] of [[features]] [[potentially indicative]] of [[suspicious URLs]] .
These [[method]]s can automatically identify [[token span]]s as [[entity mention]]s in [[document]]s and [[label their fine-grained type]]s (e.g., [[people]], [[product]] and food) in a [[scalable way]] .
[[These method]]s represent various trade-offs in terms of the [[setting]] ([[laboratory]] versus <i>[[in the wild</i>]]), [[object of measurement]] ([[user behaviour]], [[affect or cognition]]) and [[scale of data collected]] .
These [[methods]] together enable us to obtain an enriched [[vector representation]] of each new [[email message]], which consists of both standard [[features]] of an [[email message]] (such as [[words]] in the [[title]] or [[body]], [[sender]] and [[receiver]] [[IDs]], etc.) and the [[induced]] [[social]] [[features]] from the [[sender]] and [[receivers]] of the [[message]] .
These models allow for the use of [[very large sets]] of [[arbitrary]], [[overlapping]] and [[non-independent feature]]s.
These [[model]]s are constructed using [[preference learning]], based on [[questionnaire]]s administered to [[player]]s after playing different [[level]]s.
These models are used to perform many [[reasoning task]]s, such as [[scheduling]], [[planning]] and [[learning]], [[diagnosis]] and [[prediction]], [[design]], [[hardware]] and [[software verification]], and [[bioinformatics]] .
[[These model]]s are usually [[intractable]] and thus require [[approximate inference]] .
[[These models]] combine [[global parameter]]s that instantiate dense patches of connectivity ([[blockmodel]]) with [[local parameter]]s that instantiate [[node-specific variability]] in the [[connection]]s ([[mixed membership]]).
These [[model]]s make use of the [[unsupervised learning machinery]] of [[topic model]]s to [[discover]] the [[hidden topic]]s within each [[label]], as well as [[unlabeled]], [[corpus-wide]] [[latent topic]]s.
These models use [[predeﬁned score]]s that are [[invariant]] to the speciﬁc [[structure]] of the [[input]] [[graph]], and thus do not involve any [[learning]] .
These [[model]]s were transferred to the [[ISH image domain]] and used directly as [[feature extractor]]s to [[compute]] [[image representation]]s.
[[These neural networks]] were trained by [[supervised learning]] from [[human expert movse]]s, and by [[reinforcement learning]] from [[self-play]] .
These new [[regularized objective]]s provide better [[data mining result]]s than existing [[low-rank regression]] in both [[theoretical]] and [[empirical validation]]s.
These [[non-conforming pattern]]s are often referred to as [[anomali]]es, [[outlier]]s, [[discordant observation]]s, [[exception]]s, [[aberration]]s, [[surprise]]s, [[peculiariti]]es or [[contaminants in different application domain]]s.
These [[non-intentional action]]s observed in the [[web log]]s severely [[bias]] both [[analytic]]s and the [[models created]] from the [[data]] .
The sensitivity to the [[specification]] of the [[prior]] in a [[hidden Markov model]] describing [[homogeneous segment]]s of [[DNA sequence]]s is considered.
These observations motivate us to [[systematically model]] [[user psychological desire]] in order for a [[precise]] [[prediction]] on [[ad click]]s.
These [[operator]]s condense such [[execution]]s according to possible [[parallel]] or possible [[mutual exclusive execution]]s.
These [[papers]] cover [[topics]] in [[physical chemistry]] and [[biochemistry]] and were provided by the [[Royal Society of Chemistry (RSC) Publishing]] .
These [[parameters]] may be [[continuous measurements]] or [[binary]] or [[categorical measurements]] recorded in [[one second]] [[intervals]] for the [[duration]] of the [[flight]] .
[[These phenomena]] are discussed in the context of [[adaptive network]]s, along with examples from a variety of areas including [[distributed sensing]], [[intrusion detection]], [[distributed estimation]], [[online adaptation]], [[network system theory]], and [[machine learning]] .
These problem formulations are: 1) [[identifying]] [[anomalous sequence]]s with respect to a [[database of normal sequence]]s; [[2) identifying]] an [[anomalous subsequence]] within a [[long sequence]]; and 3) identifying a [[pattern]] in a [[sequence]] whose frequency of occurrence is [[anomalous]] .
These [[problem]]s are important because without knowing what [[products]] each [[sentence]] talks about the [[opinion]] [[mined]] from the [[sentence]] is of little use.
[[These problem]]s can be stated as the [[formal task]]s of [[constraint satisfaction and satisfiability]], [[combinatorial optimization]], and [[probabilistic inference]] .
These [[properti]]es present unique opportunities for an [[integrative system design]], built on [[bounded-latency network synchronization]] and [[dynamic load-balancing scheduling]], which is [[efficient]], [[programmable]], and enjoys [[provable correctness guarantee]]s.
[[These quantities]] are [[estimated]] solely from [[binary-valued (correct / incorrect)]] graded [[learner]] [[response data]] and the specific [[action]]s each [[learner]] performs (e.g., [[answer]]ing a [[question]] or [[study]]ing a [[learning resource]]) at each [[time instant]] .
These range from the [[syntactic]], such as [[part-of-speech tagging]], [[chunking and parsing]], to the [[semantic]], such as [[word-sense disambiguation]], [[semantic-role labeling]], [[named entity extraction]] and [[anaphora resolution]] .
These [[relation]]s are instantiated in the [[abstract]] / [[introduction]] part of the [[paper]]s and can be identified [[automatically]] using [[textual data]] and [[external ontologi]]es.
These [[relationship]]s are [[represented as]] a [[deterministic Bayesian network]] .
These [[representation]]s can be used to induce [[similarity measure]]s by [[computing]] [[distances between the vector]]s, leading to many useful applications, such as [[information retrieval]] ([[Manning et al., 2008]]), [[document classification]] ([[Sebastiani, 2002]]) and [[question answering]] ([[Tellex et al., 2003]]).
These [[RE]]s are further enriched with [[named entiti]]es, [[represented as]] [[URI]]s to the [[linked open data cloud]], by integrating the [[DBpedia Spotlight tool]] into our [[workflow]] .
These [[researcher]]s are engaged in activities ranging from [[natural language dialog]], [[information retrieval]], [[topic-tracking]], [[named-entity detection]], [[document classification]] and [[machine translation]] to [[bioinformatics]] and [[open-domain question answering]] .
These [[resource]]s are used to supports tasks such as [[entity recognition]] (i.e., the [[identification]] of [[biomedical entiti]]es in [[text]]) and [[relation extraction]] (i.e., the [[identification]] of [[relationship]]s among [[biomedical entiti]]es).
These results establish [[proof of principle]] for [[automated hypothesis generation]] and [[discovery]] based on [[text mining]] of the [[scientific literature]] .
[[These results]] highlight an odd juxtaposition of [[motive]]s, suggesting that while making [[charitable contribution]]s to [[support good works in society]], [[CEO]]s use [[aggressive]] and perhaps fraudulent [[tax evasion strategi]]es.
These [[result]]s may help [[establish guideline]]s for the [[choice]] of [[outlier detection method]]s in [[skewed data]], which are often sen in the [[public health field]] .
These results will catalyse research in numerous [[discipline]]s, such as [[neuroscience]] and [[genetics]], and enhance our ability to [[harness]] [[evolution]] for [[engineering]] purposes.
These [[rules]] [[represent]] the [[graph rewrite rule]]s that the first [[graph]] must go through to be [[isomorphic]] to the second [[graph]] .
The [[service terms]], such as [[price]] and [[time to completion]] of the [[service terms]], depend on the [[consumer's]] particular [[specifications]] .
These [[scalable methods]] may have [[applications]] in other [[contexts]] stipulating [[balanced allocation]] .
These [[scores]] are computed by an [[iterative algorithm]] that updates the [[scores]] of a [[page]] based on the [[scores]] of [[pages]] in its [[immediate neighborhood]] .
These [[service]]s are generally organized as [[pipeline]]s, using [[dedicated API]]s and different [[taxonomy]] for [[extracting]], [[classifying]] and [[disambiguating named entiti]]es.
These simplifying [[assumption]]s do not reflect the [[characteristic]]s of [[scenario]]s in [[real application]]s and thus the useful [[information of episode]]s in terms of [[utiliti]]es such as [[profit]]s is lost.
These [[sites]] tend to have [[stable]] [[circles of influence]] with more general [[news media sites]] acting as [[connectors]] between them.
These [[social dimension]]s [[describe]] diverse [[affiliation]]s of [[actor]]s hidden in the [[network]], and the [[discriminative]] [[learning]] can [[automatic]]ally determine which [[affiliation]]s are better aligned with the [[class]] labels.
These [[spectral signature]]s contain descriptive [[characteristics]] of [[bacterial colonies]], which can be used to identify [[Bacteria culture]]s in [[real time]] .
These [[spike-and-slab]] [[Bayesian factor model]]s encourage [[sparsity]] while [[accounting for uncertainty]] in a principled manner and avoiding unnecessary [[shrinkage]] of [[non-zero value]]s.
These studies evaluate our [[CCA]] variants and alternatives including those derived from [[deep neural network]]s, including [[C&W]], [[HLB]], [[SENNA]], and [[word2vec]] on problems in [[POS tagging]], [[word similarity]], [[generalized sentiment classification]], [[NER]], [[cross-lingual WSD]] and [[semantic & syntactic analogi]]es.
These successful results usually involve some [[end-to-end]] [[ConvNet model]] that [[learns hierarchical representation]] from [[raw pixel]]s ([[Girshick et al., 2013]]) ([[Zeiler & Fergus, 2014]]).
These [[system]]s investigate the two major [[scenario]]s for having multiple [[ontologies]] for the same [[domain]]: specializing in [[sub-domains]] and providing different [[perspectives]] .
These systems [[parsed text]] into a [[network]] of [[predefined]] [[concepts]], and created a [[knowledge base]] from which [[inference]]s can be made.
[[These systems]] [[passively track]] different sorts of [[user behavior]], such as [[purchase history]], [[watching habit]]s and [[browsing activity]], in order to [[model user preference]]s.
These [[systems passively track]] different sorts of [[user behavior]], such as [[purchase history]], [[watching habit]]s and [[browsing activity]], in order to [[model user preference]]s.
These [[system]]s use [[data]] from [[surveillance video camera]]s and [[transaction logs (TLog)]] recorded at the [[Point-of-Sale (POS)]] .
These [[tasks]] are [[structured]] and [[repetitive]], and consist of [[short]], [[atomic]], [[isolated]] [[transactions]] .
The [[set]] of all [[sense]] [[pairs]] <i>S<sub>xy</sub></i> consists of all [[pairings]] between [[senses]] in <i>S<sub>x</sub></i> and <i>S<sub>y</sub></i>.
The [[set]] of <i>N</i> + <i>K</i> [[image]]s containing only the [[common]] and the [[low-rank component]]s form a [[compact]] and [[discriminative representation]] for the [[original image]]s.
The [[set of patterns]] can be used in [[identifying]] [[functional modules]] (i.e., [[clustering]]).
These [[topic]]s are [[automatically learned]] from the [[corpus]] in an [[unsupervised manner]] and presented alongside [[search result]]s.
These [[trajectori]]es are usually generated at a [[low]] or an [[irregular frequency]] due to [[applications' characteristic]]s or [[energy saving]], leaving the [[route]]s between [[two]] [[consecutive point]]s of a [[single trajectory uncertain]] (called an [[uncertain trajectory]]).
These two [[aspect]]s were not jointly addressed in previous [[time series]] [[early classification]] [[studi]]es, such that a [[difficult choice]] of [[select]]ing one of these [[aspect]]s is required.
These two [[models]] are unified seamlessly via the [[community]] [[memberships]] .
These two types of [[queri]]es find important [[application]]s in [[graph analysis]], especially in the [[computation]] of [[graph measure]]s.
These types of [[structured metadata]] provide valuable [[evidence]] for [[learning]] how a [[community]] [[organizes knowledge]] .
These [[uncertainti]]es cannot be described by [[Turing machine]] and [[traditional]] [[interaction machine]] .
The [[seventeen]]-[[member]] [[Study Panel]], comprised of [[experts in AI]] from [[academia]], [[corporate laboratori]]es and [[industry]], and [[AI-savvy]] [[scholars in law]], [[political science]], [[policy]], and [[economic]]s, was launched in [[mid-fall]] [[2015]] .
The simplest kind of [[DBN]] is a [[Hidden Markov Model (HMM)]], which has one [[discrete]] [[hidden node]] and one [[discrete]] or [[continuous]] [[observed node]] [[per]] [[slice]] .
The [[simulation]] would [[track]] [[external event]]s and tune its [[model]]s to keep them faithful to reality.
The [[social network]] perspective has been [[developed]] over the last sixty years by [[researcher]]s in [[psychology]], [[sociology]], and [[anthropology]], and more recently, to a lesser extent, in [[physics]] .
The [[space]] of [[output attribute]]s of a [[deep web data source]] is [[partition]]ed into [[sub-space]]s.
The [[SPARQL Query Language]] for [[RDF]] and the [[SPARQL Protocol]] for [[RDF]] are implemented by a growing number of [[storage system]]s and are used within [[enterprise]] and [[open Web setting]]s.
The specific [[algorithm]] [[we]] use is [[co-EM]]: a [[multi-view semi-supervised learning algorithm]], proposed by [[Nigam & Ghani]] [13], that combines [[feature]]s from both [[co-training]] [2] and [[EM]] .
The [[spectral algorithm]] outperforms both [[SGNS]] and the [[Shifted PPMI matrix]] on the [[word similarity task]]s, and is [[scalable]] to [[large corpora]] .
... The [[SPPMI-SVD method]] simply factorizes the [[sparse SPPMI matrix]] using [[Singular Value Decomposition (SVD)]], rather than the [[gradient descent method]]s of [[word2vec]]/[[GloVe]], and uses the ([[dense]]) [[left singular vector]]s as the final [[word embedding]]s.
The [[stability]] of the [[labor share of income]] is a key [[foundation]] in [[macroeconomic model]]s.
The stakes are high: [[LAWS]] have been described as the [[third]] [[revolution]] in [[warfare]], after [[gunpowder]] and [[nuclear arms]] .
The [[standard assumption]] of [[identically distributed training]] and [[test data]] is violated when [[test data]] are [[generate]]d in response to a [[predictive model]] .
The [[standardization]] and wider use of [[electronic medical records (EMR)]] creates opportunities for better understanding [[pattern]]s of [[illness]] and [[care]] within and across [[medical system]]s.
The [[standard model of labor]] is one in which [[individual]]s [[trade]] their [[time]] and [[energy]] in return for [[monetary reward]]s.
[[The Standing Committee]] defined a [[Study Panel charge]] for the [[inaugural Study Panel]] in the [[summer]] of [[2015]] and recruited [[Professor Peter Stone]], at the [[University of Texas at Austin]], to chair [[the panel]] .
[[The Standing Committee]] expects that the [[projection]]s, [[assessment]]s, and [[proactive guidance]] stemming from [[the study]] will have [[broader]] [[global relevance]] and is making plans for future [[studi]]es to expand the scope of the [[project internationally]] .
[[The Standing Committee]] extensively discussed ways to frame the [[Study Panel]] charge to consider both recent advances in [[AI]] and potential [[societal impact]]s on [[job]]s, the [[environment]], [[transportation]], [[public safety]], [[healthcare]], [[community engagement]], and [[government]] .
The [[state-of-the-art approach]] is to choose a [[type of entiti]]es to observe in the [[behavior data]], to define a [[key metric]] for these [[observation]]s, and to [[estimate]] the [[average value]] of [[this metric]] over the [[observation]]s in each of [[the system version]]s.
The [[state-of-the-art]] [[MLN structure learners]] all involve some element of [[greedily]] generating [[candidate clauses]], and are susceptible to [[local optima]] .
The [[state-of-the-art]] of [[BT]] derives a [[linear Poisson regression model]] from [[fine-grained]] [[user behavior]]al [[data]] and [[predicts]] [[click-through rate (CTR)]] from [[user history]] .
The [[statistical tests for measuring change]] in [[true score]]s from [[pretest]] to [[posttest]] have important advantages to the classical [[raw-score difference]]s in terms of [[accuracy]], [[flexibility]], and [[control of]] [[error source]]s.
([[The statutory accounting]] that the [[state primarily]] uses is [[largely cash-based]].)
The [[streaming nature]] of [[online data]] inevitably makes an [[advertising system]] choose between [[maximizing]] its [[expected revenue]] according to its [[current knowledge]] in [[short term]] ([[exploitation]]) and trying to [[learn]] more about the unknown to improve its [[knowledge]] ([[exploration]]), since the [[latter]] might increase its [[revenue in the future]] .
The [[studi]]es are expected to develop [[synthese]]s and [[assessment]]s that provide [[expert-informed guidance]] for directions in [[AI research]], [[development]], and [[systems design]], as well as programs and [[polici]]es to help ensure that these [[system]]s broadly benefit individuals and society.1
The [[study]] of [[entities]] and [[communities]] and how they evolve in such [[large]] [[dynamic]] [[graphs]] is both important and challenging.
The subject lies at the crossroads between [[robotic]]s, [[control theory]], [[artificial intelligence]], [[algorithm]]s, and [[computer graphic]]s.
The success of the [[von Neumann model]] of [[sequential computation]] is attributable to the fact that it is an [[efficient bridge]] between [[software]] and [[hardware]]: [[high-level language]]s can be efficiently compiled on to [[this model]]; yet [[it]] can be [[effeciently implemented]] in [[hardware]] .
The [[sum]] of this [[quantity]] over all [[word]]s is the [[Kullback-Leibler divergence]] between the [[distribution of words]] in <math>c_j</math> and the [[distribution of words]] in <math>\lnot c_j</math> ([[Cover & Thomas, 1991]]).
The [[super-topic]]s represent [[clusters of topics]] that [[frequently cooccur]] .
The [[Support Vector Machine]] is a powerful new [[learning algorithm]] for solving a variety of [[learning]] and [[function estimation problem]]s, such as [[pattern recognition]], [[regression estimation]], and [[operator inversion]] .
The [[symbolist]]s’ [[master algorithm]] is [[inverse deduction]], the [[connectionist]]s’ is [[backpropagation]], the [[evolutionari]]es’ is [[genetic programming]], the [[Bayesian]]s’ is [[Bayesian inference]], and the [[analogizer]]s’ is the [[support vector machine]] .
The [[synset]]/[[lexeme embedding]]s obtained live in the same [[vector space]] as the [[word embedding]]s.
The [[system]] achieves competitive accuracy on the [[ACE English]] [[EMD]] and [[RMD task]]s.
The system continuously [[collects stream]]s of [[user data]] and is reactive to [[fresh information]], [[updating topic]]s for [[user]]s as [[interests shift]] .
The [[system design]] involves [[semantic role labelling]], [[semantic subgraph-based sentence selection]] and [[automatic post-editing]] to create a [[question-based]] 250 word [[summary]] from a set of [[documents]], all of which are relevant to the [[question topic]] .
[[The system]] identifies the important [[concept]]s in a given [[document]] and automatically [[link]]s these concepts to the corresponding [[Wikipedia page]]s.
The [[system]] is already used in [[industrial]] and [[scientific project]]s.
[[The system]] is based on [[statistical classifier]]s trained on roughly 50,000 [[sentence]]s that were [[hand-annotated]] with [[semantic roles]] by the [[FrameNet semantic labeling project]] .
[[The system]] is [[completely unsupervised]], and performs a sequence of tasks starting with [[enrichment]] of [[product title]]s and computation of [[token weight]]s, followed by matching the potentially [[co-referent]] enriched [[title]] [[pair]]s.
The [[system]] makes use of [[forecast]]s of [[future price]]s based on [[price histori]]es of the [[product]]s, incorporating [[feature]]s such as [[sales volume]], [[seasonality]], and [[competition]] in making its [[recommendation]] .
The [[system]] needs to distinguish multiple [[intruder]]s and [[track their movement]]s.
The [[system]] retrieves a <i>[[cone-mode]]</i> l from the [[historical trajectori]]es and [[track]]s multiple [[intruder]]s based on [[this model]] .
The [[system scale]]s to thousands of [[CPU]]s and [[petabytes of data]], and has thousands of [[user]]s at [[Google]] .
[[The system]] takes the author from [[literature search]] through [[domain-model creation]] and [[bibliography construction]], to the [[semantic markup]] of [[bibliographic metadata]] .
[[The Tacit Dimension]] argues that [[tacit knowledge]] - [[tradition]], [[inherited practice]]s, [[implied values]], and [[prejudgments]] - is a crucial part of [[scientific knowledge]] .
[[The talk]] will also cover the [['real world' issue]]s that [[Tracey]] has dealt with in creating a [[reusable process]], including [[data mart development]], [[sampling]], testing, [[reporting]], and [[implementation]] with [[internal customer]]s.
[[The talk]] will be a mix of [[background]], [[detailed example]]s, and [[software demonstration]] .
The [[task-based approach]] can be applied to the current [[user's search history]], or as we focus on [[here]], all [[user]]s' [[search histori]]es as so-called "[[groupization]]" (a variant of [[personalization]] whereby other [[users' profile]]s can be used to [[personalize the search experience]]).
The [[task]] differs from [[conventional recommender system]]s as the [[cold-start problem]] is [[ubiquitou]]s, and [[rich feature]]s, including [[textual content]], need to be considered.
[[The task]] has proven useful in many [[research area]]s including [[ontology learning]], [[relation extraction]] and [[question answering]] .
The [[task]] is conceived as a [[language modeling problem]] where [[per-topic model]]s are [[trained]] using [[hashtag]]s in the [[tweet stream]], which serve as [[proxi]]es for [[topic label]]s.
The [[task of record linkage (RL)]] is to find [[entries]] that [[refer]] to the same [[entity]] in different [[data source]]s.
The [[task of reference normalization]] is to [[analyze]] and [[detect]] these different [[reference]]s [6, 7].
The [[task]] provides a [[robust]] [[evaluation framework]] for measuring the [[accuracy of ontology expansion technique]]s.
The [[task to link]] the [[named entity mention]]s [[detected]] from [[tweet]]s with the corresponding [[real world entiti]]es in the [[knowledge base]] is called [[tweet entity linking]] .
The [[TBox]] describes [[terminology]], i.e., the [[ontology]] in the form of [[concept]]s and [[roles definition]]s, while the [[ABox]] contains [[assertion]]s about individuals using the terms from the [[ontology]] .
The [[technique]] has [[desirable properti]]es, such as [[high interpretability]] and [[natural sparsity]] .
[[The technique]] involves [[filtering]] out all [[non-nouns]] from the [[text]], and then [[finding the matches]] in the [[database]] .
The [[technique]] makes use of the [[variational inequality]] and provides a [[closed-form solution]] for [[screening inactive feature]]s in different [[situation]]s.
[[The Terminator movie trilogy]], for example, featured [[Skynet]], a [[self-aware artificial intelligence]] that served as the trilogy's main villain, battling [[humanity]] through its [[Terminator]] [[cyborg]]s.
The term includes the [[purchase]] of, or [[participation]] in, a [[loan]] made by another [[lender]] and financing arrangements that [[defer payment]] for more than 90 days, including the [[sale]] of a [[Federal government]] [[asset]] on [[credit]] terms.
The term [[lexeme]] also embraces [[item]]s which consist of more than [[one]] [[word-form]] .
The [[term]] [[Linked Data]] refers to a set of [[best practice]]s for [[publishing]] and [[connecting structured data]] on [[the Web]] .
The [[test-set]]s are selected to cover the variation of [[acoustic]] and [[linguistic factor]]s in [[spontaneous speech]]: [[word perplexity]], [[degree of disfluency]], and the [[speaking rate]] .
The [[theoretical result]]s are supported by [[experiment]]s on a [[problem]] from the agent navigation [[domain]] .
The theory of [[attention]], [[intention]], and [[aggregation]] of [[utterance]]s is illustrated in [[the paper]] with a number of [[example discourse]]s.
The [[theory of reinforcement learning]] provides a [[normative account]]1, deeply rooted in [[psychological]]2 and [[neuroscientific]]3 perspectives on [[animal behaviour]], of how [[agent]]s may [[optimize]] their [[control of an environment]] .
The three phases are [[preprocessing]], [[pattern discovery]], and [[patterns analysis]] .
The [[time-varying model]]s that combined [[latent topic feature]]s and [[baseline feature]]s had [[AUC]]s that reached 0.85, 0.80, and 0.77 for [[in-hospital]], 30 day [[post-discharge]] and 1 year [[post-discharge mortality]] respectively.
The [[TMEC approach]] [[train]]s a [[graph-based multi-label classifier]] for each individual [[kernel]], and then combines the [[prediction]]s of the [[individual model]]s.
[[The tool]] and [[algorithm]] improves the [[signal-to-noise ratio]] and [[pinpoints]] precisely the [[opportunities]] and [[risks]] that matter most to [[communications professionals]] and their [[organizations]] .
The [[topic]] of [[data storage]] is one that does not need to be [[well understood]] until [[something goes wrong]] ([[data disappear]]s) or [[something goes really right]] ([[too many]] [[customer]]s).
The topics treated include [[Bayesian decision theory]], [[supervised]] and [[unsupervised learning]], [[nonparametric technique]]s, [[discriminant analysis]], [[clustering]], [[preprosessing]] of [[pictorial data]], [[spatial filtering]], [[shape description technique]]s, [[perspective transformation]]s, [[projective invariant]]s, [[linguistic procedure]]s, and [[artificial intelligence technique]]s for [[scene analysis]] .
The [[topic]]s we cover include: the [[OEC (Overall Evaluation Criterion)]], [[click tracking]], [[effect trend]]s, [[experiment length]] and [[power]], and [[carryover effect]]s.
The [[traditional definition]] of [[outlier]]s, as provided by [[Hawkins]], defines [[such object]]s by their [[generative probabiliti]]es rather than the [[extremity]] in their [[value]]s.
The traditional [[entity extraction problem]] lies in the ability of [[extracting]] [[named entities]] from [[plain text]] using [[natural language processing technique]]s and intensive [[training]] from [[large document collections]] .
The traditional [[mention-pair model]] for [[coreference resolution]] cannot capture information beyond [[mention pair]]s for both [[learning]] and [[testing]] .
The [[training]] of [[CRFs]] is typically formulated as an [[unconstrained optimization]] [[problem]] that [[maximize]]s the [[conditional likelihood]] .
The [[training procedure]] for [[G]] is to [[maximize the probability]] of D making a [[mistake]] .
The [[transactions]] require [[detailed]], [[up-to-date data]], and [[read]] or [[update]] a [[few (tens of)]] [[records]] accessed typically on their [[primary keys]] .
The [[transition probability]] and the [[reset probability]] of the [[Markov model]] are defined as [[parametric model]]s based on [[feature]]s on [[node]]s and [[edge]]s.
The treatment is comprehensive and [[self-contained]], targeted at [[researcher]]s and [[student]]s in [[machine learning]] and [[applied statistics]] .
The [[tree]] is [[learned]] from the <i>[[Trajectory Pattern]]s</i> that hold a certain [[area]] and it may be used as a [[predictor]] of the next [[location]] of a new [[trajectory]] finding the best [[matching path]] in the [[tree]] .
The [[tree search]] in [[AlphaGo]] evaluated [[position]]s and selected [[move]]s using [[deep neural network]]s.
[[The tutorial]] covers [[input encoding]] for [[natural language task]]s, [[feed-forward network]]s, [[convolutional network]]s, [[recurrent network]]s and [[recursive network]]s, as well as the [[computation graph abstraction]] for [[automatic gradient computation]] .
The two [[approach]]es [[jointly learn]] a [[discriminative segmentation]] of the [[input space]] and a [[generative output prediction model]] for each [[segment]] .
The two [[network]]s are [[trained jointly]] to [[maximize the conditional probability]] of the [[target sequence]] given a [[source sequence]] .
The [[unbiased data]] also provides the [[ground truth]] for [[quantitative evaluation]] of different [[method]]s.
The [[uncovered structure]]s facilitate browsing [[information]] and [[retrieving knowledge]] from the [[data]] .
The underlying [[assumption]] is that [[windows]] that belong to the same [[task]] share some common [[properties]] with one another that we can [[detect]] [[from data]] .
The underlying [[problem setting]] resembles [[discriminative clustering]], however, [[existing approach]]es focus on the [[prediction]] of [[univariate cluster label]]s.
The [[unintentional transport]] of [[invasive speci]]es (i.e., [[non-native]] and [[harmful speci]]es that [[adversely affect]] [[habitat]]s and [[native speci]]es) through the [[Global Shipping Network (GSN)]] causes substantial losses to [[social]] and [[economic welfare]] (e.g., [[annual loss]]es due to [[ship-borne invasion]]s in the [[Laurentian Great Lake]]s is estimated to be as high as [[USD 800 million]]).
The [[unique characteristic]]s of [[graph data]] are challenging [[traditional]] [[search technique]]s like [[SQL]] and [[keyword search]] .
::The [[unit]]s for [[acceleration]] can be implied from the [[definition]] to be [[meter]]s/[[second]] divided by [[second]]s, usually written [[m/s^2]] .
The [[US Department of Defense Dictionary]] of [[Military term]]s defines the [[Information Environment]] as ' the [[aggregate of individual]]s, [[organization]]s, and [[system]]s that collect, [[process]], [[disseminate]], or act on [[information]].' The [[decision]]s and [[action]]s we take both as individuals and [[collectively simultaneously shape]] and are shaped by the [[information environment]] in which we live.
The usefulness of [[our approach]] is further verified by the [[observations]] in a [[bucket]] [[test]] launched in [[December 2008]] .
The use of a [[Semantic Wiki]] additionally helps to target the [[first dilemma]] — the [[single]] / [[multiple expert]]s [[dilemma]] .
The use of [[L1 regularisation]] for [[sparse learning]] has generated immense [[research interest]], with successful application in such diverse areas as [[signal acquisition]], [[image coding]], [[genomics]] and [[collaborative filtering]] .
The use of [[web standard]]s also makes it possible for [[BRAT]] to uniquely identify any [[annotation]] using [[Uniform Resource Identifiers (URIs)]], which enables [[linking]] to individual [[annotation]]s for discussions in [[e-mail]], [[document]]s and on [[web page]]s, facilitating easy [[communication regarding annotation]]s.
The [[user behavior]] is more [[complex]] when her [[action]]s to satisfy her information needs form a [[search session]], which may include multiple [[queri]]es and [[subsequent]] [[click behavior]]s on various [[item]]s on [[search result page]]s.
The [[user]] can easily [[modify]] / [[extend]] the [[content]] of [[the article]] by changing into the [[edit mode]] of the [[article]], which is usually possible due to a [[mandatory Edit button]] placed on the [[page]] .
The [[user involvement theory]] implies that [[user]]s prefer [[content]] [[recommend]]ed by a [[process in which they have explicit involvement]] .
The [[U.S. labor market]] has become increasingly [[polarized]] since the [[1980s]], with the [[share of employment]] in [[middle-wage occupation]]s [[shrinking over time]] .
The [[value]] from the growing availability of [[online documents]] and [[ontologies]] will increase significantly once these two [[resource]]s become deeply [[interlinked]] at the [[semantic level]] .
The [[vanilla version]] of [[data swapping]] selects [[pairs of record]]s and [[exchange]]s some of their [[attribute value]]s.
The variety of [[approach]]es tends to pull [[system]]s and [[algorithm]]s [[design]] in different [[direction]]s, and it remains difficult to [[find]] a [[universal platform]] applicable to a wide range of different [[ML]] [[program]]s at [[scale]] .
The vast majority of [[real world classification problem]]s are [[imbalanced]], [[meaning there]] are far fewer [[data]] from the [[class of interest]] (the [[positive class]]) than from other [[class]]es.
The [[volume of Web video]]s have increased [[sharply]] through the past several years because of the [[evolvement]] of [[Web video sites.Enhanced algorithm]]s on [[retrieval]], [[classification]] and [[TDT]] ([[abbreviation]] of <i>[[Topic Detection and Tracking]] </i>) can bring lots of convenience to [[Web user]]s as well as [[release tedious work]] from the [[administrator]]s.
The [[voluminous]] [[malware variant]]s that appear in the [[Internet]] have posed [[severe threat]]s to its [[security]] .
The [[VSM]] relates [[term]]s to [[document]]s, and since different [[term]]s have different importance in a given [[document]], a [[term weight]] is associated with every [[term]] [18].
The wealth of data available in [[Radiology]] gives rise to opportunities for applying this [[methodology]] to [[map out]] the [[trajectory]] of several other [[disease]]s, e.g. other [[neuro-degenerative disease]]s and [[cancer]]s, most notably, [[breast cancer]] .
[[The web]] provides an unprecedented opportunity to [[evaluate ideas]] quickly using [[controlled experiment]]s, also called [[randomized experiment]]s, [[A/B test]]s (and their generalizations), [[split test]]s, [[Control/Treatment tests]], [[MultiVariable Tests (MVT)]] and [[parallel flight]]s.
[[The Web]], teeming as it is with [[language data]], of all manner of [[varieti]]es and [[language]]s, in [[vast quantity]] and [[freely available]], is a fabulous [[linguist]]s’ playground.
The [[Wikify system]] combines these two [[method]]s to [[automatically enrich a text]] with [[link]]s to [[Wikipedia content]] .
The [[window-based]], [[dependency-based]] and [[word2vec vector set]]s were all trained on 112M words from the [[British National Corpus]], with preprocessing steps for [[lower-casing]] and [[lemmatising]] .
The [[word confidence measure (WCM)]] estimates the [[likelihood]] a [[word]] is correctly [[recognized]] .
The word "[[feature]]" is used to refer to a [[concrete]], [[linguistic input]] such as a [[word]], a [[suffix]], or a [[part-of-speech tag]] .
The [[word sense]] is drawn from [[Wiktionary.2]] For each of these [[word sense]]s, a [[system’s task]] is to identify a [[point]] in the [[WordNet’s subsumption]] (i.e., [[is-a) hierarchy]] which is the most [[plausible point]] for placing the new [[word sense]] .
[[The workshop]] avoided dealing with [[socioeconomic ramification]]s and with [[philosophical issue]]s such as [[theory of mind]], [[identity]] or [[ethics]] .
The [[world knowledge]] used includes the [[known entities]] (most [[articles in Wikipedia]] are associated to an [[entity/concept]]), their [[entity class]] when available ([[Person]], [[Location]], [[Organization]], and [[Miscellaneous]]), their known [[surface form]]s ([[term]]s that are used to [[mention the entities in text]]), [[contextual evidence]] (words or other entities that describe or co-occur with an entity), and [[category tags]] (which describe topics to which an entity belongs to)
The [[WWW]] is the largest [[database]] on [[earth]], and the [[context information]] entered by [[millions]] of [[independent]] [[users]] averages out to provide automatic [[semantics]] of [[useful]] [[quality]] .
They also apply when the [[sample]]s of different [[data set]]s are [[independent]] or [[coordinated]] .
They also are required to [[classify]] their [[transaction]]s in [[conformity]] with the fund, [[balance sheet]], [[revenue]], and [[expenditure classification description]]s contained in [[this chart]] .
They are apt to [[accept]] [["confirming" evidence]] at [[face value]] while subjecting [["disconfirming" evidence]] to [[critical evaluation]], and, as a result, draw undue support for their [[initial position]]s from [[mixed]] or [[random empirical finding]]s.
[[They]] are [[bundles of cells]] that support [[perception]] and [[action]] by constantly attempting to [[match]] incoming [[sensory input]]s with [[top-down expectation]]s or [[prediction]]s.
They are compelled to [[navigate]] much more [[complicated]] [[social enviornment]]s.
[[They]] are increasingly [[socially]] and [[spatially isolated]] and particularly [[vulnerable]] to [[adverse effect]]s from [[structural economic change]] .
They are of interest in their own right, and are also used as [[inputs]] in several [[higher-level]] [[data mining algorithm]]s including [[classification]], [[clustering]], [[rule-discovery]] and [[summarization]] .
[[They]] are relevant to all aspects of [[scientific]] and [[engineering practice]], including [[discovery]], [[analysis]], and [[application]] .
They are replacing [[neural networks]] in a variety of [[field]]s, including [[engineering]], [[information retrieval]], and [[bioinformatic]]s.
They are used for [[detection]] and [[localization]] of change for the purpose of [[anomaly detection]], [[monitoring]], or [[planning]] .
They are viewed as the [[silver bullet]] for many [[application]]s, such as [[database integration]], [[peer-to-peer system]]s, [[e-commerce]], [[semantic web service]]s, or [[social network]]s.
They can be [[approximated efficiently]] by [[noisy]], [[rectified linear unit]]s.
[[They]] can be difficult to [[design]], difficult to [[manage]], and, above all, difficult to [[test]] .
They can [[cause]] significant [[slowdowns]], [[exhaustion]] of available [[storage space]] and, eventually, [[application crashes]] .
They consist of [[tokenization]], [[sentence segmentation]], [[morphological processing]], [[part-of-speech tagging]], [[noun phrase identification]], [[named entity recognition]], [[nested noun phrase extraction]], and [[semantic class determination]] .
They divide [[feature space]]s into [[region]]s with [[sparse oblique tree splitting]] and assign [[local sparse additive expert]]s to individual [[region]]s.
They [[empirical]]ly exhibit the [[interpretability]], [[adaptability]] and [[feature selection]] [[properti]]es of [[supervised decision tree]]s while incurring slight loss in [[accuracy]] over other [[nonparametric density estimator]]s.
They examine [[scarcity debate]]s across three of the most important [[resource]]s - food, [[water]] and [[energy -]] and their [[implications for theory]], [[institutional arrangement]]s, [[policy response]]s and [[innovation system]]s.
They examine the patterns of [[currency crash]]es, high and [[hyperinflation]], and [[government default]]s on [[international]] and [[domestic debt]]s -- as well as the [[housing]]s in [[housing]] and [[equity price]]s, [[capital flow]]s, [[unemployment]], and [[government revenue]]s around these crises.
They find [[book]]s, [[movi]]es, [[job]]s, and [[date]]s for us, [[manage our investment]]s, and discover new [[drug]]s.
They focus on [[smoothing]] [[role change]]s between [[adjacent time instance]]s.
They further provide evidence that [[wage]]s and [[educational attainment]] exhibit a strong [[negative relationship]] with an [[occupation]]’s [[probability of computerisation]] .
They generalise [[hidden Markov models (HMMs)]] and [[linear dynamical systems (LDSs)]] by representing the [[hidden]] (and [[observed]]) [[state]] in terms of [[state variable]]s, which can have [[complex interdependenci]]es.
[[They]] look at games through a series of eighteen "[[game design schema]]s," or [[conceptual framework]]s, including [[game]]s as [[systems of emergence]] and [[information]], as [[context]]s for [[social play]], as a [[storytelling medium]], and as sites of [[cultural resistance]] .
They might not be able to [[fully perceive a scene]], [[recognize]] or [[manipulate]] any [[object]], [[understand]] all [[spoken]] or [[written language]], or [[navigate]] in any [[terrain]] .
[[They]] offer a [[unified model]] for looking at all kinds of [[game]]s, from [[board game]]s and [[sport]]s to [[computer]] and [[video game]]s.
[[They]] offer a way toward a new [[political outlook]], shifting from [[self-interested]] [[consumerism]] to a [[friendlier]], more [[sustainable]] [[society]] .
They're [[systematic]] and [[predictable]] making us [[predictably irrational]] .
They specifically describe [[word case]], [[punctuation]], [[numerical value]] and [[special characters]] .
They typically use a [[diffusion]] to [[propagate label]]s from a [[small set]] of [[node]]s with known [[class label]]s to the remaining [[nodes of the graph]] .
They were initially based on [[demographic]], [[content-based]] and [[collaborative filtering]] .
They will probably occupy their days with a variety of [[social]], [[recreational]] and [[artistic pursuit]]s, not unlike today’s [[comfortable]] [[retiree]]s or the [[wealthy]] [[leisure class]]es.
[[Third]], it allows [[partial cross-domain mapping]] so that [[graph]]s in different [[domain]]s may have different [[size]]s.
[[Thirdly]], several related [[learning setting]]s are [[briefly summarized]] .
Third, our [[system]] significantly improves [[stability]] of [[consensus clustering]] built on top of the [[randomized clustering]] [[solution]]s.
[[Third]], we design an [[iterative learning algorithm]], [[AEClass]], to dynamically refine the [[classification result]] by [[continuously adjusting the weight]]s on different [[activity-based edge classification scheme]]s from [[multiple]] [[activity graph]]s, while constantly [[learning]] the contribution of the [[structure affinity]] and the [[label vicinity]] in the [[unified classifier]] .
Third, [[we]] design an [[iterative learning algorithm]], [[SI-Cluster]], to [[dynamically]] refine the [[K cluster]]s by continuously [[quantifying]] and [[adjusting the weight]]s on [[self-influence similarity]] and on multiple [[co-influence similarity score]]s towards the [[clustering convergence]] .
Third, [[we]] evaluate [[our approach]] using [[real sensor data]] collected from 24 [[smart home testbed]]s.
Third, [[we]] introduce a [[query length model]] for [[capturing bias]] in [[recommendation click behavior]] .
[[This activity]] requires the analysis of the existing [[literature]] to [[identify]] the involved [[concept]]s and [[actor]]s and [[track relevant topic]]s.
This advantage is also demonstrated for the [[real world application]] of [[gene regulatory network]] [[discovery]] from [[time-course microarray]] [[data]] .
This [[algorithm]] builds a [[tree]] by [[recursively splitting]] the [[training sample]] into smaller [[subset]]s.
[[This algorithm]] delivers a [[predictive model]] in [[less time]] than [[current approach]]es in [[task]]s where not all [[column]]s are relevant.
This allowed us to gain important [[insight]]s into [[strength]]s and [[weakness]]es of both [[approach]]es.
This allows [[CBC]] to [[discover]] the [[less frequent]] [[senses]] of a [[word]] and to avoid [[discovering]] [[duplicate]] [[senses]] .
This allows [[organization]]s to effectively complete [[beneficial project]]s from different [[domain]]s, while also helping individual [[experts position themselve]]s and succeed in [[highly competitive]] [[job market]]s.
This allows us to quickly [[compute moment]]s of an important [[set of graph properti]]es, such as the [[expected number of edge]]s, [[star]]s, and [[clique]]s for [[graphs generated]] using [[MFNG]] .
This allows us to [[represent words]] not as [[low-dimensional vector]]s, but as [[densiti]]es over a [[latent space]], [[directly representing]] notions of [[uncertainty]] and enabling a [[richer geometry]] in our [[embedded space]] .
[[This]] allows us to [[score]] individual [[itemsets]] with the [[probability]] of them occuring in [[random models]] built on the [[data]] .
This allows us to solve [[sparse]] l<sub>1</sub> [[program]]ming [[problem]]s <i>exactly</i> and <i>without [[preprocess]]ing</i> at a [[cost]] that is identical to [[dense linear algebra]] both in terms of [[memory]] and [[speed]] .
This also opens the way to exploit [[coupling method]]s for [[establishing properti]]es required of a [[good]] [[similarity function]] as [[per]] [[their definition]] .
This applies to [[vendor]]s selling their [[products via Internet]], or to [[brick-and-mortar store]]s that make use of [[digital price]] [[tag]]s.
[[This approach]] allows us to define a [[criterion]] for [[active learning]] based on [[sampling set selection]] which aims at [[maximizing]] the [[frequency]] of the [[signal]]s that can be [[reconstructed]] from their [[sample]]s on the [[set]] .
[[This approach]] assumes a [[social network]] among [[user]]s and makes [[recommendation]]s for a [[user based on the ratings of the user]]s that have [[direct]] or [[indirect social relation]]s with the given [[user]] .
[[This approach]] can complement and extend [[traditional method]]s, providing [[researcher]]s with an additional [[measure]] that can quickly and [[cheaply assess]] [[large groups]] of [[participant]]s with [[minimal burden]] .
[[This approach]] can discover different [[evolving patterns]] of [[clusters]], including [[emergence]], [[disappearance]], [[evolution]] within a [[corpus]] and across different [[corpora]] .
[[This approach]] [[clusters words into groups]] based on the [[distribution]] of [[class label]]s associated with each [[word]] .
[[This approach]] decomposes [[model parameter]]s into a [[summation]] of two [[component]]s and imposes separate [[block-wise]] [[LASSO penalti]]es to each [[component]] when building a [[linear model]] in terms of the [[past Ï measurement]]s of [[feature]]s.
[[This approach]] enables [[full posterior inference]] including [[point]] and [[interval estimate]]s of the [[unknown regression function]] as well as the [[marginal effect]]s of [[potential predictor]]s.
[[This approach]] has the advantage of directly expressing the [[approximation]]s in terms of [[prior assumption]]s about the [[function]], which makes the [[consequence]]s of the [[approximation]]s much easier to [[understand]] .
[[This approach]] uses ideas from [[computer science]] about [[inheritance]] and from [[coordination theory]] about [[managing dependenci]]es.
[[This approach]] yields sufficient [[pseudo-randomness]] to provide the same [[convergence rate]] as [[random-order in-memory access]] .
[[This approximation]] must be [[robust]] to the [[kernel parameter]]s, for example the [[bandwidth]] for the [[Gaussian kernel]] .
[[This architecture]] gives flexibility to the [[application developer]]: whereas in previous "[[parameter server]]" designs the management of [[shared state]] is built [[into the system]], [[TensorFlow]] enables developers to [[experiment]] with novel [[optimization]]s and [[training algorithm]]s.
[[This article]] describes the [[architecture]] and [[design principles]] of [[IkeWiki]], a [[Semantic Wiki]] we developed at [[Salzburg Research]] .
[[This article]] discusses the formalisation of different forms of ([[non-]]) [[monotone induction]] by the [[well-founded semantics]] and illustrates the use of the logic for [[formalizing]] [[mathematical]] and [[common-sense knowledge]] .
[[This article]] employs [[conditional random fields (CRFs)]] for the task of extracting various common [[fields]] from the [[headers]] and [[citation]] of [[research papers]] .
This [[article]] introduces the [[mainstream method]]s for [[question answering]] over [[knowledge base]]s, describing typical [[semantic meaning representation model]]s and [[state-of-the-art system]]s for [[converting question]]s to predefined [[logical form]]s.
[[This article]] is about a novel [[probabilistic model]] that represents both [[author]]s and [[topic]]s.
[[This article]] is a [[retrospective]] on the theme of [[knowledge harvesting]]: [[automatically constructing]] [[large]] [[high]-quality knowledge base]]s from [[Internet source]]s.
[[This article]] overviews [[state-of-the-art approaches]] in [[term identification]] .
[[This article]] presents [[YAGO]], a [[large ontology]] with high [[coverage]] and [[precision]] .
[[This article]] provides an overview of [[recommender system]]s as well as [[collaborative filtering methods and algorithms]]; it also explains their evolution, provides an original [[classification]] for [[these system]]s, identifies areas of future [[implementation]] and develops certain [[areas selected for [[past]], [[present]] or [[future importance]] .
[[This article]] reports on an [[experiment]] with 22 [[reader]]s aimed at [[finding lexical cohesive patterns]] in 10 [[text]]s.
[[This article]] tests [[this hypothesis]] on a [[French]]-[[Japanese]] [[bilingual term extraction task]] .
[[This book]] discusses how [[model-based approach]]es can improve the [[daily practice]] of [[software professional]]s.
[[This book]] originated as lectures for a course on [[political philosophy]] that [[Rawls]] taught regularly at [[Harvard]] in the [[1980s]] .
[[This book]] presents some of the most important [[modeling]] and [[prediction technique]]s, along with relevant [[application]]s.
This [[bound]] suggests that [[increasing]] the [[strength]] and/or [[decreasing]] the [[correlation]] of an [[ensemble]]'s [[base classifiers]] may yield [[improved performance]] under the [[assumption]] of [[equal error costs]] .
[[This brief]] updates [[the 2011 report, The Re-Emergence of Concentrated Poverty,]] using the latest [[neighborhood-level]] [[poverty data]] from the [[American Community Survey]] for [[2008]] through [[2012]], to examine where and how [[poor populations]] have shifted at the [[neighborhood level]] .
This calls for an [[efficient]], [[scalable]] and [[effective solution]] that can enable a [[citywide]] and [[real-time travel time estimation]] .
This calls for finding a [[small number]] of [[representative pattern]]s to best [[approximate]] all other [[pattern]]s.
This calls for new [[tools]] that assist the [[analyst]] to be <i>[[efficient]]</i> in the [[tasks]] that she needs to [[execute]] .
This can produce [[name ambiguity]] which can affect the [[performance]] of [[document retrieval]], [[web search]], and [[database integration]], and may cause [[improper attribution]] of [[credit]] .
[[This chapter]] presents a comprehensive [[survey]] of [[neighborhood-based method]]s for the [[item recommendation problem]] .
[[This chart]] does not, for instance, include specific [[detailed revenue]] and [[expense classification]]s for utilities, [[transportation system]]s, [[airport]]s, [[hospital]]s, and numerous other activities commonly accounted for in [[proprietary fund]]s.
This chart therefore requires all [[local government]]s to [[charge salari]]es and benefits directly to the various [[function]]s, [[activiti]]es, and [[departments receiving]] the [[benefit]] .
This [[classifier]], called [[CHIRP]], is an [[iterative sequence]] of three stages ([[projecting]], [[binning]], and [[covering]]) that are designed to deal with the [[curse of dimensionality]], [[computational complexity]], and [[nonlinear separability]] .
This [[classifier]] is [[relational]] in [[nature]] : it relies on having the [[data]] presented as a [[partially labeled]] [[graph]] (also known as a [[within-network learning problem]]).
[[This class-propagation probability]] captures the [[node's intrinsic likelihood]] of belonging to each [[class]], and serves as a [[prior weight]] for each [[class]] when [[aggregating]] the [[neighbors' class label]]s in the [[collective inference procedure]] .
[[This collaboratively]] edited [[knowledgebase]] provides a common source of [[data]] for [[Wikipedia]], and [[everyone else]] .
[[This combination of idea]]s produces a new [[method]] for [[ranking]] a [[set of item]]s.
[[This combination]] of [[simplicity]] and powerful [[performance]] allows [[restreaming algorithm]]s to be easily adapted to [[efficiently]] tackle more challenging [[partitioning objective]]s.
[[This compositionality]] is what allows us to have a [[succinct characterization]] of [[meaning]] for a [[combinatorial range]] of possible [[utterance]]s.
[[This compression-based]] view automatically balances [[goodness-of-fit]] and [[model complexity]], making [[input parameter]]s [[redundant]] .
This [[computerized decision support system (CompTMAP)]] provides support in [[diagnosis]], [[treatment]], [[follow-up]], and [[preventive care]] and can be incorporated into the [[clinical setting]] .
[[This conception]] of [[social justice]] combines [[freedom]], [[equality]], and [[efficiency]] .
This [[conclusion]] seems to undermine existing [[work]] on [[data anonymization]] .
This, [[coupled]] with [[practical constraint]]s such as the [[budget]], [[campaigns' life time]], etc. makes the [[theoretical result]] from [[optimal auction theory not necessarily applicable]] and a further [[empirical study]] is required to confirm its [[optimality]] from the [[real-world system]]; 2) in [[RTB]] an [[advertiser]] is facing nearly [[unlimited supply]] and the [[auction]] is almost done in " last second ", which encourages spending less on the [[high cost]] [[ad placement]]s.
This creates a [[distributional divergence]] between the [[latent population distribution]]s of two [[cluster]]s: [[spammer]]s and [[non-spammer]]s.
This creates an [[opportunity]] for [[designer]]s to build [[item]]s that are likely to attract desirable [[tag]]s when [[published]] .
This [[data]] includes the [[citation graph]], [[authorship]] and [[publication venues]] of [[papers]], as well as their [[word content]] .
This [[data]] may show [[unusual behavior]] because of [[malicious activity]] .
This [[decoupled strategy]] is highly ineffective since the [[error]]s in [[Web]] [[classification]] will be [[propagated]] to [[Web]] [[information extraction]] and eventually [[accumulate]] to a [[high]] level.
[[This definition]] makes it clear that we need to [[represent formally]] and [[explicitly]] our [[model]] of the [[knowledge]] we are interested in (typically, a [[domain]]) and that [[this model]] should be agreed among [[user]]s, [[expert]]s, [[communiti]]es, etc. In other words, we can say that an [[ontology]] is a set of [[definition]]s in a [[formal language for concept]]s that [[describe the world of interest]], including the [[relationship]]s that connect these [[concept]]s.
[[This definition]] represents a novel [[type of outlier]]s considering both [[temporal dimension]] and [[community pattern]]s.
This [[demonstrates]] both the importance of broad [[query]] [[aspects]], and the efficacy of [[our algorithm]]s for [[extracting]] them.
[[This deployed product]] acts both to [[filter out]] the [[fraudulent traffic]] from the [[input data]] and to insure that [[we]] don't serve [[ads]] during [[unintended website visit]]s.
This distinguishes the [[problem]] from [[concept]] [[drift]] [[explorations]], where mostly a [[single]] [[concept]] is [[tracked]] .
This [[doctoral work]] aims at solving these [[limitation]]s by providing a novel [[approach]] for the [[early detection]] and [[forecasting]] of [[research trend]]s that will take advantage of the [[rich variety]] of [[semantic relationship]]s between [[research entiti]]es (e.g., [[author]]s, [[workshop]]s, [[communiti]]es) and of [[social media data]] (e.g., [[tweet]]s, [[blog]]s).
[[This document]] contains a [[uniform chart of account]]s for [[local government]]s in [[Georgia]] .
[[This document]] describes an [[open]] [[text-mining system]] that was developed for the Asian-European [[project KYOTO]] .
[[This document]] describes [[best practice recipe]]s for [[publishing vocabularies]] or [[ontologies]] on the Web (in [[RDF Schema]] or [[OWL]]).
This dose-effect study extends previous observations showing that [[psilocybin]] can occasion [[mystical-type experience]]s having persisting [[positive effects]] on [[attitudes]], [[mood]], and [[behavior]] .
This enables us to achieve [[dependent clustering]] and [[disparate clustering]] using the same [[optimization framework]] by merely [[maximizing]] versus [[minimizing]] the [[objective function]] .
This enables us to effectively [[identify]] [[network communiti]]es and important [[node]]s participating in the [[dynamics]] .
[[This]] enables us to not only [[browse information]] based on [[location]]s, but also discover [[pattern]]s in the [[location-based behavior]]s of [[user]]s.
[[This essay]] reviews what [[economist]]s have learned about the impact of [[labor market institution]]s, defined broadly as [[government regulation]]s and [[union activity]] on [[labor outcome]]s in [[developing countri]]es.
[[This estimation]] is usually based on the [[rating]]s given by this [[user]] to other [[item]]s, [[rating]]s given to [[this item]] by other [[user]]s, and possibly on some other [[information]] as well (e.g., [[user demographic]]s, [[item characteristic]]s).
[[This expertise analytics system]] has been deployed for key [[employee population segment]]s, yielding large [[reduction]]s in [[manual effort]] and the ability to [[continually]] and consistently serve [[up-to-date]] and [[accurate data]] for several [[business function]]s.
[[This fascinating problem]] is increasingly important in [[business]] and [[society]] .
This first comprehensive overview of [[SSL]] presents [[state-of-the-art algorithms]], a [[taxonomy of the field]], selected [[applications]], [[benchmark experiments]], and perspectives on [[ongoing]] and [[future research]] .
This [[flexible framework]] introduces challenges of [[scalability]] and [[performance–Many KBC system]]s built with [[DeepDive]] contain [[statistical inference]] and [[learning task]]s over terabytes of [[data]], and the [[iterative]] [[protocol]] also requires executing similar [[inference problem]]s multiple [[time]]s.
This [[formalization]] describes how a [[machine]] contributed to [[scientific knowledge]] .
This form of [[intrinsic dimensionality]] can be particularly useful in [[search]], [[classification]], [[outlier detection]], and other contexts in [[machine learning]], [[database]]s, and [[data mining]], as it has been shown to be equivalent to a [[measure]] of the [[discriminative power]] of [[similarity function]]s.
[[This formulation]] is [[non-convex]]; [[we]] convert it into its [[convex]] [[surrogate]], which can be [[routinely solved]] via [[semidefinite programming]] for [[small-size problems]] .
[[This framework]] allows us to combine different types of [[feature]]s ([[lab result]]s, [[vital sign]]s [[reading]]s, [[doctor]] and [[nurse note]]s, etc) into a single [[state]], which is updated each time new [[patient data]] is [[observed]] .
[[This framework]] assumes [[DSMS server]] is [[secure]] and [[trustable]], and [[protects providers' data]] from [[illegal access]] by [[data user]]s.
[[This framework]] can leverage [[prior]]s of any form, ranging from [[formal ontologi]]es (e.g., [[ICD9 code]]s) to [[data-derived similarity]] .
[[This framework]] first [[converts time series]] into [[time-interval sequence]]s of [[temporal abstraction]]s.
[[This framework]] for [[clustering]] and [[visualization]] will enable enterprises to prioritize the [[issue]]s in their [[IT infrastructure]] and improve the [[reliability]] and availability of their [[service]]s.
[[This framework]] has been implemented into two [[approach]]es for different [[application scenario]]s, and has been deployed to serve over 300 pilot [[household]]s in [[Dubuque]], [[IA]] .
[[This framework]] is able to [[transfer]] [[high dimensional data]] without the need for [[decomposition]] by finding shared [[implicit]] [[cluster-level tensor]] from multiple [[domain]]s.
[[This framework]] is suitable for [[detecting time]] and [[duration]] of [[event]]s, as well as their [[impact]] and [[dynamics]], from [[time series]] of [[publication volume]] .
[[This functional architecture]], which in key respects resembles that of a [[classical]] [[computer]], may play a critical role in enabling [[human]]s to [[flexibly generate]] [[complex thought]]s.
This gives a [[streaming algorithm]] whose [[error]] [[decay]]s [[proportional]] to 1 / <i>l</i> using <i>O</i> (<i> ml </i>) [[space]] .
This greatly limits the potential of [[ATM]]s on many [[application]]s such as [[medical prediction]] and [[business intelligence]], where [[practitioner]]s need suggestions on [[action]]s that can lead to [[desirable outcome]]s with [[minimum cost]]s.
This [[greedy algorithm]] is suboptimal for both [[advertiser]]s and [[publisher]]s when [[advertiser]]s have a [[finite budget]] .
[[This]] [[greedy strategy]] results in [[low efficiency]] when [[parameter learning]] is itself [[non-trivial]], such as in [[MRFs]], in which [[parameter learning]] depends on an expensive [[subroutine to calculate gradients]] .
[[This group]] is even [[larger than]] the [[Baby Boomers]], but [[Millennials]] have taken a back seat in our [[City's economy]], earning about [[20 percent]] [[less than]] [[the generation before them]] .
This has fueled the [[growth]] of different kinds of [[data set]]s such as [[data stream]]s, [[spatio-temporal data]], [[distributed stream]]s, [[temporal network]]s, [[and time series data]], [[generated by]] a [[multitude of application]]s.
[[This]] has implications for how to [[exploit]] and [[transfer knowledge]] from [[readily available]] [[auxiliary]] [[tagging resource]]s to improve another [[social web retrieval system]] .
[[This hierarchical prior]] introduces a [[sharing mechanism]] across [[sample]]s and allows for [[identifying]] [[local realization]]s of [[class]]es across [[sample]]s.
This [[hierarchy]] allows us to [[identify]] [[anomalies]] in the [[data set]] at different [[scales]], which are then posed as [[queries]] to the [[user]] .
This, however, leads to the [[problem]] of whether the [[results]] found by one [[method]] are a reflection of the [[phenomenon]] shown by the [[results]] of another [[method]], or whether the [[results]] depict in some sense [[unrelated]] [[properties]] of the [[data]] .
[[This]] implies that [[content recommendation]] would be more useful for [[knowledge management system]]s, where [[user]]s are often looking for [[specific knowledge]], rather than for [[general purpose Web site]]s, whose [[customer]]s often come for [[scanning]] .
This includes [[assigning]] the [[base form]] ([[lemmatization]]) and [[determining]] the [[morphosyntactic properti]]es ([[categorization]]).
[[This inference]] is performed over a [[full-scale stream]] of [[Twitter data]], whose [[statistical distribution]] evolves rapidly over [[time]] .
[[This information]] can be divided into two [[categori]]es related to its [[source]]: [[rich side information]] concerning [[user]]s and [[item]]s, and [[interaction information]] associated with the [[interplay of users and items]] .
This [[initialization]] works well in [[supervised setting]]s where a [[network]] can subsequently modify these [[vector]]s to capture certain [[label distribution]]s.
This interest spans [[theory]] from the perspective of [[understanding]] the [[etiology of crime]], and [[practice]] from the perspective of developing [[effective]] [[criminal justice intervention]]s to reduce [[crime]] .
This interface isolates an [[application]] from many of the [[idiosyncrasies]] of a specific [[KRS]] and enables the development of tools (e.g., [[graphical browser]]s, [[frame editor]]s, [[analysis tool]]s, [[inference tool]]s) that operate on many [[KRS]]s.
This intuition leads to a [[new sketch]], [[Discrete Max-count]], and the [[analysis]] of a [[class of sketch]]es, [[self-similar area cutting decomposition]]s that have attractive [[properti]]es and [[unbiased estimator]]s for both [[streaming]] and [[non-streaming setting]]s.
This in turn corresponds to the [[problem]]s of performing [[entity resolution]], [[link prediction]], and [[node labeling]] to [[infer]] the [[hidden graph]] .
This in turn motivates two new [[algorithms]], whose [[performance]] we study [[empirically]] using [[citation data]] and [[web hyperlink data]] .
[[This]] is a book about how [[information technologies]] are affecting [[jobs]], [[skills]], [[wages]], and [[the economy]] .
This is achieved by [[learning a shared subspace]] between the two [[source]]s under a [[joint]] [[Nonnegative Matrix Factorization]] in which the level of [[subspace sharing]] can be [[explicitly]] [[controlled]] .
This is an effective [[real-time]] [[prediction]] [[runtime]] of 9.7 [[us]] per [[rating]] which is far superior to [[previously reported result]]s.
This is a standard requirement of [[US]] [[commencement speech]]es, the deployment of [[didactic]] little [[parable-ish stori]]es.
This is demonstrated by a [[study]] with [[predicting product categori]]es from an [[Amazon]] [[co-purchasing network]] .
This is especially important for most [[biological entities]], which have corresponding [[entries]] in various [[databases]], e.g., ''[[Swiss-Prot]]'' for [[proteins]] .
This is evident to me from [[working]] in [[nation]]s, [[state]]s, [[region]]s, [[inner citi]]es, and [[compani]]es at widely varying [[stages of development]] .
This is, however, [[unavoidable for planning]] using [[partial]] and [[incomplete model]]s (e.g., considering [[planning]] using [[action models learned]] from [[partial]] and [[noisy plan trace]]s).
This is important for many [[website]]s with [[tagging]] capabilities like [[last.fm]] or [[delicious]] .
This is known as a [[generative approach]] since by [[sampling]] from the [[joint distribution]] it is possible to [[generate]] [[synthetic example]]s of the [[feature vector]] <math>\bf{x}</math>.
This is made feasible by employing a [[gradient-bounded]] [[coordinate-descent algorithm]] for [[efficiently]] [[selecting]] [[discriminative subsequence]]s without having to expand the whole [[space]] .
[[Thi]]s is not always true: for example, a [[user]] could [[install]] and use both [[Firefox]] and [[Google Chrome]] as [[browser]]s.
This is particularly important in [[cases]] where due to [[finite]] [[resource]]s or [[domain]] [[requirement]]s, one wants to make [[decisions]] based only on the most reliable rather than on the entire [[set]] of [[prediction]]s.
This is problematic because [[word]]s are often [[polysemous]] and [[global context]] can also provide [[useful information]] for [[learning word meanings]] .
This is the [[case]] when [[ranking]] [[search result]]s for [[page]]s on the [[World Wide Web]] or for [[merchandize]] on an [[e-commerce site]].[[In this work, we]] present a new [[online]] [[ranking algorithm]], called [[NoRegret KLRank]] .
This is the first [[algorithm]] for [[inferring]] [[communiti]]es in [[dynamic networks]] with a [[provable]] [[approximation]] [[guarantee]] .
This is the first [[logically precise]], [[computationally implementable]], [[book-length]] [[account]] of [[rational belief revision]] .
[[This]] is the handbook for the [[KONECT project]], the "[[Koblenz Network Collection"]], a [[scientific project]] to [[collect]], [[analyse]], and [[provide]] [[network dataset]]s for [[researcher]]s in all related [[fields of research]], by the [[Namur Center for Complex Systems (naXys)]] at the [[University of Namur]], [[Belgium]], with web hosting provided by the [[Institute for Web Science]] and [[Technologies (WeST)]] at the [[University of Koblenz]] -- [[Landau]], [[Germany]] .
This is usually [[modeled]] in the [[literature]] as an <i>[[online allocation problem]] </i>, where [[contract]]s are [[represented by]] overall [[delivery constraint]]s over a [[finite time horizon]] .
[[This journal]] was [[founded]] in [[1880]] by [[Thomas Edison]] and continues as one of the most influential [[scientific journals]] today.
[[This knowledge]] consists of [[information]] (e.g., [[who knows what]]) and of [[know-how]] (e.g., how to organize a [[research team]]).
This [[knowledge]] is usually [[evolvable]] and therefore an [[ontology maintenance process]] is required to keep the [[ontological knowledge]] [[up-to-date]] .
[[This]] [[lazy strategy]] is [[guaranteed]] to converge to the [[global optimum]] and can effectively [[select significant features]] .
This leads to interesting [[connection]]s and [[contrast]]s with [[active learning]] and the [[trade-off]]s of [[exploration]] and [[exploitation]] .
This [[learning setting]] is [[ubiquitou]]s in [[online system]]s (e.g., [[ad placement]], [[web search]], [[recommendation]]), where an [[algorithm]] makes a [[prediction]] (e.g., [[ad ranking]]) for a given [[input]] (e.g., [[query]]) and observes [[bandit feedback]] (e.g., [[user clicks on presented ads]]).
[[This lecture]] gives an introduction to the area including the [[fundamental problem]]s, existing [[approach]]es, [[theori]]es, [[application]]s, and [[future work]] .
This [[legal determination]] hinges on a definition of a [[protected class]] ([[ethnicity]], [[gender]]) and an [[explicit description]] of the [[process]] .
[[This level of performance]] is several times [[faster than]] state-of-the-art implementations both [[CPU]] and [[GPU platform]]s.
[[This list]] of [[feature]]s suggests that [[information filtering]] is a [[well-defined]] and [[unique process]] .
[[This loss]] is inadequate for [[task]]s like [[information retrieval]] where we prefer [[ranked list]]s with [[high precision]] on the [[top of the list]] .
This [[low dimensional embedding]] not only [[preserve]]s the [[semantic closeness]] of [[words and document]]s, but also has a [[strong predictive power]] for [[the particular task]] .
This makes [[FMs easily]] applicable even for [[user]]s without [[expert knowledge]] in [[factorization model]]s.
This makes [[just consequentialism]] a [[practical]] and [[theoretically sound approach]] to [[ethical problem]]s of [[computer]] and [[information ethics]] .
This makes the [[learn]]ed [[hash code]]s flexibly cope with the [[characteristic]]s of different [[data domain]]s.
This [[manual labor]] [[scales]] [[linearly]] with the number of [[target]] [[relations]] .
This [[mechanism]] aims to drive a [[benchmarking assessment tool]] for the [[current state-of-the-art]] of [[ontology population]], and to set a [[standard]] for [[best practice]] for future [[evaluation]] of [[human language technology]] for the [[semantic web]] .
This [[method]] is similar to the use of [[association rule]]s as [[predictive rule]]s in [[rule]] based [[classifier]]s.
[[This method]] is the [[easiest to implement]] for [[insert]]s into the [[XXX table]] and for certain [[simple queri]]es.
This [[method]] maximizes an [[objective function]] that is formulated by decomposing the [[traditional]] [[maximum likelihood method]] for [[estimating]] [[heritability]] of a [[quantitative trait]] .
This [[methodology]] addresses unbalanced [[data stream]]s, [[data]] where change occurs on different [[time scale]]s, and the question of how to [[split the data]] between [[training]] and [[testing]], over multiple [[model]]s.
[[This method]] provides an [[automated]], [[highly generalizable framework]] for [[identifying the underlying control mechanisms]] responsible for the [[dynamic regulation of growth]] and [[form]] .
[[This method]] rapidly identifies [[road]]s with elevated [[crash-rate]], potentially due to [[skid resistance]] [[deficit]], for investigation.
This [[mismatch]] between [[spoken dialogue]]s and written [[encyclopedia]] could bring about [[inaccuracy]] in [[selecting proper Wikipedia article]]s as sources for [[domain knowledge]] .
This mitigates the [[problem]] of [[representing uncertainty]] in [[deep learning]] without sacrificing either [[computational complexity]] or [[test accuracy]] .
[[This model]] automatically [[balance]]s the [[prestige]] and the [[diversity]] of the [[top ranked]] [[vertice]]s in a [[principled way]] .
This [[model]] can be used to [[simultaneously]] recognize the [[target]] [[Web page]]s and [[extract]] the corresponding [[metadata]] .
[[This model]] can help us to understand [[information diffusion]] from both [[macroscopic]] and [[microscopic perspective]]s.
This [[modeling]] enables various [[application]]s including [[automating]] [[repetitive search task]]s, and [[helping search engine]] [[developer]]s [[design]] [[micro-segment]]s of [[factoid question]]s.
[[This model]] initially incorporates any [[prior]] (possibly incorrect) [[belief]]s a [[data miner]] has about the [[data]] .
[[This model]] should support: [[digital summarization]], [[evidence examination]], [[challenge]], [[verification]] and [[remix]], and [[incremental adoption]] .
This [[modular setup]] allows [[re-use of component]]s across several different [[pipeline]]s.
This need to [[extract]] [[useful information]] from large [[review corpora]] has spawned considerable [[prior work]], but so far all have [[drawback]]s.
[[This new book]] reports on a decade of work [[developing]] [[SUMO]] and its associated [[tools]], [[models]] and [[domain ontologies]] .
[[This new data model]] enables to compare the [[node]]s in a [[broader context]] and [[rank]] them at a finer [[granularity]] .
This opens up new possibilities to [[explore]], [[quantify]], and [[visualize]] [[sequential data]] .
[[this paper]] addresses a novel [[topic]], namely [[coupled behavior analysis]] in [[hidden groups]] .
[[This paper]] addresses [[estimating]] the number of the [[user]]s of a specific [[application]] behind [[IP addresses (IPs)]] .
[[This paper]] addresses exactly such a challenge : how to [[jointly learn]] the [[accuracy]] of [[labeling]] [[source]]s and obtain the most [[informative label]]s for the [[active learning task]] at hand [[minimizing]] total [[labeling]] effort.
[[This paper]] addresses [[geospatial interpolation]] for [[meteorological measurement]]s in which we [[estimate]] the values of [[climatic metric]]s at [[unsampled site]]s with existing [[observation]]s.
[[This paper]] addresses [[Named Entity Mining (NEM)]], in which [[we]] [[mine knowledge]] about [[named entities]] such as [[movies]], [[games]], and [[books]] from a [[huge amount of data]] .
[[This paper]] addresses several key issues in the [[ArnetMiner system]], which aims at [[extracting]] and [[mining]] [[academic social networks]] .
[[This paper]] addresses the issue of [[unsupervised]] [[network]] [[anomaly detection]] .
[[This paper]] addresses [[the problem]] of [[low impact]] of [[smart city application]]s observed in the [[field]]s of [[energy]] and [[transport]], which constitute [[high-priority domain]]s for the [[development]] of [[smart citi]]es.
[[This paper]] addresses the question: Given a [[graph]], how can we automatically [[discover]] <i>[[roles]]</i> for [[node]]s?
[[This paper]] addresses the task of [[user activity classification]] in [[microblog]]s, where users can [[publish]] short [[message]]s and maintain [[social networks online]] .
[[This paper]] advances the [[state of the art]] in [[probabilistic method]]s for [[estimating]] the number of distinct [[element]]s in a [[streaming setting New streaming algorithm]]s are given that [[provably]] beat the [["optimal" error]]s for [[Min-count]] and [[HyperLogLog]] while using the same [[sketch]] .
[[This paper]] aims at [[discovering community]] [[structure]] in rich [[media]] [[social network]]s, through [[analysis]] of [[time-varying]], [[multi-relational data]] .
[[This paper]] aims to provide a [[timely review]] on [[this area]] with [[emphasis]] on [[state-of-the-art]] [[multi-label learning algorithm]]s.
[[This paper]] also addresses the gaps between [[FFD]] and the needs of the [[industry]] to encourage additional [[research]] on [[neglected topic]]s, and [[conclude]]s with several suggestions for further [[FFD research]] .
[[This paper]] also explores using the [[estimated size]]s to [[detect]] and [[filter]] [[abusive traffic]] .
[[This paper]] also provides [[implementation]] details of a [[program product]] that uses [[this framework]] and shows some illustrative examples of [[successful]] [[result]]s.
[[This paper]] analyses alternative [[technique]]s for [[deploy]]ing [[low-cost human resource]]s for [[data acquisition]] for [[classifier induction]] in [[domains]] exhibiting extreme [[class imbalance]] -- where traditional [[labeling strategies]], such as [[active learning]], can be ineffective.
[[This paper]] deals with an [[ontology-driven approach]] for [[semantic annotation of documents]] from a [[corpus]] where each [[document]] describes an [[entity]] of a same [[domain]] .
[[This paper]] deals with the [[extraction]] of [[semantic relation]]s from [[scientific text]]s.
[[This paper]] describes a [[Bayesian approach]] to [[multiclass]], [[multi-label document classification]] .
[[This paper]] describes a [[bootstrapping algorithm]] called [[Basilisk]] that [[learn]]s [[high-quality]] [[semantic lexicon]]s for multiple [[categori]]es.
[[This paper]] describes a [[learning companion system]] called [[Integration-Kid]] in the [[domain]] of [[learning]] [[indefinite integration]] .
[[This paper]] describes a [[method for linear text segmentation]] which is twice as [[accurate]] and over seven times as [[fast]] as the [[state-of-the-art]] ([[Reynar, 1998]]).
[[This paper]] describes an [[algorithm]] for [[translating]] [[technical words]] and [[term]]s from [[noisy]] [[parallel corpora]] across [[language groups]] .
[[This paper]] describes an [[automated message-understanding]] and [[routing system]] deployed by [[IBM]] at [[UNICEF]] .
[[This paper]] describes and [[evaluates]] [[privacy-friendly methods]] for [[extracting]] [[quasi-social network]]s from [[browser behavior]] on [[user-generated content sites]], for the purpose of [[finding]] good [[audience]]s for [[brand]] [[advertising]] (as opposed to [[click]] [[maximizing]], for example).
[[This paper]] describes an [[efficient algorithm]] for [[inexact graph matching]] .
[[This paper]] describes an [[efficient]] [[approach]] to [[record linkage]] .
[[This paper]] describes a new [[technique]] and [[analysis]] for using [[on-line learning algorithm]]s to solve [[active learning problem]]s.
[[This paper]] describes an [[integrated framework]] for [[minimizing]] [[false positive ticket]]s and [[maximizing]] the [[monitoring coverage for system fault]]s.
[[This paper]] describes an [[interactive system]] for [[detecting payment error]]s in [[insurance claim]]s with [[claim auditor]]s in the [[loop]] .
[[This paper]] describes a novel [[hybrid approach]] of using of [[community finding]] and [[social network analytic]] [[centrality measure]]s to [[identify good candidates for labeling]] and then using [[ERM]] to find the best [[instance]] in this [[candidate]] [[set]] .
[[This paper]] describes a novel [[theoretical]] and [[empirical approach]] to [[task]]s such as [[business process redesign]] and [[knowledge management]] .
[[This paper]] describes a practical, [[learning-driven]] [[client-side personalization]] [[approach]] for [[keyword advertising platform]]s, an [[emerging application]] previously not addressed in [[literature]] .
[[This paper]] describes a [[rational reconstruction]] of [[M5]], a method developed by [[Quinlan (1992)]] for [[inducing trees of regression models]] .
[[This paper]] describes [[FOIL]], a system that learns [[Horn clause]]s from [[data expressed as relations]] .
[[This paper]] describes the [[BID Data Suite]], a [[collection]] of [[hardware]], [[software]] and [[design pattern]]s that enable fast, [[large-scale]] [[data mining]] at very [[low cost]] .
[[This paper]] describes the [[design]], [[implementation]], and [[deployment]] of [[Metaphor]], the related [[search recommendation system]] on [[LinkedIn]], a [[professional social networking site]] with over 175~million [[members worldwide]] .
[[This paper]] describes the interaction among [[language resource]]s for an adequate [[concept annotation]] of [[domain text]]s in several [[language]]s.
[[This paper]] describes the [[MineFleet]] [[distributed]] [[vehicle performance data mining system]] designed for [[commercial fleets]] .
[[This paper]] describes three [[linear scale]], [[incremental]], and fully [[automatic]] [[semantic mining algorithm]]s that are at the [[foundation]] of the new [[Semantic Platform]] being [[released]] in the next version of [[SQL Server]] .
[[This paper]] develops [[performance model]]s that [[quantify]] the impact of these [[partitioning]] and [[provisioning decision]]s on overall [[distributed system performance]] and [[scalability]] .
[[This paper]] devises a novel [[unsupervised framework]] to solve [[this problem]], including two [[main component]]s: (1) a [[three-layer factor graph model]] and three types of [[potential function]]s; (2) a [[ranked-margin learning]] and [[inference algorithm]] .
[[This paper]] discloses the [[implementation]] details behind this [[personalized feed system]] at [[LinkedIn]] which can not be found from related [[work]], and addresses the [[scalability]] and [[data sparsity]] [[challenge]]s for [[deploy]]ing [[the system online]] .
[[This paper]] discusses [[directed first-order model]]s that require an [[aggregation operator]] when a [[parent random variable]] is [[parameterized]] by [[logical variable]]s that are not present in a [[child random variable]] .
[[This paper]] discusses the [[data mining approach]] followed in a [[project]] called [[TRAQUASwine]], aimed at the [[definition]] of [[method]]s for [[data analytical assessment]] of the [[authenticity]] and [[protection]], against [[fake version]]s, of some of the highest value [[Nebbiolo-based wine]]s from [[Piedmont region]] in [[Italy]] .
[[This paper]] examines current [[Semantic Web]] [[annotation platform]]s that provide [[annotation]] and related [[service]]s, and [[review]]s their [[architecture]], [[approach]]es and [[performance]] .
[[This paper]] examines important [[factors]] for [[link prediction]] in [[networks]] and provides a [[general]], [[high-performance framework]] for the [[prediction task]] .
[[This paper]] examines the role of the [[corporate objective function]] in [[corporate productivity]] and [[efficiency]], [[social welfare]], and the [[accountability]] of [[manager]]s and [[director]]s.
[[This paper]] explores an important and relatively unstudied [[quality measure]] of a [[sponsored search]] [[advertisement]] : [[bounce rate]] .
[[This paper]] explores the [[consequence]]s of a [[hypothesis]] concerning [[worker behavior]], which we shall call the [[fair wage–effort hypothesis]] .
[[this paper]] first introduces the [[concept]] of [[community outliers]] ([[interesting points]] or rising [[stars]] for a more [[positive sense]]), and then shows that well-known [[baseline approaches]] without considering [[links]] or [[community information]] cannot find these [[community outliers]] .
[[This paper]] focuses on developing [[effective]] and [[efficient algorithm]]s for [[top-N recommender system]]s.
[[this paper]] focuses on [[outsourcing]] [[frequent itemset mining]] and examines the issue on how to [[protect privacy]] against the case where the [[attackers]] have [[precise knowledge]] on the [[supports]] of some [[items]] .
[[This paper]] focuses on [[predicting the existence]] and the [[type]] of [[links]] between [[entities]] in such [[domains]] .
[[this paper]] formalizes [[this problem]] using [[three]] [[ingredients]]: the [[existing]] [[class labels]], the [[underlying]] [[separability in the data]], and a special [[type of input]] from the [[domain expert]] .
[[This paper]] formulates [[entity matching]] as a [[classification problem]], where the basic goal is to [[classify]] [[entity pair]]s as [[matching]] or [[non-matching]] .
[[This paper]] has described a [[dynamic programming]] [[n-best parsing algorithm]] that utilizes a [[heuristic]] coarse-to-fine refinement of [[parses]] .
[[This paper]] instantly infers the [[gas consumption]] and [[pollution emission]] of [[vehicles traveling]] on a [[city's road network]] in a [[current time slot]], using [[GPS trajectori]]es from a [[sample]] of [[vehicle]]s (e.g., [[taxicab]]s).
[[This paper]] intends to provide some insights of a [[scientific problem]]: how likely [[one's interests]] can be [[inferred]] from [[his/her]] [[social connections]] -- [[friends]], [[friends' friends]], [[3-degree friends]], etc?
[[This paper]] introduces a new [[method]] for [[estimating the local neighborhood]] and [[scale]] of [[data point]]s to improve the [[robustness]] of [[spectral clustering algorithm]]s.
[[This paper]] introduces a new [[statistical approach]] to [[automatically]] [[partitioning text into coherent segment]]s.
[[This paper]] introduces a [[nonlinear logistic regression model]] for [[classification]] .
[[This paper]] introduces a novel [[demand management solution]]: using [[data]] from dedicated [[occupancy sensor]]s an [[iteration scheme updates parking rate]]s to better [[match demand]] .
[[This paper]] introduces a novel [[image decomposition approach]] for an [[ensemble]] of [[correlated image]]s, using [[low-rank]] and [[sparsity constraint]]s.
[[This paper]] introduces a [[technique]] based on [[approximate optimization]] of a [[conservatively regularized objective function]] within each [[minibatch]] .
[[This paper]] introduces [[KOG]], an [[autonomous system]] for [[refining]] [[Wikipedia's infobox-class ontology]] towards this end.
[[This paper]] introduces [[LUDIA]], a novel [[low-rank approximation algorithm]] that utilizes [[aggregation constraint]]s in addition to [[auxiliary information]] in order to [[estimate]] or "[[reconstruct]] " the original [[individual-level value]]s from [[aggregate data]] .
[[This paper]] introduces [[mass estimation]] -- a [[base modelling mechanism]] in [[data mining]] .
[[This paper]] introduces novel and effective ways of [[ranking]] [[prediction]]s by their [[accuracy]] for [[problem]]s involving [[large-scale]], [[heterogeneous data]] with a [[dyadic]] [[structure]], i.e., where the [[independent variable]]s can be naturally decomposed into three [[groups]] associated with two [[sets]] of [[elements]] and their [[combination]] .
[[This paper]] introduces [[Open IE]] ([[OIE]]), a new [[extraction]] [[paradigm]] where [[the system]] makes a single [[data-driven]] [[pass]] over its [[corpus]] and [[extracts]] a large set of [[relational tuples]] without requiring any [[human input]] .
[[This paper]] introduces the problem of combining [[multiple partitionings]] of a [[set of object]]s into a single [[consolidated clustering]] without accessing the [[feature]]s or [[algorithm]]s that determined these [[partitioning]]s.
[[This paper]] introduces the use of [[Wikipedia]] as a resource for [[automatic]] [[keyword extraction]] and [[word sense disambiguation]], and shows how this online [[encyclopedia]] can be used to achieve [[state-of-the-art results]] on both these [[tasks]] .
[[This paper]] investigated the application of [[co-training]] and [[self-training]] to [[supervised word sense disambiguation]] .
[[this paper]] investigates [[frequent subgraph mining]] on [[uncertain graphs]] under [[probabilistic semantics]] .
[[This paper]] investigates the application of [[cotraining]] and [[self-training]] to [[word sense disambiguation]] .
[[This paper]] is about [[generating]] [[coherent]] [[summaries of scientific topics]] .
[[This paper]] is about [[representation learning]], i.e., [[learning]] [[representations of the data]] that make it easier to [[extract useful information]] when [[building classifier]]s or other [[predictor]]s.
[[This paper]] is concerned with the [[estimation]] of a [[local measure]] of [[intrinsic dimensionality (ID)]] recently proposed by [[Houle]] .
[[This paper]] is concerned with the [[joint allocation]] of [[bid price]] and [[campaign budget]] in [[sponsored search]] .
[[This paper]] is the first [[research]] to study [[EBSN]]s [[at scale]] and paves the way for [[future studi]]es on this new type of [[social network]] .
[[This paper]] lays out a [[set]] of [[design principle]]s for [[large-scale]] [[autonomous data mining]] [[system]]s and then demonstrates our application of these principles within the [[m6d automated ad targeting system]] .
[[This paper]] makes three [[contribution]]s to the [[study]] of [[relational learning]] .
[[This paper]], [[motivated]] by [[Autonomic Computing]], extends [[AP]] to the [[data streaming framework]] .
[[This paper]] presents a [[Bayesian method]] for [[constructing]] [[probabilistic networks]] from [[databases]] .
[[This paper]] presents a [[bid-optimization approach]] that is implemented in production at [[Media6Degree]]s for [[bidding]] on these [[advertising opportuniti]]es at an [[appropriate price]] .
[[This paper]] presents a]]classifier-combination]] [[experimental framework]] for [[named entity recognition]] in which four diverse [[classifier]]s ([[robust linear classifier]], [[maximum entropy]], [[transformation-based learning]], and [[hidden Markov model]]) are combined under different conditions.
[[This paper]] presents a [[Cloud-based system computing]] [[customized]] and practically fast [[driving route]]s for an [[end user]] using ([[historical]] and [[real-time]]) [[traffic condition]]s and [[driver behavior]] .
[[This paper]] presents a combination of [[strategi]]es, deployed by the [[online advertising firm Dstillery]], for [[learning]] many [[model]]s from extremely [[high-dimensional data]] efficiently and without [[human intervention]] .
[[This paper]] presents a [[fast approximate]] [[similarity search method]] for finding the most [[similar]] [[object]] to a given [[query object]] from an [[object set]] with a [[dissimilarity]] with a [[success probability]] exceeding a given value.
[[This paper]] presents a [[hybrid random field model]] for [[pseudo-likelihood]] [[estimation]] in [[high-dimensional]] [[domain]]s.
[[This paper]] presents a [[Hybrid Shilling Attack Detector]], or [[HySAD]] for short, to tackle these [[problem]]s.
[[This paper]] presents a [[large-scale system]] for the [[information extracted]] from a [[large encyclopedic collection]] and [[Web search result]]s.
[[This paper]] presents a mapping from these [[real-time]] obtainable [[feature]]s of a [[human speaker]] to [[agent]] [[listening behavior]]s.
[[This paper]] presents a [[meta path graph clustering framework]], [[VEPATHCLUSTER]], that combines [[meta path]] [[vertex-centric]] [[clustering]] with [[meta path edge-centric]] [[clustering]] for improving the [[clustering]] [[quality]] of [[heterogeneous network]]s.
[[This paper]] presents an [[architectural discussion]] of [[DBMS design principle]]s, including [[process model]]s, [[parallel architecture]], [[storage system design]], [[transaction system implementation]], [[query processor]] and [[optimizer architecture]]s, and typical [[shared component]]s and [[utiliti]]es.
[[This paper]] presents an efficient [[active-transductive approach]] for [[classification]] .
[[This paper]] presents an efficient [[feature induction method]] for [[CRFs]] .
[[This paper]] presents a new [[technique]] on [[mining]] [[emerging pattern]]s using [[streaming feature selection]] .
[[This paper]] presents a novel [[approach for extracting]] [[high-quality]] '[[thread-title, reply]]' [[pair]]s as [[chat knowledge]] from [[online discussion forum]]s so as to efficiently support the [[construction of a chatbot]] for a certain [[domain]] .
[[This paper]] presents an [[unsupervised learning algorithm]] for [[sense disambiguation]] that, when [[trained]] on [[unannotated English text]], rivals the performance of [[supervised technique]]s that require [[time-consuming]] [[hand annotations]] .
[[This paper]] presents a [[semantic annotation approach]] for [[highly accurate closed domain]] based on [[multi-ontology annotation]] ([[domain]] and [[application ontologies]]).
[[This paper]] presents a simple [[unsupervised learning algorithm]] for [[classifying reviews]] as [[recommended (thumbs up)]] or [[not recommended (thumbs down)]] .
This [[paper]] presents a [[statistical model]] which [[trains]] from a [[corpus annotated with Part-Of-Speech tags]] and [[assigns them]] to [[previously unseen text]] with [[state-of-the-art]] [[accuracy]] (96.6%).
[[This paper]] presents a tutorial introduction to the use of [[variational method]]s for [[inference]] and [[learning]] in [[graphical model]]s ([[Bayesian network]]s and [[Markov random field]]s).
[[This paper]] presents [[conditional topical coding (CTC)]], a novel [[formulation]] of [[conditional topic model]]s which is [[non-probabilistic]] .
[[This paper]] presents [[context-group discrimination]], a [[disambiguation algorithm]] based on [[clustering]] .
[[This paper]] presents [[EMeralD]], a [[highly scalable system]] for [[accurately detecting]] and [[filtering misbehaving user]]s in [[online video chat application]]s.
[[This paper]] presents [[Latent Association Analysis (LAA)]], a [[generative model]] that analyzes the [[topic]]s within [[two]] [[document set]]s simultaneously, as well as the [[correlation]]s between the two [[topic structure]]s, by considering the [[semantic association]]s among [[document pair]]s.
[[This paper]] presents new [[homogeneous series]] on [[top share]]s of [[income]] and [[wage]]s from [[1913]] to [[1998]] in the [[United State]]s using [[individual tax returns data]] .
[[This paper]] presents [[Non-Parametric Heterogeneous Graph Scan (NPHGS)]], a new approach that considers the entire [[heterogeneous network]] for [[event detection]]: we first [[model]] the [[network]] as a [[" sensor " network]], in which each [[node]] senses its "[[neighborhood environment]] " and reports an [[empirical p-value measuring]] its [[current level]] of [[anomalousness]] for each [[time interval]] (e.g., [[hour or day]]).
[[This paper]] presents the results of the first [[study]] of the <i>[[uniqueness]] of [[source code]]</i>.
[[This paper]] proposes a [[domain-independent statistical methodology]] to develop [[dialog manager]]s for [[spoken dialog system]]s.
[[This paper]] proposes a [[fast]] and [[robust parallel SGD matrix factorization algorithm]], called [[MLGF-MF]], which is [[robust]] to [[skewed matrice]]s and [[runs efficiently]] on [[block-storage device]]s (e.g., [[SSD disk]]s) as well as [[shared-memory]] .
[[This paper]] proposes a [[framework]] for the [[user-interactive exploration]] of a [[condensed representation]] of [[groups]] of executions of a given [[process]] .
[[This paper proposes algorithm]]s for [[learning]] [[location semantics]] and achieving [[semantically secure cloaking]] .
[[This paper]] proposes an [[algorithm]] named [[CRPS]] (for [[Column Removal during Progressive Sampling]]) that integrates [[sampling]] and [[column reduction]] .
[[This paper]] proposes an [[alternative approach]] to [[time series]] [[kNN search]], following a [[nontraditional]] [[pruning style]] .
[[This paper]] proposes an [[efficient]] [[algorithm]] that guarantees the same [[clustering result]]s as the original [[algorithm]] .
[[This paper]] proposes a new [[experimental]] [[data stream]] [[framework]] for studying [[concept drift]], and two new [[variant]]s of [[Bagging]] : [[ADWIN Bagging]] and [[Adaptive-Size Hoeffding Tree (ASHT) Bagging]] .
[[This paper]] proposes a new [[model for extracting]] an [[interpretable]] [[sentence embedding]] by introducing [[self-attention]] .
[[This paper]] proposes an [[online mixture modeling methodology]] in which individual [[component]]s can have different [[marginal distribution]]s and [[dependency structure]]s.
[[This paper]] proposes a novel [[multi-task learning]] [[framework]] which aims to [[concurrently]] address all the [[challenge]]s.
[[This paper]] proposes a [[probabilistic model]] for [[inferring]] the [[diffusion network]], which we call [[Probabilistic Latent Network Visualization (PLNV)]]; it is based on [[cascade data]], a [[record]] of [[observed time]]s of [[node influence]] .
[[This paper]] proposes conducting [[NEM]] by using [[click-through]] [[data]] collected at a [[web search engine]], employing a [[topic model]] that generates the [[click-through]] [[data]], and [[learning]] the [[topic model]] by [[weak supervision]] from [[human]]s.
[[This paper]] proposes [[multi-task copula (MTC)]] that can handle a much wider [[class]] of [[task]]s than [[mean regression]] with [[Gaussian noise]] in most former [[multi-task learning (MTL)]] .
[[This paper]] proposes [[StreamSVM]], the first [[algorithm for training]] [[linear Support Vector Machines (SVMs)]] which takes advantage of these [[properti]]es by integrating caching with [[optimization]] .
[[This paper]] proposes three [[model]]s to [[solve the problem]], which not only [[model]] both [[contention]] / [[agreement expression]]s and [[discussion topic]]s, but also, more importantly, [[model]] the [[intrinsic nature]] of [[discussion]]s / [[debate]]s, i.e., [[interaction]]s among [[discussant]]s or [[debater]]s and [[topic sharing]] among [[post]]s through [[quoting]] and [[replying relation]]s.
[[This paper]] proposes three significant [[modeling advance]]s: (1) [[we]] learn to [[jointly reason]] about [[relation]]s, [[entiti]]es, and [[entity-type]]s; (2) [[we]] use [[neural attention modeling]] to incorporate multiple [[path]]s; (3) [[we]] learn to share strength in a single [[RNN]] that represents [[logical composition]] across all [[relation]]s.
[[This paper]] proposes to use a [[convolution kernel]] over [[parse tree]]s to [[model syntactic structure information]] for [[relation extraction]] .
[[This paper]] proposes to utilize [[Wikification-based feature]]s for providing [[mention-level correspondence]]s to [[Wikipedia concept]]s for [[dialogue topic tracking]] .
[[This paper]] proposes two [[user mobility]] [[model]]s, namely [[Gaussian-based]] and [[distance-based mobility]] [[model]]s, to capture the [[check-in behavior]] of [[individual LBSN user]], based on which [[location-aware propagation probabiliti]]es can be derived respectively.
[[This paper]] provides a [[logical analysis]] of the [[concept of intention]] as composed of two more basic [[concept]]s, [[choice (or goal)]] and [[commitment]] .
[[This paper]] provides initial insights into [[engagement pattern]]s, allowing for a better understanding of the important [[characteristic]]s of how [[user]]s repeatedly [[interact]] with a [[service]] or [[group of service]]s.
[[This paper]] reports on [[method]]s and [[result]]s of an [[applied research project]] by a [[team]] consisting of [[SAIC]] and four [[universiti]]es to [[develop]], [[integrate]], and [[evaluate new approach]]es to [[detect]] the [[weak signals characteristic]] of [[insider threat]]s on [[organizations' information system]]s.
[[This paper]] reports on [[research]] of utilizing [[ontological consistency checking]] in the [[design process]] for [[identifying potential conflicts]] and [[improving coordination]] and [[communication]] .
[[This paper]] [[review]]s and [[compare]]s several common and [[less common]] [[outlier labeling method]]s and [[presents information]] that shows how the [[percent]] of [[outliers change]]s in each [[method]] according to the [[skewness]] and sample size of [[lognormal distribution]]s through simulations and [[application]] to [[real data set]]s.
[[This paper]] reviews recent work in the area of [[unsupervised feature learning]] and [[deep learning]], covering advances in [[probabilistic model]]s, [[autoencoder]]s, [[manifold learning]], and [[deep network]]s.
[[This paper]] seeks to investigate how [[reader]]s perceive [[software-generated content]] in relation to similar [[content]] written by a [[journalist]] .
[[this paper]] shows how one can [[accurately]] [[discover]] and [[release]] the most [[significant patterns]] along with their [[frequencies]] in a [[data set]] containing [[sensitive information]], while providing [[rigorous guarantees]] of [[privacy for the individuals]] whose [[information]] is [[stored]] there.
[[This paper]] shows that [[wage inequality]] in [[West Germany]] has increased over the past [[three]] [[decade]]s, contrary to common [[perception]]s.
[[This paper]] studies a novel [[social media venture]] and seeks to understand the [[effectivenes]]s of [[marketing strategi]]es in [[social media platform]]s by evaluating their impact on participating [[brand]]s and [[organization]]s.
[[this paper]] studies [[efficient]] [[mining]] of [[negative correlations]] that [[pace]] in [[collaboration]] .
[[This paper]] studies the [[critical alert mining problem]]: Given a set of [[alert sequence]]s, we aim to find a [[set]] of k [[critical alert]]s such that the [[number of]] [[alert]]s [[potentially triggered]] by them is [[maximized]] .
[[This paper]] studies the [[evaluation of policies]] that [[recommend an ordered set of items]] (e.g., a [[ranking]]) based on some [[context]] --- a common scenario in [[web search]], [[ad]]s, and [[recommendation]] .
[[This paper]] studies [[web object classification problem]] with the novel [[exploration]] of [[social tags]] .
[[This paper]] [[survey]]s the use of [[VSMs]] for [[semantic processing of text]] .
[[This paper]] tackles the [[efficiency problem]] of making [[recommendation]]s in the context of [[large]] [[user]] and [[item space]]s.
[[This paper]] tackles the [[problem]] of [[summarizing]] [[frequent itemset]]s.
[[This paper]] targets the [[problem]] of [[cargo pricing optimization]] in the [[air cargo]] [[business]] .
[[This paper]] targets the [[problem of computing]] meaningful [[clustering]]s from [[uncertain data set]]s.
[[This paper]] tries to [[answer a question]]: can we [[predict]] [[the opinion]] holder in a [[heterogeneous social network]] without any [[labeled data]]?
[[this paper, we]] give an [[efficient]] and [[effective]] [[Combined Regression and Ranking method (CRR)]] that [[optimizes]] [[regression]] and [[ranking objectives]] simultaneously.
[[this paper we]] propose a new [[boosting algorithm]] where [[base learners]] have [[structure relationships]] in the [[functional space]] .
[[this paper we]] propose an integrated [[bootstrapping framework]] named [[BioSnowball]] to [[automatically summarize]] [[the Web]] to generate [[Wikipedia-style pages]] for any [[person]] with a modest [[web presence]] .
[[This paper]] will illustrate that appropriate [[listening behavior]] can also be generated by other [[feature]]s of a [[speaker's behavior]] that are available in [[real time]] such as [[speech quality]], [[posture shift]]s and [[head movement]]s.
This [[parsing model]] equaled the performance of the “All” model with lower performance on the [[Jewelry (JW)]] [[title]]s and higher performance on [[FS]] and [[AU]] [[title]]s.
[[This patented technology]] has been [[adopted]], [[productized]], and [[commercially offered]] by many [[large companies]] in the [[mobile resource management]] and [[GPS fleet tracking industry]] .
This perspective suggests that a [[knowledge infrastructure]] consisting of [[technology]], [[structure]], and [[culture]] along with a [[knowledge process]] architecture of [[acquisition]], [[conversion]], [[application]], and [[protection]] are essential [[organizational capabiliti]]es or "preconditions" for effective [[knowledge management]] .
[[This phenomenon]] is [[qualitatively unaffected]] by [[explicit regularization]], and occurs even if we replace the [[true]] [[image]]s by [[completely unstructured random noise]] .
[[This practical book]] shows you how true [[data-drivenness]] involves [[processe]]s that require genuine [[buy-in]] across [[your company]], from [[analyst]]s and [[management]] to the [[C-Suite]] and [[the board]] .
[[This presentation]] will cover [[previous work]] at [[DARPA]], experience building [[real-world application]]s for [[defense]] and [[law enforcement]] to analyze [[data]], and the future of [[computer science]] as an [[enabler]] for [[content discovery]], [[information extraction]], [[relevance determination]], and [[information visualization]] .
[[This presentation]] will give insight into how [[Groupon manage]]s to grapple with these [[challenge]]s via a [[data-driven system]] in order to [[delight]] and [[surprise customer]]s.
This [[prior]] is then used to [[generate]] a [[large number]] of [[samples]] to [[simulate]] the [[space]] of missing [[class]]es.
[[This problem]] arises in a [[wide range]] of [[network application]]s, such as [[protein-complex discovery]], [[network routing]], and [[social network analysis]] .
[[This problem]] attempts to [[identify]] all [[induced subgraph]]s for which the [[probability of connectivity]] being maintained under [[uncertainty]] is higher than a given [[threshold]] .
This [[problem]] can be divided into two [[subproblems]]: [[sense discrimination]] and [[sense labeling]] .
This [[problem]] can be mitigated if the [[classification system]] is allowed to ask for the [[correct labels]] for a [[few]] of the [[nodes]] during [[inference]] .
[[This problem]] departs from the [[previous studi]]es on [[social influence maximization]] or [[seed minimization]] because it considers [[influence coverage]] with <i> [[probabilistic guarantees]]</i> instead of [[guarantees]] on <i> [[expected influence coverage]]</i>.
[[This problem]] gives rise to the [[seed expansion problem]] in [[community detection]]: given example [[community member]]s, how can the [[social graph]] be used to [[predict]] the [[identities of remaining]], [[hidden community member]]s?
[[This problem]] has many [[application]]s, e.g., [[opinion mining]], [[summarization]] and [[search]] .
[[This problem]] is a [[paradigmatic representative]] of [[prediction task]]s in [[machine learning]] that require deducing a [[latent structure]] from [[observed pattern]]s of [[activity in a network]], which often require an [[unrealistically]] [[large number]] of [[resource]]s (e.g., amount of available [[data]], or [[computational time]]).
[[This problem]] is central to combating [[abusive traffic]], such as [[DDoS attack]]s, [[ad click fraud]] and [[email spam]] .
This [[problem]] is challenging due to the [[requirement]] of [[high speed data processing]] within limited [[space]] [[cost]] .
This [[problem]] is difficult because of two [[concerns]] : (a) [[non anti-monotonic property]] of [[FT-support]] when [[relaxation]] is [[proportional]], and (b) difficulty in [[computing]] [[FT-support]] .
[[This problem]] is motivated by modern applications such as [[recommender system]]s and [[social network]]s where only "[[like]]s" or "[[friendship]]s" are observed.
[[This problem]] is of great practical importance given the massive volume of [[online text]] available through [[the World Wide Web]], [[Internet news feed]]s, [[electronic mail]], [[corporate database]]s, [[medical patient record]]s and [[digital libraries]] .
[[This problem]] remains [[under-investigated]], in spite of its [[applicability]] to many [[practical problem]]s.
This [[processing description]] specifies in these [[recognition task]]s the [[role of information]] from the [[discourse]] and from the [[participants' knowledge]] of the [[domain]] .
This provides an new perspective on a range of [[syntactic phenomena]], such as [[apposition]], [[parenthesis]], [[left-]] and [[right-dislocation]], and [[extraposition]] .
This provides the strong [[privacy guarantee]]s of [[differential privacy]], while letting [[policy maker]]s set [[parameter]]s based on the established [[privacy]] concept of [[individual identifiability]] .
This [[pruning strategy]] is admissible; giving us [[provably identical result]]s to the [[brute force]] [[algorithm]], but is at least an [[order of magnitude]] [[fast]]er.
This [[question]] arises in a large variety of [[applications]], e.g. in [[economy]], [[biology]] and [[medicine]] .
[[This question]] can be answered by comparing the [[degree of formalisation]] of [[ontologi]]es with that of other [[resource]]s such as [[terminologi]]es, [[glossari]]es, [[thesauri]] and [[taxonomi]]es.
[[This question]] has been a [[topic of central concern]] and [[debate amongst]] [[social scientist]]s and [[policy maker]]s for a [[long time]] .
This [[reduction]] is also [[computationally optimal]], both at [[training]] and [[test time]], requiring just O(log k) work to [[train]] on an [[example]] or make a [[prediction]] .
[[This redundancy]] leads to [[problem]]s in both [[computational efficiency]] and usefulness of [[MCE]] .
This refers to the [[Identifier]] of the [[Bibliographic Citation]] [[Webpage]] .
[[This report]] examines how entering [[adulthood]] during and after the [[2007-2009 Great Recession]] impacted the [[earnings]] and [[life choices]] of [[millennials]] in [[New York City]] .
[[This report]] surveys a [[two-dimensional classification]] of [[DP algorithm]]s (see Table 1): we first study two types of [[search space]]s (rows): the [[semiring framework]] ([[Mohri, 2002]]) when the underlying representation is a [[directed graph]] as in [[finite-state machine]]s, and the [[hypergraph framework]] ([[Gallo et al., 1993]]) when the [[search space is hierarchically branching]] as in [[context-free grammar]]s; then, under each of these frameworks, [[we]] study two important types of [[DP algorithm]]s (columns) with contrasting [[order]] of visiting [[node]]s: the [[Viterbi style topological-order algorithm]]s ([[Viterbi, 1967]]), and the [[Dijkstra-Knuth style best-first algorithm]]s ([[Dijkstra, 1959]]; [[Knuth, 1977]]).
[[This report]] will present the [[project]]'s [[goal]]s and [[workflow]], and information about the [[computational tool]]s that have been [[adapted]] or [[created in-house]] for [[this work]] .
[[This result]]s in significant [[tangible value]] to the [[authoriti]]es in terms of increase in [[technician efficiency]] and a [[decrease]] in the amount of [[wasted]], [[non-revenue]], [[water]] .
[[This review]] describes [[recent approach]]es to [[reverse-engineering]] [[human learning]] and [[cognitive development]] and, in [[parallel]], [[engineering]] more [[human-like]] [[machine learning system]]s.
[[This review]] documents the central role of both the [[supply]] and [[demand]] for [[skill]]s in shaping [[inequality]], discusses why [[skill demands]] have persistently risen in [[industrialized countri]]es, and considers the [[economic value]] of [[inequality]] alongside its potential [[social cost]]s.
[[This scenario]] is also used by the [[developers of system]]s that generate "[[intelligent agent]]s " for [[searching remote]], [[heterogeneous database]]s.
[[This schema]] has an almost [[unlimited set]] of [[relation]]s (due to [[surface form]]s), and [[supports integration]] with existing [[structured data]] (through the [[relation type]]s of [[existing database]]s).
This section explains the [[core concept]]s of [[logic]] and [[probability]], beginning with [[possible world]]s.
This [[set]], the [[code table]], is a [[compact description]] of the [[database]] in terms of [[local]] [[relational patterns]] .
[[This sharing of parameter]]s allows [[information]] to flow across [[context]]s through [[multivariate regression]]s among [[local factor]]s, instead of [[enforcing]] exactly the same [[factor]]s for an [[entity]], everywhere.
This [[side effect]] provides the opportunity for [[innovative methods]] that [[analyze]] the [[behavior]]s of [[movement]]s.
[[This special issue]] uses these fresh waves of [[data]] to explore the [[origin]]s, [[impact]], and [[future]] of [[inequality around the world]] .
This [[study]] deals with the [[real-time]] [[detection]] of [[samples]] from a missing [[class]] and the associated [[problem of learning]] with a [[nonexhaustive]] [[training dataset]] .
[[This study]] examines the application of [[cluster analysis]] in the [[accounting domain]], particularly [[discrepancy detection]] in [[audit]] .
[[This study]] investigates [[interpersonal process]]es underlying [[dialog]] by comparing two [[approach]]es, [[interactive alignment]] and [[interpersonal synergy]], and assesses how they [[predict collective performance]] in a [[joint task]] .
[[This study]] investigates why [[people]] choose to [[watch others]] [[play video game]]s, on services such as [[Twitch]] .
[[This study]] lays the groundwork for [[understanding]] the [[motivation]]s to consume this emerging form of [[new media]] in the context of [[online game]]s and [[video stream]]s.
[[This study]] paves the way for [[analysis]] and [[mining]] across [[social media site]]s, and facilitates the creation of novel [[online service]]s across [[site]]s.
[[This study]] proposes to give [[user]]s freedom to construct [[topical hierarchi]]es via [[interactive operation]]s such as [[expanding]] a [[branch]] and [[merging]] several [[branche]]s.
[[This study]] sets out to examine [[the relationship]] between two [[personality measure]]s - most popularly used [[measure]] in the [[consultancy]] and [[training world]] (the [[Myers-Briggs Type Indicator]]) and one of the most heavily used [[measure]]s in the [[academic research area]] on [[personality]] (the [[five factor NEO-PI]]).
This suggests that ideally each [[input]] [[feature]] should be [[linearly correlated]] with the [[target variable]] (or [[anti-correlated]]), whereas raw [[features]] may be highly [[non-linear]] .
This suggests that [[uncovering aspect]]s and [[sentiment]]s will allow us to gain a better understanding of [[user]]s, [[movi]]es, and the [[process]] involved in generating [[rating]]s.
This [[surprising property]] allows us to design efficient [[data structure]]s and [[scalable algorithm]]s with [[provable approximation guarantee]]s, despite the [[hardness of the problem]]s in [[question]] .
[[This survey]] attempts to provide a [[comprehensive]] and [[structured overview]] of the existing [[research]] for the [[problem of detecting anomali]]es in [[discrete / symbolic sequence]]s.
[[This survey]] describes several [[approaches of defining]] [[positive definite kernels]] on [[structured instance]]s [[directly]]
[[This survey]] [[identifi]]es and [[classifi]]es [[simplification]] [[research]] within the period 1998-2013.
[[This]] [[survey]] provides a [[conceptual introduction]] to [[ontologi]]es and their role in [[information system]]s and [[AI]] .
This [[system]] first uses [[heuristics]] to find "[[maximal length noun phrases]]", and then uses a [[grammar]] to [[extract]] "[[terminological units]] ."
[[This system]] is currently [[deployed]] in an [[experimental]] [[Commerce Search Engine]] and is used to [[match]] all the [[offer]]s received by [[Bing Shopping]] to the [[Bing product catalog]] .
[[This system]] underpins the [[collection]] and use of [[public resource]]s and informs [[policy maker]]s, [[managers of government agenci]]es, [[parliamentarian]]s and the public at large on [[government polici]]es and [[operation]]s.
[[This system]] uses a [[regular expression]] based [[keyword look-up]] to [[label token]]s in some [[text]] based on the [[ontology]] .
[[This talk]] focuses on the [[data model]], [[semantics]], [[computational complexity]] and [[algorithm]]s of [[[uncertain graph mining]] .
This talk will discuss how insights gained from living in [[Kenya]] became the [[genesis]] of a [[technology company]] currently working with [[global client]]s in over 50 countries, including [[P&G]], [[Google]], [[Unilever]], [[Danone, General Mill]]s, [[Nestle]], [[Johnson & Johnson, Microsoft]], the [[World Bank]], and the [[United Nation]]s.
[[This talk]] will discuss some of the [[issue]]s and [[algorithm]]s developed to [[analyze]] and [[discover pattern]]s in these [[data set]]s.
[[This task]] generalizes several important [[problem]]s such as [[multi-arm bandit]]s, [[active search]] and the [[knapsack problem]] .
[[This task]] is closely related to the [[link-based similarity problem]] that [[SimRank]] [[address]]es.
[[This task]] is of [[practical importance]] and can facilitate many different [[task]]s, such as [[personalized recommendation]] and [[user interest discovery]] .
[[This task]] is referred to as [[ordinal regression]] and is [[complementary]] to the standard [[machine learning task]]s of [[classification]] and [[metric regression]] .
[[This technique]] discovers both [[positive]] and [[negative patterns]] in [[text documents]] as [[higher level features]] in order to accurately [[weight]] [[low-level features]] ([[terms]]) based on their [[specificity]] and their [[distributions]] in the [[higher level features]] .
This [[text]] covers a wide range of topics including: the [[bootstrap]], the [[nonparametric delta method]], [[nonparametric regression]], [[density estimation]], [[orthogonal function method]]s, minimax estimation, [[nonparametric confidence set]]s, and wavelets.
This theory is then applied to construct a method to [[automatically extract]] [[similarity]], the [[Google similarity distance]], of [[words]] and [[phrase]]s from the [[WWW]] using [[Google page counts]] .
[[This theory]] provides a [[framework]] for describing the [[processing of utterances]] in a [[discourse]] .
[[This thesis]] presents a unified [[variational Bayesian (VB) framework]] which [[approximates these computations]] in [[models]] with [[latent variable]]s using a [[lower bound]] on the [[marginal likelihood]] .
[[This timing]] is more pronounced when [[executive]]s [[donate]] their own [[share]]s to their own [[family foundation]]s, which I identify using [[foundation]]s' [[IRS tax return]]s posted on [[Internet database]]s.
[[This tutorial]] is expected to be [[self-contained]], while presenting the different [[approach]]es under a [[unified notation]] and [[framework]] .
This [[two-phase strategy]] with a [[lazy call]] of the [[matching algorithm]] significantly [[reduce]]s the [[number]] of [[event]]s that need to be [[processed]] by <i>A</i> as well as the number of [[call]]s to <i>A </i>.
[[This type of input]] is [[intuitive]] and [[easy]] for [[experts]] to [[supply]] .
This unprecedented [[volume]] of [[information facilitate]]s a novel set of [[research question]]s applicable to a [[wide range]] of [[development issue]]s.
This view of [[disambiguation]] as a [[two]]-[[stage process]] may not be completely [[general]] (for example, it may not be appropriate for the [[iterative process]] by which a [[lexicographer]] arrives at the [[sense]] [[divisions]] of a [[dictionary entry]]), but it seems applicable to most [[work]] on [[disambiguation]] in [[computational linguistics]] .
[[This visual essay]] presents [[real hourly compensation]] [[data]] based on [[compensation]] [[data]] from the [[National Income and Product Accounts]], which is the same [[source]] that the [[BLS productivity program]] uses for [[output]] .
[[This visual essay]] presents [[relationship]]s that are [[definitional]] rather than [[causal]] .
This will enable us to detect [[role changes across time]], [[detect]] different [[pattern]]s of [[interaction]]s, for example, [[weekday]] and [[weekend behaviour]], and allow us to study how the [[structure]] in the underlying [[dynamic graph evolve]]s.
This [[word representation]] is better suited to [[autoencoder]]s than the [[binary number representation]]s used in previous related [[autoencoder model]]s such as the [[recursive autoassociative memory]] ([[RAAM) model]] ([[Pollack, 1990]]; [[Voegtlin and Dominey, 2005]]) or [[recurrent neural networks]] ([[Elman, 1991]]) since [[sigmoid unit]]s are inherently [[continuous]] .
[[This work]] builds upon the [[belief propagation algorithm]] for use in [[detecting]] [[collusion]] and other [[fraud]] [[scheme]]s.
[[This work]] designs novel [[approximation method]]s to construct [[scalable histogram]]s on [[probabilistic data]] .
[[This work]] extends [[standard topic model]]s by allowing each [[text stream]] to have both [[local topic]]s and [[shared topic]]s.
[[This work]] is based on the following [[premises]]: (1) [[grammars]] are too [[complex]] and [[detailed]] to develop manually for most [[interesting domains]]; (2) [[parsing models]] must rely heavily on [[lexical]] and [[contextual information]] to [[analyze sentence]]s [[accurately]]; and (3) existing [[n-gram modeling technique]]s are inadequate for [[parsing models]] .
[[This work]] is part of the [[national project XData]] (http://xdata.fr) that aims at combining [[large (anonymized) dataset]]s provided by different [[service provider]]s ([[telecom]], [[electricity]], [[water management]], [[postal service]], etc.).
[[This work]] is the first to propose a [[generative model]] which captures the [[statistical properties]] of these [[complex]] [[networks]] .
[[This work]] presents a [[probabilistic method]] for [[enforcing adherence]] of the [[marginal probabiliti]]es of a [[multi-label model]] to [[automatically discovered]] [[deterministic relationship]]s among [[label]]s.
[[This work]] proposes a novel angle to the [[problem]] by [[modeling spamicity]] as [[latent]] .
[[This work]] proposes a [[relaxing function]] as part of the [[prior]] and [[updates the parameter]]s with the [[likelihood function]] in terms of the [[consistency]] between the [[true label information]] and [[predicted result]] .
[[This work]] provides a [[large-scale]] [[evaluation]] of new [[security]] and [[privacy index]]es using a [[Facebook]] [[dataset]] .
[[This work]] provides guidance to [[practitioner]]s evaluating [[large-scale experiment]]s, and highlights the importance of [[analysis]] of [[inferential method]]s for [[complex dependence structure]]s common to [[online experiment]]s.
[[This work]] represents a [[promising step]] toward general [[early clinical warning]] which has the potential to significantly improve the [[quality]] of [[patient care]] in [[hospital]]s.
[[This work]] showed yet again that [[empirical risk minimization (ERM)]] was the best [[method]] to find the next [[instance]] to [[label]] and provided an [[efficient]] way to [[compute]] [[ERM]] with the [[semi-supervised classifier]] .
[[This work]] shows how to leverage [[causal inference]] to understand the [[behavior]] of [[complex learning systems interacting]] with their [[environment]] and [[predict]] the [[consequence]]s of changes to the [[system]] .
[[This work]] shows how to leverage [[causal inference]] to understand the [[behavior]] of [[complex learning systems]] [[interacting]] with their [[environment]] and [[predict]] the [[consequence]]s of changes to the [[system]] .
Thorough [[empirical studi]]es demonstrate that the proposed [[ELLR algorithm]]s [[outperform]] [[state-of-the-art approach]]es for [[link recommendation]] in [[signed network]]s at no [[cost]] in [[efficiency]] .
[[Thorough experiment]]s are conducted to understand the [[performance]] of [[CDAE]] under various [[component]] [[setting]]s.
[[Thorough experiment]]s with two [[real-world dataset]]s demonstrate the [[effectiveness]] of [[TAA]] in [[topic-specific authority identification]] as well as the [[generalizability]] of the [[TAA generative model]] .
Those that [[learn]] multiple [[metric]]s throughout the [[feature space]] have demonstrated superior [[accuracy]], but at a severe [[cost]] to [[computational efficiency]] .
Those which have been used in [[large-scale scenarios]] are, for example, [[iterative scaling]] ([[Darroch and Ratcliff, 1972]]; [[Pietra et al., 1997]]; [[Goodman, 2002]]; [[Jin et al., 2003]]), [[nonlinear conjugate gradient]], [[quasi Newton]] (in particular, [[limited memory BFGS]]) ([[Liu and Nocedal, 1989]]; [[Benson and Morfie, 2001]]), and [[truncated Newton]] (Komarek and Moore, 2005).
Those who [[excel]] rely on what [[Goleman]] calls [[Smart Practice]]s such as [[mindfulness meditation]], [[focused preparation]] and [[recovery]], [[positive emotion]]s and [[connection]]s, and [[mental 'prosthetics']] that help them improve [[habit]]s, add new [[skill]]s, and sustain [[excellence]] .
Though a few [[commercial tool]]s are capable of performing [[forecasting]] and [[what-if analysis]], and some [[paper]]s describe relevant [[application]]s in different [[field]]s, no attempt has been made so far to comprehensively address [[methodological]] and [[modeling issue]]s in [[this field]] .
Though being [[detrimental]] to [[average accuracy]], [[we]] show that [[our method]] improves [[user satisfaction]] with [[recommendation list]]s, in particular for [[lists generated]] using the common [[item-based collaborative filtering algorithm]] .
Though extensively [[studied]], most existing [[algorithm]]s become impractical when the [[input]] [[graph]] is too [[large]] and is [[disk-resident]] .
Though performing the two [[task]]s of [[topic discovery]] and [[learning predictive models jointly]], which significantly improves the [[classification performance]], [[our method]]s have comparable [[scalability]] as the [[state-of-the-art]] [[parallel algorithm]]s for the standard [[LDA topic model]]s which perform the single [[task]] of [[topic discovery only]] .
Though [[social media service]]s are essentially [[web]] or [[mobile application]]s and [[service]]s, they combine [[user-generated content]] and [[social network]]s together, so that [[information]] can be created, [[transmitted]], [[transformed]], and consumed in the [[cyberspace]] .
Though such [[relationship]]s are generic, [[our work]] is particularly motivated by the [[emerging topic]] of [[pattern based classification]] for [[semi-structured data]] including [[graphs]] .
Though the [[sequence]] S5 is [[anomalous]], each [[command]] in the [[sequence]] by itself is [[normal]] .
Though this [[maximum consensus principle]] has been shown to be [[successful]], simply [[maximizing consensu]]s can lead to less [[discriminative prediction]]s and [[overfit]] the inevitable [[noise]] due to [[imperfect base model]]s.
Three different best [[matching methods]] to [[classify]] a new [[moving object]] are proposed and their impact on the [[quality]] of [[prediction]] is [[studied]] extensively.
Three major [[factor]]s govern the [[intricaci]]es of [[community extraction]] in [[network]]s: (1) the [[application domain]] includes a wide variety of [[network]]s of fundamentally different natures, (2) the [[literature]] offers a multitude of disparate [[community detection algorithm]]s, and (3) there is no [[consensus]] characterizing how to [[discriminate communiti]]es from [[non-communiti]]es.
Three major [[theori]]es have been identified - [[information overload]], uses and [[gratification]]s, and [[user involvement]] .
Three new [[method]]s for [[mining]] [[frequent closed subgraph]]s are presented.
Three representative [[sampling method]]s are developed in [[this paper]], with the goal of achieving a <i>[[good estimation]]</i> of the <i> [[statistics]] </i>, including [[proportion]]s and [[center]]s, within the [[sub-space]]s of the [[output attribute]]s.
Through a [[3-]] [[Step]] [[hierarchical partitioning process]] [[IPLoM]] [[partitions]] [[log data]] into its respective [[clusters]] .
Through analysis of [[survey]]s collected from over 300 [[senior executive]]s, [[this research]] [[empirically models]] and uncovers key aspects of these [[dimension]]s.
Through a [[neural implementation]] of the [[dual stack machine]] that underlies [[Forth]], [[programmer]]s can write [[program sketches with slot]]s that can be filled with [[behaviour trained]] from [[program input-output data]] .
Through a [[questionnaire study]] (N=1097), [[we]] examine five distinct types of [[motivation]]s from the [[uses]] and [[gratification]]s perspective: [[cognitive]], [[affective]], [[personal integrative]], [[social integrative]] and [[tension release]] .
Through [[conversation]]s we [[develop]] and [[maintain social relation]]s and [[cultural practice]]s, we [[plan]] and [[coordinate]] with each other, we [[share experiences]], [[memori]]es, and [[attitude]]s, we [[educate]] and [[learn]] ([[Clark, 1996]]; [[Tyleen, 2010]]).
Through [[experiment]]s, [[we]] show that [[A2DM2]] [[converge]]s faster than [[ADMM]] on a variety of [[problem]]s.
Through [[experiment]]s, [[we]] show that [[the proposed approach]] is more successful than other [[candidate method]]s for the [[topic model]]ing of [[competition]] .
Through extensive [[experiment]]s, [[we]] show that for the same [[number of]] [[edges sampled]], [[MASCOT]] provides the best [[accuracy]] compared to the existing [[algorithm]] as well as [[MASCOT-C]] and [[MASCOT-A]] .
Through extensive [[experiment]]s, [[we]] show that [[our method]] significantly and uniformly [[outperform]]s other [[competitive methods]] and obtains relative [[lifts]] that are in the range of 10-15% in terms of [[predictive log-likelihood]], 200-300% for a [[rank correlation metric]] on a [[proprietary]] [[My Yahoo ]] !
Through interviews and examples from [[data scientist]]s and [[analytics leader]]s in a variety of [[industri]]es, author [[Carl Anderson]] explains the [[analytics value chain]] you need to adopt when building [[predictive business model]]s - from [[data collection]] and [[analysis]] to the [[insight]]s and [[leadership]] that drive [[concrete action]]s.
Through numerous [[evaluation]]s, we demonstrate the superiority of [[EMBERS]] over [[baserate method]]s and its [[capability]] to [[forecast]] significant [[societal happening]]s.
Through [[observation spots]], multiple [[periods]] in the [[movement]] can be [[retrieved]] using a [[method]] that combines [[Fourier transform]] and [[autocorrelation]] .
Through our [[analysis]], [[we]] aim to address these challenges to characterize [[network failures]], [[estimate]] the [[failure]] [[impact]], and [[analyze]] the [[effectiveness]] of [[network redundancy]] in [[data centers]] .
Through our [[research]], [[we]] build a [[patent]] [[ranking software]], named [[COA]] ([[Claim Originality Analysis]]) that rates a [[patent]] based on its value by [[measuring]] the <I>[[recency]]</I> and the <I>[[impact]]</I> of the important [[phrase]]s that appear in the "claims" [[section]] of a [[patent]] .
Throughout [[history]], [[rich]] and [[poor countri]]es alike have been [[lending]], [[borrowing]], [[crashing]] -- and [[recovering]] -- their way through an extraordinary range of [[financial crises]] .
Throughout life, the [[cell]]s in every [[individual]] accumulate many changes in the [[DNA]] [[inherited]] from his or her [[parent]]s.
Throughout the [[1980s]] and [[1990s]], the chief [[requirement]] of [[database]]s was to conserve a rare and [[expensive resource]]: the [[hard disk]] .
Throughout [[the book]] she asks a [[variation]] of this [[question]]: "Who would you [[save]], your [[mother]] or two [[stranger]]s?"
Throughout [[the paper, we]] use as examples the implementation of the [[cache]] in the [[Amdahl 470V/6]] and [[470V/7]], the [[IBM 3081]], [[3033]], and [[370/168]], and the [[DEC VAX 11/780]] .
Through several [[rounds]] of [[mutual reinforcement]] between [[topology]] and [[propinquity]], the [[community]] [[structures]] are expected to naturally [[emerge]] .
Through [[Taylor series expansion]] of the [[Bregman divergence]]s, [[we]] reveal a [[relationship]] between [[Bregman divergence]]s and [[Euclidean distance]] .
Through [[theoretical analysis]] and practical [[experiment]]s, [[we]] conclude that a [[warm start setting]] on a [[high-order optimization method]] for [[primal formulation]]s is more suitable than others for [[incremental]] and [[decremental learning]] of [[linear classification]] .
Through [[theoretical analysis]] backed by [[empirical study]], [[we]] show that our [[kernel-boundary-alignment algorithm]] works effectively on several [[datasets]] .
Through [[theoretical argument]]s and [[Monte Carlo simulation]], [[we]] show that [[LMEM]]s generalize best when they include the [[maximal random effects structure]] justified by [[the design]] .
Through [[this decomposition]], [[the proposed approach]] recovers a clean [[face image]] (the [[common component]]) for each [[subject]] and [[discover]]s the [[condition]]s (the [[condition component]]s and the [[sparse residual]]s) of the [[image]]s in the [[set]] .
Through [[this meta-analysis]] we aimed to provide an [[estimation]] of the overall effect of [[robot-enhanced therapy]] on [[psychological outcome]] for different [[population]]s, to provide [[average effect size]]s on different [[outcome]]s, such as [[cognitive]], [[behavioral]] and [[subjective]], and to test possible moderators of [[effect size]] .
Through vivid [[narrative]], lively [[presentation]]s of important [[research]], and intriguing [[example]]s, [[Elliot Aronson]] probes the [[pattern]]s and [[motive]]s of [[human behavior]], covering such diverse [[topic]]s as [[terrorism]], [[conformity]], [[obedience]], [[politics]], [[race relation]]s, [[advertising]], [[war]], [[interpersonal attraction]], and the power of [[religious cult]]s.
Thus, advances in [[top-N recommendation]] have [[far-ranging consequence]]s in [[practical application]]s.
Thus, at the time of writing, [[we]] know of nontrivial [[<code>[[DATR]]</code> lexicon]]s written for [[GPSG]], [[I_TAG]], [[PATR]], [[Unification Categorial Grammar]], and [[Word Grammar]] .
Thus, [[durable]] and [[nondurable good]]s have differing [[time utility]] [[characteristics]] which lead to differing [[demand]] [[characteristics]] .
Thus, even though [[independence]] is usually assumed between [[annotation]]s on [[data item]]s within the same [[batch]], in most cases, a [[worker's judgment]] on a [[data item]] can still be affected by other [[data item]]s within the [[batch]], leading to additional [[error]]s in [[collected label]]s.
Thus, in addition to those supported in [[CANaLI]], we now have queries with (i) [[complex aggregate]]s, (ii) [[structured conditions combined]] with [[keyword-based search]]es, and (iii) [[temporal condition]]s on [[Cliopedia]], a [[historical knowledge base]] that captures the [[evolution]] of [[Wikipedia entiti]]es and [[properti]]es.
Thus, in [[the proposed model]], [[current]] [[topic-specific distributions over words]] are assumed to be [[generated]] based on the [[multiscale word distributions]] of the previous [[epoch]] .
Thus it is essential to [[evaluate]] [[user browsing models]] with respect to [[accuracy]] in [[predicting relevance]], not just [[CTR]] .
Thus, merely using [[ontologi]]es, like using [[XML]], does not [[reduce heterogeneity]]: it just raises heterogeneity [[problem]]s to a [[higher level]] .
Thus, [[modeling]] [[temporal dynamics]] should be a key when [[designing]] [[recommender system]]s or general [[customer preference]] [[models]] .
Thus one is [[forced to hand code referential integrity checking]] via the [[scripting language available]] with a [[database product]] or with a [[third generation language]] ([[C + +]], [[Java]], [[PHP]]) in the forms processing associated with the [[database add]]s, [[delete]]s, and [[modify operation]]s.
Thus, [[the city]] [[unemployment rate]] among [[young worker]]s has not yet returned to its [[prerecession]] [[level]] .
Thus the [[data]] is in the form of a [[graph]], consisting of [[node]]s ([[entiti]]es) and [[labeled edge]]s ([[relationships between entities]]).
Thus the first [[ultraintelligent machine]] is the last [[invention]] that [[man]] need ever [[make]] .
Thus, the [[low-rank regression]] efficiently [[work]]s for the [[high-dimensional data]] .
Thus, the [[standard full‐time permanent job with benefits]] is now often replaced with different forms of [[non‐standard work arrangement]]s such as [[contingent]], [[part‐time contract]], [[unregulated underground work]] or [[home‐based work]], many of which are characterised by [[variable work schedule]]s, [[reduced job security]], [[lower wage]]s, hazards at the [[workplace]] and [[stressful psychosocial]] [[working condition]]s.4
Thus they ignore two essential [[characteristic]]s of [[event]]s in [[EBSN]]s: (1) a [[large number]] of new [[event]]s will be published every day which means many [[event]]s have few [[participant]]s in the beginning, (2) [[event]]s have [[life cycle]]s which means [[outdated]] [[event]]s should not be [[recommended]] .
Thus [[this paper]] postulates that for many [[problem]]s, [[GA<sup>2</sup>M-model]]s can yield [[model]]s that are both [[intelligible]] and [[accurate]] .
Thus, [[this paper]] suggests a general [[CBA framework]] for [[detecting]] <i>[[group-based market manipulation]]</i> by capturing more comprehensive [[coupling]]s and proposes [[two]] [[variant implementation]]s, which are <i>[[hybrid coupling(HC)]]</i> - based and <i>[[hierarchical grouping (HG)]]</i> - based respectively.
Thus [[tracking]] [[personalized PageRank]] in such [[evolving network]]s is an important [[challenge]] in [[network analysis]] and [[graph mining]] .
Thus, [[we]] adopted [[user bias]] as the basis for building [[accurate]] [[classification model]]s.
Thus, [[we]] define new [[prediction feature]]s based on the [[properti]]es of the [[places visited]] by [[user]]s which are able to [[discriminate]] potential future [[link]]s among them.
Thus, we present here a general [[diagnostic procedure]] which [[directly]] and [[automatic]]ally [[evaluate]]s the [[accuracy]] of the [[bootstrap's output]]s, determining whether or not the [[bootstrap]] is performing [[satisfactorily]] when applied to a given [[dataset]] and [[estimator]] .
Thus, [[we]] propose the Best of both Worlds-[[BoW method]], that [[automatically spot]]s the [[bottleneck]] and chooses a [[good strategy]] .
Thus, [[we]] propose to [[detect]] such [[attack]]s via unusually [[correlated]] [[temporal pattern]]s.
Thus, [[we]] propose to [[integrate]] <i>[[meta-path selection]]</i> with <i>[[user-guided clustering]]</i> to [[cluster object]]s in [[network]]s, where a [[user]] first provides a small [[set]] of [[object seed]]s for each [[cluster]] as [[guidance]] .
Thus, we reinterpret the [[algorithm]]s as “[[exact inference]] with an [[approximated prior]]”, rather than the existing (ubiquitous) [[interpretation]] “[[approximate inference]] with the [[exact prior]]”.
Thus, [[we]] see a new challenge for [[financial data mining]] : how to capture and categorize the [[migration]] of [[individual]] [[companies]], and how such [[behavior]] affects their [[return]]s.
Thus [[we]] show that proper [[cluster randomization]] can lead to [[exponentially lower estimator variance]] when [[experimentally measuring]] [[average treatment effect]]s under [[interference]] .
[[Time metric]]s mattered not because they changed how [[distal]] or [[important future event]]s [[felt]] (Study 6), but because they changed how [[connected]] and [[congruent]] their [[current]] and [[future selv]]es [[felt]] (Study 7).
[[Time Series]]; [[Missing Value]]; [[Bayesian Network]]; [[Expectation Maximization (EM)]] .
[[Time series motifs]] are sets of very [[similar]] [[subsequence]]s of a [[long]] [[time series]] .
[[Time series prediction]] is an important [[issue]] in a [[wide range]] of [[areas]] .
[[Time series shapelet]]s are small, [[local pattern]]s in a [[time series]] that are highly [[predictive]] of a [[class]] and are thus very useful [[feature]]s for building [[classifier]]s and for certain [[visualization]] and [[summarization task]]s.
[[Time varying problem]]s, [[velocity tuning]], [[multiple-robot coordination]], [[hybrid system]]s, [[manipulation planning]], [[protein folding]], [[unknotting]], [[closed chain]]s, [[Random Loop Generator (RLG)]], [[coverage planning]], [[optimal motion planning]] .
[[TM-LDA]] learns the [[transition parameter]]s among [[topic]]s by [[minimizing]] the [[prediction error]] on [[topic distribution]] in [[subsequent posting]]s.
To account for the [[dynamic properti]]es of the [[social media data]], we further generalize [[GLAD]] to its [[dynamic extension]] [[d-GLAD]] .
To a certain [[extent]], the [[consistency degree]] is an [[indicator]] of [[information reliability]] -- [[Information unanimously]] agreed by all the [[source]]s is more likely to be [[reliable]] .
To achieve better [[efficiency]], the [[propinquity]] is [[incrementally]] [[calculated]] .
To achieve [[fault tolerance]] [[efficiently]], [[RDD]]s provide a restricted form of [[shared memory]], based on [[coarse-grained transformation]]s rather than [[fine-grained update]]s to [[shared state]] .
To achieve [[<math>k</math>-support anonymity]], [[we]] introduce a [[pseudo taxonomy tree]] and have the [[third party]] [[mine]] the [[generalized frequent itemsets]] under the corresponding [[generalized association rules]] instead of [[association rules]] .
To achieve [[this]] [[we]] propose a new [[objective function]] that [[encourages smoothness]] of [[inferred instance-level label]]s based on [[instance-level similarity]], while at the same time respecting [[group-level label constraint]]s.
To adapt these [[algorithms]], [[we]] explicitly [[factor]] them into two parts, an [[aggregation]]/[[learning]] phase that can be performed with [[differential privacy]] [[guarantee]]s, and an [[individual]] [[recommendation]] phase that uses the [[learned correlation]]s and an individual's [[data]] to provide [[personalized recommendation]]s.
To address [[performance optimization]] in this context, [[we]] have developed a [[lower bound]] for the [[entire]] [[ROC curve]] that can be expressed in terms of the [[class-specific]] [[strength]] and [[correlation]] of the [[base classifiers]] .
To address such needs, [[this paper]] presents a novel [[topic modeling method]] based on [[joint nonnegative matrix factorization]], which [[simultaneously discovers]] [[common]] as well as [[discriminative topic]]s given [[multiple]] [[document sets]] .
To address the [[computational challenge]], we develop a [[screening rule]] which can quickly [[identify]] many [[zero block]]s in the [[estimate]]d [[graphical model]], thereby dramatically reducing the [[computational cost]] of [[solving]] [[the proposed model]] .
To address the deficiency of existing [[work]], [[this paper]] presents [[Highways-on-Disk (HoD)]], a [[disk-based index]] that supports both [[SSD]] and [[SSSP queri]]es on [[directed and weighted graph]]s.
To address the [[interdependency]] among [[data instance]]s, [[relational learning]] has been proposed, and [[collective inference]] based on [[network connectivity]] is adopted for [[prediction]] .
To address these challenges, in [[this paper, we]] propose a [[Low-rank]], [[Windowed]], [[Incremental SVD (LWI-SVD) algorithm]], which (a) leverages [[efficient]] and [[accurate]] [[low-rank approximation]]s to speed up [[incremental SVD update]]s and (b) uses a [[window-based approach]] to [[aggregate multiple]] [[incoming update]]s ([[insertion]]s or [[deletion]]s of [[row]]s and [[column]]s) and, thus, [[reduce]]s on - line [[processing cost]]s.
To address these challenges, [[we]] present [[the algorithm]] [[Scenic]] for [[dependency clustering]] across [[measurement scale]]s.
To address these challenges, [[we]] propose [[Session-based Temporal Graph (STG)]] which [[simultaneously models]] [[users]]' [[long-term]] and [[short-term preference]]s over [[time]] .
To address these fundamental [[questions]], [[we]] propose [[Topical Affinity Propagation (TAP)]] to [[model]] the [[topic-level]] [[social]] [[influence]] on [[large]] [[networks]] .
To address these issues, [[we]] propose an improved [[ranking algorithm]] on [[manifold]]s using [[Green's function]] of an [[iterated unnormalized graph Laplacian]], which is more [[robust]] and [[density adaptive]], as well as [[pointwise continuous]] in the [[limit]] of [[infinite samples]] .
To address these issues, [[we]] propose a novel [[bias-smoothed tensor model]] and [[empirically]] show that [[our model]] significantly [[outperform]]s a number of [[alternative]]s based on [[Yahoo ! New]]s, [[Yahoo ! Buzz]] and [[Epinions dataset]]s.
To address this, a [[recent paper]] [1] proposes to [[maximize]] [[recall]] of the [[classifier]] under the [[constraint]] that its [[precision]] should be greater than a specified [[threshold]] .
To address [[this challenge]], [[we]] demonstrate how one can simultaneously [[estimate]] the usefulness of a [[query graph]] and a [[set]] of [[subgraph feature]]s.
To address this challenge, [[we]] investigated the [[distribution of error]]s in the [[forecast]]s delivered by [[standard]] [[SVM]]s.
To address this challenge, [[we]] propose a [[hybrid]] [[subspace clustering algorithm]] called [[Hydra]] .
To address [[this challenge]], [[we]] propose an efficient [[dynamic semi-supervised clustering framework]] that casts the [[clustering problem]] into a [[search problem]] over a [[feasible convex set]], <i>i.e. </i>, a [[convex hull]] with its [[extreme point]]s being an [[ensemble]] of m [[data partition]]s.
To address this [[deficiency]], [[we]] present [[NIMBLE]], a [[portable infrastructure]] that has been specifically designed to enable the [[rapid implementation]] of [[parallel ML-DM algorithm]]s.
To address [[this gap we]] have developed a novel [[tool]], [[Rexplore]], which [[integrates statistical analysis]], [[semantic technologi]]es, and [[visual analytic]]s to provide effective support for [[exploring]] and making sense of [[scholarly data]] .
To address [[this issue]], a [[generalized AUC (GAUC)]] which can [[measure]] both the [[head]] and [[tail]] of a [[ranking list]] has been introduced.
To address this issue, [[post-processing technique]]s that applied several [[translation rule]]s on specific contexts (e.g., the [[number type]], [[conjunction]]s, [[enumeration]]s) to [[tokenize]] [[gene name]]s are proposed to enhance the performance of the [[general-purpose]] [[gene name recognition]] .
To address this [[issue]], [[we]] present a [[principled approach]] for [[picking]] a [[set]] of [[posts]] that best covers the important [[stories]] in the [[blogosphere]] .
To address this issue, [[we]] propose a novel [[constrained nonlinear optimization formulation]] in which the [[prediction]] [[accuracy]] of [[cross-validation]] [[sets]] are included as [[constraint]]s.
To address this [[limitation]], [[we]] first relax [[the problem]] to a [[matrix optimization problem]] with a [[label-dependent loss]] .
To address this [[problem]], in [[this paper, we]] propose a [[Dual Regularized Co-Clustering (DRCC)]] [[method]] based on [[semi-nonnegative]] [[matrix tri-factorization]] .
To address [[this problem]], techniques have been developed to [[automate]] the [[construction of semantic lexicons]] from [[text corpora]] using [[bootstrapping method]]s ([[Riloff and Shepherd, 1997]]; [[Roark and Charniak, 1998]]; [[Phillips and Riloff, 2002]]; [[Thelen and Riloff, 2002]]; [[Ng, 2007]]; [[McIntosh and Curran, 2009]]; [[McIntosh, 2010]]), but [[accuracy]] is still far from [[perfect]] .
To address [[this problem]], we generalize recently advances in [[deep learning]] from [[i.i.d. input]] to [[non-i.i.d. (CF-based) input]] and propose in [[this paper]] a [[hierarchical Bayesian model]] called [[collaborative deep learning (CDL)]], which [[jointly]] [[perform]]s [[deep representation learning]] for the [[content information]] and [[collaborative filtering]] for the [[ratings (feedback) matrix]] .
To address [[this problem]], [[we]] have built a prototype named [[Swish]] that: (1) constantly monitors [[users' desktop]] [[activities]] using a stream of [[windows events]]; (2) [[logs]] and [[processes]] this [[raw]] [[event stream]], and (3) implements two [[criteria]] of [[window "relatedness"]], namely the [[semantic similarity]] of their [[titles]], and the [[temporal closeness]] in their [[access patterns]] .
To address [[this problem]], [[we]] investigate the [[temporal relation]]s among both [[object truth]]s and [[source reliability]], and propose an [[incremental]] [[truth discovery]] [[framework]] that can [[dynamically update]] [[object truth]]s and [[source weight]]s upon the [[arrival of new data]] .
To address [[this problem]], we present a [[novel framework]] to [[post-process]] any [[ATM classifier]] to extract an [[optimal actionable]] plan that can change a given [[input]] to a [[desired class]] with a [[minimum cost]] .
To address [[this problem]], [[we]] propose a novel <i>[[Ensemble-tree]]</i> ([[E-tree]] for short) [[indexing structure]] to organize all [[base classifier]]s in an [[ensemble]] for fast [[prediction]] .
To address [[this problem]], [[we]] propose <i>[[FeaFiner]]</i> (short for [[Feature Refiner]]), an efficient [[formulation]] that simultaneously generalizes [[low-level feature]]s into [[higher level concept]]s and then selects relevant [[concept]]s based on the [[target variable]] .
To address [[this question]], we model the [[content]] of [[news article]]s and [[blog post]]s by [[attribute]]s of the [[people]] who are likely to [[share]] them.
To address two key [[challenges]] in [[Ensemble Learning]] -- (1) [[learning weights]] of [[individual classifiers]] and (2) the [[combination rule]] of their [[weighted responses]], [[this paper]] proposes a [[novel]] [[Ensemble classifier]], [[EnLR]], that computes [[weights of responses]] from [[discriminative classifiers]] and combines their [[weighted responses]] to produce a [[single response]] for a [[test instance]] .
To aid [[peer evaluation]], the [[code]] for all the [[algorithm]]s has been made [[publicly available]] .
To [[analyze]] [[complex]] [[real-world data emerging]] in many [[data-centric application]]s, the [[problem of non-exhaustive]], [[overlapping clustering]] has been studied where the goal is to find [[overlapping]] [[cluster]]s and also detect [[outliers simultaneously]] .
To [[analyze]] the [[model]]'s [[predictive accuracy]], [[the algorithm]]s are applied to multiple [[real-world data sets]] involving [[email communication]], [[international]] [[political events]], and [[animal behavior data]] .
To [[annotate]] the [[list item]]s in a [[Web list]] with [[entiti]]es that they likely [[mention]], [[we]] leverage the [[prior probability]] of an [[entity being mentioned]] and the [[global coherence]] between the [[types of entiti]]es in the [[Web list]] .
To a [[psychologist]], it is [[self-evident]] that [[people]] are neither fully [[rational]] nor completely [[selfish]], and that their [[taste]]s are anything but stable.
To assess the [[predictive value]] of [[classification schemes]] that [[estimate stroke risk]] in [[patient]]s with [[AF]] .
To avoid [[costly]] [[pairing]], [[we]] address the [[task of discovering cross-domain relations]] given [[unpaired data]] .
To avoid the [[assumption]] of a certain [[data distribution]], [[CoCo]] relies on a very [[general]] [[data model]] combining the [[Exponential Power Distribution]] with [[Independent Components]] .
To better [[understand]] and [[simulate human mobility]] during [[the disaster]]s, [[we]] develop a [[probabilistic model]] that is able to be effectively [[train]]ed by the discovered [[evacuation]]s via [[machine learning technique]] .
To capture interesting [[word pair]]s, [[we]] sample different [[sense]]s of words using [[WordNet]] (Miller, 1995).
To capture the above [[intuition]]s, we [[directly model]] query [[temporal pattern]]s using a [[special class]] of [[point process]]es called [[Hawkes process]]es, and combine [[topic model]]s with [[Hawkes process]]es for simultaneously [[identifying]] and [[labeling search task]]s.
To capture various [[expertise level]]s on different [[topic]]s, [[we]] propose [[FaitCrowd]], a [[fine grained]] [[truth discovery model]] for the [[task of aggregating]] conflicting [[data]] collected from multiple [[user]]s / [[source]]s.
To circumvent this [[computational issue]], [[we]] propose a novel [[framework]] that allows us to make an [[inference]] about the [[updated classifier]] without actually [[re-optimizing]] it.
To [[cluster]] based on [[frequent term sets]], [[we]] measure the [[mutual overlap]] of [[frequent set]]s with respect to the sets of supporting [[document]]s.
To compare using [[KB]]s, [[information extraction]] or [[Wikipedia document]]s directly in a [[single framework]] [[we]] construct an [[analysis tool]], [[WikiMovies]], a [[QA dataset]] that contains [[raw text]] alongside a [[preprocessed KB]], in the [[domain of movies]] .
To cope with these problems, [[we]] propose [[CoCo]], an [[technique]]s for [[parameter-free]] [[outlier detection]] .
To cope with [[this challenge]], [[viewing mobility record]]s on [[location-based social networks (LBSNs)]] as [[implicit feedback]] for [[POI recommendation]], [[we]] first propose to exploit [[weighted matrix factorization]] for [[this task]] since it usually serves [[collaborative filtering]] with [[implicit feedback]] better.
To cope with [[this challenge]], viewing [[mobility record]]s on [[location-based social networks (LBSNs)]] as [[implicit feedback]] for [[POI recommendation]], [[we]] first propose to exploit [[weighted matrix factorization]] for [[this task]] since it usually serves [[collaborative filtering]] with [[implicit feedback]] better.
To correct this mismatch we formalize a [[semi-Markov]] [[extraction process]], which is based on [[sequentially classifying]] [[segments]] of several [[adjacent words]], rather than single [[words]] .
To create this [[sense of imminence]], [[we]] manipulated [[time metric]] - the [[unit]]s (e.g., [[day]]s, [[year]]s) in which [[time]] is considered.
To date, most [[research]] on [[relation harvesting]] has focused on [[is-a]] and [[part-of]] .
To [[date]], there has been limited [[research]] on studying [[dynamic blockmodel]]s.
Today [[mass incarceration]] defines the meaning of [[blackness in America]]: [[black people]], especially [[black]] [[men]], are [[criminal]]s.
Today, more of those [[resident]]s live in [[suburb]]s than in [[big citi]]es or [[rural communiti]]es, a significant shift compared to [[2000]], when the [[urban poor]] still outnumbered [[suburban resident]]s living in [[poverty]].1
Today’s [[DBMS]]s embody decades of [[academic]] and [[industrial research]] and intense [[corporate software development]] .
Todays [[enterprise]]s need to [[make decision]]s based on [[analyzing]] [[massive]] and [[heterogeneous data source]]s.
Today's popular [[web search engine]]s expand the [[search process]] beyond [[crawled web page]]s to specialized [[corpora ("verticals")]] like [[image]]s, [[video]]s, [[news]], [[local]], [[sport]]s, [[finance]], [[shopping]] etc., each with its own [[specialized search engine]] .
Today there are several [[burst definitions]] and [[detection algorithms]], and their [[difference]]s can produce very different [[results]] in [[topic streams]] .
To decide which [[product]]s to produce, [[manufacturer]]s need to [[analyze]] the [[consumers' requirement]]s and how [[consumer]]s make their [[purchase decision]]s so that the new [[product]]s will be competitive in the [[market]] .
To demonstrate a potential [[downstream application]] for the identified [[high-risk patient]]s, [[we]] explore the association between the [[physician]] treating these [[patient]]s and the [[treatment outcome]], and propose a [[system]] that can assist [[healthcare provider]]s in [[optimizing the match]] between a [[patient]] and a [[physician]] .
To demonstrate the [[effectiveness]] and [[language independence]] of [[our approach]], [[we]] present [[experimental result]]s on [[Greek]], [[English]], and [[Chinese data]] .
To demonstrate the latter, [[we]] show that combining [[DNN]]s with [[novelty search]], which was designed to encourage exploration on tasks with [[deceptive]] or [[sparse reward function]]s, can solve a [[high-dimensional problem]] on which [[reward-maximizing algorithm]]s (e.g. [[DQN]], [[A3C]], [[ES]], and the [[GA]]) fail.
To demonstrate the power and generality of [[this approach]], [[we]] apply [[the method]] in two very different [[application]]s: [[named entity recognition]] and [[query classification]] .
To [[demostrate this concept]], we furnish [[our method]] with a [[large set]] of [[structural properti]]es and multiple [[community detection algorithm]]s.
To describe [[Adam]]'s [[research]], [[we]] have developed an ontology and [[logical language]] .
To develop [[MASCOT]], [[we]] first present two [[naive]] [[local triangle counting algorithm]]s in a [[graph stream]]: [[MASCOT-C]] and [[MASCOT-A]] .
To discover [[shaker]]s, [[we]] first introduce the concept of a [[cascading graph]] to capture the [[causality relationship]]s among evolving [[entiti]]es over some [[period of time]], and then [[infer]] [[shaker]]s from the [[graph]] .
To do so, we measured the [[VOC]]s from a [[movie theater]] over a whole month in [[interval]]s of thirty [[second]]s, and [[annotate]]d the [[screened film]]s by a [[controlled vocabulary]] compiled from multiple [[source]]s.
To effectively [[reduce the uncertainty]] of [[missing data]], [[we]] present a [[regularized EM algorithm]] that [[penalizes the likelihood]] with the [[mutual information]] between the [[missing data]] and the [[incomplete data]] (or the [[conditional entropy]] of the [[missing data]] given the [[observation]]s).
To [[efficiently]] [[solve]] [[FLSA]], [[we]] propose to [[reformulate]] it as the [[problem of finding an "appropriate" subgradient]] of the [[fused penalty]] at the [[minimizer]], and develop a [[Subgradient Finding Algorithm (SFA)]] .
To enable [[large-scale]] [[training]], [[we]] develop a [[parallel-iterative optimization scheme]] that can handle [[dataset]]s with hundreds of thousands of [[class]]es and millions of [[instance]]s and [[learning]] [[terabyte]]s of [[parameter]]s.
To enable [[train]]ing of [[extremely large DNN]]s, [[model]]s are [[partition]]ed across [[machine]]s.
To enable versatile [[publishing]], [[we]] introduce the [[Guardian Normal Form (GNF)]], a novel [[method]] of [[publishing]] [[multiple]] [[sub-tables]] such that each [[sub-table]] is [[anonymized]] by an existing [[QI-SA publishing algorithm]], while the combination of all [[published tables]] guarantees all [[privacy rules]] .
To ensure [[scalable computation]], these [[subset]]s are induced through a [[decision tree]], [[we]] call this [[Treed-LMMH]] .
To [[estimate sharer reputation]], it is intuitive to leverage [[data]] that [[record]]s how [[recipients respond]] (through [[clicking]], [[liking]], etc.) to [[content item]]s shared by a [[sharer]] .
To evaluate the [[performance]] of [[our algorithm]], [[we]] have performed [[experiment]]s on two [[large-scale]] [[dataset]]s, including [[real-time data]] collected from [[peering router]]s at a large [[Tier-1 ISP]] .
To expedite training on [[very large]] [[data set]]s, [[multiple model]] [[replica]]s are [[train]]ed in [[parallel]] on different [[subset]]s of the [[train]]ing [[example]]s with a [[global parameter server]] maintaining [[shared weight]]s across these [[replica]]s.
To [[explicitly]] address these [[shortcomings]], [[we]] propose a [[discriminative model]] for combining the [[link]] and [[content analysis]] for [[community detection]] .
To explore the [[huge (quadratic) number]] of [[pairs of feature]]s, [[we]] develop a novel, [[computationally efficient method]] called [[FAST]] for [[ranking]] all possible [[pairs of feature]]s as [[candidate]]s for inclusion into [[the model]] .
To facilitate the [[choice]] of [[mobile App]]s, existing [[mobile App recommender system]]s typically recommend popular [[mobile App]]s to [[mobile user]]s.
To facilitate the [[search]] and [[comparison]] of [[Drosophila]] [[gene expression pattern]]s during [[Drosophila]] [[embryogenesis]], it is highly desirable to [[annotate]] the [[tissue-level anatomical ontology]] terms for [[ISH image]]s.
To fill the [[representability]] and [[scalability]] gap between [[linear]] and [[nonlinear SVM]]s, [[we]] propose the [[Adaptive Multi-hyperplane Machine (AMM) algorithm]] that accomplishes fast [[training]] and [[prediction]] and has capability to solve [[nonlinear classification problem]]s.
To filter [[explicit noise]], [[we]] propose our own [[biomedical term detection/normalization method]]: it resolves [[misspelling]], [[term variation]]s, and [[arbitrary abbreviation]] of [[terms by nurses]] .
To find a [[fair price]] for [[storage]], [[we]] first need a [[stochastic model]] for the [[gas pricing process]] .
To fix thoughts, [[we]] use the [[World Wide Web (WWW)]] as the [[database]], and [[Google]] as the [[search engine]] .
To [[formalize]] this intuition, [[we]] propose a [[domain adaptation method]] that [[parameterizes]] this [[concept space]] by [[linear transformation]] under which [[we]] explicitly [[minimize]] the [[distribution difference]] between the [[source domain]] with sufficient [[labeled data]] and [[target domains]] with only [[unlabeled data]], while at the same time [[minimizing]] the [[empirical loss]] on the [[labeled data]] in the [[source domain]] .
To fulfill [[this gap]], in [[this paper, we]] aim for [[maximizing]] the [[revenue]] by considering the [[quantity constraint]] on the [[promoted commodity]] .
To fulfill [[users' search need]]s, the [[search engine]] must have [[good performance]], [[easy-to-use functionaliti]]es, and [[good search result quality]] .
To further alleviate the [[data sparsity issue]], [[Geo-SAGE]] exploits the [[geographical correlation]] by smoothing the [[crowd's preference]]s over a [[well-designed]] [[spatial index]] structure called [[spatial pyramid]] .
To further analyze these [[sequence]]s, [[we]] designed an approach where a [[partial order]] is [[mined]] from [[frequent sequence]]s of [[code]]s.
To further [[improve the model]], [[we]] propose two [[non-convex formulation]]s to [[reduce]] the [[shrinkage bias]] inherent in the [[convex formulation]] .
To gain a better understanding of the [[data]] and to reveal [[unknown relationship]]s, we have built [[prediction model]]s for so-called [[forward prediction]] (the [[prediction]] of [[future VOC]]s from [[the past]]), [[backward prediction]] (the [[prediction]] of [[past scene label]]s from [[future VOC]]s), which is some form of [[abductive reasoning]], and [[Granger causality]] .
To [[gather]] such [[statistic]]s, all [[sentence]]s in the [[corpus]] must be [[syntactically parsed]] .
[[Together]], [[our model]] and [[experiment]]s provide a [[formal reputational account of TPP]], and demonstrate how the [[cost]]s of [[punishing]] may be recouped by the [[long-run benefit]]s of [[signalling]] one's [[trustworthiness]] .
Together, these [[algorithm]]s enable [[HL-MRF]]s and [[PSL]] to model [[rich]], [[structured data]] at [[scale]]s not previously possible.
Together these components extract [[raw data]] from [[disparate source]]s, fuse that [[data]] into [[information]], and then [[automatically mine]] that [[information]] for [[actionable sales lead]]s driven by a combination of [[expert-defined]] and [[statistically derived trigger]]s.
[[Together]], these contributions lead to [[multi-faceted advance]]s in [[sketch construction]], [[cardinality]] and [[error estimation]], the [[theory]], and intuition for the [[problem of approximate counting]] of distinct [[element]]s for both the [[streaming]] and [[non-streaming case]]s.
Together with our [[Australian partner]], [[we]] perform comprehensive study on [[data collected]] for the [[mental health cohort]], and the [[experiment]]s validate that [[our proposed framework]] [[outperform]]s [[risk assessment instrument]]s by [[medical practitioner]]s.
To guarantee a [[satisfactory learning performance]], [[ERM]] requires that the [[training data]] are i.i.d. [[sampled]] from the unknown [[source distribution]] .
To handle [[big data]], [[we]] have implemented [[the algorithm]] using [[Map]] / [[Reduce]] .
To handle [[large database]]s, [[CURE]] employs a combination of [[random sampling]] and [[partitioning]] .
To handle long [[entity name]]s with extraneous [[token]]s, [[we]] propose [[technique]]s to effectively [[map]] long [[entity name]]s to [[short queri]]es in [[query log]] .
To help [[users]] understand the [[topic-based]] [[summarization results]], [[TIARA]] employs several [[interactive]] [[text visualization technique]]s to explain the [[summarization results]] and seamlessly [[link]] such [[results]] to the original [[text]] .
To help us understand the difference between [[people]]'s [[social interaction]]s with an [[agent]] and a [[robot]], [[we]] experimentally compared [[people]]'s responses in a [[health interview]] with (a) a [[computer agent]] projected either on a [[computer monitor]] or [[life-size]] on a [[screen]], (b) a [[remote]] [[robot]] projected [[life-size]] on a [[screen]], or (c) a [[collocated robot]] in the same room.
To [[identify]] [[abrupt concept drift]] in [[data stream]]s, [[PCA]] and [[statistics based heuristic approach]]es are employed.
To illustrate the proposed [[approach]]es, we classify [[patient]]s from the [[ADNI study]] into groups with [[Alzheimer's disease (AD)]], [[mild cognitive impairment (MCI)]] and [[normal control]]s, based on the [[multi-modality data]] .
To illustrate the richness of [[our model]], [[we]] explore two powerful [[computational paradigm]]s for studying [[large graph]]s, namely, [[sketching]] and [[sampling]], and focus on some key problems in [[social network]]s and show [[efficient algorithm]]s in the [[public-private graph model]] .
To incorporate all [[sources of information]] like [[users' profile]], [[tweet quality]], [[interaction history]], etc, [[node]]s and [[edge]]s are represented by [[feature vector]]s.
To increase the [[scale]] even further, we need to [[explore]] [[automatic method]]s for [[constructing]] [[knowledge base]]s.
To investigate the [[utility]] of large (<i> k </i>, <i>m </i>) [[string kernel]]s, [[we]] consider several [[sequence classification problem]]s, including [[protein remote homology detection]], [[fold prediction]], and [[music classification]] .
To keep [[WordNet]] current with [[folk wisdom]], [[we]] propose a [[method]] to enhance [[WordNet]] automatically by [[merging]] [[Wikipedia entiti]]es into [[WordNet]], and construct an enriched [[ontology]], named as [[WorkiNet]] .
To learn an [[<i>l</i><sub>1</sub>-M<sup>3</sup>N]], [[we]] present three [[methods]] including [[projected sub-gradient]], [[cutting-plane]], and a novel [[EM]]-style [[algorithm]], which is based on an [[equivalence]] between [[<i>l</i><sub>1</sub>-M<sup>3</sup>N]] and an [[adaptive]] [[M<sup>3</sup>N]] .
To learn from [[MIML example]]s, [[we]] propose the [[MimlBoost]] and [[MimlSvm algorithm]]s based on a simple [[degeneration strategy]], and [[experiment]]s show that solving [[problems involving complicated object]]s with multiple [[semantic meaning]]s in the [[MIML framework]] can lead to [[good performance]] .
To [[learn the weight]]s of different types of [[node]]s and [[edge]]s, [[we]] propose an [[optimization framework]], called [[OptRank]] .
To leverage [[statistical strength]] and [[infer]] [[latent interrelationship]]s between [[authors and venue]]s, the [[Dirichlet process]] is utilized to [[cluster author]]s and [[venue]]s.
To make the [[extract]]ed [[feature]]s [[generic]], [[the model]]s were [[train]]ed using a [[natural image set]] with millions of [[labeled example]]s.
To make these [[prediction]]s more [[efficient]] for a [[realistic online setting]], [[we]] develop an [[efficient]] [[updating algorithm]] to [[adjust]] the [[topic transition parameter]]s, as new [[documents stream in]] .
To make this [[tractable]] (there are [[exponentially]] many [[paths]] in the [[hypergraph]]), [[we]] lift the [[hypergraph]] by jointly [[clustering]] the [[constant]]s to form [[higher-level concept]]s, and find [[paths]] in [[it]] .
To [[maximize]] both the [[execution path depth]] and the [[degree of freedom]] in [[input parameter]]s for [[exploitation]], [[we]] define a novel [[method]] to assign [[probabiliti]]es to [[program path]]s.
To [[measure]] the [[effectiveness]] of [[TIARA]], [[we]] have conducted several [[experiment]]s.
To meet [[this requirement]], most [[existing method]]s use the [[nuclear norm]] as a [[convex proxy]] of the [[rank function]] and [[minimize it]] .
To [[mine periodicity]] in an [[event]], [[we]] have to face [[real-world challenge]]s of [[inherently]] complicated [[periodic behavior]]s and [[imperfect data collection problem]] .
To [[minimize]] the [[human-labeling]] efforts, [[we]] propose a novel [[multi-label active learning]] [[approach]] which can reduce the required [[labeled data]] without sacrificing the [[classification]] [[accuracy]] .
To [[minimize]] the [[objective function]], [[we]] propose the <i>[[cocoa algorithm]]</i>, which will generate both the [[suite]] of [[predictive model]]s for all the [[pair]]s of [[origination]]s and [[destination]]s, as well as the [[co-cluster]]s consisting of similar [[pair]]s.
To mitigate the high [[computational cost]] of [[EMD]], [[we]] propose an [[approximation]] that reduces the [[cost]] to [[linear time]] .
To [[model difference]]s between the [[severity distribution]] of a [[subpopulation]] (with significant [[atrophy]] in certain [[ROI]]s) and the [[severity distribution]] of the entire [[population]], [[we]] developed the concept of [[Distributional Association Rule]]s.
To model the [[retweet behavior]], we build a [[graph made up of three]] types of [[nodes: user]]s, [[publisher]]s and [[tweet]]s.
To obtain the [[improved result]], as a primary step [[we]] prove a [[connection]] between the [[generalization error]] of a [[classifier]] and its [[incompatibility]], which [[measure]]s the [[fitness]] between the [[classifier]] and the [[sample distribution]] .
To obtain these results, [[we]] had to abandon the original [[iterative scaling]] [[CRF training algorithm]] for [[convex optimization algorithm]]s with better [[convergence]] [[properties]] .
[[Tool]]s for [[automatically aligning]] these [[knowledge base]]s would make it possible to unify many [[source]]s of [[structured knowledge]] and answer [[complex queri]]es.
To [[optimize]] [[the proposed function]] we provide an [[additive approximation algorithm]] and a [[local-search heuristic]] .
To [[optimize]] the [[PSLF model]], [[we]] develop a [[scalable]] [[expectation-maximization (EM) algorithm]], which utilizes a novel [[approximate mean-field technique]] for fast [[expectation computation]] .
To our [[best knowledge]], this is the [[first paper]] to introduce a [[practical solution]] for [[publishing large volume]] of [[sequential data]] under [[differential privacy]] .
To our [[knowledge]] this is the first [[ontology]] and [[interlinked corpus]] for a [[subdiscipline]] within [[computing science]] .
To output [[normalized]] [[attribute value]]s, [[we]] explore several [[string comparison algorithm]]s and found [[n-gram substring matching]] to work well in practice.
To overcome the challenge of [[noisy signal]]s, [[UDI]] captures how likely a [[user connect]]s to a [[signal]] with respect to 1) the [[distance]] between the [[user]] and the [[signal]], and 2) the [[influence]] scope of the [[signal]] .
To overcome the challenge of [[scarce signal]]s, [[UDI]] integrates [[signal]]s observed from both [[social network (friends)]] and [[user-centric data]] ([[tweet]]s) in a unified [[probabilistic framework]] .
To overcome the lack of [[annotated data]], [[we]] propose a [[labelling heuristic]] based on [[information extracted]] from the [[ontology]] .
To overcome these [[drawback]]s, [[this paper]] presents a novel [[scheme]] of [[Online Multiple Kernel Regression (OMKR)]], which [[sequentially learn]]s the [[kernel-based regressor]] in an [[online]] and [[scalable fashion]], and [[dynamically explore]] a [[pool]] of [[multiple diverse kernel]]s to avoid suffering from a [[single fixed poor kernel]] so as to remedy the [[drawback]] of [[manual / heuristic kernel selection]] .
To overcome these [[problem]]s, [[we]] propose a near [[linear tensor factorization approach]], which decompose a [[tensor]] into [[factor tensor]]s in order to [[model]] the [[higher-order relation]]s, without loss of important [[information]] .
To overcome this [[limitation]], [[we]] develop an [[efficient]] [[alternating minimization algorithm]] and show that its [[time complexity]] is only [[approximately proportional]] to the number of [[nonzero element]]s in the [[purchase records tensor P]] .
To overcome [[this problem]], [[we]] propose an [[online learning model]] and [[algorithm]]s for [[learning diversified recommendation]]s and [[retrieval function]]s from [[implicit feedback]] .
To overcome [[this problem]], [[we]] propose a [[system for contextual ad matching]] based on a combination of [[semantic]] and [[syntactic feature]]s.
To overcome this, [[we]] propose a novel, [[content-based]], [[trust propagation framework]] that relies on [[signal]]s from the [[textual content]] to ascertain [[veracity]] of [[free-text claim]]s and [[compute]] [[trustworthiness]] of their [[source]]s.
To perform [[Bayesian inference]] for [[TAA]] with the new [[logistic likelihood]], we extend typical [[Gibbs sampling]] by introducing [[auxiliary variable]]s.
To [[personalize PageRank]], one adjusts [[node weight]]s or [[edge weight]]s that determine [[teleport probabiliti]]es and [[transition probabiliti]]es in a [[random surfer model]] .
[[Topic Detection and Tracking: Event-based Information Organization]] is an excellent reference for [[researcher]]s and [[practitioner]]s in a variety of [[fields related]] to [[TDT]], including [[information retrieval]], [[automatic speech recognition]], [[machine learning]], and [[information extraction]]
[[Topic modeling]], [[Dimensionality reduction]], [[Document classification]], [[Semi-supervised learning]] .
[[Topic modeling]] has been popularly used for [[data analysis]] in various [[domains]] including [[text documents]] .
[[Topic modeling]] has been widely used to [[mine topic]]s from [[document]]s.
[[Topic model]]s are [[effective probabilistic tool]]s for [[process]]ing large [[collection]]s of [[unstructured data]] .
[[Topic models]] (e.g., [[Blei, Ng, & Jordan, 2003]]; [[Griffiths & Steyvers, 2002]]; [[2003]]; [[2004]]; [[Hofmann, 1999]]; [[2001]]) are based upon the idea that [[documents]] are [[mixtures of topics]], where a [[topic]] is a [[probability distribution]] over [[words]] .
[[Topic models]], [[Gibbs sampling]], [[unsupervised learning]], [[author models]], [[perplexity]] .
[[Topic model]]s have played a [[pivotal role]] in analyzing large [[collection]]s of complex [[data]] .
[[Topic model]]s ([[PLSA]] / [[LDA]]) do not explicitly [[model missing words]] .
[[Topic model]]s provide an [[unsupervised method]] for [[extracting]] an [[interpretable representation]] from a [[collection of documents]] .
[[Topic models]] provide a powerful [[tool]] for [[analyzing]] large [[text collections]] by representing [[high dimensional data]] in a [[low dimensional subspace]] .
[[topic relevancy]], [[redundancy]] and [[language style]]s) in an [[unified optimization framework]] .
Topics discussed included: [[Swiss-Prot]], [[PPLRE]], [[NER - Protein]], [[Swissknife]], [[IE - Life Science]]s
[[Topic]]s include [[linear regression]], [[classification]], [[resampling method]]s, [[shrinkage approach]]es, [[tree-based method]]s, [[support vector machine]]s, [[clustering]], and more.
Top: [[named entity recognition]], middle: [[dependency syntax]], bottom: [[verb frame]]s.
[[Topological space]]s, [[manifold]]s, [[path]]s, The [[C-space]] of [[rigid bodi]]es, [[chains of bodies]], and [[trees of bodies]], [[Configuration space]], [[Quaternion]]s, [[C-space obstacle]]s, [[closed kinematic chain]]s, [[algebraic varieti]]es.
[[Toponym Mention Normalization Task]], [[Toponym Mention Normalization Algorithm]], [[Toponym Record]], [[Gazetteer]] .
To [[populate a database]] of such [[schema]] we present [[matrix factorization model]]s that [[learn]] [[latent feature vector]]s for [[entity tuple]]s and [[relation]]s.
[[Top]] [[ranking page]]s from a [[search engine]] may not contain any [[description of the topic]] .
To preserve the [[clustering quality]], [[CAP]] uses the same [[optimization procedure]] as used in [[clustering]] .
To prevent [[privacy breach]]es, even previously [[published result]]s have been removed from [[public database]]s, impeding [[researchers' access]] to the [[data]] and hindering [[collaborative research]] .
To prevent such [[undesirable behavior]], [[we]] modify the [[hidden layer]] such that the resulting [[parent representation]] always has [[length one]], after [[computing]] p as in [[Eq]] .
To provide better [[customer relationship management]] and [[business intelligence]] for [[customer-oriented]] [[cooperates]], millions of [[free-text]] [[addresses]] need to be [[converted]] to a [[standard format]] for [[data integration]], [[de-duplication]] and [[householding]] .
To provide [[text-based pattern search]]ing, the [[images]] in the [[Berkeley Drosophila Genome Project (BDGP) study]] are annotated with [[ontology]] [[term]]s [[manual]]ly by [[human curators]] .
To provide [[user]]s with instant [[repli]]es, a [[push model]] or [[server-initiated model]] is becoming an [[inevitable computing model]] in the [[next-generation location-based service]]s.
To [[quantify]] the [[stochastic nature]] of [[Twitter topic propagation]], we combine [[principle]]s of [[geometric Brownian motion]] and [[traditional]] [[network graph theory]] .
To [[rational economist]]s, these [[assumption]]s seem so [[basic]], [[logical]], and [[self-evident]] that they do not need any [[empirical scrutiny]] .
To [[reduce overriding]] in the [[fully-connected layers we]] employed a [[recently-developed]] [[regularization method]] called [[" dropout]] " that proved to be very [[effective]] .
To reduce the [[dimensionality]] and generalize well, one wishes to [[cluster]] [[keyword]]s or [[queri]]es into [[meaningful group]]s, and [[set parameter]]s at the [[keyword-cluster level]] .
To reduce the [[quantization error]] caused by the [[bag-of-words representation]], [[we]] propose an improved [[feature representation scheme]] based on the [[sparse learning]] [[technique]] .
To [[reduce]] this undesirable [[weighting effect]], [[we]] use a [[log-determinant function]] as a [[non-convex rank approximation]] which [[reduce]]s the [[contributions of large singular value]]s while keeping those of [[small singular value]]s close to [[zero]] .
To [[resolve these issue]]s, [[we]] propose a novel [[temporal]] and [[personalized topic model]] that jointly considers [[temporal dependenci]]es between [[video semantics]], [[users' interaction in commenting]], and [[users' preference]]s as [[prior knowledge]] .
To [[reverse the question]], what are the necessary [[condition]]s so that a [[predefined percentage]] of the [[network]] is [[infected]]?
To [[scale]] to [[real]] [[large]] [[networks]], [[TAP]] is designed with efficient [[distributed]] [[learning algorithm]]s that is implemented and tested under the [[Map-Reduce framework]] .
To show the [[practical value]] of [[this approach]], [[we]] describe how it was used to [[test Helix]], a [[distributed]] [[cluster manager deployed]] at [[LinkedIn]] .
To solve [[OIM]], [[we]] propose a [[multiple-trial approach]], where (1) some [[seed node]]s are selected based on [[existing influence information]]; (2) an [[influence campaign]] is started with these [[seed node]]s; and (3) [[user feedback]] is used to update [[influence information]] .
To solve such a [[problem]], [[we]] propose a [[new method]] that entails a high level of [[security]] - [[full-privacy]] .
To solve the [[convex optimization problem]], [[we]] use an [[alternating method]] in which each [[subproblem]] has an [[efficient solution]] .
To solve the [[IOC problem]], a novel [[unsupervised method Create (ChArT REcovEr)]] is proposed in [[this paper]], which consists of 3 steps: (1) [[social stratification]] of [[ESN]] [[user]]s into different [[social class]]es, (2) [[supervision link inference]] from [[manager]]s to [[subordinate]]s, and (3) consecutive [[social class]]es [[matching]] to [[prune]] the [[redundant]] [[supervision link]]s.
To solve these [[issue]]s, in [[this paper, we]] present a novel [[framework]] of [[Cost-Sensitive Online Active Learning (CSOAL)]], which only [[queri]]es a small fraction of [[training data]] for [[labeling]] and directly optimizes two [[cost-sensitive measure]]s to address the [[class-imbalance issue]] .
To solve the [[TTL problem]], [[we]] propose a [[learning framework]] to [[mimic]] the [[human learning process]] .
To solve this [[problem]], [[we]] propose an [[adaptive]] [[kernel approach]] that [[maps]] the [[marginal distribution]] of [[target-domain]] and [[source-domain]] [[data]] into a [[common]] [[kernel space]], and utilize a [[sample selection strategy]] to draw [[conditional probabilities]] between the two [[domains]] closer.
To solve this [[problem]], [[we]] propose a [[Vague One-Class Learning (VOCL) framework]] which employs a [[double]] [[weighting approach]], at both [[instance]] and [[classifier]] levels, to build an [[ensembling framework for learning]] .
To solve [[this problem]], [[we]] propose [[Diversified RBM (DRBM)]] which diversifies the [[hidden unit]]s, to make [[them]] cover not only the [[dominant topic]]s, but also those in the [[long-tail region]] .
To speed this [[process]], [[we]] develop [[methods]] to [[prune rectangles]] without computing their associated [[LRT statistics]] .
To [[speed]] up the [[basic]] [[outlier detection technique]], [[we]] develop two [[distributed algorithm]]s ([[DOoR]] and [[iDOoR]]) for modern [[distributed multi-core cluster]]s of [[machine]]s, [[connected]] on a [[ring topology]] .
To [[speed]] up this [[online process]], a [[scalable]] [[query processing technique]] is developed by extending the [[classic]] [[Threshold Algorithm (TA)]] .
To tackle the challenges of [[community evolution]] and [[outlier detection]], [[we]] propose an [[integrated optimization framework]] which conducts [[outlier-aware community matching]] across [[snapshot]]s and [[identification]] of [[evolutionary outlier]]s in a tightly coupled way.
To [[tackle]] the [[revenue maximization problem]], [[we]] first introduce a [[strategic searching algorithm]], referred to as [[Algorithm PRUB]], which is able to derive the [[optimal solution]]s.
To tackle [[this challenge]], [[truth discovery]], i.e., to [[integrate]] [[multi-source]] [[noisy information]] by [[estimating the reliability]] of each [[source]], has emerged as a [[hot topic]] .
To tackle [[this problem]], [[we]] propose an [[efficient]] [[greedy-based approximation algorithm]] with a [[provable solution guarantee]] .
To take the first step beyond [[keyword-based search]] toward [[entity-based search]], suitable [[token spans]] (""[[spots]]"") on [[documents]] must be [[identified reference]]s to [[real-world entities]] from an [[entity catalog]] .
[[Total variation (TV) regularization]] has important applications in [[signal processing]] including [[image denoising]], [[image deblurring]], and [[image reconstruction]] .
To [[test]] [[our algorithm]] [[we]] conduct innovative [[experiment]]s over a large [[Twitter]] [[dataset]] .
To that [[end]], in [[this paper, we]] provide a focused [[study]] of [[extracting]] [[energy-efficient]] [[transportation patterns]] from [[location trace]]s.
To that end, [[named entity recognition]] (the [[task]] of [[identifying]] [[words and phrases]] in [[free text]] that belong to certain [[classes]] of [[interest]]) is an important first step for many of these larger [[information management goal]]s.
To that end, [[we]] combine [[temporal attribute]]s of [[node]]s and [[edges of the network]] with a [[Pagerank based algorithm]] to find the [[trendsetter]]s for a given [[topic]] .
To the best of [[our knowledge]], [[SimCareer]]s is the first [[framework]] to model [[professional similarity]] between two [[people]] taking account their [[career trajectory information]] .
To the best of [[our knowledge]], the influence of these [[parameter]]s on the [[quality]] of the [[prediction]]s of [[recommender system]]s has rarely been [[report]]ed in the [[literature]] .
[[To the best of our knowledge]], this is the first [[approach]] addressing this [[problem]] under [[possible world]]s [[semantics]] .
To the best of our [[knowledge]] this is the first [[work]] on [[mining]] [[frequent closed subgraph]]s in [[non-stationary data stream]]s.
To this end, in [[this paper, we]] propose a [[geographic method]], named [[ClusRanking]], for [[estate appraisal]] by leveraging the [[mutual enforcement]] of [[ranking]] and [[clustering power]] .
To this end, the [[kddo1 ontology]] is focused on [[concept]]s and [[relationship]]s for the [[domain of knowledge discovery in database]]s (which in turn inherits concepts from [[machine learning theory]], [[optimization theory]], [[numerical analysis]], [[statistics]], and [[computational linguistics]]).
To this end, [[we]] first [[mine topic]]s from [[# hashtag]]s, the [[human-annotated semantic tag]]s in [[tweet]]s.
To this end, [[we]] first use a general purpose [[knowledgebase]] and [[keyword search]] to [[supply]] the required [[knowledge]] and [[context]] .
To this end [[we]] introduce a [[probabilistic graphical model]] that jointly learns [[user trustworthiness]], [[statement credibility]], and [[language objectivity]] .
To this end, [[we]] propose a novel problem called <b> [[RecMax]] </b> that aims to [[select]] a set of "[[seed]]" [[user]]s for a [[marketing campaign]] for a new [[product]], such that if they [[endorse the product]] by providing relatively [[high rating]]s, the number of other [[user]]s to whom the [[product]] is recommended by the underlying [[RS algorithm]] is [[maximum]] .
To this end, [[we]] propose [[model]]ing [[user psychological desire]] in [[sponsored search]] according to [[Maslow's desire theory]], which [[categorize]]s [[psychological desire]] into five [[level]]s and each one is [[represented by]] a [[set]] of [[textual pattern]]s automatically [[mined]] from [[ad text]]s.
To this end, [[we]] propose simple [[combinatorial formulation]]s that encapsulate efficient [[compressibility]] of [[graphs]] .
[[Tourism industry]] has become a key [[economic driver]] for [[Singapore]] .
To use the best [[value]]s effectively]], [[I]] advocate [[ethics education]] in [[STEM]] fields be based on [[analysis of values]] in four dimensions of [[research]] and [[practice]] .
To [[validate]] [[the proposed model]], [[we]] perform extensive [[experiment]]s on the [[real-world data]] from the world's largest [[P2P lending]] [[marketplace]] .
To [[validate]] [[the proposed RaHH method]], [[we]] conduct extensive [[evaluation]]s on two [[large dataset]]s; one is [[crawl]]ed from a popular [[social media site]]s, <b> [[Tencent Weibo]] </b>, and the other is an [[open dataset]] of <b> [[Flickr]] </b> ([[NUS-WIDE]]).
Towards an [[efficient]] [[incorporation]] of the [[structure information]], [[we]] have designed a [[general model]] where [[we]] use an [[undirected graph]] to capture the relationship of [[subgraph-based base learners]] .
Towards [[learning a scorer]] based on the [[decoupled feature]]s, we require that [[our framework]] fulfill [[inferred sparsity]] to eliminate the [[interference]] of [[noisy keyword]]s, and employ [[distant supervision]] to tackle the lack of [[keyword label]]s.
Towards this end, [[we]] build a [[feature vector]] in the [[training phase]] for an [[‘unrelated’ relation]] by [[randomly selecting]] [[entity pair]]s that do not appear in any [[Freebase relation]] and [[extracting]] [[features]] for them.
[[Tracking]] new [[topics]], [[ideas]], and "[[memes]]" across [[the Web]] has been an [[issue]] of considerable interest.
[[Tracking]] such [[discussion]] on [[weblog]]s, provides useful insight on how to improve [[product]]s or [[market]] them more effectively.
Traditional [[anomaly detection]] on [[social media]] mostly focuses on [[individual point anomalies]] while [[anomalous phenomena]] usually [[occur]] in [[group]]s.
Traditional [[approaches]] mainly focus on [[scheduling]] the [[revisiting strategy]] of each individual [[page]] .
Traditional approaches to the problem have factored in [[demographic]]s, [[revenue]] and [[aggregated human flow statistic]]s from [[nearby]] or [[remote area]]s.
Traditional approaches use a [[similarity measure]] that compares [[tuples' attribute values]]; [[tuples]] with [[similarity scores]] above a certain [[threshold]] are declared to be [[matches]] .
Traditional [[clustering algorithm]]s either favor [[cluster]]s with [[spherical shape]]s and [[similar size]]s, or are very [[fragile]] in the [[presence of outliers]] .
Traditional [[co-clustering method]]s identify [[block structure]]s from [[static data matrice]]s.
Traditional [[counterfactual estimator]]s, such as [[capped importance sampling]] or n[[ormalised importance sampling]], exhibit unsatisfying [[bias-variance compromise]]s when [[experimenting on]] [[personalized product recommendation system]]s.
Traditional [[data mining technique]]s are designed to model a single [[type of heterogeneity]], such as [[multi-task learning]] for [[modeling task heterogeneity]], [[multi-view learning]] for [[modeling view heterogeneity]], etc. [[Recently]], a variety of [[real application]]s [[emerged]], which exhibit [[dual heterogeneity]], namely both [[task heterogeneity]] and view [[heterogeneity]] .
Traditional [[distance measures]] such as [[Euclidean distance]] or [[dynamic time warping]] are not always [[effective]] for [[analyzing]] [[uncertain time series data]] .
Traditional [[EMST methods]] [[scale quadratically]], and more [[advanced methods]] lack [[rigorous runtime guarantees]] .
Traditionally, [[feature construction]] and [[feature selection]] are two important but separate [[process]]es in [[data mining]] .
Traditionally, [[Information Extraction]] ([[IE]]) has focused on satisfying precise, narrow, [[pre-specified requests]] from [[small]] [[homogeneous]] [[corpora]] (e.g., [[extract]] the [[location]] and [[time]] of [[seminars]] from a set of [[announcements]]).
Traditionally, [[information extraction]] proceeds by first [[segmenting]] each [[candidate]] [[record]] separately, and then [[merging records]] that refer to the same [[entities]] .
Traditionally, the [[problem]] was usually addressed by first [[extracting]] major [[keyword]]s corresponding to the [[source entity]] and then [[query relevant]] [[entiti]]es from the [[target domain]] using those [[keyword]]s.
Traditionally, when [[one]] wants to [[learn]] about a particular [[topic]], one [[read]]s a [[book]] or a [[survey paper]] .
[[Traditional method]]s install either a high [[granularity smart meter]] or [[sensor]]s at every [[appliance]], which is either too [[expensive]] or requires [[technical expertise]] .
Traditional [[method]]s used [[handcrafted local feature]]s combined with [[high-level image representation]]s to generate [[image-level representation]]s.
Traditional [[method]]s usually consider the [[behavior]]s as simple [[user]] and [[item connection]]s, or represent them with a [[static model]] .
Traditional [[named entity recognition (NER) task]] has expanded beyond identifying [[people]], [[location]], and [[organization]] to [[book title]]s, [[email address]]es, [[phone number]]s, and [[protein name]]s ([[Nadeau and Sekine, 2007]]).
Traditional [[ranking metric]]s, e.g., area under the [[receiver operating characteristic curve (AUC)]], are however not suitable for [[quantifying]] such a [[ranking list]] which includes [[positive]], [[negative]], and [[unknown status link]]s.
Traditional [[relation extraction]] predicts [[relation]]s within some [[fixed]] and [[finite]] target [[schema]] .
Traditional [[system]]s make [[recommendation]]s based on a single [[domain]] e.g., [[movie]] or [[book domain]] .
Traditional [[unsupervised feature selection methods]] address [[this issue]] by [[selecting]] the [[top ranked]] [[features]] based on certain [[scores]] computed independently for each [[feature]] .
[[Trained]] in [[physics]] and [[astronomy]], [[Feyerabend]] was best known as a [[philosopher of science]] .
[[Training joint-sequence based G2P]] require explicit [[grapheme-to-phoneme alignment]]s which are not straightforward since [[grapheme]]s and [[phoneme]]s don't correspond [[one-to-one]] .
[[Training our system]] using [[pairs]] of [[question]]s and [[structured representation]]s of their [[answer]]s, and pairs of [[question paraphrase]]s, yields [[competitive result]]s on a [[competitive benchmark]] of [[the literature]] .
Training these [[DNN]]s using a [[cluster]] of [[commodity machine]]s is a promising [[approach]] since [[training]] is [[time consuming]] and [[compute-intensive]] .
[[Transfer learning]], which [[leverages knowledge]] from [[source domain]]s to enhance [[learning ability]] in a [[target domain]], has been proven effective in various [[application]]s.
[[Transforming unstructured text into structured units]] (e.g., [[semantically meaningful phrase]]s) will substantially reduce [[semantic ambiguity]] and enhance the [[power]] and [[efficiency]] at manipulating [[such data]] using [[database technology]] .
[[Treasury Appropriation Fund Symbol]]: The [[component]]s of a [[Treasury Account Symbol]] - [[allocation agency]], [[agency]], [[main account]], [[period of availability]] and availability type -that directly correspond to an [[appropriations account]] established by [[Congress]] .
[[Treatment]]s of the role of [[culture]] in [[economic prosperity]] tend to focus on [[generic]] [[cultural attribute]]s that are deemed desirable, such as [[hard work]], [[initiative]], [[belief]] in the [[value of education,]] as well as [[factor]]s drawn from [[macroeconomic]]s, such as a propensity to [[save and invest]] .
[[Tree boosting]] has [[empirically proven]] to be a highly [[effective approach]] to [[predictive modeling]] .
Tree-LSTMs outperform all [[existing system]]s and strong [[LSTM baseline]]s on two [[task]]s: [[predicting]] the [[semantic relatedness]] of two [[sentence]]s ([[SemEval 2014]], [[Task]] 1) and [[sentiment classification (Stanford Sentiment Treebank]]).
[[Triangle listing]] is one of the fundamental [[algorithmic problem]]s whose [[solution]] has numerous [[application]]s especially in the [[analysis]] of [[complex network]]s, such as the [[computation]] of [[clustering coefficient]], [[transitivity]], [[triangular connectivity]], etc. [[Existing algorithm]]s for [triangle listing]] are mainly [[in-memory algorithm]]s, whose [[performance]] cannot [[scale]] with the massive [[volume]] of [[today]]'s fast growing [[network]]s.
::*[[trivial group]]: the mathematical [[group]] containing only the [[identity element]]
[[Trust-based recommendation methods]] assume the additional [[knowledge]] of a [[trust network]] among [[users]] and can better deal with [[cold start users]], since [[users]] only need to be simply [[connected]] to the [[trust network]] .
[[Trust network]]s, where [[people]] leave [[trust]] and [[distrust feedback]], are becoming increasingly common.
[[Trust/other account]]: Unit in [[MMARS]] which records the [[status of monies authorized]] to be spent by various [[statute]]s, other than [[appropriations]], [[capital outlay authorization]]s and [[federal grants]] .
[[TTL]] is aimed at breaking the [[large domain distance]]s and [[transfer knowledge]] even when the [[source]] and [[target domain]]s share few [[factor]]s directly.
[[Tucker decomposition]] has [[strong capacity of expression]], but the [[time complexity]] is [[unpractical]] for the [[large-scale]] [[real problem]]s.
[[TurboGraph]] also provides [[engine-level operator]]s such as [[BFS]] which are implemented under the [[pin-and-slide model]] .
[[TurboGraph]] is the first truly [[parallel graph engine]] that exploits 1) <i>[[full parallelism]]</i> including [[multi-core parallelism]] and [[FlashSSD IO parallelism]] and 2) <i>[[full overlap]]</i> of [[CPU processing]] and [[I/O processing]] as much as possible.
[[TVI]] is an [[unsupervised Bayesian Network]] that incorporates multiple [[factor]]s, such as [[travel speed]], [[weather condition]]s and [[geographical feature]]s of a [[road]] .
[[Tweet]]s are [[ranked]] in [[chronological order]] regardless of their potential [[interestedness]] .
[[Twitter]], [[Facebook]] etc) typically do not contain [[instance]]s of [[large scale]] collaboration]], [[we]] have crawled millions of [[software repositori]]es spanning a [[period]] of four years and hundreds of thousands of [[developer]]s from [[GitHub]], a popular [[open-source]] <i>[[social coding network]] </i>.
[[Twitter]] generates a huge [[volume]] of [[instant message]]s (i.e. [[tweet]]s) carrying [[users' sentiment]]s and [[attitude]]s every [[minute]], which both necessitates [[automatic opinion summarization]] and poses great challenges to the [[summarization system]] .
[[Twitter]] is a [["what's-happening-right-now " tool]] that enables [[interested parti]]es to [[follow thoughts]] and [[commentary]] of [[individual user]]s in [[nearly real-time]] .
[[Two]] [[aspects]] are crucial when constructing any [[real world]] [[supervised classification task]]: the [[set of classes]] whose [[distinction]] might be useful for the [[domain expert]], and the [[set of classifications]] that can actually be distinguished by the [[data]] .
Two common types of [[algorithms from machine learning research]] that may be applied to [[on-line classification task]]s make use of either [[lazy instance-based]] ([[k-NN]], [[IB1]]) or [[eager model-based]] ([[C4.5]], [[CN2]]) [[approaches]] .
Two [[complementary]] [[visualization view]]s are designed and implemented: [[cluster view]]s and [[time-zone view]]s.
Two different [[curators]] independently [[curated]] and [[scored]] all [[interactions]] .
Two [[display impression]]s with [[similar]] or [[identical]] [[effectiveness]] (e.g., [[measure]]d by [[conversion]] or [[click-through rate]]s for a [[targeted audience]]) may sell for quite different [[price]]s at different [[market segment]]s or [[pricing scheme]]s.
Two important [[contributions]] of [[our approach]] are: 1) it [[propagates]] [[scarce]] [[prior affinities]] between [[data]] to the [[global scope]] and incorporates the [[full affinities]] into the [[metric learning]]; and 2) it uses an efficient [[alternating]] [[linearization method]] to directly optimize the [[sparse metric]] .
Two [[node]]s have the same role if they [[interact]] with [[equivalent set]]s of [[neighbor]]s.
Two [[solutions are proposed]] to take [[instance]] [[weight values]] into the [[classifier training process]] .
Two [[studi]]es investigated the [[tendency]] of [[people]] to be [[unrealistically optimistic]] about future [[life event]]s.
Two well-known [[upper ontologi]]es are [[SUMO]] ([[Pease and Niles, 2002]]) and [[CyC]] ([[Lenat, 1995]]).
[[Typed dependency parse trees]] are [[edge]] and [[node labeled]] [[parse trees]] whose [[labels]] and [[topology]] contains valuable [[semantic]] [[clues]] .
Typically, the [[classification behavior]] may only be [[inferred]] from the overall [[pattern]]s of [[data distribution]], and very little [[information]] is embedded in any given [[record]] for [[classification]] purposes.
Typically, the [[relevance]] of an ad to [[page content]] is indicated by a [[tf-idf score]] that [[measure]]s the [[word overlap]] between the [[page]] and the ad [[content]], so [[this problem]] is transformed into a [[similarity search]] in a [[vector space]] .
Typically these capture [["standard" scholarly entiti]]es and their [[connection]]s, such as [[author]]s, [[affiliation]]s, [[venue]]s, [[publication]]s, [[citation]]s, and others.
Typically, the [[statistician]]'s job ends there, and the [[model]] or [[inference]] is unlikely to be [[revisited]] except if improvements to the [[methodology]] are desired, or in rare [[follow-up]] or [[meta analyse]]s.
[[Unaggregated data]], in a [[streamed]] or [[distributed form]], is prevalent and comes from diverse sources such as [[interactions of user]]s with [[web service]]s and [[IP]] [[traffic]] .
[[Uncertain Database]]s, [[Frequent Itemset Mining]], [[Probabilistic Data]], [[Probabilistic Frequent Itemset]]s
Under certain [[dependency assumption]]s, [[we]] derive simple [[bidding function]]s that can be [[calculated]] in [[real time]]; [[our finding]] shows that the [[optimal bid]] has a [[non-linear relationship]] with the [[impression level evaluation]] such as the [[click-through rate]] and the [[conversion rate]], which are [[estimated]] in [[real time]] from the [[impression level feature]]s.
Underpinning this [[evolution]] is a set of [[best practice]]s for [[publishing]] and [[connecting structured data]] on [[the Web]] known as [[Linked Data]] .
[[Understanding]] and [[predicting]] the [[choice]]s and [[preference]]s of [[user]]s is a challenging task: [[real-world scenario]]s involve users behaving in complex situations, where [[prior belief]]s, specific [[tendenci]]es, and [[reciprocal influence]]s jointly contribute to determining the [[preferences of users]] toward [[huge amounts of information]], [[service]]s, and [[product]]s.
[[Understanding]] and [[predicting]] these [[movement]]s is critical for planning effective [[humanitarian relief]], [[disaster management]], and [[long-term societal reconstruction]] .
[[Understanding]] and [[quantifying]] the impact of [[unobserved process]]es is one of the major [[challenge]]s of [[analyzing]] [[multivariate time series data]] .
Understanding how the [[disease progress]]es and [[identifying]] related [[pathological biomarker]]s for the [[progression]] is of primary importance in [[Alzheimer's disease]] [[research]] .
Understanding [[large-scale]] [[document collection]]s in an [[efficient manner]] is an important [[problem]] .
[[Understanding]], [[measuring]], and [[leveraging]] the [[similarity]] of [[binari]]es ([[executable code]]) is a [[foundational challenge]] in [[software engineering]] .
[[Understanding natural language]] requires identifying whether different [[mentions of a name]], [[within]] and [[across]] [[document]]s, represent the same [[entity]] .
[[Understanding]] the [[behaviors of tourist]]s is very important for the [[government]] and [[private sector]]s, e.g., [[restaurant]]s, [[hotel]]s and [[advertising compani]]es, to improve their [[existing service]]s or create new [[business opportuniti]]es.
Understanding the [[dynamic mechanisms]] that drive the [[high-impact scientific work]] (e.g., [[research paper]]s, [[patent]]s) is a [[long-debated research topic]] and has many important [[implication]]s, ranging from [[personal career development]] and [[recruitment search]], to the [[jurisdiction]] of [[research resource]]s.
Understanding what [[factor]]s determine whether [[people]] [[succeed]] or [[fail]] in achieving [[desired outcome]]s is a fundamental concern in both [[basic]] and [[applied psychology]] .
Under the [[ACE]] (NIST 2003) and [[DARPA TIDES]] (TIDES 2004) Programs, the [[Linguistic Data Consortium]] at the [[University of Pennsylvania]] develops [[annotation guidelines]], [[corpora]] and other [[linguistic resource]]s to support [[information extraction]] [[research]] (LDC 2004).
Under the [[ERM principle]], one [[minimize]]s an [[upper bound]] of the [[true risk]], which is approximated by the [[summation]] of [[empirical risk]] and the [[complexity]] of the [[candidate]] [[classifier class]] .
Under the <i>[[Wise Market]]</i> [[framework]], [[we]] define an [[optimization problem]] to [[minimize expected cost]] of [[paying out reward]]s while [[guarantee]]ing a [[minimum confidence level]], called the <i>[[Effective Market Problem (EMP)]] </i>.
Under this scheme, [[we]] have [[fully annotated]] an [[English web corpus]] for [[multiword expression]]s, including [[those]] containing [[gap]]s.
Unfortunately [[aggregation]] can severely diminish the utility of [[such data]] when [[modeling]] or [[analysis]] is desired at a [[per-individual basis]] .
Unfortunately, along with the [[development]] of the [[malware]] [[writing]] [[technique]]s, the number of [[file]] [[samples]] in the [[gray list]] that need to be [[analyzed]] by [[virus]] [[analysts]] on a [[daily basis]] is constantly [[increasing]] .
Unfortunately, [[EHR]] [[data]] do not always [[directly]] and reliably map to [[phenotype]]s, or [[medical concept]]s, that [[clinical researcher]]s need or use.
Unfortunately for [[humanlike robot]]s, [[computer]]s are at their [[worst]] trying to do the things most [[natural]] to [[human]]s, such as [[seeing]], [[hearing]], [[manipulating objects]], [[learning language]]s, and [[commonsense reasoning]] .
Unfortunately [[KB]]s often suffer from being too [[restrictive]], as the [[schema]] cannot support certain [[types of answers]], and too [[sparse]], e.g. [[Wikipedia]] contains much more [[information]] than [[Freebase]] .
Unfortunately most advanced [[gradient method]]s do not tolerate the [[sampling noise]] inherent in [[stochastic approximation]]: it collapses [[conjugate search]] [[direction]]s ([[Schraudolph & Graepel, 2003]]) and confuses the [[line search]]es that both [[conjugate gradient]] and [[quasi-Newton method]]s depend upon.
Unfortunately, such kind of [[knowledge]] is often [[hidden]] in a [[network]] where different kinds of [[relationships]] are not [[explicitly]] [[categorized]] .
Unfortunately, the existing [[software system]]s in the [[IP]] [[domain]] do not address this [[task]] directly.
Unfortunately, the [[fastest algorithm]] known for [[graph automorphism]] is [[nonpolynomial]] .
Unfortunately the [[generative model]] loses [[sparsity]] as the amount of [[data increase]]s, requiring O (k) [[operation]]s per word for k [[topic]]s.
Unfortunately, [[their]] [[representation]] cannot be easily translated into [[our needs]] .
Unfortunately, the [[optimization]] is [[non-convex]], so [[we]] propose a [[convex]] [[approximation]] .
Unfortunately, [[this tuning]] is often a "[[black art]]" requiring [[expert experience]], [[rules of thumb]], or sometimes [[brute-force search]] .
[[Unfunded promise]]s set the stage for [[political conflict]] and [[spending growth]] outstrips [[revenue policy]] in many [[public organization]]s.
[[United States]], [[Canada]], [[job quality]], [[Mexico]], [[part-time]], [[retail]], [[schedule]], [[working hours]]
[[Unlabeled sample]]s can be [[intelligently selected]] for [[labeling]] to [[minimize]] [[classification error]] .
Unlike almost all [[existing method]]s that map [[heterogeneous data]] in a common [[Hamming space]], mapping to different [[space]]s provides more [[flexible]] and [[discriminative ability]] .
Unlike [[binary unit]]s, [[rectified linear unit]]s [[preserve information]] about [[relative intensiti]]es as [[information travel]]s through multiple [[layer]]s of [[feature detector]]s.
Unlike [[classical graph mining work]], [[we]] show that it suffices to [[mine]] the [[dominator tree]] of the [[heap dump]], which is significantly [[smaller]] than the [[ underlying graph]] .
Unlike [[clustering methods for local dimensionality reduction]], [[LLE]] maps its inputs into a [[single]] [[global coordinate system]] of [[lower dimensionality]], and its [[optimizations]] do not involve [[local minima]] .
Unlike existing [[approach]]es that incorporate additional [[content-based objective term]]s, [[we]] instead focus on the [[optimization]] and show that [[neural network model]]s can be [[explicitly trained]] for [[cold start]] through [[dropout]] .
Unlike in [[PageRank]], [[we]] do not specify the [[edge resistance]]s (equivalently, [[conductance]]s) and ask for [[node visit rate]]s.
Unlike [[least-squares estimation]] which is primarily a [[descriptive tool]], [[MLE]] is a [[preferred method]] of [[parameter estimation]] in [[statistics]] and is an indispensable tool for many [[statistical modeling technique]]s, in particular in [[non-linear modeling]] with [[non-normal data]] .
Unlike [[LSA]], the initial [[word-by-document co-occurrence matrix]] is replaced by a [[word-by-word PMI matrix]], in which [[word]]s are [[represented as]] [[vector]]s of [[PMI score]]s relative to other [[word]]s in the [[vocabulary]] ([[Niwa & Nitta, 1994]]).
Unlike other [[ecological inference technique]]s, our [[method]] makes use of [[unlabeled individual-level data]] by [[embedding the distribution]] over these [[predictor]]s into a [[vector]] in [[Hilbert space]] .
Unlike other [[methods]], [[associative classification]] tries to find all the [[frequent patterns]] existing in the [[input]] [[categorical data]] satisfying a [[user-specified]] [[minimum support]] and/or other [[discrimination measures]] like [[minimum confidence]] or [[information-gain]] .
Unlike other [[nonlinear model]]s such as [[kernel method]]s, the [[FFD model]] is [[interpretable]] as it gives [[importance weight]]s on the original [[feature]]s.
Unlike other similar [[algorithm]]s [[IPLoM]] is not based on the [[Apriori algorithm]] and it is able to [[find clusters]] in [[data]] whether or not its [[instance]]s appear [[frequently]] .
Unlike past [[published work]] that requires [[high-resolution trace]]s with [[dense sampling]], [[we]] focus on situations with [[coarse granularity data]], such as that obtained from thousands of [[taxi]]s in [[Shanghai]], which transmit their [[location]] as seldom as [[once per minute]] .
Unlike [[previous analyses]] of other [[classifiers]] in this setting, [[we]] avoid the [[unnatural effects]] that arise when one insists that all [[pairwise distance]]s are [[approximately preserved]] under [[projection]] .
Unlike previous [[parallel approach]]es, [[the algorithm]]s use simple [[intersection operation]]s to [[compute frequent itemsets]] and do not have to maintain or [[search]] [[complex hash structure]]s.
Unlike previous [[RNN model]]s, [[QANTA]] [[learn]]s [[word]] and [[phrase-level representation]]s that combine [[across sentence]]s to [[reason]] about [[entiti]]es.
Unlike previous [[sparse learning method]]s, [[our model]] [[FHIM]] recovers both the main [[effect]]s and the [[interaction term]]s accurately without imposing [[tree-structured hierarchical constraint]]s.
Unlike previous [[system]]s, [[Shark]] shows that it is possible to achieve these [[speedup]]s while retaining a [[MapReduce-like]] [[execution engine]], and the [[fine-grained]] [[fault tolerance properti]]es that such [[engine]] provides.
Unlike [[previous work]] on [[frequent itemset mining]], [[our technique]]s do not rely on the [[output]] of a [[non-private mining algorithm]] .
Unlike previous [[work]], [[we]] consider [[proactive thermal management]], whereby [[server]]s can [[predict]] potential [[overheating]] [[event]]s due to [[dynamic]]s in [[data center configuration]] and [[workload]], giving [[operator]]s enough time to [[react]] .
Unlike [[prior work]] [[our approach]] is entirely [[unsupervised]] and does not require [[knowledge]] of the aspect specific [[rating]]s or [[genre]]s for [[inference]] .
Unlike prior [[work]] that views [[constraints]] as [[boolean criteria]], [[we]] present a [[formulation]] that allows [[constraints]] to be [[satisfied]] or [[violated]] in a [[smooth manner]] .
Unlike [[stratification]], [[MetaCost]] is applicable to any number of [[class]]es and to arbitrary [[cost matrice]]s.
Unlike their [[corpus-specific method]], which is specific to a (single) [[Wikipedia page]], [[our algorithm]] allows us to [[extract]] [[evidence]] for a [[relation]] from many different [[document]]s, and from any [[genre]] .
Unlike the much more extensively [[researched explicit feedback]], [[we]] do not have any [[direct input]] from the [[user]]s regarding their [[preference]]s.
Unlike the [[Netflix-like setting]] that provides both [[positive]] and [[negative feedback]] ([[high]] and [[low rating]]s), no [[negative feedback]] is available in many [[e-commerce system]]s.
Unlike the other forms of [[group]]s, users join this [[type]] of groups mainly for participating [[offline events organized]] by [[group member]]s or [[inviting]] other [[user]]s to [[attend event]]s sponsored by them.
Unlike [[traditional]] [[method]]s, the new [[setting]] aims to [[explicitly represent]] and [[learn]] an [[intrinsic structure]] from [[data]] in a [[high-dimensional space]], which can greatly facilitate [[data visualization]] and [[scientific discovery]] in [[downstream analysis]] .
[[Unsupervised approaches]] have been applied to [[shallow semantic tasks]] (e.g., [[paraphrasing]] ([[Lin and Pantel, 2001]]), [[information extraction]] ([[Banko et al., 2007]])), but not to [[semantic parsing]] .
[[Unsupervised extraction]] is often applied to [[specialized domain]]s, since the [[manual construction of knowledge bases]] or [[training examples]] for [[such domains]] is costly in terms of [[effort]] and [[expertise]] .
[[Unsupervised models]] compute [[score]]s for [[pairs of nodes]] based on [[topological properties of the graph]] .
[[Unsupervised text embedding method]]s, such as [[Skip-gram]] and [[Paragraph Vector]], have been attracting increasing attention due to their [[simplicity]], [[scalability]], and [[effectiveness]] .
[[Unsupervised word representations]] are very useful in [[NLP task]]s both as [[input]]s to [[learning algorithm]]s and as extra [[word feature]]s in [[NLP system]]s.
[[Unusual pattern]]s in such [[data]] typically reflect [[disease condition]]s.
Upon [[examination]], many of the [[dense subgraphs output]] by [[our algorithm]] are <i>[[link spam]] </i>, i.e., [[website]]s that attempt to manipulate [[search engine ranking]]s through [[aggressive interlinking]] to [[simulate]] [[popular content]] .
[[UPON Lite]] focuses on [[user]]s, typically [[domain expert]]s without [[ontology expertise]], [[minimizing]] the role of [[ontology engineer]]s.
Upon [[NRP]], [[we]] determine the [[probability]] of a [[category tag]] for each place by exploring the [[relatedness of place]]s.
[[U-report]] is an [[open-source SMS platform]] operated by [[UNICEF Uganda]], designed to give [[community]] members a [[voice]] on [[issue]]s that impact them.
[[Use case]]s are a [[method for describing interactions]] between [[human]]s and/or [[system]]s.
[[User behavioral analysis]] and [[user feedback]] (both [[explicit]] and [[implicit]]) [[modeling]] are crucial for the improvement of any [[online recommender system]] .
[[User behavior]] can often be determined by [[individual]]'s [[long-term]] and [[short-term preference]]s.
[[User]] [[browsing information]], particularly their non-[[search]] [[related]] [[activity]], reveals important [[contextual information]] on the [[preference]]s and the intent of [[web]] [[user]]s.
[[User engagement]] is a key concept in the [[design of online application]]s (whether for [[desktop]], [[tablet]] or [[mobile)]], motivated by the observation that [[successful application]]s are not just [[used]], but are [[engaged]] with.
[[User engagement]] refers to the [[quality]] of the [[user experience]] that emphasizes the [[positive aspect]]s of [[interacting]] with an [[online application]] and, in particular, the desire to use that [[application]] [[longer]] and [[repeatedly]] .
[[user engagement]], [[user experience]], [[method]]s, [[measurement]], [[online activity]], [[eye tracking]], [[loyalty]], [[mouse tracking]], [[popularity]], [[physiological measurement]], [[self-report measurement]], [[web analytic]]s, [[intra-session]], [[inter-session]], [[reliability]] and [[validity]], [[multi-tasking]], [[dwell time]], [[mixed method]]s
[[user expectation]], sensitivity to various [[item qualiti]]es, [[reading speed]], are involved into the [[casual behavior]] of [[online reading]] .
[[User interfaces for Digital Libraries]] - [[Collection building]] - [[management and integration]] - [[System architecture]]s - [[integration and interoperability]]
[[User modeling]] has found [[application]]s in [[mobile APP recommendation]]s, [[social networking]], [[financial product marketing]] and [[customer service]] in [[telecommunication]]s.
[[user modeling]], [[machine learning]], [[concept drift]], [[computational complexity]], [[World Wide Web]], [[information agent]]s
[[User recommender system]]s are a key [[component]] in any [[on-line social networking]] [[platform]]: they help the [[user]]s growing their [[network]] [[faster]], thus driving [[engagement]] and [[loyalty]] .
[[Users' characteristics extracted]] from their [[public profile]]s in [[microblog]]s and [[products' demographics learned]] from both [[online product review]]s and [[microblog]]s are fed into [[learning]] to [[rank algorithm]]s for [[product recommendation]] .
[[Users' daily activiti]]es, such as [[dining]] and [[shopping]], inherently reflect their [[habit]]s, [[intent]]s and [[preference]]s, thus provide [[invaluable information]] for [[service]]s such as [[personalized information recommendation]] and [[targeted advertising]] .
[[User]]s flock these [[network]]s, [[creating profile]]s and [[linking themselve]]s to other [[individual]]s.
[[User]]s in [[online social network]]s play a variety of [[social role]]s and [[status]]es.
[[User]]s invest [[time]], [[attention]], and [[emotion]] in their [[use of technology]], and seek to satisfy [[pragmatic]] and [[hedonic]] needs.
[[Users' location]]s are important to many [[application]]s such as [[targeted advertisement]] and [[news recommendation]] .
[[Users of popular service]]s like [[Twitter]] and [[Facebook]] are often simultaneously [[overwhelmed]] with the amount of [[information delivered]] via their [[social connection]]s and [[miss out]] on [[much content]] that they might have liked to see, even though it was [[distributed outside]] of their [[social circle]] .
[[User]]s on an [[online social network]] site generate a [[large number]] of [[heterogeneous activiti]]es, ranging from connecting with other [[user]]s, to [[sharing content]], to updating their [[profile]]s.
[[User]]s perceive more [[change]] and [[freshness]] but less [[accuracy]] and [[familiarity]] .
[[User]]s [[sequentially]] [[generate]] [[queries]] in a [[digital library]] .
[[User study]] and [[experimental result]]s demonstrate that [[SSGSelect]] significantly [[outperform]]s [[manual coordination]] in both [[solution quality]] and [[efficiency]] .
[[User]]s typically [[rate]] only a [[small fraction]] of all available [[items]] .
[[User]]s who are popular, [[active]], and [[influential]] tend to create [[traffic-based shortcut]]s, making the [[information diffusion process]] more [[efficient]] in the [[network]] .
Using a combination of [[locally-connected convolutional unit]]s and [[globally-connected unit]]s, as well as a few [[trick]]s to reduce the effects of [[overfitting]], [[we]] achieve [[state-of-the-art performance]] in the [[classification task]] of the [[CIFAR-10 subset]] of the [[tiny images dataset]] .
Using a [[gold standard]] of [[MEDLINE abstract]]s manually tagged and normalized for mentions of [[human gene]]s, a [[combined tagging and normalization system]] achieved 0.669 [[F-measure]] (0.718 [[precision]] and 0.626 [[recall]]) at the [[mention]] level, and 0.901 [[F-measure]] (0.957 [[precision]] and 0.857 [[recall]]) at the [[document level]] for [[document]]s used for [[tagger training]] .
Using a [[large scale]] [[dataset]] [[recorded]] from [[retail store]]s, [[our approach]] discovers [[semantically meaningful]] [[cashier scan pattern]]s.
Using a [[natural model]] of [[opinion formation]], we analyze the [[effect]] of these [[interaction]]s on an [[individual's opinion]] and [[estimate]] her [[propensity to conform]] .
Using an [[open-source]], [[Java toolkit]] of [[name-matching method]]s, [[we]] experimentally compare [[string distance metrics]] on the [[task of matching entity names]] .
Using a [[retrieval-based approach]] to find [[relevant article]]s, [[we]] [[instantiate]] [[the framework]] to compute [[trustworthiness]] of [[news source]]s and [[article]]s.
Using a straight-forward [[triangle counting algorithm]] as a [[black box]], [[we]] performed 166 [[experiments]] on real-world [[networks]] and on [[synthetic datasets]] as well, where [[we]] show that our [[method]] works with [[high]] [[accuracy]], typically more than 99\% and gives significant [[speedup]]s, resulting in even <math>\approx</math> 130 [[times]] faster [[performance]] .
Using both [[synthetic]] and [[real data]], we have done extensive [[experiment]]s given different [[real life scenario]]s to [[verify the effectiveness]] of [[our model]]s.
Using [[Catalyst]], we have built a variety of [[feature]]s (e.g. [[schema inference for JSON]], [[machine learning type]]s, and [[query federation]] to [[external database]]s) tailored for the [[complex]] needs of [[modern data analysis]] .
Using [[cell phone]] [[location data]], as well as [[data]] from two [[online location-based social network]]s, [[we]] aim to understand what [[basic]] [[law]]s govern [[human motion]] and [[dynamics]] .
Using clear, sharp analysis and comprehensive data, [[Reinhart]] and [[Rogoff]] document that [[financial fallout]]s occur in [[cluster]]s and [[strike]] with surprisingly consistent [[frequency]], [[duration]], and [[ferocity]] .
Using comprehensive [[experimental study]], [[we]] have demonstrated the effectiveness of [[proposed]] [[learning methods]] .
Using [[daily S\&P500 data]] for the past 21 years and a [[benchmark NYSE dataset]], [[we]] show that [[MA]]s [[outperform]] existing [[portfolio selection algorithm]]s with provable [[guarantee]]s by several [[orders of magnitude]], and match the [[performance]] of the best [[heuristic]]s in the [[pool]] .
Using [[datasets of user rating]]s, [[we]] provide [[evidence]] for the existence of [[aversion]] and attraction in [[real-life data]], and show that our [[optimal strategy]] can lead to significantly improved [[recommendation]]s over systems that ignore [[aversion]] and [[attraction]] .
Using [[email]] [[spam-filtering]] [[data]], [[we]] demonstrate that [[class noise]] can have substantial [[content specific bias]] .
Using [[Flickr dataset]]s of more than ten million [[image]]s of 40 [[topic]]s, our [[empirical result]]s show that [[the proposed algorithm]] is more successful in [[predicting]] unseen [[Web image]]s than other [[candidate method]]s, including [[forecasting]] on [[semantic meaning]]s only, a [[PageRank-based image retrieval]], and a [[generative]] [[author-time topic model]] .
Using <i>[[Trajectory Pattern]]s</i> as [[predictive rule]]s has the following [[implication]]s : (I) the [[learning]] depends on the [[movement]] of all available [[object]]s in a certain [[area]] instead of on the [[individual history]] of an [[object]]; (II) the [[prediction tree]] intrinsically contains the [[spatio-temporal properties]] that have emerged from the [[data]] and this allows us to define [[matching methods]] that striclty depend on the [[properties]] of such [[movement]]s.
Using [[maximum entropy]] and [[SVM]] as the [[base classifier]] (for [[classifying]] the individual [[word token]]s), a [[hidden Markov model (HMM)]] is trained on the the [[probabilistic output]] of the [[base classifier]], and a [[sequential label prediction]] is obtained using a [[Viterbi decoding]] .
Using more [[unlabeled data]] for [[pre-training]] only yields additional [[gain]]s.
Using new techniques from [[unconstrained submodular maximization]], we develop an [[approximation algorithm]] for [[SII]] and present a suite of [[experimental result]]s - including [[test]]s on [[real-world police data]] from [[Chicago]] .
Using our [[MapReduce]] <b> [[Set Cover]] </b> [[heuristic]] as a [[building block]], we present the first [[large-scale seed generation algorithm]] that [[scale]]s to â¼ 20 billion [[node]]s and [[discover]]s new [[page]]s at a [[rate]] â¼ 4x [[faster]] than that obtained by [[prior art heuristic]]s.
Using our [[streaming partitioning method]]s, we are able to [[speed up]] [[PageRank computation]]s on [[Spark]], a [[distributed computation system]], by 18% to 39% for [[large]] [[social network]]s.
Using [[pipelining]], [[we]] then combine this [[pre-filtering stage]] with a [[conventional frequent item algorithm]] (<i> [[Space-Saving]] </i>) that will [[process]] the [[remainder]] of the [[data]] .
Using [[real data trace]]s, [[we]] show that [[ThermoCast]] [[forecast]]s [[temperature]] better than a [[machine learning approach]] solely driven by [[data]], and can successfully [[predict]] [[thermal alarm]]s 4.2 minutes ahead of [[time]] .
Using several public [[domain dataset]]s, [[we]] illustrate how our approach overcomes many [[limitation]]s of current [[system]]s and enables the analyst to [[efficiently]] narrow down to [[hypothese]]s of [[interest]] and reason about alternative [[explanation]]s.
Using [[standard metric]]s, [[we]] show [[result]]s that are significantly more [[accurate]] than the current [[state-of-the-art]] while being faster to compute.
Using [[synthetic dataset]]s and three [[real dataset]]s including those from the [[platforms Meetup]] and [[Plancast]], we [[experimentally]] demonstrate that our [[greedy heuristic]]s are [[scalable]] and furthermore [[outperform]] the [[baseline algorithm]]s significantly in terms of achieving superior [[social welfare]] .
Using the [[discovered relation]]s, [[our proposed network]] successfully transfers style from one [[domain]] to another while preserving key [[attribute]]s such as [[orientation]] and [[face identity]] .
Using the [[Distributional Association Rule]]s, [[we]] [[clustered]] [[ROI]]s into [[disease subsystem]]s.
Using the enriched [[vector representation]] as the [[input]] in [[SVM]] [[classifiers]] to [[predict]] the [[importance level]] for each [[test]] [[message]], [[we]] obtained significant [[performance improvement]] over the [[baseline system]] (without [[induced]] [[social]] [[features]]) in our [[experiments]] on a [[multi-user]] [[data collection]] .
Using the new [[experimental]] [[framework]], an [[evaluation study]] on [[synthetic]] and [[real-world dataset]]s comprising up to ten million [[examples]] shows that the new [[ensemble methods]] perform very well [[compared]] to several [[known methods]] .
Using the [[real data]], [[GeBM]] produces [[brain activity pattern]]s that are strikingly similar to the [[real one]]s, and the [[inferred functional connectivity]] is able to provide [[neuroscientific]] insights towards a better [[understanding]] of the way that [[neuron]]s [[interact with]] each other, as well as [[detect]] [[regulariti]]es and [[outlier]]s in [[multi-subject brain activity measurement]]s.
Using these [[feature]]s, which are [[mined]] from a small initial [[seed]] of [[labeled data]], we are able to profile the [[Web site]]s of [[forty-four]] distinct [[affiliate program]]s that account, [[collectively]], for hundreds of millions of [[dollar]]s in [[illicit e-commerce]] .
Using these [[model]]s, [[we]] show that some [[feature]]s carry [[supplementary information]], and the [[effectiveness]] of different [[feature]]s vary in different [[type]]s of [[forums]] .
Using [[these prediction]]s, early [[corrective manufacturing action]]s may be taken to increase the [[speed]] of [[expected slow wafer]]s (a [[collection]] of [[microprocessor]]s) or [[reduce]] the [[speed]] of [[fast wafer]]s.
Using these techniques, [[we]] build a [[rich ontology]], integrating [[Wikipedia's infobox]]-[[class schemata]] with [[WordNet]] .
Using the [[social graph]] of the [[experiment user]]s, [[we]] also explore how [[user]]s are affected by their [[friend]]s who are [[exposed to ad]]s.
Using the type of roof of a home, [[thatched]] or [[metal]], as a [[proxy]] for [[poverty]], we develop a new [[remote sensing approach]] for [[selecting extremely poor village]]s to [[target]] for [[cash transfer]]s.
Using [[this congestion model]], [[we]] develop an [[efficient algorithm]] for [[non-myopic adaptive routing]] to [[minimize]] the <i>[[collective travel time]]</i> of all [[vehicle]]s in [[the system]] .
Using [[this framework]], [[we]] derive a [[Propensity-Weighted Ranking SVM]] for [[discriminative learning]] from [[implicit feedback]], where [[click model]]s take the role of the [[propensity estimator]] .
Using [[this information]], [[the algorithm]] updates only those [[estimate]]s which are relatively [[inaccurate]] and whose [[update]] would most likely remove [[clustering uncertainty]] .
Using this large benchmark result set, [[we]] discuss a comparison of [[TPC-C]] [[performance]] and [[price-performance]] to [[Moore's Law]] .
Using [[this model]], [[our system]] predicts the [[traffic condition]]s of a [[future time]] (when the [[computed route]] is [[actually driven]]) and performs a [[self-adaptive driving direction service]] for a particular [[user]] .
Using [[this search algorithm]], [[our program AlphaGo]] achieved a 99.8% [[winning rate]] against other [[Go program]]s, and defeated the [[human]] [[European]] [[Go]] [[champion]] by 5 games to 0.
Using three [[factor]]s in the [[factorization]], we can [[explicitly model]] and [[learn]] the [[community membership]] of each [[node]] as well as the [[interaction]] among [[communiti]]es.
Using [[tool]]s from [[online loss minimization]], [[we]] derive an [[adaptive online boosting algorithm]] that is also [[parameter-free]], but not [[optimal]] .
Using [[unsupervised learning methods]], [[the system]] automatically creates [[patterns]] and performs [[extraction]] based on a [[topic]] that has been specified by a [[user]] .
Using [[WordNet]] to [[expand queries]] to an [[information retrieval system]], the [[expansion]] of ''[[computer]]'' includes [[words]] like ''[[estimator]]'' and ''[[reckoner]]''.
Using [[word senses]] versus [[word form]]s is useful in many [[applications]] such as [[information retrieval]] [20], [[machine translation]] [5] and [[question-answering]] [16].
[[USP]] substantially outperforms [[TextRunner]], [[DIRT]] and an [[informed baseline]] on both [[precision]] and [[recall]] on [[this task]] .
Usually, [[document data]] are associated with other [[information]] (e.g., an [[author]]'s [[gender]], [[age]], and [[location]]) and their [[link]]s to other [[entiti]]es (e.g., [[co-authorship]] and [[citation network]]s).
“[[Utility function]]” is a [[technical term]] not of [[engineer]]s but of [[economist]]s.
[[Vacuuming robot]]s ought to beget [[smarter]] [[cleaning robot]]s with [[dusting]], [[scrubbing]] and [[picking-up]] [[arm]]s, followed by larger [[multifunction]] [[utility robot]]s with stronger, more [[dexterous arm]]s and better [[sensor]]s.
[[Value iteration]] for [[planning under sensing uncertainty]], [[Robot localization]], [[mapping]], [[navigation]], [[searching]], [[visibility-based pursuit-evasion]], [[manipulation with sensing uncertainty]] .
[[Value]]s, therefore, translate into facts that can be [[scientifically understood]]: regarding [[positive]] and [[negative social emotion]]s, [[retributive impulse]]s, the effects of specific [[law]]s and [[social institution]]s on [[human relationship]]s, the [[neurophysiology]] of [[happiness]] and [[suffering]], etc. The most important of these facts are bound to transcend [[culture]] — just as facts about [[physical]] and [[mental health]] do.
[[Variable selection]] and [[feature extraction]] are fundamental for [[knowledge discovery]] and [[predictive modeling]] with [[high-dimensionality]] (Fan and Li [13]).
[[Variational approximations]] based on [[Kalman filters]] and [[nonparametric wavelet regression]] are developed to carry out [[approximate posterior inference]] over the [[latent]] [[topics]] .
[[Variational inference]] involves defining a [[parametric family]] of [[distributions]] that forms a [[tractable]] [[approximation]] to an [[intractable]] [[true joint distribution]] .
[[Variational inference]] is also used for [[detecting]] [[hidden motifs]] .
[[Variational inference (VI)]] lets us [[approximate]] a [[high-dimensional Bayesian posterior]] with a [[simpler variational distribution]] by solving an [[optimization problem]] .
Various [[network architecture]]s and [[nonlineariti]]es are assessed on [[task]]s of [[regression]] and [[classification]], using [[MNIST]] as an example.
Various " [[no-flattening theorem]]s " show when these [[efficient]] [[deep network]]s cannot be accurately [[approximated]] by shallow ones without [[efficiency loss]] - even for [[linear network]]s.
Various [[properties of discourse]] are described, and [[explanation]]s for the [[behavior]] of [[cue phrase]]s, [[referring expression]]s, and [[interruption]]s are [[explored.
Various types of [[value]]s can be involved in each [[domain]] including [[ethical value]]s (the good of [[society]], [[equity]], [[sustainability]]), [[aesthetic value]]s ([[simplicity]], [[elegance]], [[complexity]]), or [[epistemic value]]s ([[predictive power]], [[reliability]], [[coherence]], [[scope]]).
[[Vector-space models (VSM)]] [[represent word meanings]] with [[vectors]] that capture [[semantic]] and [[syntactic information of word]]s.
[[Verbal fluency]] has been demonstrated to be sensitive to [[lesion]]s in the [[frontal lobe]], [[temporal lobe]], and [[caudate nucleus]] [[Benton 1968]], [[Butters et al. 1987]], [[Miceli, Caltagirone, Gainotti, Masullo, & Silveri 1981]], [[Milner 1964]], [[Perret 1974]] and [[Ramier & Hecaen 1970]]; [[Alzheimer's disease]] [[Appell, Kertesz, & Fisman 1982]], [[Bayles & Tomoeda 1983]], [[Butters et al. 1987]], [[Chertkow & Bub 1990]], [[Cummings, Benson, Hill, & Read 1985]], [[Hodges et al. 1992]], [[Martin & Fedio 1983]], [[Ober, Dronkers, Koss, Delis, & Friedland 1986]], [[Pachana, Boone, Miller, Cummings, & Berman 1996]] and [[Rosen 1980]]; [[Huntington's disease]] [[Butters, Sax, Montgomery, & Tarlow 1978]] and [[Butters et al. 1987]]; [[amnesia]] [[Butters et al. 1987]] and [[Weingartner, Grafman, Boutelle, Kaye, & Martin 1983]], and [[traumatic brain injury]] ([[Raskin & Rearick, 1996]]).
[[Vertical cell decomposition]], [[shortest-path roadmap]]s, [[maximum-clearance roadmap]]s, [[cylindrical algebraic decomposition]], [[Canny's algorithm]], [[complexity bound]]s, [[Davenport-Schinzel sequence]]s.
[[Vertice]]s playing the same [[role]] have [[similar pattern]]s of [[interaction]]s with [[vertice]]s in other [[role]]s.
Very often the [[collected data]] comes with [[block-wise missing entri]]es; for example, a [[patient]] without the [[MRI scan]] will have no [[information]] in the [[MRI]] [[data block]], making his / her overall [[record incomplete]] .
Very quickly, she discovered that no [[job]] is truly "[[unskilled]]," that even the [[lowliest occupation]]s require [[exhausting]] [[mental]] and [[muscular effort]] .
[[Veto]]: [[Action taken]] by the [[Governor]], [[authorized]] by the [[Constitution]], to disapprove a [[legislative bill]] .
[[Veto override]]: [[Legislative power]] to nullify a [[Governor]]'s [[veto]] .
[[Video dissemination]] through [[site]]s such as [[YouTube]] can have widespread impacts on [[opinion]]s, [[thought]]s, and [[culture]]s.
[[virus]]es, [[meme]]s, [[information propagation]], [[viral propagation]], [[topic structure]], [[topic characterization]], [[blog]]s.
[[Visualization]] of [[high-dimensional data]] such as [[text document]]s is widely applicable.
[[Visual text analytic]]s, [[exploratory analytics]], [[document summarization]], [[topic analysis]], [[interactive visualization]]
[[VOC]]s are [[molecule]]s of relatively [[small mass]] that quickly [[evaporate]] or [[sublimate]] and can be [[detect]]ed in the air that surrounds us.
[[Voting variant]]s, some of which are introduced in [[this paper]], include: [[pruning]] versus [[no pruning]], use of [[probabilistic estimates]], [[weight perturbations ([[Wagging)]], and [[backfitting of data]] .
[[Vulnerabilities]] can be addressed by [[patches]], [[reconfigurations]], and other [[workarounds]]; however, these [[actions]] may incur [[down-time]] or [[unforeseen side-effects]] .
Wargames are generally categorized as [[historical]], [[hypothetical]], [[fantasy]], or [[science fiction]] .
[[Ways and Means Committee]]s: [[Legislative bodi]]es in the [[House]] and [[Senate]], which consist of members of the respective [[branch]]es appointed by the [[House Speaker]] and [[Senate President]] .
[[We]] achieved the highest performance using [[dependency-based vector representation]]s, which [[outperformed neural network]] and [[window-based model]]s.
[[We]] address it through [[extracting and analyzing content]] from the [[websites]] of the [[service providers]] listed in [[business]] [[directories]] .
[[We]] address the problem of [[inferring road map]]s from [[large-scale]] [[GPS trace]]s that have relatively [[low resolution]] and [[sampling frequency]] .
[[We]] address the [[problem]] of [[predicting new drug-target interaction]]s from three [[input]]s: known [[interaction]]s, [[similariti]]es over [[drug]]s and those over [[target]]s.
[[We]] address these issues using a [[matrix factorization]] [[analogue]] of [[logistic regression]], and by applying a principled [[confidence-weighting scheme]] to its [[objective]] .
[[We]] address this [[challenge]] through employing a [[multilabel statistical clustering approach]] within an [[expectation-maximization framework]] .
[[We]] address this issue by [[developing sparse graph regression (SpaGraphR)]], a [[non-parametric estimator]] incorporating [[kernel smoothing]], [[maximum likelihood]], and [[sparse graph structure]] to gain fast [[learning algorithm]] .
[[We]] address [[this problem]] and develop [[sequential]] and [[distributed algorithm]]s that are significantly more [[efficient]] than [[state-of-the-art method]]s while still guaranteeing the same [[outlier]]s.
[[We]] address [[this problem]] by considering the [[privacy]] and the algorithmic [[requirements]] simultaneously, focusing on [[decision tree induction]] as a [[sample application]] .
[[We]] address [[this problem]] by extending [[factorized asymptotic Bayesian inference]] .
[[We]] address [[this problem]] by proposing [[NERD]], a [[framework]] which unifies 10 popular named entity extractors available on the [[web]], and the [[NERD ontology]] which provides a [[rich]] [[set of axioms]] aligning the [[taxonomi]]es of these [[tool]]s.
[[We]] address three [[prediction problem]]s, namely [[subject risk prediction]], [[drug recommendation]], and [[future risk prediction]], by using [[machine learning technique]]s; [[our multiple-classifier approach]] successfully reduced the [[cost]]s of [[health checkup]]s, a [[multi-task learning method]] provided [[accurate]] [[recommendation]] for specific [[type]]s of [[drug]]s, and an [[active learning method]] achieved an [[efficient assignment]] of [[healthcare worker]]s for the [[follow-up care]] of [[subject]]s.
[[We]] adopt the [[generic framework]] of [[Random Walk with Restarts]] in order to provide with a more [[natural]] and [[efficient way]] to [[represent social network]]s.
[[We]] aim at reducing the [[complexity]] of [[that model]], while preserving most of its [[accuracy]] in describing the [[data]] .
We aim to give a wide view of [[ontology mapping]] including [[integration]], [[merging]], and [[alignment]] because this concept of [[ontology mapping]] is broad in scope[5] and [[ontology mapping]] is required in the process of [[integration]], [[merging]], and [[alignment]] .
[[We]] aim to model [[people]]'s [[cognitive state]]s, namely their [[belief]]s as expressed through [[linguistic mean]]s.
[[We]] also add a [[regularization term]] to the [[loss function]] for avoiding [[over-ﬁtting in
[[We]] also analyze the [[theoretical bound]]s of the proposed [[OMKR method]] and conduct extensive [[experiment]]s to [[evaluate]] its [[empirical performance]] on both [[real-world regression]] and [[times series prediction task]]s.
[[We]] also applied [[our approach]] with a [[real]] [[ad serving system]] and compared the [[online performance]] using [[A/B testing]] .
[[We]] also apply [[SNARE]] to the [[task]] of [[graph]] [[labeling]] in general on [[publicly-available]] [[dataset]]s.
[[We]] also apply the [[discovered]] [[advisor-advisee relationships]] to [[bole search]], a specific [[expert finding task]] and [[empirical study]] shows that the [[search performance]] can be effectively improved (+4.09% by [[NDCG]]@5).
[[We]] also apply the [[regularized EM algorithm]] to [[fit]] the [[finite mixture model]] .
[[We]] also assume that [[atypical]] [[sequence]]s of [[event]]s in the [[discrete stream]]s can lead to [[off-nominal]] [[system performance]] .
[[We]] also briefly describe the [[reasoning architecture]] of [[KnowWE]] .
[[We]] also compute the [[entropy]] of the [[geographic distribution]] of the [[queri]]es as means of [[detecting]] their [[location affinity]] .
[[We]] also [[correlate]] the [[probabilistic output]] with other [[failure signal]]s from [[passive monitoring]] to increase the [[confidence]] of the [[probabilistic analysis]] .
[[We]] also define two [[constraint]]s in our [[structured learning framework]] to ensure that [[search result]]s are both [[diversified]] and consistent with a [[user's interest]] .
[[We]] also demonstrate that applying the [[integration result]]s produced by [[our method]] can improve the [[accuracy]] of [[expert finding]], an important [[task]] in [[social network]]s.
[[We]] also demonstrate that in a [[top-k setting]], the removal of not [[delta-relevant]] [[pattern]]s improves the [[quality]] of the [[result set]] .
[[We]] also demonstrate that [[noise detection technique]]s based on [[classifier]] confidence tend to identify [[instance]]s that [[human assessor]]s are likely to [[label in error]] .
[[We]] also demonstrate that [[TM-LDA]] is able to highlight interesting variations of common [[topic transition]]s, such as the [[difference]]s in the [[work-life rhythm of citi]]es, and [[factor]]s associated with [[area-specific problem]]s and [[complaint]]s.
[[We]] also demonstrate the effect of [[attribute]] [[drift]], that is, the importance of individual [[attributes]] in forming [[links]] [[change over time]] .
[[We]] also demonstrate the [[real-world applicability]] of [[our approach]] on an [[operational search engine]], where it substantially improves [[retrieval performance]] .
[[We]] also derive an [[efficient]] [[learning]] and [[inference procedure]] that allows for [[large scale]] [[optimization of the model parameters]] .
[[We]] also describe an [[algorithm]] to [[rank multiple answer]]s [[extract]]ed from multiple [[webpage]]s.
[[We]] also describe [[how script]]s are [[compiled into efficient]], [[parallel execution plan]]s and [[executed]] on [[large cluster]]s.
[[We]] also describe interesting [[research problem]]s in [[this domain]] as well as [[design choice]]s made to make [[system]] easily deployable across [[health insurance companies]] .
[[We]] also describe the [[tools]] used for [[corpus annotation]] and [[management]] .
[[We]] also develop an [[alternating minimization algorithm]] for [[efficiently]] [[optimizing the proposed formulation]] .
[[We]] also develop and [[analyze]] a [[mathematical model]] for the kinds of [[temporal variation]] that the [[system]] exhibits.
[[We]] also develop a robust [[predictive modeling]] [[technique]] that identifies and [[models]] only the most [[coherent]] [[region]]s of the [[data]] to give [[high]] [[predictive accuracy]] on the selected [[subset]] of [[response]] values.
[[We]] also developed a novel [[semi-supervised transductive learning algorithm]] that [[propagates]] importance [[labels]] from [[training examples]] to [[test examples]] through [[message]] and [[user]] [[nodes]] in a [[personal email]] [[network]] .
[[We]] also developed two [[extension]]s of the [[MVMT learning algorithm]] .
[[We]] also develop new [[coordinate descent method]]s when [[error]] in [[NMF]] is measured by [[KL-divergence]] by applying the [[Newton method]] to solve the [[one-variable sub-problem]]s.
[[We]] also discuss a [[projected subgradient method]] for solving [[this problem]], which offers additional [[computational saving]]s in certain [[setting]]s.
[[We]] also discuss general [[distributed optimization]], extensions to the [[nonconvex setting]], and [[efficient implementation]], including some details on [[distributed MPI]] and [[Hadoop MapReduce implementation]]s.
[[We]] also draw conclusions between [[academic]] versus [[commercial annotator]]s.
[[We]] also embed the [[learned predictor]] into a [[mobile device based activity prompter]] and evaluate the [[app]] on [[multiple participant]]s living in [[smart home]]s.
[[We]] also [[empirically demonstrate]] the effectiveness of [[the proposed framework]] by conducting [[comprehensive experiments]] on [[challenging real-world dataset]]s.
[[We]] also [[empirically show]] that [[BioSnowball]] [[outperform]]s the [[decoupled methods]] .
[[We]] also employ strategies to address the challenges of [[learning]] from [[highly skewed data]] [[at scale]], allocating the effort of [[human expert]]s, leveraging [[domain expert knowledge]], and [[independently assessing]] the [[effectiveness]] of [[our system]] .
[[We]] also establish [[condition]]s under which the [[methodology]] will achieve the [[global maximum]] .
[[We]] also evaluated the effectiveness of the [[induced]] [[lexicon]]s with respect to [[instance-based semantic tagging]] .
[[We]] also evaluate [[semantic generalization]] on the [[SemEval 2012 task]], and [[outperform]] the previous [[state-of-the-art]] .
[[We]] also evaluate [[the model]]'s ability to [[predict sentiment distribution]]s on a new [[dataset]] based on [[confession]]s from the [[experience project]] .
[[We]] also examine the effect that [[calibrating the models]] via [[Platt Scaling]] and [[Isotonic Regression]] has on their [[performance]] .
[[We]] also examine the [[evolution]] of [[viewpoint neighborhoods]] for different [[entities]] over [[time]] to identify key [[structural]] and [[behavioral transformations]] that occur.
[[We]] also examine [[training set size]] , and [[alternative document representation]]s.
[[We]] also experimented with a [[method]] that finds better [[solution]]s to Eq.5 based on [[CKY-like]] beam [[search algorithm]]s ([[Socher et al., 2010]]; [[Socher et al., 2011]]) but the [[performance]] is similar and the [[greedy version]] is much faster.
[[We]] also explore additional [[variant]]s of the fundamental [[problem formulation]], in order to account for [[constraint]]s and [[consideration]]s that emerge in [[realistic cluster-hiring scenario]]s.
[[We]] also explore [[hashing method]]s to perform fast nearest [[neighbor search]] on a [[map-reduce framework]], in order to [[efficiently]] obtain [[recommendation]]s.
[[We]] also explore some interesting conjectures suggested by [[the results]] of our [[experiment]]s that relate the [[properti]]es of the [[user preference]]s, the [[dominance relation]], and the [[algorithm]]s.
[[We]] also extend [[the proposed method]] to [[large-scale]] [[L2-loss linear support vector machines]] ([[SVM]]).
[[We]] also extend [[this family of formulation]]s to [[bipartite graph]]s by introducing the <i>[[(p,q)- biclique densest subgraph problem]]</i> ([[(p, q) - Biclique-DSP]]), and devise an [[exact algorithm]] that can treat both [[clique]] and [[biclique densiti]]es in a [[unified way]] .
[[We]] also find that [[robots]] increased both [[wages]] and [[total factor productivity]] .
[[We]] also find that [[social integrative motivation]]s are the [[primary predictor]] of [[subscription behaviour]] .
[[We]] also find that [[variability]] of [[individual behavior]] over [[time]] is significantly [[less than]] [[variability]] across the [[population]], suggesting that [[individuals]] can be [[classified]] into [[persistent]] "[[types]]".
We also give an [[empirical evaluation]] of [[our rank]] and [[stable rank algorithm]]s on [[real]] and [[synthetic dataset]]s.
[[We]] also had excellent luck and timing: as we were [[building]] the [[company]], [[real-time ad impression-level auction]]s with [[machine-to-machine buying]] and [[selling]] became [[commonplace]], and [[marketer]]s became increasingly focused on delivering better results for their [[company]] and delivering better [[personalized]] and relevant [[digital experience]]s for their [[customer]]s.
[[We]] also illustrate some interesting [[phenotype]]s derived from our [[data]] .
[[We]] also included five legal drugs of misuse ([[alcohol]], [[khat]], [[solvent]]s, [[alkyl nitrite]]s, and [[tobacco]]) and one that has since been classified ([[ketamine]]) for reference.
[[We]] also introduce a [[hybrid]] [[method]] that utilizes both [[quantification]] and [[semi-supervised learning]] .
[[We]] also introduce a new [[search algorithm]] that combines [[Monte Carlo simulation]] with [[value]] and [[policy network]]s.
[[We]] also introduce a [[probabilistic form]] of [[Skolemization]] for [[handling evidence]] .
[[We]] also introduce [[MalGen]], which is a [[utility for generating data]] on [[clouds]] that can be used with [[MalStone]] .
[[We]] also introduce three typical [[application]]s: [[reconstruction of network]]s, [[evaluation of network evolving mechanism]] and [[classification of partially labeled network]]s.
[[We]] also normally assume that the [[parameters]] do not [[change]], i.e., the [[model is time-invariant]] .
[[We]] also perform an [[experimental evaluation]] on [[real data]] in order to understand the [[value]] of [[coverage]] for [[user]]s.
[[We]] also perform [[longitudinal stability selection]] to [[identify]] and [[analyze]] the [[temporal pattern]]s of [[biomarker]]s in [[disease progression]] .
[[We]] also plan to do [[experiment]]s on more [[real world application]]s in the [[future]] .
[[We]] also present a [[debiasing technique]] to remove the [[effect]] of such [[annotation bias]] from [[adversely]] affecting the [[accuracy of labels]] obtained.
[[We]] also present a [[generalization bound]] showing the [[consistency]] and the [[asymptotic behavior]] of the [[learning process]] of [[our proposed formulation]] .
[[We]] also present an [[application]] of [[tree mining]] to analyze [[real]] [[web log]]s for [[usage pattern]]s.
[[We]] also present an [[effective]] [[flow-based predicting algorithm]], offer [[formal bounds]] on [[imbalance]] in [[sparse network]] [[link prediction]], and employ an [[evaluation method]] appropriate for the observed [[imbalance]] .
[[We]] also present an [[efficient]] [[heuristic approach]] for [[anonymizing]] [[large-scale]] [[social network]]s against [[friendship attack]]s.
[[We]] also present an [[evaluation methodology]] for automatically [[measuring]] the [[precision]] and [[recall]] of discovered [[senses]] .
[[We]] also present an [[LWI-SVD]] with [[restart]]s [[(LWI2-SVD) algorithm]] which leverages a novel [[highly efficient partial reconstruction]] based [[change detection scheme]] to [[support timely refreshing]] of the [[decomposition]] with significant changes in the [[data]] and prevent accumulation of [[error]]s over [[time]] .
[[We]] also present a number of [[statistics]] about the network including the most [[cited author]]s, the most central [[collaborator]]s, as well as [[network statistic]]s about [[the paper citation]], [[author citation]], and [[author collaboration network]]s.
[[We]] also present [[association rules]] and [[related algorithm]]s for an efficient [[training process]] .
[[We]] also present [[empirical justification]] for some of the [[modeling]] [[assumptions]] made in [[dynamic]] [[network]] [[analysis]] (e.g., [[first-order]] [[Markovian assumption]]) and develop [[metrics]] to [[measure]] the [[alignment]] between [[links]] and [[attributes]] under different [[strategies]] of using the historical [[network data]] .
[[We]] also present several new [[streamlined architecture]]s for both [[residual]] and [[non-residual Inception network]]s.
[[We]] also present the [[empirical evaluation]] of [[the proposed method]]s.
[[We]] also present the results of experiments using [[VAMPIRE]], [[SPASS]], and the [[E Theorem Prover]] on the [[first-orderized Cyc KB]] .
[[We]] also present the [[results]] that [[validate]] its [[effectiveness]] .
[[We]] also propose a fast [[greedy algorithm]] based on the [[selection]] of <i>[[composite atom]]s</i> in each [[iteration]] and provide a [[performance guarantee]] for [[it]] .
[[We]] also propose a [[map-reduce implementation]] of [[our framework]] and demonstrate that this can analyze [[cascade]]s with millions of [[infection]]s in [[minute]]s.
[[We]] also propose an [[entropy-minimization model]] to [[suggest]] the best [[location]]s to [[establish]] new [[monitoring station]]s.
[[We]] also propose a new [[index structure]], [[Social R-Tree]] to further improve the [[efficiency]] .
[[We]] also propose an [[intuitive and simple]] [[probabilistic model]] to directly [[quantify]] the [[attribution]] of different [[advertising channel]]s.
[[We]] also propose a [[scheme]] for [[pruning spurious node]]s from [[Wikipedia]]'s [[crowd-sourced]] [[category hierarchy]] .
[[We]] also propose a [[self-attention mechanism]] and a special [[regularization term]] for [[the model]] .
[[We]] also propose a [[supervised algorithm]] that [[identifies]] and [[links concept mentions]] that are (or should be) in the [[ontology]], and also [[identify mentions]] of [[binary relations]] that are (or should be) in the [[ontology]] .
[[We]] also propose the use of a [[low-rank matrix factorization]] approach with [[generalized loss function]]s as a [[practical method]] for [[sign inference]] - this approach yields [[high accuracy]] while being [[scalable]] to [[large signed network]]s, for instance, [[we]] show that [[this analysis]] can be performed on a [[synthetic graph]] with 1.1 million [[node]]s and 120 million [[edge]]s in 10 minutes.
[[We]] also provide a [[catalogue]] of currently available [[BMA software]] .
[[We]] also provide a [[generic learning algorithm]] for [[optimizing model]]s with respect to [[BPR-Opt]] .
[[We]] also provide [[analysis]] of [[finding]] [[real-world event]]s from the [[topic]]s obtained by our [[model]] .
[[We]] also provide an [[ensemble method]] for combining diverse [[cluster-based model]]s.
[[We]] also provide an [[extension]] of [[the algorithm]] which [[learns sparse nonlinear representations]] using [[kernel]]s.
[[We]] also provide an improved [[learning]] and [[prediction method]] with [[runtime complexity analysis]] for [[RTF]] .
[[We]] also provide insights on the [[pragmatics]] of [[ontology mapping]] and elaborate on a [[theoretical approach]] for defining [[ontology mapping]] .
[[We]] also provide [[normalization]] [[solutions]] for several [[measures]] .
[[We]] also provide [[provable approximation guarantees]] for [[our algorithm]] .
[[We]] also provide [[theoretical convergence analysis]] based on [[Gibbs sampling]] with [[asynchronous variable update]]s.
[[We]] also provide [[theoretical justification]] for [[the proposed algorithm]] for several existing [[time series model]]s.
[[We]] also provide the simple [[linear-time algorithm]] which [[we]] developed to [[identify]] and [[extract]] the [[structure]]s from [[our data]] .
[[We]] also [[quantitatively]] demonstrate the generalization power of [[the proposed method]] for three [[prediction task]]s.
[[We]] also refer to [[tools for ontology integration]], [[merging]], and [[alignment]] as [[ontology mapping tool]]s in [[this paper]] .
[[We]] also reveal that both [[long-term]] and [[short-term context]]s are very important in improving [[search performance]] for [[profile-based personalized search strategi]]es.
[[We]] also reveal the [[property]] of the [[CVB0 inference]] in terms of the [[leave-one-out]] [[perplexity]], which leads to the [[estimation algorithm]] of the [[Dirichlet distribution]] [[parameter]]s.
[[We]] also show [[experimentally]] the [[quality]] of the [[set]] of [[blocks proposed]], and the interest of the [[rewriting]] to understand [[actual trace data]] .
[[We]] also show how an [[effective framework]] can be [[trained without manually labeled data]], and conduct several [[experiment]]s to [[verify]] the [[effectiveness]] of [[the whole process]] .
[[We]] also show how certain [[social actor]]s, [[author]]s, impact these [[topic]]s and propose new ways for [[evaluating]] [[author impact]] .
[[We]] also show how [[our method]] is [[deterministic]], [[fully incremental]], and has a [[limited time complexity]], so that it can be used on [[web-scale real network]]s.
[[We]] also show how [[privacy]] weakens if [[newly accepted friend]]s are unguarded or [[unprotected]] .
[[We]] also show how to [[combine]] these [[latent]] and [[observable model]]s to get improved [[modeling power]] at decreased [[computational cost]] .
[[We]] also show how to combine these [[latent]] and [[observable model]]s to get [[improved modeling power]] at decreased [[computational cost]] .
[[We]] also show prelimary results on the problem of [[normalizing]] the different ways that the same [[concept]]s are [[expressed]] within a set of [[citance]]s, using and improving on existing [[technique]]s in [[automatic]] [[paraphrase generation]] .
[[We]] also show [[superior prediction accuraci]]es on multiple [[real-world data set]]s.
[[We]] also show that a particular [[type]] of [[small network motif]]s that we call [[3-path]]s are the major source of [[loss]] in [[reciprocity]] for [[real network]]s.
[[We]] also show that [[less aggressive]] [[clustering]] sometimes results in improved [[classification accuracy]] over [[classification]] without [[clustering]] .
[[We]] also show that [[our approach]] can flag [[anomalous team behavior]] which has many potential [[application]]s.
[[We]] also show that [[our method]] reduces the [[prediction error]] as fast as [[distributed]] [[stochastic gradient descent]], achieving a 4.1% improvement in [[RMSE]] for the [[Netflix]] [[dataset]] and an 1.8% for the [[Yahoo music]] [[dataset]] .
[[We]] also show that our novel [[feature]]s add [[significant value]] over and above basic [[keyword]] / [[expression count feature]]s.
[[We]] also show that our [[routing algorithm]] generates significantly faster [[route]]s compared to [[standard baseline]]s, and achieves <i>[[near-optimal performance]]</i> compared to an [[omniscient]] [[routing algorithm]] .
[[We]] also show that [[performance]] improves significantly when combining multiple [[feature]]s in [[supervised learning algorithm]]s, suggesting that the [[retail success]] of a [[business]] may depend on multiple [[factor]]s.
[[We]] also show that [[ranking]] [[news article]]s based on [[trustworthiness]] [[learn]]ed from the [[content-driven framework]] is significantly better than [[baseline]]s that ignore either the [[content quality]] or the [[trust framework]] .
[[We]] also show that [[RSC]] can be used to [[spot outlier]]s and [[detect user]]s with [[non-human behavior]], such as [[bot]]s.
[[We]] also show that [[RWDISK]] returns more desirable ([[high conductance]] and [[small size]]) [[clusters]] than the popular [[clustering algorithm METIS]], while requiring much [[less]] [[memory]] .
[[We]] also show that the [[mined]] [[minimal DNF pattern]]s are very effective when used as [[feature]]s for [[classification]] .
[[We]] also show that the [[ranking function]] may [[diverge]] to [[infinity]] at the [[query point]] in the [[limit]] of [[infinite samples]] .
[[We]] also study the [[empirical characteristic]]s of [[our scheme]]s and provide useful insights on the [[strength]]s and [[weaknesse]]s of each.
[[We]] also study the [[influence]] of three types of [[lexical information]]: [[lemma]]s, [[morphological feature]]s, and [[word cluster]]s.
[[We]] also suggest a [[scalable optimization procedure]], which [[scales linearly]] with the [[data size]] .
[[We]] also summarize the [[ERD workshop]] that followed [[the challenge]], including the [[oral]] and [[poster presentation]]s as well as the [[invited talk]]s.
[[We]] also test [[our algorithm]]s on several [[real web-scale graph]]s, one of which contains 118.14 million [[node]]s and 1.02 billion [[edge]]s, to demonstrate the [[high efficiency]] of [[the proposed algorithm]]s.
[[We]] also test the [[performance]] of [[the three method]]s on [[binary classification]] using two [[model]]s: a [[Bayes classifier]] and [[kernel ridge regression]] .
[[We]] also use [[distance weighted KNN]], and [[locally weighted averaging]] .
[[We]] also use two [[applications-identity resolution]] and [[structural hole spanner finding]] -- to [[evaluate]] the [[accuracy]] of the [[estimated similariti]]es.
[[We]] also utilize the [[linear model]] to capture [[explicit feature]]s.
[[We]] analyze a number of [[phenomena]] that may contribute to [[rich-get-richer]], including the [[first-mover advantage]], and [[search bias]] towards [[popular video]]s.
[[We]] analyze [[data]] from a [[collaborative search experiment]], and based on these [[data we]] propose three techniques that can enhance the value of [[collaborative search tool]]s using [[personalization]]: <i> [[groupization]], [[smart splitting]], and [[group hit-highlighting]] </i>.
[[We]] analyze [[methods]] for ''[[progressive sampling]]'' - using progressively larger [[samples]] as long as [[model]] [[accuracy]] improves.
[[We]] analyze [[our algorithm]]s in terms of [[optimality]], [[correctness]], and [[complexity]] .
[[We]] analyze [[relative merit]]s and [[pitfall]]s of these [[algorithm]]s through extensive [[experimentation]] on a [[large-scale data set]] and [[baseline them]] against existing ideas from [[color science]] .
[[We]] analyze [[skip-gram with negative-sampling (SGNS)]], a [[word embedding method]] introduced by [[Mikolov et al.]], and show that it is implicitly [[factorizing]] a [[word-context matrix]], whose cells are the [[pointwise mutual information (PMI)]] of the respective [[word]] and [[context pair]]s, shifted by a [[global constant]] .
[[We]] analyze the [[accuracy]] of the proposed [[local low-rank modeling]] .
[[We]] analyze the application of [[ensemble learning]] to [[recommender system]]s on the [[Netflix Prize dataset]] .
[[We]] analyze the [[caption pair]]s and find [[significant difference]]s between the [[funnier]] and [[less-funny caption]]s.
[[We]] analyze the [[low-rank approximation]] of [[graph kernel]]s, which enables the [[online algorithms scale]] to large [[graph]]s.
[[We]] analyze the [[performance]] in [[simulation experiment]]s and on two [[real-world offline dataset]]s.
[[We]] analyze the [[problem of reconstructing documents]] when we only have access to the [[n-gram]]s (for n [[fixed]]) and their [[count]]s from the original [[document]]s.
[[We]] analyze the structure of [[conversations]] in three different [[social datasets]], namely, [[Usenet groups]], [[Yahoo! Groups]], and [[Twitter]] .
[[We]] analyze the [[trade-off]] between the [[sample size]] and [[estimation error]] and [[validate]] [[our algorithm]]s using both [[real data]] obtained from [[online user experiment]]s and [[synthetic data]] .
[[We]] answer [[this question]] by developing a new [[probabilistic framework]] for [[deep learning]] based on a [[Bayesian generative probabilistic model]] that explicitly [[capture]]s [[variation]] due to [[nuisance variable]]s.
[[We]] applied [[CatchSync]] on two [[large]], [[real dataset]]s 1-[[billion-edge]] [[Twitter social graph]] and 3-[[billion-edge]] [[Tencent Weibo social graph]], and several [[synthetic one]]s; [[CatchSync]] consistently outperforms existing [[competitor]]s, both in [[detection accuracy]] by 36% on [[Twitter]] and 20% on [[Tencent Weibo]], as well as in [[speed]] .
[[We]] applied [[our model]] to [[post]]s collected from [[Twitter]] on two [[topic]]s: the [[2010 Brazilian Presidential Election]]s and the 2010 season of [[Brazilian Soccer League]] .
[[We]] applied our novel [[kernel]] in a [[supervised]] and a [[semi-supervised setting]] to [[regression]] and [[classification problem]]s on a [[number]] of [[real-world dataset]]s of [[molecular graph]]s.
[[We]] applied [[the algorithm]] to [[derive]] [[vector]]s for the [[sense]]s in a [[Swedish semantic network]], and [[we]] evaluated their quality extrinsically by using them as features in a [[semantic classification task]] – mapping senses to their corresponding [[FrameNet frame]]s.
[[We]] applied this [[approach]] on four [[classical IE tasks]]: [[named entity recognition]] ([[German]] and [[English]]), [[part-of-speech tagging]] ([[English]]) and one [[gene name recognition]] [[corpus]] .
[[We]] apply an understanding of what [[computer]]s do to study how [[computerization]] alters [[job skill demand]]s.
[[We]] apply [[ConvNets]] to various [[large-scale dataset]]s, including [[ontology classification]], [[sentiment analysis]], and [[text categorization]] .
[[We]] apply [[deep learning]] to the [[problem of discovery and detection of characteristic pattern]]s of [[physiology]] in [[clinical]] [[time series data]] .
[[We]] apply [[our algorithm]] on both [[synthetic dataset]]s and [[real-world dataset]]s ([[TripAdvisor.com]]), and show that [[our algorithm]] can easily achieve five or more [[orders of speedup]] over the [[exhaustive search]] and achieve about 96% of the [[optimal solution]] on [[average]] .
[[We]] apply [[our algorithm]] to characterize the [[large]], [[dense subgraphs of a graph]] showing connections between hosts on [[the World Wide Web]]; [[this graph]] contains over 50M hosts and 11B [[edge]]s, gathered from 2.1B [[web page]]s.
[[We]] apply [[our algorithm]] to identify [[highly heritable trait]]s of [[complex human-behavior disorder]]s including [[opioid]] and [[cocaine]] use [[disorder]]s, and [[highly heritable trait]]s of [[dairy cattle]] that are [[economically important]] .
[[We]] apply [[our approach]] to [[benchmark]] [[movie recommender data set]]s.
[[We]] apply our [[approach]] to [[extract relations]] from [[the New York Times corpus]] and use [[Freebase]] as [[knowledge base]] .
[[We]] apply our approach to the challenging [[political science problem]] of [[modeling]] the [[voting behavior]] of [[demographic group]]s based on [[aggregate]] [[voting data]] .
[[We]] apply [[our framework]] to [[health care data]] of 13, 558 [[diabetic patient]]s and show its benefits by [[efficiently]] [[finding]] useful [[pattern]]s for [[detecting]] and [[diagnosing]] [[adverse medical condition]]s that are associated with [[diabete]]s.
[[We]] apply [[our method]] to [[robot vision data]], [[slot car inertial sensor data]] and [[audio event classification data]], and show that in these [[application]]s, [[embedded HMM]]s exceed the previous [[state-of-the-art]] [[performance]] .
[[We]] apply [[our model, QANTA,]] to a [[dataset of questions]] from a [[trivia competition]] called [[quiz bowl]] .
[[We]] apply our proposed [[streaming algorithm]] to the [[Japanese Twitter stream]] and successfully demonstrate that, compared with other [[online nonnegative matrix factorization method]]s, [[our framework]] both [[track]]s [[real-world event]]s with [[high accuracy]] in terms of the [[perplexity]] and simultaneously eliminates [[irrelevant]] [[topic]]s.
[[We]] apply [[Poisson process function]]s to model [[information diffusion]] outside of the [[Twitter mentions network]] .
[[We]] apply [[RainMon]] to three [[real-world dataset]]s from [[production system]]s and show its [[utility]] in [[discovering]] [[anomalous machine]]s and [[time period]]s.
[[We]] apply recent advances in the [[theory]] and [[algorithm]]s of [[matrix completion]] to [[skew-symmetric matrice]]s.
[[We]] apply [[sub-document classification]] to two different [[problem]]s in [[contextual advertising]] .
[[We]] apply [[temporal ConvNet]]s ([[LeCun et al., 1998]]) to various [[large-scale text understanding task]]s, in which the [[input]]s are [[quantized character]]s and the [[output]]s are [[abstract properties of the text]] .
[[We]] apply the [[method of augmented Lagrangian multiplier]]s to [[optimize]] this [[non-convex rank approximation-based objective function]] and obtain [[closed-form solution]]s for all [[subproblem]]s of [[minimizing]] different [[variables]] [[alternatively]] .
[[We]] apply the model on both [[synthetic data]] and [[DBLP data sets]], and the [[results]] demonstrate importance of [[this concept]], as well as the [[effectiveness]] and [[efficiency]] of [[proposed approach]] .
[[We]] apply the proposed [[algorithm]] to a [[collection]] of [[FDG]]-[[PET]] [[image]]s from 232 [[NC]], [[MCI]], and [[AD]] [[subject]]s.
[[We]] apply the proposed method to [[FDG-PET image]]s of 42 [[AD]] and 67 [[NC subject]]s, and identify the [[effective connectivity model]]s for [[AD]] and [[NC]], respectively.
[[We]] apply the ''[[relational Markov network]]'' [[framework]] of [[Taskar]] ''et al''.
[[We]] apply the tool to a [[corpus]] of [[high school textbook]]s from [[India]] and [[empirically examine]] its effectiveness in helping [[author]]s identify [[sections of textbook]]s that can benefit from [[reorganizing]] the [[material]] presented.
[[We]] apply [[this algorithm]] to [[deep]] or [[recurrent neural network training]], and provide [[numerical evidence]] for its superior [[optimization performance]] .
[[We]] apply [[this algorithm]] to [[numerous samples]] from two [[real-world social network]]s and a [[terrorism network data set]] whose [[node]]s have associated [[geospatial location]]s.
[[We]] apply [[this approach]] to the [[problem of predicting label]]s for [[sentence]]s given [[label]]s for [[review]]s, using a [[convolutional neural network]] to [[infer sentence similarity]] .
[[We]] apply [[this methodology]] to the [[task of extracting rare]] or [[unknown side-effect]]s of [[medical drug]]s --- this being one of the problems where [[large scale]] [[non-expert data]] has the potential to [[complement]] [[expert medical knowledge]] .
[[We]] apply [[this method]] to two new [[relational datasets]], one involving [[university]] [[webpages]], and the other a [[social network]] .
[[We]] apply [[this model]] to completing the [[annotation]]s appropriately for [[partially annotated image]]s in the [[training data]] and then to [[predicting]] the [[annotation]]s appropriately and completely for all the [[unannotated image]]s either in the [[training data]] or in any [[unseen data]] beyond the [[training process]] .
We are also studying how [[robot]] [[appearance]] enhances [[interaction]]s, in particular how [[indicator light]]s may reveal more of a [[robot's inner state]] to [[human]]s.
[[We]] are currently [[experimenting]] with an [[iterative process]]: after [[annotating concept]]s in [[the corpus]], we extract [[sequential pattern]]s, which are used to identify [[instances of known relations]] and to [[discover new types of relations]] .
We are dealing with [[Big Data]] and [[must scale]] on both dimensions: each [[experiment]] typically exposes several million [[user]]s to a [[treatment]], and over 200 [[experiment]]s are running [[concurrently]] .
[[We]] are deeply indebted to [[Debmalya Mandal]], [[Jean Pouget-Abadie]] and [[Yaron Singer]] for bringing to our attention a [[counter-example]] to that [[claim]] .
[[We]] are further presented with a [[pool]] of [[experts ensuremath{X}]], where each [[expert]] has his own [[skillset]] and [[compensation demand]]s.
We are interested in [[organizing]] a [[continuous stream]] of [[sparse and noisy text]]s, known as "[[tweet]]s ", in [[real time]] into an [[ontology]] of hundreds of [[topic]]s with <i>[[measurable]]</i> and <i>[[stringently high precision]]</i>.
[[We]] are interested in the [[problem of tracking]] broad [[topic]]s such as "[[baseball]]" and "[[fashion]] " in [[continuous stream]]s of [[short text]]s, exemplified by [[tweet]]s from the [[microblogging service]] [[Twitter]] .
[[We]] are particularly interested in [[complex reasoning]] about [[entiti]]es and [[relations in text]] and [[large-scale knowledge bases (KBs)]] .
[[We]] argue instead that [[stereotype]]s are captured by two [[dimension]]s ([[warmth and competence]]) and that subjectively [[positive stereotype]]s on one [[dimension]] do not contradict [[prejudice]] but often are [[functionally]] consistent with [[unflattering stereotype]]s on the other [[dimension]] .
[[We]] argue that [[computer capital]] (1) substitutes for [[worker]]s in performing [[cognitive]] and [[manual task]]s that can be accomplished by following [[explicit rule]]s; and (2) complements [[worker]]s in performing [[nonroutine problem-solving]] and [[complex communications task]]s.
[[We]] argue that [[consensus learning]] is an [[NP-hard problem]] and thus propose to solve it by an [[efficient]] [[heuristic method]] .
[[We]] argue that [[dirty data]] can be [[repaired]] and utilized as [[strong support]]s in [[clustering]] .
[[We]] argue that [[penalti]]es are usually introduced into an [[incomplete contract]], [[social]] or [[private]] .
[[We]] argue that the [[impact of application]]s depends primarily on their [[ontology]], and [[secondarily]] on [[smart technology and programming feature]]s.
[[We]] argue that this [[representation]] has drawbacks leading to [[redundancy]] in the [[output]] .
[[We]] [[assess]] [[our method]] empirically over a [[large data collection]] obtained from [[Flickr]], using [[interest groups]] as the initial [[information]] .
[[We]] assess their [[trade-off]]s and effectiveness in both [[qualitative]] and [[quantitative comparison]]s for [[region]]s of [[Shanghai]] and [[Chicago]] .
[[We]] assume that each [[node]] in the [[network]] has [[latent coordinate]]s in the [[visualization space]], and [[diffusion]] is more likely to [[occur]] between [[node]]s that are [[placed close together]] .
[[We]] assume that [[node]]s of different [[profile]]s can [[infect]] one another and we prove that under [[realistic condition]]s, apart from the [[weak profile (great sensitivity)]], the [[stronger profile (low sensitivity)]] will get [[infected]] as well.
[[We]] avoid [[redundancy]] in the result by [[selecting]] only the most [[interesting]], [[non-redundant cluster]]s for the [[output]] .
[[Web appearance]], [[name disambiguation]], [[social network]], [[document clustering]], [[link structure]], [[information bottleneck]] .
[[We]] base the [[evaluation]] on the [[opinion]]s of a panel of [[expert]]s.
[[Web]] [[content analysis]] often has two [[sequential]] and separate [[step]]s : [[Web]] [[Classification]] to [[identify]] the [[target]] [[Web page]]s, and [[Web]] [[Information Extraction]] to [[extract]] the [[metadata]] contained in the [[target]] [[Web page]]s.
[[Web content mining]], [[domain concept mining]], [[definition mining]], [[knowledge compilation]], [[information integration]] .
[[We]] begin by providing an [[overview]] of [[Benford’s law]], and how it applies to [[accounting]] and in particular [[fraud detection]] .
[[We]] begin with a [[formal model]] of the [[setting]], noting that the utility of a [[POI]] may be discounted by (i) the presence of [[competing businesses]] nearby as well as (ii) its [[position]] in the [[set]] of [[establishments ordered]] by [[distance]] from the [[user]] .
[[We]] believe availability of such [[data-driven]] [[multi-touch attribution metric]] and [[model]]s is a [[break-through]] in the [[digital advertising]] [[industry]] .
[[We]] believe it is therefore beneficial to [[integrate]] [[classification]] and [[ranking]] in a [[simultaneous]], [[mutually enhancing]] [[process]], and to this end, propose a novel [[ranking-based iterative classification framework]], called [[RankClass]] .
[[We]] believe our [[finding]]s on the interplay of [[mobility pattern]]s and [[social]] ties offer new [[perspective]]s on not only [[link prediction]] but also [[network dynamics]] .
[[We]] believe that a [[community]] depends not only on the [[group of people]] who [[actively participate]], but also the [[topic]]s they [[communicate]] about or [[collaborate]] on.
[[We]] believe that [[density estimation tree]]s provide a new tool for [[exploratory]] [[data analysis]] with unique [[capabiliti]]es.
[[We]] believe that [[intuitive]] and [[userfriendly interface]]s as well as the judicious application of NLP technology to support, [[not supplant]], [[human judgement]]s can help maintain the [[quality of annotation]]s, make annotation more accessible to [[non-technical user]]s such as [[subject domain expert]]s, and [[improve annotation productivity]], thus reducing both the [[human]] and [[financial cost]] of [[annotation]] .
[[We]] believe that many of the lessons that we [[learned]] are applicable to building [[large-scale]] [[enterprise-level]] [[data-management system]]s in general.
[[We]] believe that [[our framework]] will not only allow [[law enforcement agenci]]es to improve their [[crime reduction]] and [[prevention strategi]]es, but also offers new [[criminological insight]]s into [[criminal link formation]] between [[offender]]s.
[[We]] believe that our [[model]] can be easily generalized to many [[Web]] [[applications]] .
[[We]] believe that our [[technique]]s, especially with [[personalization]], can dramatically reduce [[information overload]] .
[[We]] believe that [[our work]] is a [[significant]] advance in [[routine]]s with [[rigorous theoretical guarantee]]s for [[scalable extraction]] of [[large near-clique]]s from [[network]]s.
[[We]] believe that [[our work]] makes significant contributions to solving [[large-scale]] [[machine learning]] [[problem]]s of [[industrial relevance]] in general.
[[We]] believe the reason why [[rule discovery]] in [[real-valued time series]] has failed thus far is because most [[effort]]s have [[more or less]] [[indiscriminately]] applied the [[idea]]s of <i>[[symbolic stream rule discovery]]</i> to <i>[[real-valued rule discovery]]</i>.
[[We]] [[benchmark PBC]] against existing [[high-precision]] [[document classification algorithm]]s and conclude that it is most useful in <i>[[multilabel classification]]</i>.
[[Web-facing compani]]es, including [[Amazon]], [[eBay]], [[Etsy]], [[Facebook]], [[Google]], [[Groupon]], [[Intuit]], [[LinkedIn]], [[Microsoft]], [[Netflix]], [[Shop Direct]], [[StumbleUpon]], [[Yahoo]], and [[Zynga]] use [[online controlled experiment]]s to guide [[product development]] and accelerate [[innovation]] .
[[web graph]]s, [[social network]]s), where an underlying [[network connect]]s the [[units of the population]] .
[[Web of knowledge]] - [[web of data]] - [[semantic web]] - [[ontology learning]] - [[information extraction]] - [[free-form query specification]]
[[We]] [[bootstrap]] the [[algorithm]] based on the [[language identification]] based on the [[site]] alone, a [[methodology]] suitable for any [[supervised language identification algorithm]] .
[[We]] briefly review [[Hochreiter's (1991) analysis]] of [[this problem]], then address it by introducing a novel, [[efficient]], [[gradient based method]] called [[long short-term memory (LSTM)]] .
[[Web search log]]s have been studied mainly at [[session]] or [[query level]] where [[user]]s may submit several [[queri]]es within one [[task]] and handle several [[task]]s within [[one session]] .
[[Web site owner]]s, from [[small web site]]s to the largest [[properti]]es that include [[Amazon]], [[Facebook]], [[Google]], [[LinkedIn]], [[Microsoft]], and [[Yahoo]], attempt to improve their [[web site]]s, [[optimizing for criteria]] ranging from [[repeat usage]], time on site, to [[revenue]] .
[[Website traffic]] varies through [[time]] in [[consistent]] and [[predictable ways]], with highest [[traffic]] in the middle of the [[day]] .
[[Websumm]] [Mani and Bloedorn, 2000], [[LexPageRank]] ([[Erkan and Radev, 2004]]) and Mihalcea and Tarau [2005] are three such [[system]]s using [[algorithm]]s similar to [[PageRank]] and [[HITS]] to compute [[sentence]] [[importance]] .
[[We]] build [[evolutionary tree]]s in a [[Bayesian online filtering framework]] .
[[We]] build on [[previous work]], where [[model]]s were derived that predicted [[player experience]] based on [[features of level design]] and on [[playing style]]s.
[[We]] build on [[technique]]s from [[combinatorial bandits]] to introduce a new practical [[estimator]] .
[[We]] build on the [[theory]] of [[self-exciting point process]]es to develop a [[statistical model]] that allows us to make [[accurate prediction]]s.
[[We]] build [[social selection model]]s, [[Bipartite Markov Random Field (BiMRF)]], to [[quantitatively evaluate]] the [[prediction]] [[performance]] of those [[feature]] [[factor]]s and their [[relationship]]s.
[[We]] build up this [[dynamic]] perspective by suggesting that [[firm]]s [[learn new skills]] by recombining their current [[capabiliti]]es.
We built various [[views]] basing our selection of [[text features]] on a [[heterarchy]] of [[concepts]] .
[[Web usage mining]] consists of three [[phase]]s, namely <i>[[preprocessing]], [[pattern discovery]] </i>, and <i> [[pattern analysis]] </i>.
[[We]] call our [[method]] the <i>[[Clustering Agreement Process (CAP)]]</i>.
[[We]] call such [[tendenci]]es [[present-biased preference]]s: When considering [[trade-off]]s between two [[future moment]]s, [[present-biased preference]]s give [[stronger]] [[relative weight]] to the earlier moment as it gets closer.
[[We]] call the [[method]] <i>[[One-Sided Convolutional NMF (OSC-NMF)]]</i>.
[[We]] call [[this problem]] [[Online Influence Maximization (OIM)]], since we [[learn influence probabiliti]]es at the same time we run [[influence campaign]]s.
[[We]]call this subset a <i>[[conceptual design]]</i> for the [[annotated collection]] .
[[We]] call “variable” the “raw” [[input variables]] and “[[features]]” variables constructed for the [[input variables]] .
[[We]] can also [[estimate]] the [[human brain]]'s [[communication performance]] in terms of [[TEPS]], and use this to meaningfully compare [[brain]]s to [[computer]]s.
[[We]] can [[co-create]], or [[join into], [[something larger than ourselves]] .
[[We]] can [[join]] [[others]] in pursuit of [[common goal]]s, nested in [[shared tradition]]s and [[common value]]s.
[[We]] cannot [[blindly pursue]] the [[goal of machine intelligence]] without [[pondering its consequence]]s.
[[We]] can obtain a [[numeric score]] for an [[entire sequence]], and then turn this into a [[probability]] for the [[entire sequence]] by [[normalizing]] .
[[We]] can run [[word2vec]] without any changes on the [[reformatted corpus]] to learn embeddings for [[SkipB]]s.
[[We]] can talk about the latest [[development]]s and [[limitation]]s, provide the big picture and [[demystify]] the [[technology]] .
[[We]] can’t [[create]] [[them]] [[on our own]] any more than we can [[create]] a [[language]] [[on our own]] .
[[We]] can [[train our neural network]] using [[batch gradient descent]] .
[[We]] can view the [[constraint]]s as restricting (either directly or [[indirectly]]) the [[search space]] of a [[clustering algorithm]] to just feasible [[clustering]]s.
[[We]] capture the [[intrinsic relatedness]] among different [[task]]s by a [[temporal group Lasso regularizer]] .
[[We]] carry out detailed [[experiment]]s on a [[new data set]] we have created consisting of about 33K [[Wikipedia user]]s (including both a [[black list]] and a [[white list]] of [[editor]]s) and containing 770K [[edit]]s.
[[We]] carry out extensive [[empirical studi]]es on [[real-world dataset]]s and show that [[the proposed model]] achieves [[state-of-the-art]] [[classification result]]s with [[sparsity]], [[interpretability]], and [[exceptional scalability]] .
[[We]] cast the problem as a [[probabilistic inference task]], in which we must [[infer the node]]s, [[edge]]s, and [[node label]]s of a [[hidden graph]], based on [[evidence]] provided by the [[observed network]] .
[[We]] cast [[web object classification problem]] as an [[optimization problem]] on a [[graph]] of [[objects]] and [[tags]] .
[[We]] characterize each [[topic]] in the [[hierarchy]] by an [[integrated ranked list of mixed-length phrase]]s.
[[We]] characterize the [[margin distribution]] by the first - and [[second-order statistics]], i.e., the [[margin]] [[mean]] and [[variance]] .
[[We]] characterize [[user]]s with a [[set of parameter]]s associated with different [[link creation strategi]]es, [[estimated]] by a [[Maximum-Likelihood approach]] .
[[We]] claim that [[sparsification]] is a fundamental [[data-reduction operation]] with many applications, ranging from visualization to [[exploratory]] and [[descriptive]] [[data analysis]] .
[[We]] classify [[predicate]]s into one of three [[possibiliti]]es: [[committed belief]], [[non committed belief]], or [[not applicable]] .
[[We]] close by pointing out that our [[utility-distribution estimation]] strategy can also be applied to convert [[pool-based]] [[active learning technique]]s into [[budget-sensitive]] [[online active learning technique]]s.
[[We]] [[cluster]] the [[queri]]es into [[trending topic]]s and assign the [[topic]]s to their corresponding [[location]] .
[[We]] combine both static features derived from a [[predefined]] [[vocabulary]] by [[domain expert]]s and [[dynamic feature]]s generated from [[dynamic query expansion]] in a [[multi-task feature learning]] [[framework]]; we investigate different [[strategi]]es to balance [[homogeneity]] and diversity between [[static]] and [[dynamic term]]s.
[[We]] combine ideas from [[decision theory]], [[cost-sensitive learning]], and [[online density estimation]] .
[[We]] combine multiple [[word representation]]s based on [[semantic clusters]] extracted from the ([[Brown et al., 1992]]) [[algorithm]] and [[syntactic clusters]] obtained from the [[Berkeley parser]] ([[Petrov et al., 2006]]) in order to improve [[discriminative dependency parsing]] in the [[MST-Parser framework]] [[(McDonald et al., 2005)]] .
[[We]] combine [[natural language processing technique]]s with [[statistical term extractor]]s and [[external]] [[ontological resource]]s.
[[We]] combine the above two [[diversification concept]]s by [[modeling]] [[the latter approach]] as a ([[partition]]) [[matroid constraint]], and study [[diversity maximization problem]]s under [[matroid constraint]]s.
[[We]] combine the [[PageRank algorithm]] and [[random walks]] on [[graph]] to [[derive]] the [[community tree]] from the [[social network]] .
[[We]] compare against the following baselines: [[tf-idf]] represents [[word]]s in a [[word-word matrix capturing co-occurrence counts]] in all [[10-word]] [[context window]]s.
[[We]] compare all these [[technique]]s on [[a unique corpus (TASA)]] and, for [[PMI]] and [[GLSA]], [[we]] also report performance on a [[larger]] [[web-based corpus]] .
[[We]] compare [[CTL]] with several [[baseline approach]]es on [[large publication dataset]]s from different [[domain]]s.
[[We]] compare [[discriminative]] and [[generative learning]] as typified by [[logistic regression]] and [[naive Bayes]] .
[[We]] compared [[ItemRank]] with other [[state-of-the-art ranking technique]]s (in particular the [[algorithm]]s [[described]] in ([[Fouss et al., 2005]]).
[[We]] compare its [[performance]] with that of the [[method]] developed by [[Buckley and LeNir (1985)]], which combines [[cyles]] of [[BFGS step]]s and [[conjugate direction step]]s.
[[We]] compare [[our approach]] with the latest [[social recommendation approach]]es on two [[real dataset]]s, [[Flixter]] and [[Douban]] (both with [[large social network]]s).
[[We]] compare [[our embedding]]s to [[Skip-Gram]] on seven [[standard word similarity task]]s, and evaluate the ability of [[our method]] to [[learn unsupervised lexical entailment]] .
[[We]] compare [[our method]]s on several [[dataset]]s, including a realistic example where [[similariti]]es are [[expensive]] and [[noisy]] .
[[We]] compare our method with the [[state-of-the-art]] [[active learning approach]]es on several [[text classification task]]s: [[sentiment classification]], [[newsgroup classification]] and [[email spam filtering]] .
[[We]] compare the [[accuracy]] obtained using such [[training data]] against manually [[labeled training data]] .
[[We]] compare the effectiveness of five different [[automatic learning algorithm]]s for [[text categorization]] in terms of [[learning speed]], [[realtime classification speed]], and [[classification accuracy]] .
[[We]] compare the [[empirical performance]] of [[our algorithm]]s to several [[sequential]] and [[parallel algorithm]]s for the [[k-median problem]] .
[[We]] compare the [[performance]] of [[our hazard based model]] in [[predicting]] the [[user return time]] and in [[categorizing user]]s into [[bucket]]s based on their [[predicted return time]], against several [[baseline regression]] and [[classification method]]s and find the [[hazard based approach]] to be [[superior]] .
[[We]] compare the [[performance]] of three [[statistical parsing architecture]]s on the [[problem of deriving typed dependency structure]]s for [[French]] .
[[We]] compare this to [[similar approach]]es for [[inference-based attack]]s on other forms of [[anonymized data]] .
[[We]] compare various [[document clustering technique]]s including [[K-means]], [[SVD-based method]] and a [[graph-based approach]] and their performance on [[short text data]] collected from [[Twitter]] .
[[We]] compare with and [[outperform]] several [[state-of-the-art baseline]]s.
[[We]] complement [[our experimental result]]s with a [[bias-variance analysis]] that explains how different [[shape model]]s influence the [[additive model]] .
[[We]] complement these results by performing [[experiment]]s using [[our algorithm]]s.
[[We]] complement these results with a [[lower bound]] on the [[query complexity]] of [[influence estimation]] in this [[model]] .
[[We]] compute embeddings for [[generalized phrase]]s and show in [[experimental evaluation]]s on [[coreference resolution]] and [[paraphrase identification]] that such embeddings perform better than [[word form embedding]]s.
[[We]] [[compute self-influence]] and [[co-influence]] based [[similarity]] based on [[social graph]] and its associated [[activity graph]]s and [[influence graph]]s respectively.
[[We]] compute these [[statistics]] for a [[corpus]] consisting of Gigaword and [[Wikipedia]] .
[[We]] conclude by reflecting on [[interacting factor]]s which motivate [[participation]] in the different [[site]]s
[[We]] conclude that [[communication]] [[patterns]] may prove useful as an additional [[class]] of [[attribute data]], complementing [[demographic]] and [[network data]], for [[user]] [[classification]] and [[outlier detection]] --- a point that we illustrate with an [[interpretable]] [[clustering]] of [[users]] based on their [[inferred model parameters]] .
[[We]] conclude that [[perceptual fluency]] affects [[judgments of truth]] .
[[We]] conclude with a [[brief update]] on the latest [[ALE development]]s.
[[We]] conclude with advice on [[running prediction market]]s, including writing good [[question]]s, [[market duration]], motivating [[trader]]s and [[protecting confidential information]] .
[[We]] conduct a [[large scale]] [[live experiment]] on [[YouTube traffic]], and demonstrate that augmenting [[collaborative filtering]] with [[topical representation]]s significantly improves the [[quality]] of the [[related video suggestion]]s in a [[live setting]], especially for [[categories]] with fresh and [[topically-rich video content]] such as [[news video]]s.
[[We]] conduct an extensive [[experimental evaluation]] of [[the proposed framework]] where [[we]] demonstrate its effectiveness over a number of [[high-dimensional problem]]s from the [[fields of biology]] and [[text-mining]] .
[[We]] conduct an extensive [[set of experiment]]s to examine the [[empirical performance]] of [[the proposed algorithm]]s for a [[large-scale]] challenging [[malicious URL detection task]], in which the encouraging [[result]]s showed that [[the proposed technique]] by querying an extremely [[small-sized labeled data]] (about 0.5% out of 1-million [[instance]]s) can achieve better or highly comparable [[classification performance]] in [[comparison]] to the [[state-of-the-art]] [[cost-insensitive]] and [[cost-sensitive online classification algorithm]]s using a huge amount of [[labeled data]] .
[[We]] conduct an [[online field experiment]] to test two ways of manipulating [[top-N recommendation]]s with the goal of improving [[user experience]]: cycling the [[top-N recommendation]] based on their [[past presentation]] and [[serpentining]] the [[top-N list]] mixing the best [[item]]s into later [[recommendation request]]s.
[[We]] conduct detailed [[experiment]]s on [[synthetic]] and [[real data crawled]] from the [[web]] to [[evaluate]] the [[efficiency]] and [[quality]] of [[our proposed algorithm]]s.
[[We]] conduct detailed [[experiments]] to test [[the]] [[performance]] and [[scalability]] of these [[methods]] .
[[We]] conducted an [[empirical study]] using a variety of [[learning method]]s ([[SVMs]], [[neural nets]], [[k-nearest neighbor]], [[bagged]] and [[boosted trees]], and [[boosted stumps]]) to compare nine [[boolean classification performance metrics]]: [[Accuracy]], [[Lift]], [[F-Score]], [[Area under the ROC Curve]], [[Average Precision]], [[Precision/Recall Break-Even Point]], [[Squared Error]], [[Cross Entropy]], and [[Probability Calibration]] .
[[We]] conducted comprehensive [[experiment]]s on [[real dataset]]s [[crawl]]ed from [[EBSN]]s and [[the results]] demonstrate [[our proposed model]] is [[effective]] and [[outperform]]s several [[alternative method]]s.
[[We]] conducted [[experiment]]s on two [[publicly available dataset]]s, i.e., [[Delicious]] and [[Last.fm]] .
[[We]] conducted fairly extensive [[experiments]] based on different evaluation metrics such as [[misclassification index]], [[F-measure]], [[cluster purity]], and [[Entropy]] on very [[large]] [[article sets]] from [[MEDLINE]], the largest [[biomedical digital library]] in [[biomedicine]] .
[[We]] conducted [[large-scale experiment]]s on both [[synthetic]] and [[real world news article]]s, and show that [[Dirichlet-Hawkes process]]es can recover both meaningful [[topic]]s and [[temporal dynamics]], which leads to better [[predictive performance]] in terms of [[content perplexity]] and [[arrival time]] of future [[document]]s.
[[We]] conduct [[experiments]] based on billions of [[real]] [[navigation trails]] collected by a major [[search engine's]] [[browser toolbar]] .
[[We]] conduct [[experiment]]s on an open [[personalized diversification dataset]] and find that our [[supervised learning strategy]] outperforms [[unsupervised]] [[personalized diversification method]]s as well as other [[plain personalization]] and [[plain diversification method]]s.
[[We]] conduct [[experiment]]s on both [[synthetic dataset]]s and [[real dataset]]s.
[[We]] conduct [[experiment]]s on real [[user rating]]s collected from [[Orbitz]], [[Priceline]] and [[TripAdvisor]] on all the [[hotel]]s in [[Las Vegas and New York City]] .
[[We]] conduct [[experiment]]s on the [[Corel database]] of [[image]]s and captions, assessing performance in terms of [[held-out likelihood]], [[automatic annotation]], and [[text-based image retrieval]] .
[[We]] conduct [[experiment]]s using [[data collected]] for this [[advertisement problem]] along with [[experiment]]s using [[standard dataset]]s.
[[We]] conduct [[experiment]]s using [[real]] and [[simulated data]] to demonstrate the [[scalability]] of [[our method]] and the [[robustness]] of [[explanation]]s.
[[We]] conduct extensive [[experimental evaluation]]s on two [[real CQA data set]]s to demonstrate the [[effectiveness]] and [[efficiency]] of [[our algorithm]]s.
[[We]] conduct extensive [[experiment]]s and the [[experimental result]]s clearly demonstrate [[our Geo-SAGE model]] [[outperform]]s the [[state-of-the-art]] .
[[We]] conduct extensive [[experiment]]s on a [[real-life]] [[data set]] to [[evaluate]] the [[performance]] of individual [[opinion]] [[summarization module]]s as well as the [[quality]] of the produced [[summary]] .
[[We]] conduct extensive [[experiment]]s on four [[dataset]]s, and [[the results]] show that [[the proposed model]] is effective in making [[group recommendation]]s, and [[outperform]]s [[baseline method]]s significantly.
[[We]] conduct extensive [[experiment]]s on one [[synthetic dataset]] and two [[public real-world dataset]]s (i.e. [[DBLP]] and [[NUS-WIDE]]).
[[We]] conduct extensive [[experiments]] on [[social media]] [[data]] (one from a [[real-world]] [[blog]] [[site]] and the other from a [[popular content sharing]] [[site]]).
[[We]] conduct extensive [[experiment]]s to [[evaluate]] [[our model]]s on both [[synthetic]] and [[real world dataset]]s.
[[We]] conduct extensive [[experiment]]s to [[validate]] and compare its [[performance]] against several popular [[l1-minimization solver]]s, including [[interior-point method]], [[Homotopy]], [[FISTA]], [[SESOP-PCD]], [[approximate message passing]], and [[TFOCS]] .
[[We]] conduct extensive [[experiment]]s using [[real-world data]] with [[pattern matching algorithm]]s for, respectively, [[automata]] and [[join tree]]s.
[[We]] conduct [[our experiment]]s on a [[large dataset]] from a [[real system]] by using a [[newly proposed unbiased evaluation methodology]] [17].
[[We]] configure our [[scheme]]s to provide popular [[privacy guarantee]]s while [[resisting attack]]s proposed in recent [[research]], and demonstrate [[experiment]]ally that we gain a clear [[utility]] advantage over the previous [[state of the art]] .
[[We]] consider all [[bigram]]s <math>w_i w_{i+1}</math> as candidates for pairs, where <math>w_i</math> is a [[candidate]] [[value]], and <math>w_{i+1}</math> is a [[candidate]] [[attribute]], a [[reasonable heuristic]] .
[[We]] consider a novel [[problem]] called [[Activity Prediction]], where the goal is to [[predict]] the [[future activity occurrence time]]s from [[sensor data]] .
[[We]] consider a [[search task]] as a [[set]] of [[queri]]es that serve the same [[user information]] need.
[[We]] consider both improved [[feature representation]] and novel [[learning]] formulation to boost the [[annotation]] [[performance]] .
[[We]] consider [[cross validation]] for the [[selection process]], so several [[optimization problem]]s under different [[parameter]]s must be solved.
[[We]] considered three [[prediction regime]]s: (1) [[baseline prediction]], (2) [[dynamic (time-varying) outcome prediction]], and (3) [[retrospective outcome prediction]] .
[[We]] consider [[identifying]] [[users]]' [[heterogenous]] [[preference]]s from millions of [[click]]/[[view events]] and building [[predictive models]] to [[classify]] new [[users]] into [[segments]] of distinct [[behavior]] [[pattern]] .
[[We]] consider [[job role]] and [[specialty prediction]] and pose them as [[supervised classification problem]]s.
[[We]] consider [[morality]] to be the vast realm of [[social action]]s, [[intention]]s, [[emotion]]s, and [[judgment]]s aimed at [[providing benefits]] (and [[preventing damage]]) to [[people]], [[society]], and the [[world beyond the self]] .
[[We]] consider [[problems involving groups of data]], where each [[observation within a group]] is a [[draw]] from a [[mixture model]], and where it is desirable to share [[mixture component]]s between [[group]]s.
[[We]] consider [[random projections]] in conjunction with [[classification]], specifically the [[analysis]] of [[Fisher's Linear Discriminant (FLD)]] [[classifier]] in [[randomly projected data space]]s.
[[We]] consider [[rank-one]] [[binary]] [[matrix approximations]] that [[identify]] the dominant [[patterns]] of the [[data]], while preserving its [[discrete property]] .
[[We]] consider the [[2012]] [[US Presidential election]], and ask: what was the [[probability]] that members of various [[demographic group]]s supported [[Barack Obama]], and how did this vary spatially across [[the country]]?
[[We]] consider the [[characterization]] of [[muscle fatigue]] through [[noninvasive sensing mechanism]] such as [[surface electromyography (SEMG)]] .
[[We]] consider the [[estimation]] of a [[joint [[probability density function]] of a [[d-dimensional random vector X]] and define a piecewise [[constant estimator]] structured as a [[decision tree]] .
[[We]] consider [[the framework]] to use the [[world knowledge]] as [[indirect supervision]] .
[[We]] consider the problem of a [[user]] navigating an [[unfamiliar corpus]] of [[text document]]s where [[document metadata]] is limited or [[unavailable]], the [[domain]] is specialized, and the [[user base]] is small.
[[We]] consider the [[problem of building compact]], [[unsupervised representation]]s of [[large]], [[high-dimensional]], [[non-negative data]] using [[sparse coding]] and [[dictionary learning scheme]]s, with an [[emphasis]] on [[executing the algorithm]] in a [[Map-Reduce environment]] .
[[We]] consider the [[problem of computing low rank approximation]]s of [[matrice]]s.
[[We]] consider the [[problem of data mining]] with [[formal]] [[privacy guarantees]], given a [[data access interface]] based on the [[differential privacy framework]] .
[[We]] consider the [[problem of embedding entiti]]es and [[relationship]]s of [[multi-relational data]] in [[low-dimensional vector space]]s.
[[We]] consider the [[problem of estimating rates]] of [[rare events]] for [[high dimensional]], [[multivariate categorical data]] where several [[dimensions]] are [[hierarchical]] .
[[We]] consider the [[problem of estimating rates of rare event]]s obtained through [[interaction]]s among several [[categorical variable]]s that are [[heavy-tailed]] and [[hierarchical]] .
[[We]] consider the problem of [[finding dense subgraphs]] with specified [[upper]] or [[lower bound]]s on the [[number of vertice]]s.
[[We]] consider the problem of improving [[named entity recognition (NER) system]]s by using [[external dictionaries]] --- more specifically, the problem of extending [[state-of-the-art]] [[NER system]]s by incorporating information about the [[similarity]] of extracted [[entities]] to [[entities]] in an [[external dictionary]] .
[[We]] consider the [[problem of learning]] [[incoherent]] [[sparse]] and [[low-rank patterns]] from [[multiple tasks]] .
[[We]] consider the [[problem of mining activity network]]s to [[identify]] [[interesting event]]s, such as a big concert or a demonstration in a [[city]], or a [[trending keyword]] in a [[user community]] in a [[social network]] .
[[We]] consider the [[problem of modeling annotated data]] --- [[data]] with multiple types where the instance of one [[type]] (such as a [[caption]]) serves as a [[description]] of the other [[type]] (such as an [[image]]).
[[We]] consider the [[problem of offline]], [[pool-based]] [[active semi-supervised learning]] on [[graph]]s.
[[We]] consider the [[problem]] of [[producing recommendation]]s from collective [[user behavior]] while simultaneously providing [[guarantee]]s of [[privacy]] for these [[users]] .
[[We]] consider the [[problem]] of [[simultaneously selecting]] a [[learning algorithm]] and [[setting]] its [[hyperparameter]]s, going beyond previous work that attacks these [[issue]]s separately.
[[We]] consider the [[similarity function]] given by [[optimal]] and [[near-optimal matching]]s with respect to [[Euclidean distance]] of [[the corresponding embeddings of the graphs]] in [[high dimension]]s.
[[We]] consider the [[task]] of [[evaluating]] the [[spread of influence]] in [[large network]]s in the well-studied [[independent cascade model]] .
[[We]] consider the [[unbiasedness property]] in the [[sampling process]], and design [[optimal]] [[instrumental distribution]]s to [[minimize]] the [[variance]] in the [[stochastic process]] .
[[We]] consider three [[hypothese]]s about [[relatedness]] and [[well-being]] including the [[hive hypothesis]], which says [[people]] need to [[lose themselve]]s occasionally by becoming part of an [[emergent]] [[social organism]] in order to reach the [[highest level]]s of [[human flourishing]] .
[[We]] consider two [[approximation method]]s: [[Nystrom]] and an [[algebraic treecode]] developed in [[our group]] .
[[We]] consider two [[attack model]]s: a [[free-range attack model]] that permits [[arbitrary]] [[data corruption]] and a [[restrained attack model]] that anticipates more [[realistic attack]]s that a [[reasonable adversary]] would devise under [[penalti]]es.
[[We]] construct a [[belief graph]] to first [[propagate]] [[predictions]] from [[supervised models]] to the [[unsupervised]], and then negotiate and reach consensus among them.
[[We]] constructed a [[dictionary]] for [[disease]] and [[gene name]]s from six public databases and extracted relation candidates by [[dictionary matching]] .
[[We]] contextualize the papers collected in [[this special issue]] by providing a [[detailed overview]] of previous work on [[collaborative]], [[semi-structured resource]]s.
[[We]] [[contrast]] our [[findings]] to [[previous related work]], and [[discuss]] our [[insights]] .
[[We]] contrast [[TreeMiner]] with a [[pattern matching tree mining algorithm]] ([[PatternMatcher]]).
[[We]] corroborate these [[experimental finding]]s with a [[theoretical construction]] showing that simple [[depth two]] [[neural network]]s already have [[perfect]] [[finite sample]] [[expressivity]] as soon as the [[number of parameter]]s exceeds the [[number of data point]]s as it usually does in practice.
[[We]] created a [[gold standard corpus]] from [[computer science conference proceeding]]s and [[journal article]]s, where [[Claim]] and [[Contribution]] [[sentence]]s are [[manually annotated]] with their respective types using [[LOD URI]]s.
[[We]] deal with both [[implicit]] and [[explicit attribute]]s and formulate both kinds of [[extraction]]s as [[classification problem]]s.
[[We]] deem that not only the [[data points]], but also the [[features]] are [[sampled]] from some [[manifolds]], namely [[data manifold]] and [[feature manifold]] respectively.
[[We]] defend the use of <i>[[Predictive Sequential]]</i> [[methods]] for [[error estimate]] -- the <i>[[prequential error]]</i>.
[[We]] define a [[disease subsystem]] as a [[contiguous set]] of [[ROI]]s that are [[collectively implicated]] in [[AD]] .
[[We]] [[define]] an intuitive [[outlier]] [[factor]] based on the [[principle of the Minimum Description Length]] together with an novel [[algorithm for outlier detection]] .
[[We]] define an [[objective function]] which [[jointly model]]s the [[messages' utility score]]s and their [[entropy]] .
[[We]] define a [[probabilistic generative model]] that represents the [[multi-label nature]] of a [[document]] by indicating that the [[words in a document]] are [[produced by]] a [[mixture]] of [[word distribution]]s, one for each [[topic]] .
[[We]] define a [[random field]] over the [[structure]] of each [[sentence]]'s [[syntactic parse tree]] .
[[We]] define a rather general [[convolutional network architecture]] and describe its application to many well known [[NLP task]]s including [[part-of-speech tagging]], [[chunking]], [[named-entity recognition]], [[learning a language model]] and the task of [[semantic role-labeling]] .
[[We]] [[define]] a [[simple and elegant]] [[notion]] of [[coverage]] and [[formalize]] it as a [[submodular]] [[optimization problem]], for which we can [[efficiently]] [[compute]] a [[near-optimal solution]] .
[[We]] defined over [[100]] [[data feature]]s in seven [[categori]]es based on approximately 5.5 million [[actions per day]] from approximately 5, 500 [[user]]s.
[[We]] define four classes to classify words into: <i>[[unassigned]]</i>, <i>[[attribute]]</i>, <i>[[value]]</i>, or <i>neither</i>.
[[We]] define [[local]] and [[global feature]]s that capture both the [[local relatedness]] and [[global coherence embedded]] in that [[textbook]] .
[[We]] define [[measure]]s that capture the [[characteristic]]s of [[queri]]es, and [[we]] propose a [[method]] for [[generating]] [[workload]]s with the desired [[properti]]es, that is, [[effective]]ly [[evaluating]] and [[comparing]] [[data series summarization]]s and [[index]]es.
We define [[service computing]] (alternatively termed [[service-oriented computing]]) as the discipline that seeks to develop [[computational abstraction]]s, [[architecture]]s, [[technique]]s, and [[tool]]s to support [[service]]s broadly.
[[We]] define the [[confidence measure]] [[GAMMA]] to be <math>\prod_{i=u}^v \gamma_i(s_i)</math>, where <math>u</math> and <math>v</math> are the [[start]] and [[end indice]]s of the [[extracted field]] .
[[We]] define the <i>[[trace complexity]]</i> as the [[number of distinct trace]]s required to achieve high [[fidelity]] in [[reconstructing the topology]] of the [[unobserved network]] or, more generally, some of its [[properti]]es.
[[We]] [[define]] the [[problem of learning]] a [[personalized]] [[coverage function]] by providing an appropriate [[user-interaction]] [[model]] and [[formalizing]] an [[online learning framework]] for this [[task]] .
[[We]] define two [[rule]]s that [[reduce]] [[this graph]], preserving all [[possible reconstruction]]s while at the same time increasing the [[label length}length]] of the [[edge label]]s.
[[We]] demonstrate (1) how to [[reformulate]] the [[EM algorithm]] to be able to [[exchange information]] using [[Async-EM]] and (2) how to exploit the [[special memory]] and [[processor architecture]] of a modern [[GPU]] in order to share [[this information]] among [[thread]]s in an [[optimal way]] .
[[We]] demonstrate a comprehensive set of [[quality control process]]es that allow us [[monitor]] and maintain thousands of distinct [[classification model]]s automatically, and to [[add new model]]s, take on new [[data]], and [[correct poorly-performing model]]s without [[manual intervention]] or [[system disruption]] .
[[We]] [[demonstrate]] and [[evaluate]] [[CPPC]] on [[artificial data]] and on our motivating [[domain]] of [[land cover classification]] .
[[We]] demonstrate an [[EMR mining system]] called [[EMRView]] that enables [[exploration]] of the [[precedence relationship]]s to quickly [[identify]] and [[visualize]] [[partial order information]] [[encoded]] in key [[class]]es of [[patient]]s.
[[We]] demonstrate [[DeepWalk's latent representation]]s on several [[multi-label network classification task]]s for [[social network]]s such as [[BlogCatalog]], [[Flickr]], and [[YouTube]] .
[[We]] demonstrated our approach by [[inferring]] complete [[regulatory network]]s explaining the outcomes of the main [[functional regeneration experiment]]s in the [[planarian]] [[literature]]; By analyzing all [[the datasets]] together, [[our system]] [[inferred]] the first [[systems-biology]] comprehensive [[dynamical model]] explaining [[patterning]] in [[planarian regeneration]] .
[[We]] demonstrate [[EMeralD]]'s improvement via [[experimental evaluation]]s on [[real-world data set]]s obtained from [[Chatroulette.com]] .
[[We]] demonstrate how such a [[design]] in light of [[ML-first principle]]s leads to significant [[performance]] improvements versus [[well-known]] [[implementation]]s of several [[ML program]]s, allowing them to [[run]] in much less time and at considerably [[larger model size]]s, on [[modestly-sized computer cluster]]s.
[[We]] demonstrate how the resulting [[ontology]] may be used to enhance [[Wikipedia]] with improved [[query processing]] and other [[features]] .
[[We]] demonstrate how these powerful [[regional feature]]s can be used in [[within-network]] and [[across-network classification]] and [[de-anonymization task]]s - without relying on [[homophily]], or the availability of [[class label]]s.
[[We]] demonstrate [[instance]]s of [[our framework]] on three popular [[graph kernel]]s, namely [[Graphlet kernel]]s, [[Weisfeiler-Lehman subtree kernel]]s, and [[Shortest-Path graph kernel]]s.
[[We]] demonstrate its use in [[automatic image annotation]], [[automatic region annotation]], and [[text-based image retrieval]] .
[[We]] demonstrate nearly a [[50%]] [[error]] [[reduction]] for [[coreference]] and a [[40%]] [[error]] [[reduction]] for [[schema matching]] .
[[We]] demonstrate on [[real dataset]]s including various [[genre]]s (e.g., [[news article]]s, discussion forum posts, and [[tweet]]s), [[domain]]s (general vs. [[bio-medical domain]]s) and languages (e.g., [[English]], [[Chinese]], [[Arabic]], and even [[low-resource language]]s like [[Hausa]] and [[Yoruba]]) how these [[typed entiti]]es aid in [[knowledge discovery]] and [[management]] .
[[We]] demonstrate, on [[simulation]], [[benchmark]], and [[real world dataset]]s that, in terms of [[accuracy]], [[OT-SpAM]]s outperform [[state-of-the-art]] [[interpretable model]]s and perform competitively with [[kernel SVM]]s, while still providing [[result]]s that are highly [[understandable]] .
[[We]] demonstrate [[our approach]] in [[detecting abnormal]] [[manipulative]] [[trading behaviors]] on [[orderbook-level]] [[stock data]] .
[[We]] demonstrate [[our approach]] [[theoretically]] and [[experimentally]] on both [[simulation data]] and [[real]] [[epidemiological record]]s.
[[We]] demonstrate [[our work]] on the largest [[set]] of [[time series experiment]]s ever attempted.
[[We]] demonstrate several [[synthetic]] as well as [[real-world examples]] of [[heap dumps]] for which [[our approach]] provides more insight into [[the problem]] than [[state-of-the-art tools]] such as [[Eclipse's MAT]] .
[[We]] demonstrate some important [[nugget]]s [[learned]] through [[our approach]] and also outline key [[challenge]]s for future [[research]] based on [[our experience]]s.
[[We]] demonstrate that by combining [[generated]] and existing [[lexica]] we can [[collaboratively]] develop [[rich lexical description]]s of [[ontology entiti]]es.
[[We]] demonstrate that even under [[Differential Privacy]], such [[classifier]]s can be used to [[infer]] [["private" attribute]]s accurately in [[realistic data]] .
[[We]] demonstrate that many [[real network]]s exhibit [[reciprociti]]es surprisingly close to the [[upper bound]], which implies that [[user]]s in these [[social network]]s are in a sense more "[[social]]" than suggested by the [[empirical reciprocity]] alone in that they are more willing to [[reciprocate]], subject to their "[[social capacity]]" [[constraint]]s.
[[We]] demonstrate that [[parameters]] of the [[model]] can be learnt by [[Collapsed Gibbs Sampling]] .
[[We]] demonstrate that [[syntactic feature]]s improve performance in both [[disambiguation task]]s.
[[We]] demonstrate that the [[inference quality prediction]] increases the [[inference quality]] of [[implicit]] [[interests]] by 42.8%, and [[inference quality]] of [[explicit interests]] by up to 101%.
[[We]] demonstrate that the results [[produced]] by [[ClickRank]] for [[web search]] [[ranking]] are highly [[competitive]] with those [[produced]] by other [[approach]]es, yet achieved at better [[scalability]] and substantially lower [[computational cost]]s.
[[We]] demonstrate that the [[word vector]]s capture [[semantic regulariti]]es by using the [[vector offset method]] to answer [[SemEval-2012 Task 2]] [[question]]s.
[[We]] demonstrate that this [[pre-processing]] is beneficial for [[linear]] [[SVM]] [[classifiers]] on a large [[benchmark]] of [[text classification task]]s as well as [[UCI]] [[dataset]]s.
[[We]] demonstrate that [[transformation]] into this new [[data space]] can improve [[classification accuracy]], whilst retaining the [[explanatory power]] provided by [[shapelet]]s.
[[We]] demonstrate the advantages of [[mixed membership stochastic blockmodel]]s with [[application]]s to [[social network]]s and [[protein interaction network]]s.
[[We]] demonstrate the benefits of these [[semantic representation]]s in two [[task]]s: [[entity ranking]] and [[computing document semantic similarity]] .
[[We]] demonstrate the benefits of this [[component]] using [[experiment]]s [[we]] conducted on [[advertising]] at [[LinkedIn]] .
[[We]] demonstrate the [[bootstrapping]] and [[algorithm]] on [[eBay email data]] and on [[Twitter status updates data]] .
[[We]] demonstrate the [[capabiliti]]es of [[our model]] by applying it to a [[real-world]] [[COPD patient cohort]] and deriving some [[interesting clinical insight]]s.
[[We]] demonstrate the capability of [[our proposed model]] on both [[simulated]] and [[patient data]] from a [[publicly available clinical database]] .
[[We]] demonstrate the [[effectiveness]] and [[efficiency]] of [[the proposed algorithm]]s on [[real]] and [[synthetic data set]]s.
[[We]] demonstrate the [[effectiveness]] and [[scalability]] of [[SPEAGLE]] on three [[real-world review dataset]]s from [[Yelp.com]] with [[filtered (spam)]] and [[recommended (non-spam) review]]s, where it significantly [[outperform]]s several [[baseline]]s and [[state-of-the-art method]]s.
[[We]] demonstrate the effectiveness of [[CRR]] for both [[families]] of [[metrics]] on a range of [[large-scale tasks]], including [[click prediction]] for [[online advertisements]] .
[[We]] demonstrate the effectiveness of <i>[[RolX]]</i> on several [[network-mining task]]s: from [[exploratory data analysis]] to [[network transfer learning]] .
[[We]] demonstrate the effectiveness of [[our algorithm]] using actual [[Yahoo!]] [[data]] .
[[We]] demonstrate the effectiveness of [[our approach]] by [[tracing]] [[information cascades]] in a [[set]] of 170 million [[blogs]] and [[news articles]] over a [[one year period]] to [[infer]] how [[information]] [[flows]] through the [[online media space]] .
[[We]] demonstrate the effectiveness of [[our approach]] by validating its [[prediction]]s with previous [[court decision]]s of [[litigated patent]]s.
[[We]] demonstrate the effectiveness of [[our framework]] on [[real world]] [[social network data]] .
[[We]] demonstrate the effectiveness of [[proposed method]] in terms of [[predictive performance]] and [[computational efficiency]] by [[examining]] [[collections]] of [[real documents with timestamps]] .
[[We]] demonstrate the effectiveness of [[the proposed method]] on both [[synthetic data]] and [[real-world problem]]s.
[[We]] demonstrate the [[efficacy]] of [[our approach]]es through [[experiment]]s on [[real datasets of expert]]s, and demonstrate their advantage over [[intuitive baseline]]s.
[[We]] demonstrate the efficacy of [[the proposed approach]] with extensive [[experiment]]s carried out on [[six]] [[datasets]], obtaining clear [[performance gains]] over the [[state-of-the-arts]] .
[[We]] demonstrate the [[efficiency]] of [[our algorithm]] and the need for using [[closed episode]]s [[empirically]] on [[synthetic]] and [[real-world dataset]]s.
[[We]] demonstrate the [[efficiency]] of [[our approach]] by analyzing [[data]] from several [[properti]]es of a major [[Internet portal]] .
[[We]] demonstrate the [[efficiency]] of [[our approach]] on the classic [[benchmark dataset]]s used for these [[problem]]s, and show several [[case studi]]es where [[logical shapelet]]s significantly [[outperform]] the original [[shapelet representation]] and other [[time series classification technique]]s.
[[We]] demonstrate the [[empirical efficacy]] of both [[technique]]s on two [[real-world]] [[hospital data set]]s and show that the resulting [[neural net]]s learn [[interpretable]] and [[clinically relevant]] [[feature]]s.
[[We]] demonstrate the following [[extremely unintuitive fact]]; in [[large dataset]]s we can exactly search under [[DTW]] much more quickly than the current [[state-of-the-art]] <i>[[Euclidean distance]]</i> [[search algorithm]]s.
[[We]] demonstrate their usefulness and [[superior accuracy]] over existing [[work]] on [[synthetic]] and [[real dataset]]s.
[[We]] demonstrate the [[merit]]s of [[our framework]] through [[simulation study]] and the [[analysis]] of [[real financial]] and [[genetic]]s [[data]] .
[[We]] demonstrate the novel application of these [[technique]]s on [[real Twitter dataset]]s related to [[mass protest adoption]] in [[social communiti]]es.
[[We]] demonstrate the [[schema]] and [[ontology matching tool]] [[COMA++]] .
[[We]] demonstrate the utility of [[our algorithm]]s with a variety of [[case studies]] in the [[domains]] of [[robotics]], [[acoustic monitoring]] and [[online compression]] .
[[We]] demonstrate the utility of [[our idea]]s in domains as diverse as [[gesture recognition]], [[robotic]]s, and [[biometric]]s.
[[We]] demonstrate the utility of [[our idea]]s with both [[single]] and [[multidimensional case studi]]es in the [[domain]]s of [[astronomy]], [[speech physiology]], [[medicine]] and [[entomology]] .
[[We]] demonstrate the utility of [[our idea]]s with [[experiment]]s in [[domain]]s as diverse as [[medicine]], [[entomology]], [[wildlife monitoring]], and [[human behavior analyse]]s.
[[We]] demonstrate the [[utility]] of [[RSC]] by showing that it can [[accurately fit]] [[real time-stamp data]] from [[Reddit]] and [[Twitter]] .
[[We]] demonstrate the utility of the proposed [[algorithm]], processing a [[flow]] [[cytometry data set]] containing two extremely [[rare cell population]]s, and [[report result]]s that significantly [[outperform competing technique]]s.
[[We]] demonstrate through the [[experimental results]] using two [[benchmark]] [[data sets]] and a [[simulation]] [[data set]] that [[anomalies]] of a whole [[network]] and [[nodes]] responsible for them can be [[detect]]ed by the proposed [[method]] .
[[We]] denote by rank_{ui} the [[percentile-ranking]] of [[program]] i within the [[ordered list]] of all [[program]]s prepared for [[user]] u.
[[We]] deployed [[the model]] in the [[LinkedIn job recommender system]], and generated [[20%]] to [[40%]] more [[job application]]s for [[job seeker]]s on [[LinkedIn]] .
[[We]] deployed the [[PBC system]] on the [[task]] of <i>[[job title classification]] </i>, as a part of [[LinkedIn]]'s [[data standardization]] effort.
[[We]] deploy the [[predictive model]] to construct a [[fine-scale heat map]] of [[poverty]] and integrate this [[discovered knowledge]] into the [[process]]es of [[GiveDirectly's operation]]s.
[[We]] derive a [[feature evaluation criterion]], named [[gSemi]], to estimate the usefulness of [[subgraph features]] based upon both [[labeled]] and [[unlabeled graphs]] .
[[We]] derive an [[efficient algorithm]] for [[learning the factorization]], [[analyze its complexity]], and provide [[proof of convergence]] .
[[We]] derive an [[optimization problem]] to determine the [[solution]] of [[this game]] and present several [[instance]]s of the [[Stackelberg prediction game]] .
[[We]] derive a novel form of [[upper bound]] for the [[true risk]] in the [[active learning setting]]; by [[minimizing]] this [[upper bound]] [[we]] develop a [[practical]] [[batch mode active learning method]] .
[[We]] derive conditions under which [[our estimator]] is [[unbiased]] --- these conditions are weaker than prior [[heuristic]]s for [[slate evaluation]] --- and [[experimentally demonstrate]] a smaller [[bias]] than [[parametric approach]]es, even when [[these condition]]s are [[violated]] .
[[We]] derive [[efficient]] <i>[[multiplicative update rule]]s <i> for [[OSC-NMF]], and also prove [[theoretical]]ly its [[convergence]] .
[[We]] derive [[efficient]] [[online inference procedures]] based on a [[stochastic EM algorithm]], in which the [[model]] is [[sequentially updated]] using [[newly obtained data]]; this means that [[past data]] are not required to make the [[inference]] .
[[We]] derive novel [[estimator]]s for [[estimating]] <math>L_p</math> distance from [[sampled data]] .
[[We]] derive the [[dynamic sensor selection strategy]] by [[minimizing]] the [[error rate]]s in [[tracking]] and [[predicting sensor readings over time]] .
[[We]] describe a [[completely]] [[automated]] [[large scale]] [[visual recommendation system]] for [[fashion]] .
[[We]] describe a general and robust [[machine learning framework]] for [[large-scale]] [[classification]] of [[social media user]]s according to [[dimensions of interest]] .
[[We]] describe a [[general framework]] for [[generating]] [[variational transformation]]s based on [[convex duality]] .
[[We]] describe [[algorithm]]s for the different [[variant]]s [[we]] consider, and, whenever possible, [[we]] provide [[approximation guarantee]]s with respect to the [[optimal solution]] .
[[We]] describe a [[manually created]] [[benchmark]] for [[OPTM]] and discuss several factors which determine the difficulty of [[the task]] .
[[We]] describe a means of [[extracting]] the <i>[[k best shapelets]]</i> from a [[data set]] in a [[single pass]], and then use these [[shapelet]]s to [[transform data]] by [[calculating the distance]]s from a [[series]] to each [[shapelet]] .
[[We]] describe a [[method]] for the [[automatic acquisition]] of the [[hyponymy lexical relation]] from [[unrestricted text]] .
[[We]] describe a [[method whereby we]] mine [[historic search-engine log]]s to find other [[user]]s performing [[similar task]]s to the current [[user]] and leverage their [[on-task behavior]] to [[identify Web page]]s to promote in the current [[ranking]] .
[[We]] [[describe]] a [[metric]] called [[pSkip]] that aims to [[quantify]] the [[ranking]] [[quality]] by [[estimating]] the [[probability]] of [[user]]s encountering non [[relevant]] results that [[cost]] them the efforts to [[read and skip]] .
[[We]] describe an [[an approach to folksonomy learning]] based on [[relational clustering]], which exploits [[structured metadata]] contained in [[personal hierarchies]] .
[[We]] describe an [[approach for unsupervised learning]] of a [[generic]], [[distributed sentence encoder]] .
[[We]] describe an [[automated system]] for the [[large-scale]] [[monitoring]] of [[Web site]]s that serve as [[online storefront]]s for [[spam-advertised good]]s.
[[We]] describe an [[evaluation framework]] that uses data from [[Wikipedia]] .
[[We]] describe a new [[algorithm]] for [[efficient]] [[training]] an [[unsmoothed error count]] .
[[We]] describe an [[exponential time algorithm]] for finding the [[optimal solution]] to this [[optimization problem]], and a [[polynomial-time]] [[heuristic]] for [[identifying solution]]s that perform well in practice.
[[We]] describe an [[interactive claims prioritization component]] that uses an [[online cost-sensitive learning approach]] ([[more-like-this]]) to make [[the system efficient]] .
[[We]] describe a [[nonlinear generalization]] of [[PCA]] that uses an [[adaptive]], [[multilayer]] [["encoder" network]] ...
[[We]] describe a novel [[method to compute]] [[accurate]] [[dwell time]] based on [[client-side]] and [[server-side logging]] and demonstrate how to [[normalize]] [[dwell time]] across different [[device]]s and [[context]]s.
[[We]] describe a novel [[sampling approach]] that can be used to design [[scalable algorithm]]s with [[provable]] [[performance guarantee]]s.
[[We]] describe a [[pre-trained deep neural network hidden Markov model]] ([[DNN-HMM]]) hybrid architecture that trains [[the DNN]] to produce a [[distribution]] over [[senone]]s (tied triphone states) as its [[output]] .
[[We]] describe a [[probabilistic factor model]] as a [[general principled approach]] to studying these [[exogenous]] and often overwhelming [[phenomena]] .
[[We]] describe a [[real-time]] [[bidding algorithm]] for [[performance-based display ad allocation]] .
[[We]] describe a [[real-world]], [[deployed application]] of [[AL]] to the [[problem of biomedical citation screening]] for [[systematic reviews]] at the [[Tufts Medical Center]]'s [[Evidence-based Practice Center]] .
[[We]] [[describe]] a [[recommender system]] in the [[domain]] of [[grocery shopping]] .
[[We]] describe a single [[convolutional neural network architecture]] that, given a [[sentence]], outputs a host of [[language processing prediction]]s: [[part-of-speech tag]]s, [[chunk]]s, [[named entity tag]]s, [[semantic role]]s, [[semantically similar word]]s and the likelihood that the [[sentence]] makes sense ([[grammatically]] and [[semantically]]) using a [[language model]] .
[[We]] describe a [[streaming]], [[distributed inference algorithm]] which is able to handle [[tens of million]]s of [[user]]s.
[[We]] describe a [[system]] that helps [[reduce these errors]] using [[machine learning technique]]s by [[predicting claim]]s that will need to be [[reworked]], generating [[explanations]] to help the [[auditors]] correct these [[claims]], and [[experiment]] with [[feature selection]], [[concept drift]], and [[active learning]] to [[collect]] [[feedback]] from the [[auditors]] to improve [[over time]] .
[[We]] describe a [[system]] that [[monitors]] [[social]] and [[mainstream media]] to [[determine shifts]] in what [[people]] are [[thinking]] about a [[product]] or [[company]] .
[[We]] describe a way to answer these [[questions]], quickly and accurately, without [[randomized experiments]], [[surveys]], [[focus groups]] or [[expert data analysts]] .
[[We]] describe a web application, <em>GlossExtractor</em>, that receives in input the output of a terminology extraction web application, <em>TermExtractor</em>, or a [[user-provided terminology]], and then [[search]]es several [[repositori]]es ([[on-line glossari]]es, [[web document]]s, [[user-specified]] [[web page]]s) for [[sentence]]s that are [[candidate]] [[definition]]s for each of the [[input]] [[term]]s.
[[We]] describe common architectures for [[experimentation system]]s and analyze their [[advantage]]s and [[disadvantage]]s.
[[We]] describe different [[analyst]]-centric [[tools]] using a [[case study]] from [[real-life engagements]] and demonstrate their [[effectiveness]] .
[[We]] describe [[distributed query execution]] in the presence of [[resharding]], [[query restart]]s upon [[transient failure]]s, [[range extraction]] that drives [[query routing]] and [[index seek]]s, and the improved [[blockwise-columnar storage format]] .
[[We]] describe [[experimental result]]s on several [[large data set]]s which suggest that the [[approach]] is [[effective]] and [[accurate]] in [[quantifying]] how [[diverse]] a [[document]] is relative to other [[document]]s in a [[corpus]] .
[[We]] describe [[experiment]]s on [[Latent Dirichlet Allocation]], achieving [[speed]]s similar to the [[fast]]est reported [[method]]s ([[online Variational Baye]]s) and lower [[cross-validated loss]] than other [[LDA]] [[implementation]]s.
[[We]] describe [[experiments]] on [[real data]] that demonstrate the usefulness of [[our approach]] .
[[We]] describe five related techniques: [[generalized additive models]], [[trees]], [[multivariate adaptive regression splines]], the [[patient rule induction method]], and [[hierarchical mixtures of experts]] .
[[We]] describe here an [[N-gram-based approach]] to [[text categorization]] that is tolerant of [[textual error]]s.
[[We]] describe how to [[train]] a [[two-layer]] [[convolutional Deep Belief Network (DBN)]] on the 1.6 million [[tiny images dataset]] .
[[We]] describe [[ISIS]], a [[high-performance-computing-based application]] to support [[computational epidemiology]] of [[infectious disease]]s.
[[We]] describe [[latent Dirichlet allocation (LDA)]], a [[generative probabilistic model]] for collections of [[discrete data]] such as [[text corpora]] .
[[We]] describe [[method]]s for [[continual prediction]] of [[manufactured product quality prior]] to [[final testing]] .
[[We]] describe new [[algorithms for training]] [[models]], as an alternative to [[maximum-entropy models]] or [[conditional random fields]] ([[CRFs]]).
[[We]] describe [[our experience]], including both the [[strong and weak correlation]]s found between [[prediction]]s and [[real world result]]s.
[[We]] describe [[our framework]], [[problem formulation]], [[evaluation metrics]], and [[experimental results]] on [[claims data]] from a [[large]] [[US]] [[health insurer]] .
[[We]] describe [[our method]] for [[benchmarking]] [[Semantic Web knowledge base system]]s with respect to use in [[large]] [[OWL application]]s.
[[We]] describe [[our work]] on a system capable of [[inferring]] both kinds of [[attribute]]s to [[enhance product databases]] .
[[We]] describe [[our work]] on [[extracting attribute and value pairs]] from [[textual product description]]s.
[[We]] describe six roles for [[predictive analytics]]: new [[theory generation]], [[measurement development]], [[comparison of competing theories]], improvement of existing [[model]]s, [[relevance assessment]], and [[assessment of the predictability of empirical phenomena]] .
[[We]] describe some [[possible approach]]es to this [[task]] and propose an [[alternative procedure]] that our extensive [[experiment]]s on both [[artificial]] and [[real world domain]]s show to have clear advantages.
[[We]] describe specific behaviors that distinguish between [[vandal]]s and [[non-vandal]]s.
[[We]] describe [[technique]]s that are [[well-suited]] for [[this task]] and present a comprehensive [[evaluation]] of their [[relative merit]]s using [[retail sales data]] for [[electronic product]]s.
[[We]] describe the [[challenge]]s that a [[system]] for [[matching]] [[unstructured offer]]s to [[structured product description]]s must address, drawing upon our experience from building such a [[system]] for [[Bing Shopping]] .
[[We]] describe [[the CoNLL-2003 shared task]]: [[language-independent named entity recognition]] .
[[We]] describe the construction and annotation of [[GENETAG]], a [[corpus]] of 20K [[MEDLINE®]] [[sentence]]s for [[gene/protein NER]] .
[[We]] describe the [[design]] and [[implementation]] of a [[domain-specific language]] that enables [[extracting data]] from a [[webpage]] based on its [[structure]], [[visual layout]], and [[linguistic pattern]]s.
[[We]] describe the [[design]], [[implementation]], and [[evaluation]] of [[EMBERS]], an [[automated]], 24 x 7 [[continuous system]] for [[forecasting civil unrest]] across 10 countries of [[Latin America]] using [[open source indicator]]s such as [[tweet]]s, [[news source]]s, [[blog]]s, [[economic indicator]]s, and other [[data source]]s.
[[We]] describe the key [[feature]]s of [[game physics engine]]s and their [[parallel]]s in [[human mental representation]], focusing especially on the [[intuitive physic]]s of [[young infant]]s where the [[hypothesi]]s helps to unify many [[classic]] and otherwise puzzling [[phenomena]], and may provide the basis for a [[computational account]] of how the [[physical knowledge of infants develop]]s.
[[We]] describe the [[mining algorithm]]s and [[architecture]], and also outline some [[compelling user experience]]s that are enabled by the [[index]]es.
[[We]] describe the [[OntoNotes methodology]] and its result, a [[large]] [[multilingual]] [[richly-annotated corpus]] constructed at 90% [[interannotator agreement]] .
[[We]] describe the overall [[environment]] and individual [[modules]] and demonstrate how they can be used for the [[generation of a sample]], [[complex]] [[customer/product database]] with corresponding [[shopping basket data]], including various [[artifacts]] and [[outliers]] .
[[We]] describe the [[practical technique]]s used to [[optimize performance]] in our [[DSGD]] [[implementation]] .
[[We]] describe two major applications of [[our finding]]s: a [[conceptual change]] to [[our search ads auction]] that further increased the importance of [[ads quality]], and a 50% [[reduction]] of the [[ad load]] on [[Google']]s [[mobile search interface]] .
[[We]] describe two new techniques, a [[centroid-based summarizer]], and an [[evaluation scheme]] based on [[sentence]] [[utility]] and [[subsumption]] .
[[We]] design a [[classifier]] using only these <i>N</i> + <i>K</i> [[image]]s.
[[We]] [[design]] and [[evaluate]] a [[number]] of [[methods]] for coping with this [[problem]] and compare the [[performance]] of these [[methods]] .
[[We]] [[design]] and implement a general [[topic-driven framework]] for [[analyzing]] and [[mining]] the [[heterogeneous]] [[patent network]] .
[[We]] [[design]] and implement an [[All-Hazard Disaster Situation Browser (ADSB) system]] that runs on [[Apple's mobile operating system (iOS)]] and [[iPhone]] and [[iPad mobile device]]s.
[[We]] [[design]] and [[implement]] a [[web based]] [[prototype implementation]] of a [[Business Continuity Information Network (BCIN) system]] utilizing the latest advances in [[data mining technologies]] to create a [[user-friendly]], [[Internet-based]], [[information-rich service]] and acting as a vital part of a [[company]]'s [[business continuity process]] .
[[We]] design a novel [[cascade classification approach]], which copes with the severe [[scalability]] and [[accuracy constraint]]s we are [[facing]] .
[[We]] design a [[semi-supervised inference model]] utilizing existing [[monitoring data]] together with [[heterogeneous city dynamics]], including [[meteorology]], [[human mobility]], [[structure of road network]]s, and [[point of interests (POIs)]] .
[[We]] design a [[space efficient algorithm]] that approximates the [[transitivity]] ([[global clustering coefficient]]) and [[total triangle count]] with only a [[single pass through a graph]] given as a [[stream of edge]]s.
[[We]] designed and implemented a highly [[scalable]] and [[efficient]] [[solution]] to [[BT]] using [[Hadoop MapReduce]] [[framework]] .
[[We]] [[designed]] and [[implement]]ed an [[instance]] of [[GBASE]] using [[MapReduce]] / [[Hadoop]] .
[[We]] designed a new [[model evaluation paradigm]] that [[simulate]]s the [[online behavior]] of [[predictive model]]s.
[[We]] design our [[model]] to correctly address three major challenges: [[multiview]] [[representation of text]] and [[image]]s, [[modeling of competitiveness]] of multiple [[brand]]s over [[shared topic]]s, and [[track]]ing their [[temporal evolution]] .
[[We]] design [[the algorithm]] for [[rank-2 NMF]] by exploiting the [[fact]] that an [[exhaustive search]] for the [[optimal]] [[active set]] can be performed extremely fast when solving these [[NNLS problem]]s.
[[We]] design two [[heterogeneous network]]s, each of which [[encode]]s either [[supervision]] or [[unsupervised data structure]] respectively.
[[We]] detailedly investigate the relationship between [[optimal solution]]s of [[logistic regression]] / [[linear SVM]] and [[regularization parameter]]s.
[[We]] detail our [[implementation]]s and [[optimization]]s that lead to the ability to [[factor matrice]]s with more than [[100]] million [[row]]s and billions of [[non-zero entri]]es in just a few [[hour]]s on a small [[commodity cluster]] .
[[We]] develop [[0-bit]] [[consistent weighted sampling (CWS)]] for [[efficiently]] [[estimating min-max kernel]], which is a [[generalization]] of the [[resemblance kernel]] originally designed for [[binary data]] .
[[We]] develop a [[Bayesian "sum-of-trees" model]] where each [[tree]] is constrained by a [[regularization]] prior to be a [[weak learner]], and [[fitting]] and [[inference]] are accomplished via an [[iterative Bayesian backfitting MCMC algorithm]] that [[generates sample]]s from a [[posterior]] .
[[We]] develop a [[framework]] for [[tracking]] [[short]], [[distinctive]] [[phrases]] that travel relatively intact through [[on-line text]]; developing [[scalable algorithm]]s for [[clustering]] [[textual]] [[variants]] of such [[phrase]]s, [[we]] [[identify]] a broad [[class]] of [[memes]] that exhibit wide [[spread]] and rich [[variation]] on a [[daily basis]] .
[[We]] develop a [[framework]] for understanding how [[dependence]] affects [[uncertainty]] in [[user-item experiment]]s and evaluate how [[bootstrap method]]s that account for differing [[level]]s of [[dependence]] perform in [[practice]] .
[[We]] develop a fully [[Bayesian]], [[generative model]] that describes [[this interaction]], while allowing us to avoid the [[pitfall]]s associated with having [[positive-only data]] .
[[We]] develop a general, [[quantitative method]] to assess [[lexical]], [[prosodic]], and [[speech / pause pattern]]s related to the two [[approach]]es and their impact on [[collective performance]] in a [[corpus of task-oriented conversation]]s.
[[We]] develop a [[graph-based]] <i>[[transductive multi-label classifier(TMC)]]</i> that is evaluated on a [[composite kernel]], and also propose a [[method]] for [[data integration]] using the [[ensemble framework]], called <i>[[transductive multi-label ensemble classifier(TMEC)]] </i>.
[[We]] develop a [[learning principle]] and an [[efficient algorithm]] for [[batch learning]] from [[logged bandit feedback]] .
[[We]] develop [[algorithm]]s that, given any [[user-supplied]] arbitrary [[likelihood function]], conduct a [[likelihood ratio hypothesis test]] ([[LRT]]) over each [[rectangular]] [[region]] in the [[grid]], [[rank]] all of the [[rectangles]] based on the [[computed]] [[LRT statistics]], and [[return]] the top few most interesting [[rectangles]] .
[[We]] develop a [[method]] to identify how a [[piece of text]] [[evolve]]s as it moves through an [[underlying network]] and how [[substring information]] can be used to narrow down where in the [[evolutionary process]] a particular [[observation]] at a [[node li]]es.
[[We]] develop an [[active learning algorithm]] for [[this problem]] which uses [[information-theoretic technique]]s to choose which [[node]]s to explore.
[[We]] develop an [[algorithm]] based on the [[Alternating Direction Method of Multipliers (ADMM)]] to solve [[this problem]] in a [[distributed and scalable manner]], which allows for [[guaranteed]] [[global convergence]] even on [[large graph]]s.
[[We]] develop an [[analytics algorithm]] that [[estimates housing quality]] and [[density]] in [[patch]]es of [[publicly-available satellite imagery]] by [[learning]] a [[predictive model]] with sieves of [[template matching]] [[result]]s combined with [[color histogram]]s as [[feature]]s.
[[We]] develop and apply the [[Balcan-Blum-Srebro (BBS)]] [[theory of classification]] via [[similarity function]]s (which are not necessarily [[kernel]]s) to the [[problem of graph classification]] .
[[We]] develop and [[evaluate]] a [[data-driven approach]] for [[detecting unusual]] (anomalous) [[patient-management action]]s using past [[patient cases stored]] in an [[electronic health record (EHR) system]] .
[[We]] develop and [[evaluate]] a [[data-driven approach]] for [[detecting unusual (anomalous)]] [[patient-management decision]]s using past [[patient case]]s stored in [[electronic health records (EHRs)]] .
[[We]] develop an [[efficient]] and [[easy-to-implement]] [[coordinate descent learning algorithm]], of which each [[coding substep]] has a [[closed-form solution]] .
[[We]] develop an efficient [[Classification EM algorithm]] to [[infer]] the [[model parameter]]s.
[[We]] develop an [[iterative procedure]] to [[compute]] the [[solution]] .
[[We]] develop a novel [[approach]] utilizing the [[technique]] developed in [[frequent itemset mining]], [[set cover]], and [[max k-cover]] to [[approximate]] the [[minimal biclique set cover problem]] .
[[We]] develop a novel [[message passing-based]], [[blind]], [[approximate Kalman filter]] for [[sparse factor analysis (SPARFA)]] that [[jointly]] traces [[learner]] [[concept knowledge]] [[over time]], [[analyze]]s [[learner]] [[concept knowledge state transition]]s (induced by [[interacting]] with [[learning resource]]s, such as [[textbook section]]s, [[lecture video]]s, etc., or the [[forgetting effect]]), and [[estimate]]s the [[content organization]] and difficulty of the [[question]]s in [[assessment]]s.
[[We]] develop a [[predictive framework]] based on the [[multivariate point process]], which employs a [[stochastic parametric model]] to solve the [[relation]]s between [[image occurrence]] and the [[covariate]]s that influence it, in a [[flexible]], [[scalable]], and [[globally optimal]] way.
[[We]] develop a [[principle]]d, [[utility]]-based [[formulation]] for the [[problem of iteratively updating user profile]]s stored [[client-side]], which relies on [[calibrated prediction]] of future [[user activity]] .
[[We]] develop a [[streaming algorithm]] for a [[sequence]] of [[document-frequency table]]s; [[our algorithm]] enables [[real-time monitoring]] of the [[top-10]] [[topic]]s from approximately 25% of all [[Twitter message]]s, while [[automatically]] [[filtering noisy]] and [[meaningless topic]]s.
[[We]] developed [[PSORTb v.3.0]] with improved [[recall]], higher [[proteome-scale]] [[prediction coverage]], and more refined [[localization sub-categories]] .
[[We]] develop [[efficient algorithm]]s based on the [[concept]] of [[LPS]] .
[[We]] develop [[efficient]] and [[scalable optimization procedure]]s for both [[the proposed method]]s and demonstrate their [[effectiveness]] for [[link prediction]] (on [[real-world network]]s consisting of over 2 million [[node]]s and 90 million [[link]]s) and [[semi-supervised clustering task]]s.
[[We]] develop [[efficient]] and [[scalable technique]]s leveraging the [[MapReduce framework]] to [[discover synonym]]s at [[large scale]] .
[[We]] develop fast [[clustering algorithm]]s with [[constant factor approximation guarantee]]s.
[[We]] develop in [[this article]] the argument that what [[firm]]s do better than [[market]]s is the [[sharing]] and [[transfer of the knowledge]] of [[individual]]s and [[group]]s within an [[organization]] .
[[We]] develop [[NoNClu]]s, a novel [[method]] based on [[non-negative matrix factorization (NMF)]], to [[cluster]] an [[NoN]] .
[[We]] develop [[privacy-preserving algorithm]]s for [[computing]] the [[number and location of SNP]]s that are significantly associated with the [[disease]], the significance of any [[statistical test]] between a given [[SNP]] and the [[disease]], any [[measure of correlation]] between [[SNP]]s, and the [[block structure]] of [[correlation]]s.
[[We]] develop [[robust]], [[MCMC algorithm]]s for [[learning]] [[the models]] under [[the framework]] .
[[We]] develop [[specialized procedure]]s for solving the [[maximum]] [[subgraph variant]]s that are far more [[efficient]] than [[previously proposed inference method]]s for [[MLN]]s that solve [[variant]]s of [[MAX-SAT]] .
[[We]] develop [[technique]]s within [[this framework]] for [[classifying]] in an [[online setting]], for example, for [[classifying]] the [[stream of web page]]s where [[online advertisement]]s are [[being served]] .
[[We]] develop the first efficient [[streaming algorithm]] with [[constant factor]] <math>1/2-ε</math> [[approximation guarantee]] to the [[optimum solution]], requiring only a [[single pass]] through the [[data]], and [[memory]] independent of [[data size]] .
[[We]] develop three [[orthogonal timeline quality criteria]] that an ideal timeline should satisfy: (1) it shows [[event]]s that are <i>[[relevant]]</i> to the [[entity]]; (2) it shows [[event]]s that are <i>[[temporally diverse]] </i>, so [[they distribute]] along the [[time axis]], [[avoiding visual crowding]] and allowing for [[easy user interaction]], such as [[zooming in and out]]; and (3) it shows [[event]]s that are <i>[[content diverse]] </i>, so they contain many different [[types of event]]s (e.g., for an [[actor]], it should show [[movie]]s and [[marriage]]s and [[award]]s, not just [[movi]]es).
[[We]] develop two [[approaches]], [[exact match]] and [[relatedness-match]], to map [[text documents]] to [[Wikipedia concepts]], and further to [[Wikipedia categories]] .
[[We]] develop [[variational methods]] for performing [[approximate inference]] on our [[model]] and demonstrate that our [[model]] can be practically [[deployed]] on [[large]] [[corpora]] such as [[Wikipedia]] .
[[We]] devise a [[method]] for [[labeling a population]] according to [[criteria]] based on a [[postulated set]] of [[shopping behavior]]s specific to a [[descriptive segment]] .
[[We]] devise a [[procedure]], [[called RegularMine]], for [[mining]] a [[set]] of [[regular itemsets]] that is a [[concise representation]] of [[frequent itemsets]] .
[[We]] devise efficient [[filtering algorithm]]s and develop effective [[pruning technique]]s to improve [[filtering efficiency]] .
[[We]] devise two [[algorithms]], [[Guardian Decomposition (GD)]] and [[Utility-aware Decomposition (UAD)]], for decomposing a [[microdata table]] into [[GNF]], and present extensive [[experiment]]s over [[real-world datasets]] to demonstrate the [[effectiveness]] of both [[algorithms]] .
[[We]] devise two novel [[method]]s to [[jointly learn]] the [[classifier]] and the [[clustering]] using [[alternating optimization]] and [[collapsed inference]], respectively.
[[We]] discover [[patterns of homophily]], the [[tendency of user]]s to connect with [[user]]s with similar [[social role]]s and [[statuse]]s.
[[We]] discover several [[interesting]] [[social strategi]]es that [[mobile user]]s frequently use to maintain their [[social connection]]s.
[[We]] discover that [[singleton review]] is a significant source of [[spam review]]s and largely affects the [[rating]]s of [[online store]]s.
[[We]] discuss approaches for [[training]] and [[testing]] in [[this scenario]] and introduce new [[metrics for evaluating individual examples]], [[class recall]] and [[precision]], and [[overall accuracy]] .
[[We]] discuss a successful [[application]] of [[our approach]] that [[mines]] the [[published literature]] to identify new [[protein kinase]]s that [[phosphorylate]] the [[protein tumor suppressor p53]] .
[[We]] discuss differences between [[generative]] and [[discriminative modeling]], [[latent-variable conditional models]], and practical aspects of [[CRF implementation]]s.
[[We]] discuss how the [[techniques presented herein]] can be [[deployed]] to help a [[mobile app market operator]] such as [[Google]] as well as individual [[app developer]]s and [[end-user]]s.
[[We]] discuss how this is embedded in the literature on [[dynamic pricing]] in general, but do not review all relevant [[research topic]]s associated with [[dynamic pricing]]; for this we refer to [[Bitran and Caldentey (2003)]], [[Elmaghraby and Keskinocak (2003]]), [[Talluri]] and van [[Ryzin (2004)]], [[Phillips (2005)]], [[Heching]] and [[Leung (2005]]), [[G¨onsch et al. (2009)]], [[Rao (2009)]], [[Chenavaz et al. (2011]]), [[Deksnyte]] and [[Lydeka (2012]]) and ¨Ozer and [[Phillips (2012]]).
[[We]] discuss our [[results]] within the context of the [[service]]s needed to design a [[marketing campaign]] for a [[small business]] .
[[We]] discuss recent [[evolutionary thinking]] about [[multilevel selection]], which offers a [[distal reason]] why the [[hive hypothesis]] might be [[true]] .
[[We]] discuss relations with methodologically related [[research area]]s, and [[identify direction]]s for future [[research]] .
[[We]] discuss several [[case studi]]es of [[real-world annotation project]]s using [[pre-release]] versions of [[BRAT]] and present an evaluation of [[annotation assisted]] by [[semantic class]] disambiguation on a [[multicategory entity mention annotation task]], showing a 15% [[decrease]] in [[total]] [[annotation time]] .
[[We]] discuss some of the issues in [[designing]] and [[interpreting]] [[A/B test]]s.
[[We]] discuss the [[application domain]], [[novel algorithms]], and also discuss [[results]] on [[real-world data sets]] .
[[We]] discuss the [[estimation method]]s and issues that arise when constructing [[top income share series]], including [[income]] [[definition]] and [[comparability]] over [[time]] and across countries, tax avoidance, and tax evasion.
[[We]] discuss the [[general notion of context]] and how it can be [[modeled]] in [[recommender system]]s.
[[We]] discuss the [[problem of parsing natural language commands]] to [[action]]s and [[control structure]]s that can be [[readily implemented]] in a [[robot execution system]] .
[[We]] discuss these [[extension]]s in detail, and [[we]] describe a [[case study]] in the field of [[medical emergency system]]s.
[[We]] discuss the [[trade-off]]s, [[design choice]]s and [[challenge]]s in applying such techniques in a [[real-world deployment]] .
[[We]] discuss tradeoffs for [[HRI research]] of using [[collocated robot]]s, [[remote robot]]s, and [[computer agent]]s as proxies of [[robot]]s.
[[We]] discuss why [[negative experiment]]s, which degrade the [[user experience]] [[short term]], should be run, given the [[learning value]] and [[long-term benefit]]s.
[[We]] distinguish three [[factor]]s: [[social]], [[external]] and [[intrinsic influence]] which can explain the [[emergence]] of every [[specific event]] .
[[We]] do not dive into specific details associated with particular application area such as [[pricing]] in [[queueing]] or [[telecommunication environment]]s ([[Mutlu et al., 2012]]), [[road pricing]] (de [[Palma et al., 2006]], [[Tsekeris and Voß, 2009]], [[Lou, 2013]]), or [[electricity pricing]] ([[Garcia et al., 2005]], [[Roozbehani et al., 2010]]), to name a few.
[[We]] do this by: i) forming a [[representation]] of [[team behavior]] by chunking the [[incoming spatiotemporal signal]] into a [[series]] of [[quantized bin]]s, and ii) generate an [[expectation model]] of [[team behavior]] based on a [[code-book of past performance]]s.
[[We]] do this by [[mining]] the [[search query log]]s to [[detect trending local topic]]s.
[[We]] elaborate on [[rule-based]] and [[statistical method]]s for [[entity]] and [[relationship extraction]] .
[[We]] emphasize two distinctions: Do [[activiti]]es involve [[immediate cost]]s or [[immediate reward]]s, and are [[people sophisticated]] or [[naive]] about future [[self-control problem]]s?
[[We]] empirically compare [[our approach]] with several state-of-the-art [[pattern summarization approach]]es along the [[axes]] of [[storage cost]], [[query accuracy]], [[query flexibility]], and [[efficiency]] using [[real data]] from [[Twitter]] .
[[We]] [[empirically demonstrate]] that [[C3]] is superior, in terms of both [[predictive accuracy]] and [[runtime]], to [[state-of-the-art]] [[probabilistic approach]]es on three [[real-world problem]]s.
[[We]] [[empirically demonstrate]] using [[artificial data set]]s that [[our proposed method]] is able to [[detect cluster change]]s significantly more [[accurately]] than an existing [[statistical-test based method]] and [[AIC]] / [[BIC-based method]]s.
[[We]] empirically evaluate [[client-side]] [[keyword profile]]s for [[keyword advertising]] on a [[large-scale dataset]] from a major [[search engine]] .
[[We]] empirically show the influence of [[pre-training]] with respect to [[architecture depth]], [[model capacity]], and [[database record count]] [[training example]]s.
[[We]] [[empirically verified]] [[our theory]] by designing two [[proof-of-concept]] [[multi-view learning algorithm]]s, one based on [[active view sensing]] and the other based on [[online co-regularization]], with [[real-world data set]]s.
[[We]] empirically [[verify]] the [[effectiveness]] and [[efficiency]] of [[our algorithm]] on two [[real world]] [[large dataset]]s, one [[publicly]] available [[dataset of Flickr]] and the other [[MIRFLICKR-Yahoo Answers dataset]] .
[[We]] employ a [[subset]] of [[empty region graph]]s - the [[Î²-skeleton]] - and [[non-linear diffusion]] to define a [[locally-adapted affinity matrix]], which, as [[we]] demonstrate, provides higher quality [[clustering]] than [[conventional approach]]es based on Îº [[nearest neighbor]]s or [[global scale parameter]]s.
We employ [[Maximum Entropy models]] to combine diverse [[lexical]], [[syntactic]] and [[semantic features]] derived from the [[text]] .
[[We]] employ the [[alternating direction method of multipliers (ADMM)]] and difference of [[convex function]]s [[(DC) programming]] to solve [[the proposed formulation]]s.
[[We]] established the following [[definition]] of [[courage]]: [[Courage]] is the [[voluntary willingness to act]], with or without varying [[levels of fear]], [[in response to]] a [[threat]] to [[achieve]] an [[important, perhaps moral, outcome or goal]] .
We establish sufficient conditions for [[convergence]] of [[SSGD]] using [[result]]s from [[stochastic approximation theory]] and [[regenerative process theory]] .
[[We]] [[estimate the rank]] of our [[linear projections]] by taking recourse to [[online model selection]] based on [[optimizing predictive likelihood]] .
[[We]] evaluate a [[large number]] of [[feature set]]s, [[predictive model]]s and [[postprocessing algorithm]]s, and choose a combination for [[deployment]] .
[[We]] [[evaluate]] and [[compare]] [[our method]] using several general purpose and [[biomedical]] [[benchmarks of word pairs]] whose [[similarity]] has been assessed by [[human expert]]s, and several general purpose ([[WordNet)]] and [[biomedical ontologi]]es ([[SNOMED CT]] and [[MeSH]]).
[[We]] evaluate a [[scenario]] where we [[generate for each user]] an [[ordered list of the show]]s, [[sorted]] from the one [[predicted]] to be most [[preferred till]] the [[least preferred one]] .
[[We]] evaluate a variety of [[classifier]]s, and different types of [[attribute]]s and [[similarity labeling scheme]]s, on two [[benchmark]]s derived from [[open-source software]] and [[malware]] respectively.
[[We]] evaluate [[Bidirectional LSTM (BLSTM)]] and several other [[network architecture]]s on the [[benchmark task]] of [[framewise phoneme classification]], using the [[TIMIT database]] .
[[We]] evaluate [[Brown clusters]], [[Collobert and Weston ([[2008]]) embeddings]], and [[HLBL (Mnih & Hinton, 2009) embeddings of words]] on both [[NER]] and [[chunking]] .
We evaluate [[CPI]] on two [[task]]s, [[Semantic Role Labelling]] and [[Joint Entity Resolution]], while plugging in two different [[MAP inference method]]s: the current [[method of choice]] for [[MAP inference in Markov Logic]], [[MaxWalkSAT]], and [[Integer Linear Programming]] .
[[We]] [[evaluate]] [[DivRank]] using [[empirical experiments]] on three different [[networks]] as well as a [[text summarization task]] .
[[We]] evaluated [[our approach]] on common data sets, namely, the [[MUC-6]] and [[MUC-7 coreference corpora]] .
[[We]] evaluated [[our approach]] with extensive [[experiment]]s based on five [[real data source]]s obtained in [[Beijing]] and [[Shanghai]] .
[[We]] evaluated our method using [[large-scale]] and [[real-world dataset]]s, consisting of two [[POI]] [[dataset]]s of [[Beijing (in 2010 and 2011]]) and two 3-month [[GPS trajectory dataset]]s (representing [[human mobility]]) [[generated by]] over 12,000 [[taxicab]]s in [[Beijing in 2010]] and [[2011]] respectively.
[[We]] evaluated our [[proposed method]] using [[simulation experiment]]s.
[[We]] evaluated the [[candidate]] [[translation]]s using a [[reference list]], and found that taking [[discourse type]] into [[account resulted]] in [[candidate]] [[translation]]s of a better [[quality]] even when the [[corpus size]] was reduced by [[half]] .
[[We]] evaluated [[this approac]]h offline on a [[data set]] based on [[log]]s from an ad [[network]] .
[[We]] evaluate [[GBFS]] on several [[real world data set]]s and show that [[it match]]es or [[outperform]]s other [[state of the art feature selection algorithm]]s.
[[We]] evaluate [[GeBM]] by using both [[synthetic]] and [[real brain data]] .
[[We]] evaluate [[GP-BUCB]] and [[GP-AUCB]] on several [[simulated]] and [[real data set]]s.
[[We]] evaluate [[hPAM]], [[hLDA]] and [[LDA]] based on the criteria mentioned earlier.
[[We]] evaluate its effectiveness for the [[problem]] of [[web search]] [[ranking]], showing that it contributes significantly to [[retrieval]] [[performance]] as a novel [[web search]] [[feature]] .
[[We]] evaluate [[LASTA]]'s [[topic assignment system]] on an [[internal labeled corpus]] of 32, 264 [[user-topic label]]s generated from [[real user]]s.
[[We]] evaluate on several [[dataset]]s, and demonstrate substantial improvements over existing [[decision tree]] based [[sequence learning framework]]s such as [[SEARN]] and [[DAgger]] .
[[We]] evaluate [[OQA]] on three [[benchmark]] [[question set]]s and demonstrate that it achieves up to twice the [[precision]] and [[recall]] of a [[state-of-the-art]] [[Open QA system]] .
[[We]] evaluate [[our algorithm]]s by [[experiment]]s on two large [[academic collaboration]] [[graphs]] obtained from the [[online]] [[archival database]] [[arXiv.org]] .
[[We]] evaluate [[our algorithm]]s on [[real-world dataset]]s and demonstrate that they produce significantly more [[accurate]] [[result]]s than [[prior technique]]s while [[guaranteeing differential privacy]] .
[[We]] evaluate [[our approach]]es on [[functional magnetic resonance imaging (fMRI) data]] to address [[question]]s such as: "What common [[cognitive network]] does this [[group of individual]]s have?
[[We]] evaluate [[our approach]] on a [[data set]] consisting of [[fleet data]] .
[[We]] evaluate [[our approach]] on several standard [[dataset]]s where we achieve [[state-of-the art performance]] .
[[We]] evaluate [[our approach]] through both [[human]] and [[automatic experiment]]s, and demonstrate it [[outperform]]s the [[state of the art]] .
[[We]] [[evaluate]] our [[coverage]] and [[personalization algorithm]]s extensively over [[real]] [[blog]] [[data]] .
[[We]] evaluate [[our discriminant low-rank regression method]]s by six [[benchmark dataset]]s.
[[We]] [[evaluate]] [[our learning method]] on [[web-search ranking data sets]] from several [[countries]] .
[[We]] evaluate [[our method]] based on both [[synthetic data]] and [[real-world query log data]] .
[[We]] evaluate our [[method]] based on extensive [[experiment]]s, using [[GPS trajectori]]es [[generated by]] more than 32,000 taxis over a period of two [[month]]s.
[[We]] evaluate our [[method]] based on extensive [[experiment]]s using [[GPS trajectori]]es [[generated by]] over 32,000 taxis in [[Beijing]] over a [[period]] of two [[month]]s.
[[We]] evaluate [[our method]]s across multiple [[domain]]s, using [[publicly available dataset]]s with [[labeled]], [[ground-truth communiti]]es.
[[We]] evaluate [[our method]]s on [[segmenting user navigation sequence]]s from [[Yahoo ! New]]s.
[[We]] evaluate [[our method]]s on [[simulated data]], and also show their applicability to movie [[ratings prediction]] and the [[discovery]] of [[drug-target interaction]]s.
[[We]] evaluate [[our method]] using [[book recommendation data]], including [[offline analysi]]s on 361,349 [[rating]]s and an [[online study]] involving more than 2,100 [[subject]]s.
[[We]] [[evaluate]] [[our method]] using both [[vector]] and [[non-vector data sets]] at a [[large scale]] up to [[1 million]] [[samples]] .
[[We]] evaluate [[our model]] on 3 different tasks: [[author profiling]], [[sentiment classification]], and [[textual entailment]] .
[[We]] evaluate [[our model]] on a [[large-scale dataset]], consisting of [[text stream]]s from both [[Twitter]] and [[news feed]]s from [[Yahoo! New]]s.
[[We]] evaluate [[our model]] on a [[live copy]] [[crawl]]ed from [[IMDb]] .
We [[evaluate]] our [[model]] on a [[real-world data set]] of [[people]] and demonstrate that simultaneously solving these [[tasks]] reduces [[errors]] over a [[cascaded]] or isolated [[approach]] .
[[We]] evaluate our [[model]] on millions of [[tweet]]ed [[news article]]s and [[blog post]]s collected between [[September 2010]] and [[September 2012]], demonstrating that [[our approach]] is [[effective]] .
[[We]] evaluate our [[performance model]]s and [[scalability optimizer]] using a [[state-of-the-art]] [[distributed DNN training framework]] on two [[benchmark application]]s.
[[We]] evaluate [[our proposed algorithm]]s on a variety of [[data set]]s from various [[application]]s, and exhibit up to five orders of [[magnitude improvement]] in [[query time]] over the [[naive search technique]] in some [[case]]s.
[[We]] evaluate [[our result]]s empirically using [[merchant offer]]s collected from a [[search engine]], and measure the [[proximity]] of the [[price history]] [[generated by]] [[our approach]] to the [[true price history]] .
[[We]] evaluate our results using [[coverage]] and [[prediction]] [[metrics]], and compare to [[biological literature]] .
[[We]] evaluate [[our service]] using a [[real-world dataset]] [[generated by]] over 33, 000 [[taxis]] over a [[period]] of 3 months in [[Beijing]] .
[[We]] evaluate [[our technique]]s on three [[real-world data set]]s and compare them against three [[baseline]] [[approach]]es.
[[We]] evaluate [[our treecode]] by comparing it to the [[classical]] [[Nystrom method]] and a [[state-of-the-art]] [[fast approximate Nystrom method]] .
[[We]] evaluate [[Rubik]] on two [[EHR dataset]]s, one of which contains 647, 118 [[record]]s for 7, 744 [[patient]]s from an [[outpatient clinic]], the other of which is a [[public dataset]] containing 1, 018, 614 [[CMS claims record]]s for 472, 645 [[patient]]s.
[[We]] evaluate the ability of [[TMC]] and [[TMEC]] to predict the [[functions of protein]]s by using two [[yeast dataset]]s.
[[We]] evaluate the [[effectiveness]] of [[our model]] on both [[synthetic dataset]]s and [[real traffic dataset]], and the [[experimental result]]s show the superiority of our [[method]] .
[[We]] evaluate the [[LAA algorithm]] with [[real dataset]]s, specifically, the [[IT-Change]] and the [[IT-Solution document set]]s from the [[IBM [[IT service]] environment]] and the [[Symptom-Treatment document set]]s from [[Google Health]] .
[[We]] evaluate [[the method]]s with [[experiment]]s on [[real-world]] [[meteorological data]] that highlight the promise of [[the approach]] .
[[We]] evaluate the [[mKPGM]] [[learning algorithm]] by comparing it to several different [[graph model]]s, including [[KPGM]]s.
[[We]] evaluate the new [[method]] against a standard [[baseline]] for extracting [[genomic variation relations]] from [[biomedical text]] .
[[We]] [[evaluate the performance]] of [[mobility-based clustering]] based on [[real traffic situations]] .
[[We]] evaluate the [[performance]] of our [[recommender system]] on two [[large-scale real data set]]s, [[DoubanEvent]] and [[Foursquare]] .
[[We]] evaluate the performance of [[SVMs]], [[neural nets]], [[logistic regression]], [[naive bayes]], [[memory-based learning]], [[random forests]], [[decision trees]], [[bagged trees]], [[boosted trees]], and [[boosted stumps]] on eleven [[binary classification problem]]s using a variety of [[performance metric]]s: [[accuracy]], [[F-score]], [[Lift]], [[ROC Area]], [[average precision]], [[precision/recall break-even point]], [[squared error]], and [[cross-entropy]] .
[[We]] evaluate the problems and [[algorithm]]s [[experimentally]] on [[real network]]s.
[[We]] [[evaluate]] the [[proposed approach]] using [[Beijing]] [[air quality data]], resulting in clear advantages over a series of [[state-of-the-art]] and [[commonly used method]]s.
[[We]] evaluate [[the proposed framework]] on two different [[genres of dataset]]s: [[disease-gene (DG)]] and [[mobile social network]]s.
[[We]] evaluate [[the proposed method]]s on a [[real-world patent database]] .
[[We]] evaluate [[the proposed model]] on two different [[genre]]s of [[data collection]]s: [[SNS]] and [[Academia]], each consisting of multiple [[heterogeneous social network]]s.
[[We]] evaluate [[the proposed model]] using several [[city-scale mobility dataset]]s including [[location check-in]]s, [[GPS trajectories]] of [[taxi]]s, and [[public transit data]] .
[[We]] evaluate these [[neighborhood communiti]]es on a range of [[graph]]s.
[[We]] evaluate [[this approach]] in the context of following [[route instruction]]s through an [[indoor environment]], and demonstrate that [[our system]] can learn to translate [[English command]]s into [[sequence]]s of [[desired action]]s, while correctly [[capturing]] the [[semantic intent]] of [[statements involving complex control structure]]s.
[[We]] evaluate this [[hypothesis]] using data obtained from [[EHR]]s of 4486 [[post-]][[cardiac]] [[surgical patient]]s and a [[subset of]] 222 [[alert]]s generated from the [[data]] .
[[We]] examine [[data utility]] in terms of two popular [[data analysis task]]s conducted at the [[STM]], namely [[count queri]]es and [[frequent sequential pattern mining]] .
[[We]] examine our [[model]] on two [[synthetic datasets]] as well as a [[real]] [[application]] [[dataset]] for monitoring [[oil-production]] [[equipment]] to capture different stages of the [[system]], and achieve promising results.
[[We]] examine [[self-control problem]]s -- [[modeled]] as [[time-inconsistent]], [[present-biased preference]]s -- in a [[model]] where a [[person must]] do an [[activity]] [[exactly once]] .
[[We]] examine the sensitivity of [[retrieval]] [[performance]] to the [[smoothing parameter]]s and compare several popular [[smoothing method]]s on different [[test collection]] .
[[We]] examine the sensitivity of [[retrieval performance]] to the [[smoothing parameter]]s and compare several [[popular smoothing method]]s on different [[test collection]]s.
[[We]] expect that [[this framework]] will be useful for a number of [[MDS]] [[variants]] that have not yet been [[studied]] .
[[We]] expect [[this work]] will generate interests among [[data mining]] [[practitioner]]s who would like to [[efficiently]] utilize the [[nonlinear information]] of [[non-binary]] and [[nonnegative data]] .
[[We]] [[experimentally]] demonstrate the [[efficiency]] and [[effectiveness]] of these novel [[feature selection framework]]s.
[[We]] [[experimentally evaluate]] [[HoD]] on both [[directed]] and [[undirected real-world graph]]s with up to billions of [[node]]s and [[edge]]s, and [[we]] demonstrate that [[HoD]] significantly [[outperform]]s [[alternative solution]]s in terms of [[query efficiency]] .
[[We]] experimentally evaluate [[our approach]] on four [[real-life network dataset]]s and show that [[our solution]] effectively preserves essential [[network structural properti]]es like [[degree distribution]], [[shortest path length distribution]] and [[influential node]]s.
[[We]] experimentally study our [[theoretical analysis]] and show that [[adaptive subgradient method]]s outperform [[state-of-the-art]], yet [[non-adaptive, subgradient algorithm]]s.
[[We]] [[experimentally test]] [[SITA]] on the ability to retrieve similar [[compound-protein pair]]s / [[substrate-product pair]]s for a [[query]] from [[large database]]s with over 200 million [[compound-protein pair]]s / [[substrate-product pair]]s and show that [[SITA]] performs better than other possible [[approach]]es.
[[We]] experiment with [[crowdsourcing a knowledge base (KB)]] of [[scientist]]s and their [[institution]]s using two [[method]]s: the first [[recruit]]s [[expert]]s who are likely to already know the necessary [[domain knowledge]] (using [[Google Adwords]]); the second employs [[non-expert]]s who are incentivized to look up the information (using [[Amazon Mechanical Turk]]).
[[We]] [[experiment]] with [[the proposed algorithm]]s on [[real dataset]]s from a [[public bicycling system]] and a [[geolocation-enabled social network dataset]] collected from [[twitter]] .
[[We]] experiment with [[two]] [[real-world dataset]]s for cars and [[camera]]s, and a [[synthetic one]] .
[[We]] explain theoretically a curious [[empirical phenomenon]]: "[[Approximating a matrix]] by [[deterministically selecting]] a [[subset]] of its [[column]]s with the corresponding largest leverage [[score]]s results in a [[good low-rank matrix surrogate]] ".
[[We]] explicitly measure [[this difference]] based on all [[candidate]] [[subset]]s of the [[unlabeled data]] and select the best [[subset]] .
[[We]] exploit such [[potentials]] to design efficient [[inference algorithm]]s that are both [[analytically shown]] to have a [[lower complexity]] and [[empirically found]] to be comparable to [[sequential models]] for typical [[extraction tasks]] .
[[We]] exploit the [[linearity]] by using [[low-rank matrix approximation]], and the [[community structure]] by [[graph partitioning]], followed by the [[Sherman-Morrison lemma]] for [[matrix inversion]] .
[[We]] exploit this [[skew]] to design a new [[algorithm]] that uses a [[pre-filtering stage]] that can be implemented in a highly [[efficient manner]] through [[SIMD instruction]]s.
[[We]] exploit [[Wikipedia]] to create a [[massive corpus]] of [[named entity]] [[annotated text]] .
[[We]] explore an extensive [[set of feature]]s and conduct [[experiment]]s on a [[real-world dataset]], concluding that [[CoFM with ranking-based loss function]]s is superior to [[state-of-the-art method]]s and yields interpretable [[latent factor]]s.
[[We]] explore [[application]]s with [[qualitative]] [[case studi]]es of [[tag]]ged [[web page]]s from [[del.icio.us]] and [[PhD dissertation abstract]]s, demonstrating improved [[model interpretability]] over traditional [[topic model]]s.
[[We]] explore a [[real-time]] [[Twitter search application]] where [[tweet]]s are arriving at a [[rate]] of several [[thousands per second]] .
[[We]] explored four fundamental design decisions: [[text chunks representation]], [[inference algorithm]], using [[non-local feature]]s and [[external knowledge]] .
[[We]] explore how we can apply the lessons from the [[NLP]] community to [[KDD]] [[tasks]] .
[[We]] explore new directions with [[forecasting weather]] as a [[data-intensive challenge]] that involves [[inference]]s across [[space]] and [[time]] .
[[We]] explore [[the]] [[performance]] of [[our proposed approach]] on both a [[synthetic dataset]] and several [[realistic]] [[document datasets]] .
[[We]] explore the use of [[Amazon's Mechanical Turk system]], a significantly [[cheaper]] and [[faster method]] for collecting [[annotations]] from a broad base of paid [[non-expert contributors]] over [[the Web]] .
[[We]] explore three [[application]]s, i.e., [[TDT]], [[Retrieval]] and [[Classification]] based on a [[Web news video]] [[dataset]] obtained from a famous [[online video-distributing website]], <i>[[YouKu]] </i>, and [[evaluate our approach]] .
[[We]] explore ways to build [[surgery-specific]] and [[hospital-specific model]]s (the [[target task]]) using [[information]] from other kinds of [[surgeri]]es and other [[hospital]]s ([[source task]]s).
[[We]] explore which [[properti]]es of the [[seed set]] can improve [[performance]], focusing on [[heuristic]]s that one can control in [[practice]] .
[[We]] extend an [[algorithm]] for [[matrix completion]] to handle [[skew-symmetric data]] and use that to [[extract rank]]s for each [[item]] .
[[We]] extend an [[existing algorithm]] for [[BMF]] to use [[MDL]] to identify the best [[Boolean matrix factorization]], [[analyze]] the [[complexity]] of the problem, and perform an extensive [[experimental evaluation]] to study its [[behavior]] .
[[We]] extend [[our algorithm]] to maintain [[running]] [[estimate]]s of the [[true similariti]]es, as well as estimates of their [[accuracy]] .
[[We]] extend [[our analysi]]s to the [[inductive matrix completion problem]], where [[row]]s and [[column]]s of <i>M</i> have associated [[feature]]s.
[[We]] extend the current [[state-of-the-art]] [[tree-based approach]] with these two [[algorithm]]s.
[[We]] extend the [[description logic]] [[EL++]] with [[reflexive role]]s and range restrictions, and show that [[subsumption remains tractable]] if a certain [[syntactic restriction]] is [[adopted]] .
[[We]] extend the efficient [[Cholesky decomposition]] to model [[dependenci]]es in [[heterogeneous data]] and to ensure [[interpretability]] .
[[We]] extensively evaluated the [[performance]] of [[KAURI]] over [[manually annotated tweet corpus]], and the [[experimental result]]s show that [[KAURI]] significantly outperforms the [[baseline method]]s in terms of [[accuracy]], and [[KAURI]] is [[efficient]] and [[scale]]s well to [[tweet stream]] .
[[We]] extensively evaluated the performance of [[MSCMF]] by using both [[synthetic]] and [[real dataset]]s.
[[We]] extensively evaluated the performance of [[our proposed framework]] over both [[manually annotated]] [[real Web lists extracted]] from the [[Web page]]s and two [[public data set]]s, and the [[experimental result]]s show that [[our framework]] significantly [[outperform]]s the [[baseline method]] in terms of [[accuracy]] .
[[We]] extensively evaluate our [[definition]]s and [[method]]s on a number of [[real-world dataset]]s and applications, such as [[influence maximization]] and [[task-driven team formation]] .
[[We]] extensively [[evaluate]] [[our proposed framework]] and demonstrate how to enrich [[user experience]]s in [[object-level search]] using a [[real-world]] [[product search]] [[scenario]]s.
[[We]] extensively evaluate [[the proposed approach]] and compare to the [[state-of-the-art technique]]s on several [[dataset]]s.
[[We]] extensively [[evaluate the proposed method]] in two [[real dataset]]s.
[[We]] [[extract]] [[community's magnet]] features from [[heterogeneous source]]s, i.e., a [[community's standalone feature]]s and its [[dependency feature]]s with other [[communiti]]es.
[[We]] [[extract]] such broad [[latent aspects]] from [[query]] [[reformulations]] found in [[historical search session logs]] .
[[We]] find [[interesting subgroup]]s with [[deviating model]]s on [[dataset]]s from several different [[domain]]s.
[[We]] find interesting tensions between [[opt-out]]s and [[activiti]]es, [[user perceived accuracy]] and [[freshness]] .
[[We]] find our [[method]] successfully captures the [[dynamics]] of [[rating growth]], helping us separate [[social influence bias]] from [[inherent value]]s.
We find [[paths]] of [[true]] [[ground atom]]s in the [[hypergraph]] that are connected via their [[argument]]s.
[[We]] find some [[surprising linear relationship]]s between [[empirical reciprocity]] and the [[bound]] .
[[We]] find that another [[embedding method]], [[NCE]], is [[implicitly]] [[factorizing]] a [[similar matrix]], where each cell is the [[(shifted) log conditional probability]] of a [[word given its context]] .
[[We]] find that applying [[data mining technique]]s to public [[transport data]] has the potential to provide [[traveller]]s with substantial [[saving]]s.
[[We]] find that changes in three [[transition rate categori]]es are of primary importance: (i) that from [[unemployment]] to [[employment]] in [[routine occupation]]s, (ii) that from [[labor force non-participation]] to [[routine employment]], and (iii) that from [[routine employment]] to [[non-participation]] .
[[We]] find that, despite the relative [[randomness]] and [[lesser commitment]] of [[structural]] [[relationship]]s in [[online forum]]s, [[user]]s' [[community]] [[joining]] [[behaviors]] display some strong [[regularities]] .
We find that [[fluctuation]]s in [[relative]] [[supply]] go a long way in explaining trends in the [[wage differential]] between the [[medium-]] and [[low-skilled]], but only weakly predict trends in the [[wage differential]] between the [[high-]] and the [[medium-skilled]] .
[[We]] find that humans experience a combination of [[periodic movement]] that is [[geographically limited]] and seemingly [[random jump]]s [[correlate]]d with their [[social network]]s.
[[We]] find that [[industrial robot]]s increased both [[labor productivity]] and [[value added]] .
[[We]] find that [[our algorithm]] can [[estimate]] the [[3-profile of a graph]] in approximately the same time as [[triangle counting]] .
[[We]] find that the [[diffusion network]] of [[news]] tends to have a [[core-periphery structure]] with a [[small set]] of [[core]] [[media sites]] that [[diffuse information]] to [[the rest of]] [[the Web]] .
[[We]] find that [[the proposed approach]] is not only [[scalable]] but also outperforms [[existing approach]]es by a [[large]] [[margin]] .
We find that the resulting [[best-estimate parameter distributions]] for both [[data sets]] are surprisingly [[similar]], [[indicating]] that at least some [[features]] of [[communication]] dynamics generalize beyond specific [[contexts]] .
[[We]] find that the [[similarity]] between two individuals' [[movement]]s strongly [[correlate]]s with their [[proximity]] in the [[social network]] .
[[We]] find that [[this straightforward model]] can generate [[simple conversation]]s given a [[large]] [[conversational training dataset]] .
[[We]] find that within [[industri]]es, [[occupatio]]ns, and [[education group]]s, [[computerization]] is associated with reduced [[labor input]] of [[routine manual]] and [[routine cognitive task]]s and increased [[labor input]] of [[nonroutine cognitive task]]s.
[[We]] first describe [[CANaLI]], that lets people enter [[Natural Language (NL) question]]s and [[translates them]] into [[SPARQL queri]]es executed on [[DBpedia]] .
[[We]] first describe common [[preprocessing method]]s such as [[sampling]] or [[dimensionality reduction]] .
[[We]] first describe how we acquire [[judgment]]s about the [[humorousness]] of different [[caption]]s.
[[We]] first develop a robust [[sampling-based framework]] to [[systematically]] explore the [[dependenci]]es among all [[attribute]]s and [[subsequently build a dependency graph]] .
[[We]] first introduce [[standard mean field variational inference]], then [[review recent advance]]s focusing on the following aspects: (a) [[scalable VI]], which includes [[stochastic approximation]]s, (b) generic [[VI]], which extends the applicability of [[VI]] to a [[large class of otherwise intractable model]]s, such as [[non-conjugate model]]s, (c) [[accurate VI]], which includes [[variational model]]s beyond the [[mean field approximation]] or with [[atypical divergence]]s, and (d) [[amortized VI]], which implements the [[inference]] over [[local latent variable]]s with [[inference network]]s.
[[We]] first investigate the [[correlation]]s between the [[temporal dependenci]]es and other [[temporal pattern]]s, and then propose a [[generalized framework]] to [[resolve the problem]] .
[[We]] first propose a [[convex formulation]] for [[multi-task metric learning]] by [[modeling]] the [[task relationships]] in the form of a [[task covariance matrix]] .
[[We]] first propose a [[multi-latent space framework]] to [[model]] the [[complex heterogeneity]], which is used as a [[building block]] to [[stack up]] a [[multi-layer structure]] so as to [[learn]] the [[hierarchical multi-latent space]] .
[[We]] first propose an efficient [[partition-based algorithm]] for [[MCE]] that addresses the [[problem of processing large graph]]s with [[limited memory]] .
[[We]] first propose an [[intuitive]] and [[appealing]] [[expected loss minimization objective]], and give an [[efficient]] [[shortcut]] to [[evaluate]] it despite the huge [[space]] of <i>ys </i>.
[[We]] first [[quantitatively analyse]] the [[formation process of such taxonomies]], which leads to [[power law distribution]] as the [[stationary distribution]]s.
[[We]] first select the most suitable [[network]]s inside a [[composite social network]] via a [[hierarchical Bayesian model]], [[parameterized]] for individual [[user]]s, and then build [[topic model]]s for [[user behavior prediction]] using both the [[relationship]]s in the selected [[network]]s and [[related behavior data]] .
[[We]] first show that previous techniques on [[differentially private discovery]] of [[frequent itemsets]] cannot apply in [[mining frequent graph pattern]]s due to the [[inherent complexity]] of [[handling]] [[structural information in graph]]s.
[[We]] focus on [[efficient]] [[approximation (heuristic) method]]s that attempt to alleviate the [[computational problem]] and trade [[off accuracy]] for [[speed]] .
[[We]] focus on [[inferring]] [[category-specific]] [[social trust circle]]s from available [[rating data]] combined with [[social network data]] .
[[We]] focus on [[methods]] that are based on [[inference]] in a <i>[[combinatorial Markov Field]]</i> (or <i>[[Comraf]]</i>, for short) of a simple [[topology]] .
[[We]] focus on studies where [[selling price]] is a [[control variable]]; we only scarcely discuss [[learning]] in [[capacity-based revenue management]] ([[Talluri and van Ryzin, 2004]]) or [[learning]] in [[news-vendor]] / [[inventory control problem]]s.
[[We]] focus on the [[Gibbs MedLDA model]] [27] that is able to [[simultaneously discover]] [[latent structure]]s and make [[accurate prediction]]s.
[[We]] focus on the [[news domain]]: given two [[news articles]], [[our system]] automatically [[finds]] a [[coherent chain]] [[linking]] them together.
[[We]] focus on the [[practical]] and popular [[clustering problem]]s, [[k-center]] and [[k-median]] .
[[We]] focus on three in particular --- [[binary classification]], [[predicting housing price]]s, and [[event detection]] in [[time series data]] --- comparing the [[network lasso]] to [[baseline approach]]es and showing that it is both a [[fast]] and [[accurate method]] of solving [[large optimization problem]]s.
[[We]] focus on [[unsupervised latent variable model]]s, and develop [[L1 minimising factor models]], [[Bayesian variant]]s of "L1", and [[Bayesian model]]s with a stronger [[L0-like sparsity]] induced through [[spike-and-slab distribution]]s.
[[We]] focus on [[user reputation]] in a [[comment rating environment]], where [[user]]s make [[comment]]s about [[content item]]s and [[rate the comment]]s of one another.
[[We]] focus our attention on methods based on [[latent factor]]s, such as [[mixture model]]s, [[probabilistic matrix factorization]], and [[topic model]]s, for [[explicit]] and [[implicit preference data]] .
[[We]] follow up on [[previous work]] evaluating a simple [[metric]] of [[pointwise mutual information]] .
[[We]] [[formalize]] the [[characteristics]] of a [[good]] [[chain]] and provide an [[efficient algorithm]] (with [[theoretical guarantees]]) to connect two [[fixed endpoints]] .
[[We]] formalize the [[data mining process]] as a [[process]] of [[information exchange]], defined by the following key [[component]]s.
[[We]] formalize the notion of [[model selection]] [[consistency]] in [[the proposed setup]] .
[[We]] [[formalize the problem]] of [[event detection]] using two [[graph-theoretic formulation]]s.
[[We]] formalize the [[problem of finding]] a [[set of reformulation]]s of the [[input query]] by [[maximizing]] a [[linear combination]] of [[coverage]] (of the [[original query's answer set]]) and [[diversity]] among the [[specialization]]s.
[[We]] [[formalize]] [[Web]] [[news extraction]] as a [[machine learning problem]] and [[learn]] a [[template-independent]] [[wrapper]] using a very [[small number]] of [[labeled]] [[news pages]] from a single [[site]] .
[[We]] [[formally]] define several major types of [[conformity]] in individual, [[peer]], and [[group level]]s.
[[We]] [[formally show]] that under the [[kernel]]-[[mapping]] [[space]], the difference in [[distributions]] between the two [[domains]] is [[bounded]]; and the [[prediction]] [[error]] of the [[proposed approach]] can also be [[bounded]] .
[[We]] formulate and [[study search algorithm]]s that consider a [[user's prior interaction]]s with a wide variety of [[content]] to [[personalize]] that [[user's current Web search]] .
[[We]] formulate different [[types of context]]s as [[multiple view]]s of the [[partition]] of the [[corpus]] .
[[We]] formulate [[ILI]] case [[count forecasting]] as a [[convex optimization problem]], whose objective balances the [[autoregressive loss]] and [[the model similarity regularization]] [[induce]]d by the [[structure]] of the [[similarity graph]] .
[[We]] formulate [[ranking]] on [[NoN]] as a [[regularized optimization problem]]; propose [[efficient algorithm]]s and provide [[theoretical analysis]], such as [[optimality]], [[convergence]], [[complexity]] and [[equivalence]] .
[[We]] formulate the [[EvoHDP]] as a [[series]] of [[hierarchical Dirichlet processes (HDP)]] by adding [[time dependencies]] to the [[adjacent]] [[epochs]], and propose a [[cascaded Gibbs sampling scheme]] to [[infer the model]] .
[[We]] formulate the [[extraction]] as a [[classification problem]] and use [[Naïve Bayes]] combined with a [[multi-view semi-supervised algorithm (co-EM)]] .
[[We]] formulate the problem as a novel [[combinatorial optimization problem]], and we [[study]] [[it theoretically]] .
[[We]] formulate this as the [[media]] [[scheduling problem]], where we attempt to [[maximize]] [[total]] [[clicks]], given the overall [[traffic]] [[pattern]] and the [[time varying]] [[clickthrough rate]]s of available [[media]] [[content]] .
[[We]] formulate [[this problem]] as one of [[extracting]] [[temporal signatures]] from [[multivariate]] [[time series data]], where the [[signatures]] are composed of [[ordinal comparisons]] between [[time series]] [[components]] .
[[We]] found that [[communiti]]es detected in [[EBSN]]s are more [[cohesive]] than those in other types of [[social network]]s (e.g.
[[We]] found that [[definitional sentence]]s were more likely to be in the [[section]]s of [[Introduction]] and [[Background]] .
[[We]] found that [[GSDMM]] can infer the number of [[cluster]]s automatically with a good balance between the [[completeness]] and [[homogeneity]] of the [[clustering result]]s, and is fast to [[converge]] .
[[We]] found that [[latent topic-derived feature]]s were effective in determining [[patient mortality]] under three [[timelines]]: [[in-hospital]], 30 day [[post-discharge]], and 1 year [[post-discharge mortality]] .
We further advance [[the algorithm]] to [[linear time cost]], where a [[trade-off]] between [[effectiveness]] and [[efficiency]] is [[enabled]] .
[[We]] further demonstrate how proper [[activation scaling]] [[stabilizes the training]] of [[very wide]] [[residual Inception network]]s.
[[We]] further [[design]] an [[efficient]] [[learning algorithm]] to optimize the [[objective function]] .
[[We]] further extend [[the algorithm]] by presenting a [[heuristic]] to [[measure adherence to constraints]], and providing a [[criterion]] for [[determining]] the [[model complexity]] ([[number of classes]]) for [[constraint-based clustering]] .
[[We]] further extend the [[PMF model]] to include an [[adaptive prior]] on the [[model parameter]]s and show how the [[model capacity]] can be [[controlled automatically]] .
[[We]] further identify the [[intensity]] of each [[function]] in different [[location]]s.
[[We]] further organize existing [[testing method]]s and [[extract device]]s to make the [[experiment]]s more effective.
[[We]] further present a [[hierarchical algorithm]] that finds a [[clustering]] satisfying all given [[constraint]]s in [[polynomial time]] .
We further present the common [[characteristics]] of [[distributed]] [[learning algorithm]]s for [[Map-Reduce]] .
[[We]] further propose a [[matrix update algorithm]] which provides a significant [[computational saving]] for [[dynamic update]]s to the [[graph]] .
[[We]] further relax [[this problem]] and propose a highly [[scalable alternating minimization approach]] with which [[we]] can solve [[problem]]s with millions of [[user]]s and millions of [[item]]s in a single [[thread]] .
[[We]] further show that the [[low-rank model]] can be used for other [[analysis task]]s on [[signed network]]s, such as [[user segmentation]] through [[signed graph clustering]], with [[theoretical guarantee]]s.
[[We]] further study to what extent [[users' demographic]]s can be [[inferred]] from their [[mobile communication]]s.
[[We]] give a [[cogent semantic justification]] for turning [[PageRank]] thus on its [[head]] .
[[We]] give a [[heuristic algorithm]] based on [[community detection]] and provide [[experiment]]s on both [[testing]] and [[large scale]] [[collaboration network]]s.
[[We]] give an [[alternate formulation]], <i>[[Differential Identifiability]] </i>, [[parameter]]ized by the [[probability]] of [[individual identification]] .
[[We]] give an approach that first [[identifi]]es [[top-k frequent itemset]]s, then uses them to construct a [[compact]], [[differentially private]] [[FP-tree]] .
[[We]] give a new, [[efficient]] [[Monte Carlo sampling method]] to [[compute]] the [[objective]] and [[gradient]] of this [[approximation]], which can then be used in a [[quasi-Newton optimizer]] like [[LBFGS]] .
We give applications in [[hierarchical clustering]], [[classification]], and [[language translation]] .
[[We]] give [[approximation algorithm]]s for [[efficiency]], where the [[bound of approximation loss]] is [[theoretically proved]] .
[[We]] give examples of [[associations between attributes]] and [[Like]]s and discuss implication]]s for [[online personalization]] and [[privacy]] .
[[We]] give [[experimental result]]s for an [[information retrieval task]]: [[learning the order]] of [[document]]s with respect to an initial [[query]] .
[[We]] give [[experimental result]]s for an [[information retrieval task]]: [[learning]] the order of [[document]]s with respect to an initial [[query]] .
[[We]] give [[formulations]] for the [[trade-off]] between [[local]] [[spot-to-entity]] [[compatibility]] and [[measures]] of [[global]] [[coherence]] between [[entities]] .
[[We]] grounded the design of [[Mastery Grid]]s in [[self-regulated learning]] and [[learning motivation theori]]es, as well as in [[our past work]] in [[social comparison]], [[OLM]], and [[adaptive navigation support]] .
[[We]] handle missing words using a weighting scheme that distinguishes [[missing word]]s from [[observed word]]s yielding robust [[latent vector]]s for [[sentence]]s.
[[We]] [[harness Web table]]s to answer [[queri]]es whose [[target]] is a quantity with [[natural variation]], such as [[net worth of zuckerburg]], [[battery life]] of [[ipad]], [[half life]] of [[plutonium]], and calories in pizza.
[[We]] have achieved [[area]] under the [[ROC curve value]]s of up to 0.979 and [[lift value]]s of 65 on the top 50 [[user-days identified]] on two [[month]]s of [[real data]] .
[[We]] have also developed a [[visual language]] for specifying [[combination]]s of [[feature]]s, [[baseline]]s, [[peer group]]s, [[time period]]s, and [[algorithm]]s to [[detect anomalies suggestive]] of [[instance]]s of [[insider threat behavior]] .
[[We]] have also made a [[demo version]] of our [[system]] [[publicly available]] and have implemented a [[live system]] which allows [[registered user]]s to receive [[recommendation]]s in [[real time]] .
[[We]] have also [[open-sourced]] [[our implementation]] in [[TensorFlow]] .
[[We]] have applied this [[classifier]] to over 20 million [[names]] from a [[large-scale]] news [[corpus]], [[identifying]] interesting [[temporal]] and [[spatial]] [[trend]]s on the [[representation]] of particular [[cultural]]/[[ethnic group]]s.
[[We]] have applied [[TIARA]] to several [[real-world applications]], including [[email summarization]] and [[patient record analysis]] .
[[We]] have built a [[portal]] for disseminating [[blog]]s and [[tweet]]s, amplifying their reach to tens of thousands of [[follower]]s.
[[We]] have conducted a series of [[experiment]]s on a [[synthetic dataset]], a [[distance matrix dataset]] and a [[large]] [[movie rating dataset]] .
[[We]] have conducted [[experiment]]s on two [[real life data set]]s, the [[public domain Epinions.com dataset]] and a much larger [[dataset]] that we have recently [[crawled]] from [[Flixster.com]] .
[[We]] have conducted extensive [[experiment]]s on both [[synthetic]] and [[real data set]]s.
[[We]] have conducted extensive [[experiments]] on two common [[text mining problems]], namely, [[information extraction]] and [[document classification]] to demonstrate the effectiveness of our proposed [[method]] .
[[We]] have conducted extensive [[experiment]]s on two [[real dataset]]s, consisting of [[Foursquare check-in dataset]]s and [[taxi trajectori]]es.
[[We]] have conducted extensive [[experiment]]s using both [[synthetic]] and [[real-world data sets]] .
[[We]] have conducted several [[experiment]]s on very [[high-resolution satellite imagery]], representing four [[unique]] [[geographic]] [[region]]s across the [[world]] .
[[We]] have confirmed [[Adam]]'s conclusions through [[manual experiment]]s.
[[We]] have derived [[efficient]] [[optimization algorithm]]s based on [[coordinate decent]] for the new [[boosting formulation]] and [[theoretically prove]] that it exhibits a natural [[grouping effect]] for [[nearby spatial]] or [[overlapping features]] .
[[We]] have developed a [[method for recommending item]]s that combines [[content]] and [[collaborative data]] under a single [[probabilistic framework]] .
[[We]] have developed and applied multiple [[algorithm]]s for [[anomaly detection]] based on suspected [[scenario]]s of [[malicious insider behavior]], [[indicator]]s of [[unusual activiti]]es, [[high-dimensional statistical pattern]]s, [[temporal sequence]]s, and [[normal graph evolution]] .
[[We]] have developed [[Sesame]], an [[architecture]] for [[efficient storage]] and [[expressive querying]] of [[large quantities]] of [[metadata]] in [[RDF]] and [[RDF Schema]] .
[[We]] have developed the [[Biomedical Discourse Relation Bank (BioDRB)]], in which we have [[annotated]] [[explicit]] and [[implicit]] [[discourse relations]] in 24 [[open-access]] [[full-text]] [[biomedical articles]] from the [[GENIA corpus]] .
[[We]] have developed the [[Biomedical Discourse Relation Bank (BioDRB)]], which contains [[discourse-level annotations]] of [[explicit]] and [[implicit]] [[discourse relations]] and their [[abstract]] [[object arguments]], and the [[senses of discourse relations]] .
[[We]] have [[evaluated]] our [[algorithm]] on [[datasets]] from diverse [[areas]] including [[medicine]], [[anthropology]], [[computer networking]] and [[image processing]] and show that we can find [[interesting]] and [[meaningful]] [[motifs]] in [[datasets]] that are many [[orders of magnitude]] larger than [[anything]] considered before.
[[We]] have evaluated our [[system]] in a [[large dataset]] [[crawled]] from [[Sina Weibo]] .
[[We]] have explored four complementary approaches for [[extracting]] [[gene]] and [[protein]] [[synonym]]s from [[text]], namely the [[unsupervised]], [[partially supervised]], and [[supervised machine-learning technique]]s, and the [[manual knowledge-based approach]] .
[[We]] have gathered extensive [[data]] about one of these [[service]]s, [[Gowalla]], with [[periodic]] [[snapshot]]s to capture its [[temporal evolution]] .
[[We]] have implemented our approach in the [[Windows Azure production environment]] and have validated its [[effectiveness]] in terms of [[localization accuracy]], [[precision]], and time to [[localization]] using known [[network incident]]s over the past three [[month]]s.
[[We]] have implemented [[RDD]]s in a system called [[Spark]], which we [[evaluate]] through a variety of [[user application]]s and [[benchmark]]s.
[[We]] have implemented such [[imperatively defined factor graph]]s in a system we call [[FACTORIE]], a [[software library]] for an [[object-oriented]], [[strongly-typed]], [[functional language]] .
[[We]] have introduced [[BRAT]], an [[intuitive]] and [[userfriendly]] [[web-based annotation tool]] that aims to enhance [[annotator productivity]] by closely integrating NLP technology into the [[annotation process]] .
[[We]] have laid the foundation for a [[protein function]] [[prediction system]] that integrates [[protein information]] from various [[source]]s [[efficiently]] and [[effectively]] .
[[We]] have participated in the [[CoNLL-2005 shared task]] [[closed challenge]] with full [[syntactic information]] .
[[We]] have performed a detailed [[study]] of the [[NASDAQ]], [[NYSE]], and [[AMEX]] [[stock market]]s, over a 43-year [[span]] .
[[We]] have performed comprehensive [[evaluation]]s of [[the proposed model]]s on the application of [[AD]] [[diagnosis]] .
[[We]] have performed comprehensive [[experimental studi]]es of [[the proposed method]]s, and have compared our [[method]]s with the [[state-ofthe-art]] using three [[real-world data set]]s from different [[application domain]]s.
[[We]] have performed extensive [[evaluation]]s using various types of [[data]] at the [[baseline]] from the [[Alzheimer's Disease Neuroimaging Initiative (ADNI) database]] for [[predicting the future]] [[MMSE]] and [[ADAS-Cog score]]s.
[[We]] have seen in Section 1.2 how [[probability theory]] provides us with a [[consistent]] [[mathematical framework]] for [[quantifying]] and [[manipulating uncertainty]] .
[[We]] have shown a [[human-machine system design]] for [[language translation]] benefits both [[human]] users—who produce [[higher-quality translations—]] and [[machine agent]]s, which can refine their [[model]]s given [[rich feedback]] .
[[We]] have shown that our [[SpikeM]] model [[accurately]] and [[succinctly]] describes all the [[pattern]]s of the [[rise-and-fall spike]]s in these [[real dataset]]s.
[[We]] have successfully deployed [[the system]] to three different [[market]]s, and it handles multiple [[vertical]]s in each [[market]] .
[[We]] have validated [[the proposed framework]] on [[Surface Electromyogram signal]]s collected from 8 [[people]] during a [[fatigue-causing repetitive gripping activity]] .
We highlight an [[application]] for [[our approach]] and demonstrate its [[effectiveness]] and robustness in [[extracting knowledge]] from [[real-world data]] .
[[We]] hope that [[our approach]] will help [[non-expert user]]s to more effectively identify [[machine learning algorithm]]s and [[hyperparameter setting]]s appropriate to their [[application]]s, and hence to achieve improved [[performance]] .
We hope to leverage the [[asymmetric measure]]s, [[probabilistic interpretation]], and [[flexible training criteria]] of [[our model]] to tackle tasks involving [[similarity-in-context]], [[comparison of sentence]]s and [[paragraphs]], and more general [[common sense reasoning]] .
We hypothesize several different uses of [[citation sentence]]s (which we call [[citance]]s), including the creation of [[training]] and [[testing data]] for [[semantic analysis]] (especially for entity and [[relation recognition]]), [[synonym set creation]], [[database curation]], [[document summarization]], and [[information retrieval]] generally.
[[We]] [[identify]] and characterize interesting [[network diffusion properti]]es whose [[expectation]]s can be [[computed exactly]] and [[efficiently]], [[either wholly]] or in part.
[[We]] [[identify]] and construct [[multidimensional time series]] based on [[aggregate statistic]]s, in order to [[depict and mine]] such [[correlation]]s.
[[We]] [[identify]] a [[task]] - [[opinion holder bias prediction]] - which is strongly related to the [[sentiment analysis task]]; however, in constrast to [[sentiment analysis]], it builds [[accurate model]]s since the underlying [[relational data]] follows a [[stationary distribution]] .
[[We]] [[identify the importance of modeling]] a [[user's individuality]], and that of exploiting [[opinion]]s of the [[user's friend]]s for [[accurate]] [[activity classification]] .
[[We]] identify their respective [[motivation]]s and distinguish their [[advantage]]s and [[disadvantage]]s in a [[comparative review]] .
[[We]] identify the [[window]] with the [[highest]] [[likelihood ratio]] as our [[anomalous window]], and (c) [[Monte Carlo Simulations]] : to ascertain whether this [[window]] is truly [[anomalous]] and not just a [[random occurrence]] [[we]] perform [[hypothesis testing]] by computing a [[p-value]] using [[Monte Carlo Simulations]] .
[[We]] illustrate the [[benefit]]s of [[TANGENT]] with [[experiment]]s on both [[synthetic]] and [[real data set]]s.
[[We]] illustrate the [[diversity]] of [[the data]] with various [[statistic]]s and provide examples of [[task]]s that benefit from the [[dataset]] .
[[We]] illustrate the effectiveness of [[MA]]s on the [[problem]] of [[portfolio selection]] in the [[stock market]] and use several existing ideas for [[portfolio selection]] as [[base algorithm]]s.
[[We]] illustrate the effectiveness of [[the proposed method]] through extensive [[experiment]]s on [[synthetic data]] as well as on two [[real]] [[stock market dataset]]s, where major [[financial event]]s can be [[visualized]] in [[low dimension]]s.
[[We]] illustrate the [[performance]] of [[our method]] on several [[real-world dataset]]s.
[[We]] illustrate the promise of [[ALE]] by presenting a [[benchmark set]] of [[domain-independent agent]]s designed using [[well-established AI technique]]s for both [[reinforcement learning]] and [[planning]] .
[[We]] illustrate the usefulness of this [[supervision]] on various [[data set]]s and [[application]]s.
[[We]] implement an [[active friending service]] with [[SITINA]] on [[Facebook]] to validate our [[idea]] .
[[We]] implemented a [[semantic tagging approach]] that performs [[off-line reasoning]] .
[[We]] implemented our [[coreset]] and demonstrate it by turning [[Matlab]]'s [[NMF off-line function]] that gets a [[matrix]] in the [[memory]] of a single [[machine]], into a [[streaming algorithm]] that runs in parallel on 64 [[machine]]s on [[Amazon's cloud]] and returns [[sparse NMF factorization]] .
[[We]] implement our [[solution]] to retrieve [[specifications]] for 3000 [[sectors]], representing more than 300,000 [[service providers]] .
[[We]] implement several [[technique]]s to [[estimate the confidence]] of both [[extracted field]]s and entire [[multi-field record]]s, obtaining an [[average precision]] of 98% for retrieving [[correct field]]s and 87% for [[multi-field record]]s
[[We]] implement [[the algorithm]] on a [[vertex-oriented bulk synchronous parallel (BSP) model]] so that the [[mining]] [[load]] can be [[distributed]] on thousands of [[machines]] .
[[We]] implement [[this theory]] using [[conditional random fields (CRF]]s) that synthesize [[textual information]] with [[information]] about previous [[emotional state]]s and the [[emotional states of other]]s.
[[We]] impose no [[sparsity]] or underlying [[low-dimensional structure constraints]] on the [[data]]; we instead take advantage of the [[class structure]] inherent in the [[problem]] .
[[We]] impose the well-known [[group Lasso penalty]] on [[row group]]s of the first [[component]] for [[capturing]] the [[shared feature]]s among relevant [[task]]s.
[[We]] incorporate both the [[structure affinity]] and the [[label vicinity]] into a unified [[classifier]] to speed up the [[classification convergence]] .
[[We]] incorporate the [[quality]] of [[relevant content]] into [[the framework]] and present an [[iterative algorithm]] for [[propagation]] of [[trust score]]s.
[[We]] incorporate [[user feedback]] into [[our framework]], allowing the [[stories]] to be [[refined]] and [[personalized]] .
[[We]] infer the [[function]]s of each [[region]] using a [[topic-based inference model]], which regards a [[region]] as a [[document]], a [[function]] as a [[topic]], [[categori]]es of [[POI]]s (e.g., [[restaurant]]s and [[shopping mall]]s) as [[metadata]] (like [[author]]s, [[affiliation]]s, and [[key word]]s), and [[human mobility pattern]]s (when [[people]] reach / leave a [[region]] and where [[people]] come from and leave for) as [[word]]s.
[[We]] [[instantiate]] [[CAP]] as [[inference]] within a more [[complex]], [[bipartite Comraf]] .
[[We]] instantiate [[our framework]] with an [[efficient]] [[beam search algorithm]], and demonstrate its [[efficiency]] on two [[Big Data Pipeline]]s: [[parsing]] and [[relation extraction]] .
[[We]] instantiate the problem in the context of [[portfolio selection]] and [[evaluate the effectiveness]] of the [[formulation]] through extensive [[experiment]]s on five [[dataset]]s in comparison with [[existing]] [[algorithm]]s and several [[variant]]s.
[[We]] integrate the [[# hashtags]] as weakly [[supervised information]] into [[topic modeling algorithm]]s to obtain better [[interpretation]] and [[representation]] for calculating the [[similarity]] among them, and adopt [[Affinity Propagation algorithm]] to group [[# hashtags]] into [[coherent topic]]s.
[[We]] interpret [[our experimental finding]]s by comparison with [[traditional model]]s.
[[We]] introduce a [[dataset]] composed of the [[kddo1 ontology]] and the [[kdd09cma1 corpus]] .
[[We]] introduce a [[dynamic]] [[flow]] [[simulation algorithm]] to generate a [[flow pattern]] which is a unique [[characteristic]] for each [[component]] .
[[We]] introduce a [[framework]] for [[evaluating]] [[brand]] [[audience]]s, in analogy to [[predictive-modeling holdout evaluation]] .
[[We]] introduce a [[fully automated method]] for [[locating]] and [[repairing bugs in software]] .
[[We]] introduce a globally [[normalized]] [[transition-based neural network model]] that achieves [[state-of-the-art]] [[part-of-speech tagging]], [[dependency parsing]] and [[sentence compression]] [[result]]s.
[[We]] introduce a [[linguistic resource]] composed of a [[semantically annotated corpus]] and a [[lexicalized ontology]] that are [[interlinked]] on [[mention]]s of [[concept]]s and [[entiti]]es.
[[We]] introduce a [[methodology]] ([[MOBIUS]]) for finding a [[mapping]] among [[identiti]]es of [[individual]]s across [[social media site]]s.
[[We]] introduce a [[navigation cost model]] as a means to [[estimate the effort]] required to [[navigate]] the [[query-result]]s, and show that the problem of [[estimating]] the ideal amount of [[diversification]] at each step is [[NP-Hard]] .
[[We]] introduce and [[analyze]] the concept of [[writable working set]], and present the [[design]], [[implementation]] and [[evaluation]] of [[high-performance]] [[OS migration]] built [[on top of]] the [[Xen VMM]] .
[[We]] introduce and formalize a novel [[constrained path optimization problem]] that is the heart of the [[real-time]] [[ad serving task]] in the [[Yahoo!]] (formerly [[RightMedia]]) [[Display Advertising Exchange]] .
[[We]] introduce a new [[dataset with human judgments on pairs of words in sentential context]], and evaluate [[our model]] on it, showing that [[our model]] [[outperform]]s [[competitive]] [[baseline]]s and other [[neural language model]]s.
[[We]] introduce a new [[deep learning architecture]] that exploits the [[spatial]] and [[temporal]] nature of the [[neuronal activation data]] .
[[We]] introduce a new method for [[triggering vulnerabiliti]]es in [[deep layer]]s of [[binary executable]]s and facilitate their [[exploitation]] .
[[We]] introduce a new method of constructing [[kernels]] on [[sets]] whose elements are [[discrete structures]] like [[strings]], [[trees]] and [[graphs]] .
[[We]] introduce an [[interaction-based metric]] for [[estimating]] a [[user]]'s [[affinity]] to his [[contacts]] and [[groups]] .
[[We]] introduce a novel concept of [[vertex-edge homophily]] in terms of both [[vertex label]]s and [[edge label]]s and transform a general [[collaboration graph]] into an [[activity-based collaboration multigraph]] by augmenting its [[edge]]s with [[class label]]s from each [[activity graph]] through [[activity-based edge classification]] .
[[We]] introduce a novel [[graphical model]], the [[collaborative score topic model (CSTM)]], for [[personal recommendation]]s of [[textual document]]s.
[[We]] introduce a novel [[machine learning framework]] based on [[recursive autoencoder]]s for [[sentence-level prediction]] of [[sentiment label distribution]]s.
[[We]] introduce a novel [[sentence ranking problem]] called [[explanatory sentence extraction (ESE)]] which aims to [[rank]] [[sentence]]s in [[opinionated text]] based on their [[usefulness]] for [[helping users]] understand the [[detailed reason]]s of [[sentiment]]s (i.e.," explanatoriness ").
[[We]] introduce a [[relation recognition algorithm]] that can detect [[n-ary relations]] across multiple [[sentence]]s in a [[document]], and use the [[subcellular localization]] relation as a motivating [[example]] .
[[We]] introduce, as a [[case study]], two novel [[technique]]s to [[estimate]] the number of [[candidate]] [[sequence]]s.
[[We]] introduce a [[theoretical analysis]] of the quality of [[approximation]] to guarantee the [[reliability]] of [[our estimation algorithm]] .
[[We]] introduce [[Church]], a [[universal language]] for [[describing stochastic generative processes]] .
[[We]] introduce [[dependencies]] across multiple [[sample]]s by placing a [[global Dirichlet process prior]] over individual [[DPM]]s.
[[We]] introduced novel [[synopses]] to [[reduce communication cost]] when running [[our method]]s in such [[setting]]s.
[[We]] introduce <i>[[structural correspondence learning]]</i> to automatically [[induce correspondence]]s among [[feature]]s from different [[domain]]s.
[[We]] introduce [[kernel]]s defined over [[shallow parse representations of text]], and design [[efficient algorithm]]s for computing the [[kernel]]s.
[[We]] introduce [[measures]] of [[brand]] [[proximity]] in the [[network]], and show that [[audience]]s with high [[brand]] [[proximity]] indeed show substantially [[higher]] [[brand]] [[affinity]] .
[[We]] introduce [[methods]] for [[extracting]] [[quasi-social networks]] from [[data]] on [[visitations]] to [[social networking]] [[pages]], without [[collecting]] any [[information]] on the [[identities]] of the [[browsers]] or the [[content]] of the [[social-network]] [[pages]] .
[[We]] introduce novel [[measure]]s to quantify both [[concept]]s ([["synchronicity" and "normality "]]) and [[we]] propose a [[parameter-free algorithm]] that works on the resulting [[synchronicity-normality plot]]s.
[[We]] introduce [[ontologi]]es, view them from the perspective of several [[fields of knowledge]], and present existing [[ontologi]]es and the different tasks of [[ontology building]], [[learning]], [[matching]], [[mapping]] and [[merging]] .
[[We]] introduce [[ProbLog]], a [[probabilistic extension]] of [[Prolog]] .
[[We]] introduce [[TANGENT]], a novel [[recommendation algorithm]] to solve [[this problem]] .
[[We]] introduce the [[ACL Anthology Network (AAN)]], a [[manually curated networked database of citation]]s, [[collaboration]]s, and [[summari]]es in the [[field of Computational Linguistic]]s.
[[We]] introduce the [[algorithm]] [[SSCC]] that exploits different [[pruning technique]]s to [[efficiently]] [[generate]] a [[subspace correlation clustering]] .
[[We]] introduce the concept of [[edge pivoting]] which allows us to [[collect]] [[2-hop information]] without maintaining an explicit [[2-hop neighborhood list]] at each [[vertex]] .
[[We]] introduce the [[dynamic memory network (DMN)]], a unified [[neural network framework]] which [[processes input sequence]]s and [[question]]s, forms [[semantic]] and [[episodic memori]]es, and [[generates relevant answer]]s.
[[We]] introduce the first [[meta-service]] for [[information extraction]] [[in molecular biology]], the [[BioCreative MetaServer]] ([[BCMS]]; http://bcms.bioinfo.cnio.es/ webcite).
[[We]] introduce the [[intra-list similarity metric]] to assess the [[topical diversity of recommendation list]]s and the [[topic diversification approach]] for [[decreasing]] the [[intra-list similarity]] .
[[We]] introduce the [[kddo1 ontology]] and [[semantically annotated]] [[kdd09cma1 corpus]] from the [[field of knowledge discovery in database (KDD) research]] .
[[We]] introduce the [[notion]] of <i>[[state-stacked sparseness]]</i> to select a [[subset]] of the most critical [[sensor]]s as a [[function of evolving system state]] .
[[We]] introduce the [[Semantic Wiki]] [[KnowWE]], that provides the possibility to [[define]] and [[maintain ontologi]]es together with strong [[problem-solving knowledge]] .
[[We]] introduce [[TriMine]], which performs [[three-way mining]] for all three [[attribute]]s, namely, [[URL]]s, [[user]]s, and [[time]] .
[[We]] introduce two [[optimization problem]]s: the [[densest at-least-k-subgraph problem (dalks)]], which is to find an [[induced subgraph]] of highest [[average degree]] among all [[subgraph]]s with [[at least]] k [[vertice]]s, and the [[densest at-most-k-subgraph problem (damks)]], which is defined [[similarly]] .
[[We]] investigate a [[class of method]]s that we call "[[social sampling]]," where [[participant]]s in a [[poll]] respond with a [[summary]] of their [[friends' putative response]]s to the [[poll]] .
[[We]] investigate [[connection]]s between specific [[biase]]s and various [[measure]]s of [[structural representativeness]] .
[[We]] investigate five tasks: [[affect recognition]], [[word similarity]], [[recognizing textual entailment]], [[event temporal ordering]], and [[word sense disambiguation]] .
[[We]] investigate how [[SVM]]s with a very [[large number]] of [[feature]]s perform with the [[classification task]] of [[chunk]] [[labelling]] .
[[We]] investigate practical [[solutions]] based on [[local hill-climbing]], [[rounding integer linear programs]], and [[pre-clustering]] [[entities]] followed by [[local]] [[optimization]] within [[clusters]] .
[[We]] investigate the application of [[kernel density estimation (KDE)]] to [[this problem]] using a [[mixture model approach]] that can [[interpolate]] between an individual's [[data]] and [[broader pattern]]s in the [[population]] as a whole.
[[We]] investigate the [[characteristic]]s and [[behavior]]s of the [[evaluation metric]]s on [[offline]] and [[online testing]] both [[analytical]]ly and [[empirical]]ly by experimenting them on [[online advertising data]] from the [[Bing search engine]] .
[[We]] investigate the effectiveness of [[this approach]] versus [[query-based matching]] and finding related [[historic activity]] from the [[current user]] (i.e., [[group versus individual]]).
[[We]] investigate the [[problem of identifying]] [[evolutionary community outlier]]s given the [[discovered communiti]]es from two [[snapshot]]s of an [[evolving dataset]] .
[[We]] investigate the [[task of building]] [[open domain]], [[conversational dialogue system]]s based on [[large dialogue corpora]] using [[generative model]]s.
[[We]] investigate [[two]] [[implementations]] of the [[proposed framework]] for [[primal]] and [[dual SVMs]], respectively.
We juxtapose the [[effect]]s of [[trade]] and [[technology]] on [[employment]] in [[U.S. local labor market]]s between [[1990]] and [[2007]] .
[[We]] lay out the [[theoretical foundation]] of [[ClickRank]] based on an [[intentional surfer model]] and analyze its [[properties]] .
Welcome to [[my knowledge base]]: A [[semantic wiki]] of the [[concept]]s, [[publication]]s and [[people]] [[that I expect to re-encounter]] .
[[We]] [[learn]] several different [[learning tasks]] with a [[joint model]], explicitly addressing the specifics of each [[learning task]] with [[task-specific parameters]] and the commonalities between them through [[shared parameters]] .
[[We]] learn that [[user satisfaction]] is paramount: [[ads blindness]] and [[sightedness]] are driven by the [[quality of previously viewed]] or [[clicked ad]]s, as measured by both [[ad relevance]] and [[landing page quality]] .
[[We]] leverage multiple [[classical]] [[ML approach]]es, but develop 3 novel [[sets of features]] .
[[We]] leverage this theory to propose a new [[method of moments algorithm]] for [[fitting]] [[MFNG]] to [[large network]]s.
[[We]] limit the [[search space]] by providing [[provably correct]] [[anti monotone]] [[bounds]] for [[FT-support]] and develop [[practically]] [[efficient means]] of achieving them.
[[Well-engineered implementation]]s have already outperformed highly [[optimized software librari]]es for [[ubiquitous problem]]s such as [[least-squares]], 4, 35 with good [[scalability]] in [[parallel]] and [[distributed]] environments.52
Well-known principles of [[induction]] include [[monotone induction]] and different sorts of [[nonmonotone induction]] such as [[inflationary induction]], [[induction]] over [[well-founded set]]s and [[iterated induction]] .
[[We]] look at the [[comparative behaviour]] of different [[model]]s and present some [[experimental insight]]s.
[[We]] look into different techniques for computing [[item-item similariti]]es (e.g., [[item-item correlation]] vs. [[cosine similariti]]es between [[item vector]]s) and different [[technique]]s for obtaining [[recommendation]]s from them (e.g., [[weighted sum]] vs. [[regression model]]).
[[We]] make an [[empirical exploration]] of several [[factor]]s, including variations on [[Gaussian]], [[Laplace]] and [[hyperbolic-L1 priors]] for improved [[regularization]], and several classes of features.
[[We]] map [[this problem]] on a [[de Bruijn graph]], where the [[n-gram]]s form the [[edge]]s and where every [[Eulerian cycle]]s gives a [[plausible reconstruction]] .
[[We]] [[measure the distribution]] of these [[dense subgraph]]s and their [[evolution]] over [[time]] .
[[We]] [[measure]] the [[empirical]] [[trade-off]] between [[accuracy]] and [[privacy]] in these [[adaptation]]s, and find that [[we]] can provide [[non-trivial]] [[formal]] [[privacy guarantee]]s while still outperforming the [[Cinematch]] baseline [[Netflix]] provides.
[[We]] model a [[heterogeneous network]] containing <i>M</i> [[type]]s of [[meta path]]s as <i>M</i> [[vertex-centric path graph]]s and <i>M</i> [[edge-centric path graph]]s.
[[We]] [[model]] each [[sample data]] by an [[infinite mixture]] of [[Dirichlet-process Gaussian-mixture models (DPMs)]] with each [[DPM]] representing the [[noisy realization]] of its corresponding [[class distribution]] in a given [[sample]] .
[[We]] model the ability of each [[author]] and the [[bias]] of each [[reviewer]], and propose a [[two-stage probabilistic generative model]] using the [[graded response model]] in the [[item response theory]] .
[[We]] [[model]] the interaction between [[learner]] and [[data generator]] as a [[Stackelberg competition]] in which the [[learner]] plays the role of the [[leader]] and the [[data generator]] may react on the [[leader]]'s move.
[[We]] model the [[search federation]] as a [[contextual bandit problem]] .
[[We]] [[model]] the [[strength]] of their [[belief]]s and their (the [[human]]) [[degree of commitment]] to their [[utterance]] .
[[We]] motivate a [[measure of density]] based on [[minimum degree]] and [[distance constraints]], and [[we]] develop an [[<i>optimum</i>]] [[greedy algorithm]] for [[this measure]] .
[[We]] motivate, that an [[extensible SemanticWiki]] is an appropriate [[basis]] for [[building]] a new generation of [[knowledge engineering environment]]s.
[[We]] name [[the proposed algorithm]] [[SUpervision-Guided AutoencodeR (SUGAR)]] .
[[We]] need two [[properti]]es: (a) [[effectiveness]], that is, the [[pattern]]s should help us understand the [[data]], [[discover group]]s, and [[enable forecasting]], and (b) [[scalability]], that is, the [[method]] should be [[linear]] with the [[data size]] .
[[We]] next consider [[psychological phenomena]] such as the [[joy]] of [[synchronized movement]] and the [[ecstatic joy]] of [[self-loss]], which might be [[proximal mechanism]]s underlying the extraordinary [[pleasure]]s [[people]] get from [[hive-type activiti]]es.
[[We]] next discuss simultaneously conducting [[active inference]] and [[active learning]] .
We now fly around [[free]] and [[unencumbered]], calling ourselves [[atheist]]s, reading [[Waiting for Godot]], and wondering, [[what]] does [[it]] all [[mean]]?
[[We]] observed that each [[AD-related ROI]] is associated with a [[wide range]] of [[severity]] and that the difference between [[ROI]]s is merely a [[difference]] in [[severity distribution]] .
[[We]] observe large improvements in accuracy at much lower [[computational cost]], i.e. it takesa day and one CPU to derive high quality [[300-dimensional]] [[vectors]] for [[one million]] [[vocabulary]] from a 1.6 billion [[word]]s [[data set]] .
[[We]] observe [[significant deviation]]s between the [[experimental finding]]s on [[real-world network]]s and [[stochastic Kronecker graph]]s, a [[random graph model]] that [[mimic]]s [[real-world network]]s in certain [[aspect]]s.
[[We]] observe that, by [[estimating]] the [[connection probabiliti]]es between [[vertice]]s instead of considering the [[observed edge]]s directly, the [[noise scale]] [[enforced]] by [[differential privacy]] can be greatly reduced.
[[We]] observe that, in addition to producing [[near-optimal solution]]s for [[dalks]], [[the algorithm]] also produces [[near-optimal solution]]s for [[dks]] for nearly all values of k.
[[We]] [[observe]] that, in reality, a great [[portion]] (> 90% in the [[data]] we study) of the [[reviewer]]s write only one [[review]] ([[singleton review]]).
[[We]] observe that many [[people]] are members of several [[social network]]s in the same time, such as [[Facebook]], [[Twitter]] and [[Tencent's QQ]] .
[[We]] observe that previous [[notion]]s of [[summaries]] cannot be directly used for [[analyzing]] [[frequent itemset]]s.
[[We]] observe that the [[accuracy of inference]] of [[private attribute]]s for [[differentially private data]] and [[l-diverse data]] can be quite [[similar]] .
[[We]] observe that the [[KBC process]] is [[iterative]], and we develop [[technique]]s to [[incrementally produce]] [[inference result]]s for [[KBC system]]s.
We observe that the [[prequential error]] [[converge]]s to a [[holdout estimator]] when estimated over a [[sliding window]] or using [[fading factor]]s.
[[We]] observe that these two [[task]]s are often tightly [[coupled]]: [[rectifying erroneous value]]s will facilitate [[data collation]], while [[linking similar record]]s provides us with a clearer view of the [[data]] and additional [[evidence]] for [[error correction]] .
[[We]] [[observe]] that when we look in [[reversed order]] of [[time]], the [[temporal dependence structure]] of the [[time series]] is usually preserved after switching the roles of [[cause and effect]] .
[[We]] obtain a reasonably [[tight]] [[upper bound]] on the [[estimated misclassification error]] on [[average]] over the [[random choice of the projection]], which, in contrast to [[early]] [[distance preserving approaches]], tightens in a natural way as the [[number of training examples]] [[increases]] .
[[We]] obtained interesting [[experimental results]] on several [[real]] [[network data]] .
[[We]] obtained significant [[performance improvement]] over the [[baseline system]] (without [[induced]] [[social]] [[features]]) in our [[experiments]] on a [[multi-user]] [[data collection]]: the [[relative error]] reduction in [[MAE]] was 31% in [[micro-averaging]], and 14% in [[macro-averaging]] .
[[We]] offer a [[unified analysis]] of the growth of [[low-skill service occupation]]s between [[1980]] and [[2005]] and the [[concurrent polarization of US employment]] and [[wage]]s.
([[We]] opted for using [[percentile-rank]]s rather than [[absolute rank]]s in order to make [[our discussion]] general and [[independent]] of the [[number of program]]s.)
[[We]] organize the [[literature]] on [[VSMs]] according to the [[structure of the matrix]] in a [[VSM]] .
[[We]] outline several [[variant]]s of [[weighting friend]]s within circles based on their [[infer]]red [[expertise level]]s.
[[We]] outline [[the system architecture]] of [[EMBERS]], individual [[model]]s that leverage specific [[data source]]s, and a [[fusion]] and [[suppression engine]] that supports [[trading off]] specific [[evaluation criteria]] .
We outperform baseline [[methods]] (including aspect [[model]] and [[tensor analysis]]) by an [[order of magnitude]] .
[[We]] perform a detailed [[empirical study]] on numerous [[massive graph]]s, showing that [[FAST-PPR]] dramatically [[outperform]]s existing [[algorithm]]s.
[[We]] perform an extensive [[evaluation]] using [[artificially generated thesaurus-sense-tagged data]] .
[[We]] perform a [[theoretical investigation]] of the concise [[representation problem]] and [[link]] it to the [[biclique set cover problem]] and [[prove]] its [[NP-hardness]] .
[[We]] perform both [[quantitative]] and [[qualitative evaluation]]s using [[synthetic]] as well as [[real-world document data set]]s such as [[research paper collection]]s and [[nonprofit micro-finance data]] .
[[We]] perform [[clustering analysis]] on both a unified [[vertex-centric path graph]] and each [[edge-centric path graph]] to generate [[vertex clustering]] and [[edge clustering]]s of the original [[heterogeneous network]] respectively.
[[We]] performed an [[evaluation]] on the [[Epinions]] [[dataset]] and [[compared]] our [[model]] with existing [[trust-based]] and [[collaborative filtering methods]] .
[[We]] performed an extensive [[empirical study]] covering three different [[time period]]s, over [[100]] million [[message]]s, and thousands of [[candidate]] [[tag]]s [[per]] [[message]] .
[[We]] performed a [[systematic review]] of [[empirical research]] on [[trust in automation]] from [[January 2002]] to [[June 2013]] .
[[We]] performed [[experiment]]s on both [[real]] and [[synthetic dataset]]s [[spanning]] several [[megabyte]]s, including [[motion capture]] [[sequence]]s and [[chlorine level]]s in [[drinking water]] .
[[We]] perform [[experiment]]s on a [[Cantonese]] [[conversational telephone speech corpus]] and find that increasing the number of [[auto-encoder]]s in the [[network]] produces more useful [[feature]]s, but requires [[pre-training]], especially when little [[training data]] is available.
[[We]] perform [[experiment]]s on both [[synthetic data]] and [[real data]] from the [[Allen Developing Mouse Brain Atla]]s; [[result]]s demonstrate the [[effectiveness]] and [[efficiency]] of [[the proposed approach]] .
[[We]] perform [[experiment]]s on [[large scale real world dataset]] of [[news article]]s, and use [[Twitter]] as the platform providing information about the [[social context]] of these [[news article]]s.
[[We]] perform [[experiment]]s on [[real data]], and [[we]] show that there is a [[correlation]] between [[the labeling]] we obtain and [[empirical metric]]s of tie [[strength]], and that [[weak edge]]s act as bridges between different [[communiti]]es in the [[network]] .
[[We]] perform [[experiment]]s to [[assess]] [[our method]] against a [[large range]] of [[alternative clustering algorithm]]s in the [[literature]] .
[[We]] perform [[experiment]]s with [[real-world]] [[social media dataset]]s to [[evaluate]] the [[effectiveness]] of [[the proposed framework]] and probe the working of its key [[component]]s.
[[We]] perform extensive [[empirical evaluation]]s on [[large]], [[real]] [[scholarly data set]]s to [[validate]] the [[effectiveness]] and the [[efficiency]] of our [[method]] .
[[We]] perform extensive [[empirical studies]] on both [[synthetic]] and [[real data set]]s.
[[We]] perform extensive [[empirical studi]]es on [[real data collected]] from [[Amazon Mechanical Turk]] .
[[We]] perform extensive [[empirical study]] on a [[Tencent microblogging network]] of 1,000,000,000 [[edge]]s.
[[We]] perform [[large scale experiments]] on [[data]] from [[real]] [[computational advertising applications]] and illustrate [[our approach]] on [[datasets]] with several [[billion]] [[records]] and [[hundreds of millions]] of [[predictors]] .
[[We]] perform [[large scale experiment]]s on this [[data-set]] to [[evaluate]] the [[accuracy]] and [[efficiency]] of [[our algorithm]]s.
[[We]] point out that the [[QPFS formulation]] faces several [[non-trivial issue]]s, in particular, how to properly treat [[feature]] `[[self-redundancy]]' while ensuring the [[convexity]] of the [[objective function]] .
[[We]] pose a general [[anomaly detection problem]] which includes both [[discrete]] and [[continuous data streams]], where we assume that the [[discrete stream]]s have a [[causal influence]] on the [[continuous streams]] .
[[We]] pose the [[problem]] of [[network discovery]] which involves [[simplifying]] [[spatio-temporal data]] into [[cohesive region]]s ([[node]]s) and [[relationship]]s between those [[region]]s ([[edge]]s).
[[We]] posit, that using the [[temporal]] and [[structural feature]]s of a [[career trajectory]] for [[modeling]] [[profile similarity]] is a far more [[superior approach]] than using [[similarity measure]]s on [[semi-structured attribute representation]] of a [[profile]] for this [[application]] .
We [[predict]] [[functional class membership]] of [[enzyme]]s and [[non-enzyme]]s using [[graph kernel]]s and [[support vector machine classification]] on these [[protein graph]]s.
[[We]] present a [[Bayesian tensor factorization model]] for [[inferring]] [[latent group structure]]s from [[dynamic pairwise interaction pattern]]s.
[[We]] present a [[case study]] of the application of [[fact-oriented modeling]] to the [[capture]] and [[management]] of [[requirement specifications]] for the [[introduction]] of an [[information technology solution]] within [[Microsoft]] .
[[We]] present a [[clustering algorithm]] called [[CBC]] ([[Clustering By Committee]]) that [[automatically discovers word senses from text]] .
[[We]] present a [[clustering algorithm]] for [[discovering]] [[rare]] yet significant [[recurring class]]es across a [[batch of samples]] in the presence of [[random effect]]s.
[[We]] present a [[complete workflow]] to [[transform scientific literature]] into a [[semantic knowledge base]], based on the [[W3C standard]]s [[RDF]] and [[RDFS]] .
[[We]] present a [[corpus-based approach]] to the [[class expansion task]] .
[[We]] present a [[decision tree framework]] for learning an accurate [[non-parametric]] [[spatio-temporal sequence predictor]] .
[[We]] present a [[decomposition]] of the [[POEM objective]] that enables [[efficient]] [[stochastic gradient optimization]] .
[[We]] present a [[direct multi-class boosting (DMCBoost) method]] for [[classification]] with the following [[properti]]es: (i) instead of reducing the [[multi-class classification task]] to a set of [[binary classification task]]s, [[DMCBoost]] directly solves the [[multi-class classification problem]], and only requires very weak [[base classifier]]s; (ii) [[DMCBoost]] builds an [[ensemble classifier]] by directly [[optimizing]] the [[non-convex performance measure]]s, including the [[empirical classification error]] and [[margin function]]s, without resorting to any [[upper bound]]s or [[approximation]]s.
[[We]] present a [[discriminative [[latent variable]] model]] for [[classification problem]]s in [[structured domains]] where [[inputs]] can be [[represented by]] a [[graph]] of [[local]] [[observations]] .
[[We]] present a [[domain-independent topic segmentation algorithm]] for [[multi-party speech]] .
[[We]] present a [[dynamic postings allocation policy]] that [[allocates memory]] in [[increasingly-larger]] " slices " from a small number of [[large]], [[fixed pool]]s of [[memory]] .
[[We]] present [[AESOP]], a [[scalable algorithm]] that [[identifi]]es [[malicious executable file]]s by applying [[Aesop's moral]] that " a man is known by the [[company]] he keeps.
[[We]] present a [[family]] of <i>[[Noise Adaptive Stochastic Approximation (NASA)]]</i> algorithms for [[online convex optimization]] and [[stochastic convex optimization]] .
[[We]] present a [[formal definition]] of [[comprehension burden]] and propose an [[algorithmic approach]] for computing [[it]] .
[[We]] present a [[formal definition]] of the [[grouping problem]] and investigate some of its [[variant]]s.
[[We]] present a [[framework]] for [[discriminative sequence classification]] where [[linear classifier]]s work directly in the [[explicit]] [[high-dimensional predictor space]] of all [[subsequence]]s in the [[training set]] (as opposed to [[kernel-induced space]]s).
[[We]] present a [[framework]] for using [[continuous-space vector representations of word meaning]] to derive new [[vector]]s representing the [[meaning of senses]] [[listed in]] a [[semantic network]] .
[[We]] present a [[framework to handle such problem]]s and apply it to the problem of [[semantic scene classification]], where a [[natural scene]] may contain [[multiple]] [[object]]s such that the scene can be described by [[multiple]] [[class label]]s (e.g., a [[field scene]] with a [[mountain]] in the [[background]]).
[[We]] present a general [[framework]] in which we [[mine]] over different [[feature model]]s produced from a given [[dataset]] in order to extract [[pattern]]s over the [[model]]s.
[[We]] present a [[generalized framework]] for [[active inference]], the [[selective acquisition]] of [[labels]] for cases at [[prediction time]] in lieu of using the [[estimated label]]s of a [[predictive model]] .
[[We]] present a [[general method]] using [[kernel canonical correlation analysis]] to learn a [[semantic representation]] to [[web image]]s and their [[associated]] [[text]] .
[[We]] present a [[generic framework]] that [[extract]]s [[structural information]] from [[malware program]]s as [[attributed function call graph]]s, in which [[rich malware feature]]s are [[encode]]d as [[attribute]]s at the [[function level]] .
[[We]] present a [[graph mining solution]] to [[this problem]] wherein [[we]] [[analyze]] [[heap dumps]] to [[automatically identify subgraphs]] which could represent potential [[memory leak]] [[source]]s.
[[We]] present a large-scale empirical comparison between ten [[supervised learning method]]s: [[SVM]]s, [[neural nets]], [[logistic regression]], [[naive bayes]], [[memory-based learning]], [[random forests]], [[decision trees]], [[bagged trees]], [[boosted trees]], and [[boosted stumps]] .
[[We]] present a [[large-scale]] [[evaluation framework]] for [[personalized search]] based on [[query log]]s, and then evaluate five [[personalized search strategi]]es (including two [[click-based]] and three profile-based ones) using 12-day [[MSN query log]]s.
[[We]] present a [[lexicon-based approach]] to [[extracting sentiment from text]] .
[[We]] present [[algorithm]]s to find a good [[set]] of [[aspect]]s, and also to [[pick]] the best <math>k</math> [[aspect]]s [[matching]] any [[query]] .
[[We]] present a [[local algorithm]] ([[constant-time distributed algorithm]]) for approximating these [[LP]]s, and [[we]] show that the approximation factor of [[our algorithm]] is the best possible among all local [[algorithm]]s.
[[We]] present a [[method]] called [[TIMEMACHINE]] to [[generate a timeline]] of [[event]]s and [[relation]]s for [[entiti]]es in a [[knowledge base]] .
[[We]] present a [[method]] for [[computer-assisted authorship attribution]] based on [[character-level <i>n</i>-gram language model]]s.
[[We]] present a [[method]] for [[efficiently]] [[training]] [[binary]] and [[multiclass kernelized SVM]]s on a [[Graphics Processing Unit (GPU)]] .
[[We]] present a [[method for learning discriminative feature transforms]] using as criterion the [[mutual information]] between [[class label]]s and [[transformed feature]]s.
[[We]] present a [[methodology]] based on [[empirical Bayes modeling]] to combine [[ADR signal]]s [[mined]] from ~5 million [[adverse event reports]] collected by the [[FDA]], and [[healthcare data]] corresponding to 46 million [[patient]]s' the main two types of [[information source]]s currently employed for [[signal detection]] .
[[We]] present a [[method]] that naturally supports the [[recognition of semantic relations]] with more than two [[arguments]] and whose [[mentions]] can reside across multiple [[sentence]]s.
[[We]] present a [[model for the automatic extraction]] of [[comparable text]]s in [[multiple languages]] and on specific [[topics from Wikipedia]] .
[[We]] present a [[model]] in which information can reach a [[node]] via the [[link]]s of the [[social network]] or through the [[influence of external source]]s.
[[We]] present a model we call [[lemon (Lexicon Model for Ontologies)]] that supports the [[sharing]] of [[terminological]] and [[lexicon resource]]s on the [[Semantic Web]] as well as their [[linking]] to the existing [[semantic representation]]s provided by [[ontologi]]es.
[[We]] present a [[multi-document summarizer]], [[MEAD]], which generates summaries using [[cluster centroids]] produced by a [[topic detection and tracking system]] .
[[We]] present an [[algorithm]], called the [[Offset Tree]], for [[learning]] to make [[decisions]] in situations where the [[payoff]] of only one [[choice]] is [[observed]], rather than all [[choice]]s.
[[We]] present an [[algorithm]] for [[language identification]], in particular of [[short document]]s, for the case of an [[Internet domain]] with [[site]]s in multiple [[countri]]es with differing [[language]]s.
[[We]] present an [[algorithm]], [[GP-SELECT]], which utilizes [[prior]] knowledge about [[similarity]] between [[item]]s, expressed as a [[kernel function]] .
[[We]] present an [[algorithm]] to [[extract]] a [[high-quality approximation]] of the (<i>[[top-k]]</i>) [[Frequent itemsets (FIs)]] from [[random sample]]s of a [[transactional dataset]] .
[[We]] present an [[algorithm]] to [[generate such timelines]] for a given [[time period]] and [[screen size]], based on [[submodular optimization]] and [[web-co-occurrence statistic]]s with [[provable]] [[performance guarantee]]s.
[[We]] present an [[analysis]] of individual [[queri]]es, [[query duplication]], and [[query session]]s.
[[We]] present an [[analysis]] of [[user conversation]]s in [[on-line social media]] and their [[evolution]] over time.
[[We]] present an [[approach]] to [[cluster]] such [[non-homogeneous datasets]] by using the [[relationship]]s to impose either [[dependent clustering]] or [[disparate clustering constraints]] .
[[We]] present an automatic approach to the construction of [[BabelNet]], a [[very large]], [[wide-coverage]] [[multilingual]] [[semantic network]] .
[[We]] present an effective [[multifaceted system]] for [[exploratory analysis]] of [[highly heterogeneous]] [[document collection]]s.
[[We]] present an [[efficient algorithm]] to perform this [[scheduling]] under certain [[condition]]s and apply this [[algorithm]] to [[real data]] obtained from [[server log]]s, showing [[evidence]] of significant [[improvements]] in [[traffic]] from our [[algorithmic schedules]] .
[[We]] present an [[efficient]] and novel [[miner]] for [[discovering frequent and closed general episode]]s.
[[We]] present an [[efficient]] [[Bayesian inference procedure]] of [[the proposed model]] based on the [[stochastic EM algorithm]] .
[[We]] present an [[efficient]] [[learning]] [[algorithm to extract]] such [[fault]] [[signatures]] from [[noisy]] [[historical]] [[event data]], and with the help of novel [[space-time]] [[indexing structures]], [[we]] show how to perform [[efficient]], [[online]] [[signature matching]] .
[[We]] present an end-to-end system that extracts a [[user]]’s [[social network]] and its [[member]]s’ [[contact information]] given the [[user]]’s [[email inbox]] .
[[We]] present an [[ensemble-based framework]] for [[semantic [[lexicon]] induction]] that incorporates three diverse [[approaches]] for [[semantic class identification]] .
[[We]] present a new [[algorithm]] for [[finding]] [[large]], [[dense subgraph]]s in [[massive graph]]s.
[[We]] present a new approach that combines [[sequential]], [[structural]] and [[chemical information]] into one [[graph model]] of [[protein]]s.
[[We]] present a new [[approach]] to [[large-scale]] [[graph]] [[mining]] based on so-called [[backbone refinement class]]es.
[[We]] present a new [[approach]] to [[rare category detection]] based on [[hierarchical mean shift]] .
[[We]] present a new [[dimensionality reduction setting]] for a [[large family]] of [[real-world problem]]s.
[[We]] present a new [[dual-tree algorithm]] for [[efficiently]] [[computing the EMST]], use [[adaptive algorithm analysis]] to prove the [[tightest]] (and possibly [[optimal]]) [[runtime bound]] for the [[EMST problem]] to-date, and demonstrate the [[scalability]] of [[our method]] on [[astronomical data sets]] .
[[We]] present a new, [[fast]], [[general]] [[EMST algorithm]], motivated by the [[clustering]] and [[analysis]] of [[astronomical data]] .
[[We]] present a new method for accelerating [[matrix multiplication]] [[asymptotically]] .
[[We]] present a new [[structured]] [[probabilistic topic model]] built on a [[realistic]] [[figure generation scheme]] to [[model]] the structurally [[annotated biological figure]]s, and [[we]] derive an efficient [[inference algorithm]] based on [[collapsed Gibbs sampling]] for [[information retrieval]] and [[visualization]] .
[[We]] present a new theory of [[similarity]] between [[words]] and [[phrase]]s based on [[information distance]] and [[Kolmogorov complexity]] .
[[We]] present an [[experimental analysis]] across [[100]] different ad [[campaign]]s, showing that the [[transfer learning]] indeed improves [[performance]] across a [[large number of them]], especially at the start of the [[campaign]]s.
[[We]] present an extensible [[supervised Target-Word Sense Disambiguation system]] that leverages upon [[GATE (General Architecture for Text Engineering)]], [[NSP (Ngram Statistics Package)]] and [[WEKA (Waikato Environment for Knowledge Analysis)]] to present an [[end-to-end solution]] that integrates [[feature identification]], [[feature extraction]], [[preprocessing]] and [[classification]] .
[[We]] present an important [[clinical application]] of [[SympGraph]]: [[symptom expansion]], which can expand a given [[set]] of [[symptom]]s to other related [[symptom]]s by analyzing the underlying [[SympGraph structure]] .
[[We]] present an initial [[case study]] on [[KnIT]], a [[prototype system]] that [[mines the information]] contained in the [[scientific literature]], represents it explicitly in a [[queriable network]], and then further reasons upon these [[data]] to [[generate novel]] and [[experimentally testable hypothese]]s.
[[We]] present an [[integrated framework]] for using [[Convolutional Networks for classification]], [[localization]] and [[detection]] .
[[We]] present an [[online learning algorithm]], [[Amnesiac Averaged Perceptron]], that is highly [[efficient]] yet able to quickly adjust to the [[rapidly-changing]] [[distributions]] of [[bidded keywords]], [[advertisements]] and [[user behavior]] .
[[We]] present a notion of [[similarity]] based on [[provenance]] -- two [[binari]]es are [[similar]] if they are [[compile]]d from the same (or very [[similar]]) [[source code]] with the same (or [[similar]]) [[compiler]]s.
[[We]] present a novel [[algorithm]] for significant [[pattern mining]], [[Westfall-Young light]] .
[[We]] present a novel [[approach]] that uses the wild [[bootstrap]] to capture the [[spatio-temporal dependenci]]es, in the special use case of [[teleconnection]]s in [[climate data]] .
[[We]] present a novel [[approach to distant supervision]] that can alleviate this [[problem]] based on the following two ideas: First, [[we]] use a [[factor graph]] to [[explicitly model]] the [[decision whether two entities are related]], and the [[decision whether this relation is mentioned in a given sentence]]; second, [[we]] apply [[constraint-driven semi-supervision]] to [[train]] this [[model]] without any knowledge about which [[sentence]]s express the [[relation]]s in our [[training KB]] .
[[We]] present a novel [[approach]] to [[model]] the [[burstiness]] of a [[term]], using [[discrepancy theory]] [[concepts]] .
[[We]] present a novel [[approach to parallel materialisation]] (i.e., [[fixpoint computation]]) of [[datalog program]]s in [[centralised]], [[main-memory]], [[multi-core]] [[RDF system]]s.
[[We]] present a novel [[columnar storage representation]] for [[nested record]]s and discuss experiments on [[few-thousand node instance]]s of [[the system]] .
[[We]] present a novel [[learning algorithm]], [[DirectRank]], which [[directly and exactly optimize]]s [[ranking measure]]s without resorting to any [[upper bound]]s or [[approximation]]s.
[[We]] present a novel way to define a [[vulnerable friend]] from an individual [[user's perspective]] is dependent on whether or not the [[user]]'s [[friend]]s' [[privacy setting]]s protect the [[friend]] and the individual's [[network of friend]]s (which includes the [[user]]).
[[We]] present an overview on [[recommender system]]s and [[we]] sketch how to use [[Linked Open Data]] to [[build a new generation]] of [[semantics-aware recommendation engine]]s.
[[We]] present an [[unsupervised approach to symmetric word alignment]] in which two simple [[asymmetric model]]s are [[trained jointly]] to maximize a combination of [[data likelihood]] and [[agreement]] between [[the model]]s.
[[We]] present a preliminary [[pilot study]] of [[belief annotation]] and [[automatic tagging]] .
[[We]] present a [[sampling framework]] for [[unaggregated data]] that uses a single [[pass (for streams)]] or two passes (for [[distributed data]]) and [[state proportional]] to the desired [[sample size]] .
[[We]] present a [[set of practical]], [[privacy-preserving data mining algorithm]]s for [[GWAS dataset]]s.
[[We]] present a [[simple]] and [[scalable algorithm]] for [[clustering]] tens of millions of [[phrase]]s and use the resulting [[cluster]]s as [[feature]]s in [[discriminative classifier]]s.
[[We]] present a [[spectrum]] of [[topic modeling technique]]s that contribute to a [[deployed system]] .
[[We]] present [[asynchronous variant]]s of four [[standard]] [[reinforcement learning algorithm]]s and show that [[parallel actor-learner]]s have a [[stabilizing effect]] on [[training]] allowing all four [[method]]s to successfully train [[neural network controller]]s.
[[We]] present a [[system]] for the [[extraction of entity]] and [[relation mention]]s.
[[We]] present a [[system]] to automatically [[construct stori]]es in [[entity network]]s that can help form directed chains of [[relationship]]s, with [[support]] for [[co-referencing]], [[evidence marshaling]], and imposing [[syntactic constraint]]s on the [[story generation process]] .
[[We]] present a thorough evaluation comprising [[supervised]] and [[unsupervised modes]], and both [[lexical-sample]] and [[all-words tasks]] .
[[We]] present a [[two-stage]] [[multilingual]] [[dependency parser]] and evaluate it on 13 diverse languages.
[[We]] present [[CiteSeer]]: an [[autonomous]] [[citation indexing system]] which [[indexes]] [[academic literature]] in [[electronic format]] (e.g. [[Postscript ﬁle]]s on [[the Web]]).
[[We]] present ''[[conditional random fields]]'', a [[framework for building probabilistic models]] to [[segment]] and [[label sequence data]] .
[[We]] present [[DeepWalk]], a novel [[approach]] for [[learning]] [[latent representation]]s of [[vertice]]s in a [[network]] .
[[We]] present [[efficient algorithm]]s to solve the related [[optimization problem]]s based on [[bound optimization]] and [[alternating projection]] .
[[We]] present efficient [[approximate inference technique]]s based on [[variational method]]s and an [[EM algorithm]] for [[empirical]] [[Bayes parameter estimation]] .
[[We]] present efficient [[methods for evaluating]] [[competitiveness]] in [[large dataset]]s and address the [[natural problem]] of finding the [[top-k competitor]]s of a given [[item]] .
[[We]] present [[empirical analyses]] demonstrating the efficacy of these [[bounds]] in predicting [[relative]] [[classifier performance]] .
[[We]] present [[empirical result]]s on [[application]]s of [[this model]] to [[problems in text modeling]], [[collaborative filtering]], and [[text classification]] .
[[We]] present encouraging results indicating that [[classification]] is a [[viable approach]] for [[automated]] [[provenance-similarity detection]], and as an aid for [[malware analyst]]s in particular.
[[We]] present [[experimental results]] for the [[task of predicting citations]] made in [[scientific literature]] using [[relational data]] taken from [[CiteSeer]] .
[[We]] present [[experimental result]]s illustrating the [[effectiveness]] of the [[approach]] .
[[We]] present [[experimental result]]s on two [[real sensor dataset]]s, corresponding to [[oil drilling rig sensor]]s and [[intensive care unit (ICU)]] [[sensor]]s, and demonstrate the [[superiority]] of [[our approach]] with respect to [[other model]]s.
[[We]] present [[experimental result]]s showing that for this [[task our algorithm]] [[outperform]]s other popular [[dimensionality-reduction algorithm]]s across a wide variety of [[ad campaign]]s, as well as [[production result]]s that showcase its [[performance]] in practice.
[[We]] present [[experimental results]] with [[three]] different [[text corpora]]: [[UseNet news article]]s ([[20 Newsgroups]]), [[web pages]] ([[WebKB]]), and [[newswire article]]s ([[Reuters]]).
[[We]] present extensive [[comparisons]] between [[models]] and [[training methods]] that [[confirm]] and [[strengthen]] previous [[results]] on [[shallow parsing]] and [[training methods]] for [[maximum-entropy models]] .
[[We]] present extensive [[experimental results]] in [[real world]] [[accident]] [[datasets]] for various [[highways]] with known [[issues]] ([[code]] and [[data]] available from [27], [21]).
[[We]] present here a simple [[two-stage method]] for [[extracting]] [[complex relations]] between [[named entities]] in [[text]] .
[[We]] present here some [[empirical result]]s using [[Thompson sampling]] on [[simulated]] and [[real data]], and show that it is highly [[competitive]] .
[[We]] present <i>AutoExtend</i>, a system to [[learn embedding]]s for [[synset]]s and [[lexeme]]s.
[[We]] present illustrative [[examples]] of the use of [[prequential error]] [[estimators]], using [[fading factor]]s, for the [[tasks]] of : i.
[[We]] present [[inference-based, message-passing scheme]]s (e.g., [[variable-elimination]]) and [[search-based, conditioning scheme]]s (e.g., [[cycle-cutset conditioning]] and [[AND/OR search]]).
[[We]] present [[iterative]] [[parameter estimation algorithm]]s for [[conditional random fields]] and compare the [[performance]] of the resulting [[models]] to [[HMMs]] and [[MEMMs]] on [[synthetic]] and [[natural-language data]] .
[[We]] present novel and [[scalable method]]s to [[efficiently]] solve the [[approximate isomorphism problem]] .
[[We]] present novel, [[efficient]], [[model]] based [[kernel]]s for [[time series data]] [[root]]ed in the [[reservoir computation framework]] .
[[We]] present novel [[semi-supervised]] [[boosting algorithm]]s that [[incrementally]] build [[linear combinations]] of [[weak classifiers]] through [[generic]] [[functional]] [[gradient descent]] using both [[labeled]] and [[unlabeled training data]] .
[[We]] present one [[such algorithm]] that [[learn]]s disjunctive [[Boolean function]]s, along with [[variant]]s for [[learning]] other classes of [[Boolean function]]s.
[[We]] present [[our experience]] of using [[machine learning technique]]s over [[data originating]] from [[advanced meter infrastructure (AMI) system]]s for [[water consumption]] in a [[medium-size city]] .
[[We]] present our [[learning]]s as they happened: puzzling outcomes of [[controlled experiment]]s that we [[analyzed deeply to understand]] and [[explain]] .
[[We]] present preliminary evidence that a [[probabilistic language]] based upon our calculus is viable in [[application]]s involving [[massive]] [[probabilistic computation]] .
[[We]] present [[RainMon]], a novel [[end-to-end approach]] for [[mining]] [[timeseries monitoring data]] designed to handle its [[size]] and [[unique characteristic]]s.
[[We]] present [[real time]] [[algorithm]]s in order to [[minimize]] the [[power consumption]] in reducing the [[data collected]] and show that the resulting [[data]] retains almost the same [[amount of information]] at a much lower [[cost]] .
[[We]] present [[Resilient Distributed Datasets (RDDs)]], a [[distributed memory abstraction]] that lets [[programmer]]s perform [[in-memory computation]]s on [[large cluster]]s in a [[fault-tolerant manner]] .
[[We]] present results obtained with several advanced [[language modeling technique]]s, including [[class based model]], [[cache model]], [[maximum entropy model]], [[structured language model]], [[random forest language model]] and several types of [[neural network based language model]]s.
[[We]] present [[results]] on both [[synthetic data]] as well as several [[real-world datasets]] .
[[We]] present results on using [[stacking]] to [[ensemble]] multiple [[system]]s for the [[Knowledge Base Population English Slot Filling (KBP-ESF) task]] .
[[We]] present several [[benchmark problem]]s to show how the above [[element]]s combine to yield multiple [[orders-of-magnitude]] [[improvement]]s for each [[problem]] .
[[We]] present several [[exact]] and [[highly scalable]] [[local pattern sampling algorithm]]s.
[[We]] present [[ShoppingAdvisor]], a novel [[recommender system]] that helps [[user]]s in shopping for [[technical product]]s.
[[We]] present [[SimSensei Kiosk]], an implemented [[virtual human interviewer]] designed to create an [[engaging face-to-face interaction]] where the [[user]] feels comfortable [[talking]] and [[sharing information]] .
[[We]] present some [[editorial evaluation]] of the [[technique]] and [[result]]s of a [[live experiment]] .
[[We]] present [[Spine]], an [[efficient algorithm]] for finding the "backbone" of an [[influence network]] .
[[We]] present [[StarSpace]], a [[general-purpose neural embedding model]] that can solve a wide variety of [[problem]]s: [[labeling task]]s such as [[text classification]], [[ranking task]]s such as [[information retrieval]] / [[web search]], [[collaborative filtering-based]] or [[content-based recommendation]], [[embedding of multi-relational graphs]], and [[learning]] [[word]], [[sentence]] or [[document level embedding]]s.
[[We]] present technical details about [[modeling]] [[trust evolution]], and perform [[experiment]]s to show how the [[exploitation]] of [[trust evolution]] can help improve the [[performance]] of [[online application]]s such as [[rating]] and [[trust prediction]] .
[[We]] present the [[application]]s, [[abstraction]]s, and [[technique]]s used in [[DeepDive]] to accelerate the construction of such dark [[data extraction system]]s.
[[We]] present the [[derivation]] of the [[maximum likelihood estimation]] of [[pSkip]] and demonstrate its efficacy in [[describing]] the [[user]] [[study]] [[data]] .
[[We]] present the design and [[implementation]] of [[Prodcast]], an [[experimental system]] whose goal is to help [[consumer]]s decide when to buy a [[product]] .
[[We]] present the first [[constant-factor approximation algorithm]] for [[this problem]], using a new [[technique]] .
[[We]] present the first [[deterministic]], [[local algorithm]] to compute [[this diffusion]] and use that [[algorithm]] to [[study]] the [[communities]] that it produces.
[[We]] present the first large-scale [[empirical comparison]] of existing [[method]]s for [[learning]] [[GAM]]s.
[[We]] present the [[Lehigh University Benchmark (LUBM)]] as an example of how to [[design]] such [[benchmark]]s.
[[We]] present the [[Mastery Grids system]], an [[intelligent interface]] for [[online learning content]] that combines [[open learner modeling (OLM)]] and [[social comparison feature]]s.
[[We]] present the [[procedures]] for [[computing]] the [[projected gradient]] and ensuring the [[global convergence]] of the [[projected gradient scheme]] .
[[We]] present [[the results]] of an extensive [[experimental study]] on [[real-life]] and [[synthetic data set]]s.
[[We]] present the results of [[applying the model]] to two [[dataset]]s with long time [[period]]s and show its [[effectiveness]] over [[non-trivial]] [[baseline]]s.
[[We]] present the [[succinct interval-splitting tree algorithm (SITA)]] that efficiently performs [[similarity search]] in [[database]]s for [[compound-protein pair]]s with respect to both [[binary fingerprint]]s and [[real-valued properti]]es.
[[We]] present two [[algorithm]]s for frequent term-based [[text clustering]], [[FTC]] which creates flat [[clustering]]s and [[HFTC]] for [[hierarchical clustering]] .
[[We]] present [[two]] [[algorithms for inference]] in [[this model]]: an [[expectation-maximization algorithm]] as well as a [[Markov chain Monte Carlo procedure]] based on [[collapsed Gibbs sampling]] .
[[We]] present two approaches to [[efficiently]] characterize the [[difficulty of satisfying must-link (ML)]] and [[cannot-link (CL) constraint]]s: calculating the [[fractional chromatic polynomial]] of the [[constraint graph]] using [[LP]] and [[approximately]] counting the number of feasible [[clustering]]s using [[MCMC sampler]]s.
[[We]] present two complementary [[annotation schemes]] for [[sentence based annotation]] of [[full scientific papers]], [[CoreSC]] and [[AZ-II]], applied to primary [[research articles]] in [[chemistry]] .
[[We]] present two [[energy function]]s to [[train]] [[these embeddings]] .
[[We]] present two [[extensions to the algorithm]] that improve [[classification accuracy]] under these conditions: (1) a [[weighting factor]] to [[modulate]] the contribution of the [[unlabeled data]], and (2) the use of [[multiple]] [[mixture component]]s [[per]] [[class]] .
[[We]] present two methods for improving performance of [[person name recognizers]] for [[email]]: [[email]]-specific [[structural features]] and a recall-enhancing method which exploits [[name]] [[repetition]] across multiple [[documents]] .
[[We]] present two [[principled algorithm]]s for solving [[this problem]]: (a) an [[exact "two-tier" algorithm]] (based on [[top-k querying technique]]s), which performs much better than the [[naive brute-force algorithm]] and works well for [[moderate problem]] [[instance]]s, and (b) a novel [[polynomial-time]] [[approximation algorithm]] with [[provable]] [[error bound]] for [[larger problem]] [[instance]]s.
[[We]] present two [[sets]] of [[experiments]] to [[test]] [[model effectiveness]] and [[efficiency]] .
[[We]] present [[WebSAIL Wikifier]], an [[entity recognition]] and [[disambiguation system]] that [[identifi]]es and [[links textual mention]]s to their [[referent entiti]]es in [[Wikipedia]] and later maps to the [[ERD set]] of [[target entiti]]es.
[[We]] present [[YAGO]], a [[light-weight]] and [[extensible]] [[ontology]] with [[high coverage]] and [[quality]] .
[[We]] proceed by assigning [[words]] to their [[most similar]] [[clusters]] .
[[We]] proceed by [[characterizing]] a [[class]] of [[<i>monotone</i> constraints]] and we generalize [[our algorithm]] to [[compute]] [[optimum solutions]] satisfying any [[set]] of [[monotone constraints]] .
[[We]] process over 100,000 [[news articles]], [[blog posts]], [[review sites]], and [[tweets]] a [[day]] for [[mentions of item]]s (e.g., [[products]]) of [[interest]], [[extract phrases]] that are mentioned near [[them]], and determine which of the [[phrases]] are of greatest possible [[interest]] to, for example, [[brand managers]] .
[[We]] [[productionized]] and [[evaluated]] [[the system]] on [[Google Play]], a [[commercial mobile app store]] with over one billion [[active user]]s and over one million [[app]]s.
[[We]] propose a [[Bayesian framework modeling]] multiple [[clustering]]s of the [[data]] by multiple [[mixture distribution]]s, each [[responsible]] for an individual [[set]] of relevant [[dimension]]s.
[[We]] propose a [[Bayesian model]] that [[represent]]s the [[rating data]] as [[sequence]] of [[categorical mixture model]]s.
[[We]] propose a [[bispace model]] to [[capture propagation]] in the [[union of (exclusively) Twitter]] and [[non-Twitter environment]]s.
[[We]] propose a [[branch-and-bound algorithm]] to [[search]] for the [[optimal]] [[query graph]] and [[optimal feature]]s simultaneously.
[[We]] propose a [[conceptually simple]] and [[lightweight]] [[framework for deep reinforcement learning]] that uses [[asynchronous gradient descent]] for [[optimization]] of [[deep neural network controller]]s.
[[We]] propose a [[data-driven model]] leveraging the concept of [[meta-learning]] to [[maximize]] the [[expected influence]] [[in the long run]] .
[[We]] propose a [[dynamic model]] that predicts the [[growth dynamics]] and [[structural properties of conversation thread]]s.
[[We]] propose a [[dynamic topic model]] for [[monitoring]] [[temporal evolution]] of [[market competition]] by jointly leveraging [[tweet]]s and their associated [[image]]s.
[[We]] propose a [[factor model]], named [[Multiple Similarities Collaborative Matrix Factorization (MSCMF)]], which [[projects drug]]s and [[target]]s into a common [[low-rank feature space]], which is further consistent with [[weighted similarity matrice]]s over [[drug]]s and those over [[target]]s.
[[We]] propose a [[fast]] and [[effective method]], [[CatchSync]], which exploits two of the [[tell-tale signs left in graph]]s by fraudsters: (a) [[synchronized behavior]]: [[suspicious node]]s have [[extremely similar]] [[behavior pattern]], because they are often required to perform some [[task together]] (such as follow the same [[user)]]; and (b) [[rare behavior]]: their [[connectivity pattern]]s are very different from the majority.
[[We]] propose a [[framework]] for [[cross-media semantic understanding]] which contains [[discriminative modeling]], [[generative modeling]] and [[cognitive modeling]] .
[[We]] propose a [[framework]] for rigorously studying [[this question]], taking a [[computational perspective]] .
[[We]] propose a [[framework]] to [[cluster alert]]s and [[incident ticket]]s based on the [[text]] in them, using [[unsupervised machine learning]] .
[[We]] propose a [[framework]] under which the [[problem of extracting]] such [[broad latent aspects]] reduces to that of [[optimizing]] a [[formal]] [[objective function]] under [[constraints]] on the [[total number]] of [[aspect]]s the [[system]] can [[store]], and the [[number]] of [[aspect]]s that can be shown in [[response]] to any given [[query]] .
[[We]] propose a [[framework]] which integrates several critical elements that make up the [[user context]], namely, the [[user's short-term behavior]], [[semantic knowledge from ontologies]] that provide [[explicit representation]]s of the [[domain of interest]], and [[long-term user profile]]s revealing [[interest]]s and [[trend]]s.
[[We]] propose a general [[GRF framework]] that is able to (1) [[tune]] the original [[ranking function]] based on [[user feedback]] and (2) further enrich the [[query]] itself by [[mining]] new [[feature]]s from [[user feedback]] .
[[We]] propose a general, [[nonlinear mixed effects model]] for [[repeated measures data]] and define [[estimators]] for its [[parameters]] .
[[We]] propose a [[general-purpose framework]] that [[systematically]] addresses [[data]] - and [[model-parallel challenge]]s in [[large-scale ML]], by leveraging several fundamental [[properti]]es underlying [[ML program]]s that make them different from [[conventional operation-centric program]]s: [[error tolerance]], [[dynamic structure]], and [[nonuniform convergence]]; all stem from the [[optimization-centric nature]] shared in [[ML programs]]' mathematical [[definition]]s, and the [[iterative-convergent behavior]] of their [[algorith]]mic [[solution]]s.
[[We]] propose a [[general solution]] that is able to exploit [[multiple type]]s of [[context]]s without [[arbitrary manipulation]] of the [[structure of classical topic model]]s.
[[We]] propose a [[generative model]] for [[text]] and other collections of [[discrete data]] that [[generalize]]s or [[improve]]s on several previous [[model]]s including [[naive Bayes]]/[[unigram]], [[mixture of unigrams]] [6], and [[Hofmann's aspect model]], also known as [[probabilistic latent semantic indexing (pLSI)]] [3].
[[We]] propose a [[graph-based semantic model]] for [[representing document content]] .
[[We]] propose a [[hierarchical algorithm]] to [[robustly detect]] the [[time window]]s where such [[attack]]s are likely to have happened.
[[We]] propose a [[LADP (Latent Action Diffusion Path) model]] to incorporate the [[information diffusion model]] with the [[model]] of [[external trend]]s, and then design an [[EM-based algorithm]] to infer the [[diffusion probabiliti]]es, the [[external trend]]s and the [[source]]s of [[event]]s [[efficiently]] .
[[We]] propose a [[large-scale]] [[data mining approach]] to <i>[[learning]]</i> [[word-word relatedness]], where known <i>pairs</i> of related [[words impose <i>constraint]]s</i> on the [[learning process]] .
[[We]] propose a [[linear program (LP)]] [[relaxation]], which is shown to achieve a [[guaranteed]] [[approximation error bound]] .
[[We]] propose a [[measure]] for the exceptionality of [[regression model]]s ([[Cook's distance]]), and explore the possibilities to avoid having to [[fit]] the [[regression model]] to each [[candidate]] [[subgroup]] .
[[We]] propose a [[method]] based on a [[generative adversarial network]] that learns to [[discover relation]]s between different [[domain]]s ([[DiscoGAN)]] .
[[We]] propose a [[method]] based on [[Generalized Linear Dynamic Model]]s that [[model]]s the [[probability of mortality]] as a [[latent state]] that [[evolves over time]] .
[[We]] propose a [[method for discovering]] the [[dependency relationship]]s between the [[topics of documents]] shared in [[social network]]s using the [[latent social interaction]]s, [[attempting]] to answer the [[question]]: <i>given a [[seemingly new topic]], from where does [[this topic evolve]]</i>?
[[We]] propose a method for improving [[access]] to [[scientific literature]] by analyzing the content of [[research paper]]s beyond [[citation link]]s and [[topic tracking]] .
[[We]] propose a [[methodology]] for creating [[structured summaries of information]], which we call [[zoomable metro map]]s.
[[We]] propose a method that [[learn]]s to assign [[MR]]s to a [[wide range of text]] (using a [[dictionary]] of more than 70,000 [[word]]s mapped to more than 40,000 [[entiti]]es) thanks to a [[training scheme]] that combines [[learning]] from [[knowledge base]]s (e.g. [[WordNet]]) with [[learning]] from [[raw text]] .
[[We]] propose a [[method]] to [[train]] a [[cascade of classifiers]] by simultaneously [[optimizing]] all its [[stages]] .
[[We]] propose a [[model of belief]] and [[intention change]] over the [[course of a dialogue]], in the case where the [[decisions taken]] during the [[dialogue]] affect the possibly [[conflicting goals]] of the [[agent]]s involved.
[[We]] propose an [[active learning algorithm]] for [[spectral clustering]] that [[incrementally measure]]s only those [[similariti]]es that are most likely to remove [[uncertainty]] in an [[intermediate clustering solution]] .
[[We]] propose an [[algorithm]] that, at any moment in the [[time]] and by [[crawl]]ing a small portion of the [[graph]], provides an [[estimate]] of the [[PageRank]] that is close to the [[true]] [[PageRank]] of the [[graph]] at that moment.
[[We]] propose an [[algorithm]] to [[mine]] <i>multiple</i> diverse, relevant, and [[interesting text snippet]]s for [[image]]s on the [[web]] .
[[We]] propose an alternative, [[data centric, approach]] that relies on actual [[measurements]] of [[climate]] [[observations]] and [[human]] and [[natural]] [[forcing factors]] .
[[We]] propose an [[approach]] to [[automatic]]ally and [[simultaneous]]ly determine both the relevant [[feature]]s and the relevant [[temporal point]]s that impact the current [[outcome]] of the [[dependent variable]] .
[[We]] propose an [[approximation scheme]] for [[kernel k-mean]]s, [[termed approximate kernel k-mean]]s, that reduces both the [[computational complexity]] and the [[memory requirement]]s by employing a [[randomized approach]] .
[[We]] propose and [[analyze]] a [[block minimization framework]] for [[data]] [[larger than]] the [[memory size]] .
[[We]] propose and evaluate the applicability of [[classification]] to [[detect]] [[provenance-similarity]] .
[[We]] propose a [[near-optimal solution]] to the underlying [[optimization problem]], which leverages the [[submodularity property]] of the [[objective function]] .
[[We]] propose an [[efficient]] [[approximate algorithm]] to select a [[near-optimal subset]] of the [[query result]]s that [[minimize]]s the [[expected user effort]] .
[[We]] propose an [[efficient]] <i>[[data-dependent]]</i> yet differentially [[private transit data sanitization approach]] based on a <i>[[hybrid-granularity]]</i> [[prefix tree structure]] .
[[We]] propose an [[efficient method]] that uses the [[Voronoi partitioning]] of the space by the [[object]]s and a [[nearest-neighbor oracle]] .
[[We]] propose an [[efficient solution]] by [[modeling networked data]] as a [[mixture model]] composed of [[multiple]] [[normal communities]] and a [[set]] of [[randomly generated outliers]] .
[[We]] propose an [[entity-centric topic-based opinion summarization framework]], which aims to [[produce]] [[opinion summari]]es in accordance with [[topic]]s and remarkably [[emphasizing]] the [[insight]] behind the [[opinion]]s.
[[We]] propose a [[network]] [[anomaly detection]] [[method]] which [[resolves]] the above two [[task]]s in a [[unified way]] .
[[We]] propose a [[new approach]] referred to as [[<math>k</math>-support anonymity]] to protect each [[sensitive item]] with <math>k</math>-1 other [[item]]s of [[similar support]] .
[[We]] propose a new [[clustering algorithm]] called [[CURE]] that is more robust to [[outlier]]s, and [[identifies clusters]] having [[non-spherical shape]]s and wide [[variances in size]] .
[[We]] propose a new [[deterministic approach]] to [[coreference resolution]] that combines the [[global information]] and precise features of modern [[machine-learning model]]s with the [[transparency]] and modularity of deterministic, [[rule-based system]]s.
[[We]] propose a new [[dimensionality reduction]] [[framework]] that involves the [[learning]] of a [[mapping function]] that [[project]]s [[data point]]s in the original [[high-dimensional space]] to [[latent point]]s in a [[low-dimensional space]] that are then used directly to construct a [[graph]] .
[[We]] propose a new [[framework]] for [[estimating generative models]] via [[adversarial net]]s, in which [[we]] simultaneously [[train]] two [[model]]s: a [[generative model]] G that captures the [[data distribution]], and a [[discriminative model]] D that [[estimates the probability]] that a [[sample came]] from the [[training data]] rather than G.
[[We]] propose an [[online topic model]] for [[sequentially analyzing]] the [[time evolution]] of [[topic]]s in [[document collections]] .
[[We]] propose a [[nonparametric HMM]] that extends traditional [[HMM]]s to structured and [[non-Gaussian continuous distribution]]s.
[[We]] propose an [[ontology-based approach]], [[named Saupodoc]], aiming to perform this particular [[annotation process]] by combining several [[approach]]es.
[[We]] propose a [[non-uniform item sampler]] to overcome [[this problem]] .
[[We]] propose a novel [[active learning strategy]] that exploits [[a priori]] [[domain knowledge]] provided by the [[expert]] (specifically, [[labeled features]]) and extend [[this model]] via a [[Linear Programming algorithm]] for [[situations]] where the [[expert]] can provide [[ranked labeled features]] .
[[We]] propose a novel [[algorithm]] that directly [[mine]]s the [[set of block]]s.
[[We]] propose a novel [[approach]] to detect [[heat pump]]s that utilizes [[low granularity smart meter data]], [[prior sales data]] and [[weather data]] .
[[We]] propose a [[novel approach]] to improve the [[sensitivity]] of [[user engagement metric]]s (that are widely used in [[A/B test]]s) by utilizing [[prediction]] of the [[future behavior]] of an [[individual user]] .
[[We]] propose a novel [[automatic feature selection algorithm]] that selects [[robust features]] based on [[our proposed heuristic]]: [[conservative mean]] .
[[We]] propose a novel [[Co-Training method]] for [[statistical parsing]] .
[[We]] propose a novel [[data mining]] [[approach]] to [[study]] the multi-year movement of [[individual]] [[companies]] .
[[We]] propose a novel [[framework]] for [[stable]] [[feature]] selection which first identifies consensus [[feature]] [[groups]] from subsampling of [[training samples]], and then performs [[feature]] selection by treating each consensus [[feature]] [[group]] as a single [[entity]] .
[[We]] propose a [[novel framework]] for [[this problem]] based on [[our recent]] results on [[sampling theory]] for [[graph signal]]s.
[[We]] propose a novel [[framework]] in which [[privacy protection]] is pushed to [[data provider site]] .
[[We]] propose a novel [[framework]], which [[model]]s the [[behavior]] of [[news]] and [[social media]] in response to [[event]]s as a [[convolution]] between [[event's importance]] and [[media response function]], specific to [[media]] and [[event type]] .
[[We]] propose a novel [[indexing scheme]] called <i>[[Selective Hashing]]</i>, where a [[disjoint]] [[set of indice]]s are built with different [[granulariti]]es and each [[point]] is only [[stored]] in the most [[effective index]] .
[[We]] propose a novel [[latent factor model]] to [[accurately predict]] response for [[large scale]] [[dyadic data]] in the presence of [[features]] .
[[We]] propose a novel [[method]] called [[ROPSM-Growth]] to [[mine ROPSM patterns]] .
[[We]] propose a[[novel method]] to [[automatically acquire]] a [[term-frequency-based taxonomy]] from [[a corpus]] using an [[unsupervised method]] .
[[We]] propose a novel [[model]] called [[Regularized Consensus Maximization (RCM)]], which is [[formulate]]d as an [[optimization problem]] to combine the [[maximum consensu]]s and large [[margin principle]]s.
[[We]] propose a novel [[model]] for [[reasoning across component]]s of [[Big Data Pipeline]]s in a [[probabilistic]]ally well-founded manner.
[[We]] propose a novel [[model]] which captures the [[co-evolution]] of [[social]] and [[affiliation networks]] .
[[We]] propose a novel [[probabilistic model]] to capture the [[semantic region]]s where [[people]] [[post message]]s with a [[coherent topic]] as well as the [[pattern]]s of movement between the [[semantic region]]s.
[[We]] propose a novel [[ranking algorithm]], [[DivRank]], based on a [[reinforced random walk]] in an [[information network]] .
[[We]] propose a [[novel solution]] to preserve the [[joint distribution]] of a [[high-dimensional dataset]] .
[[We]] propose a novel [[statistical framework]] for [[disaggregation]] on [[coarse granular smart meter reading]]s by [[modeling]] [[fixture characteristic]]s, [[household behavior]], and [[activity correlation]]s.
[[We]] [[propose]] a [[novel]] [[statistical method]] that [[models]] the the [[popularity of events over time]], taking into consideration the [[burstiness]] of [[user]] [[interest]], [[information diffusion]] on the [[network structure]], and the [[evolution]] of [[textual topics]] .
[[We]] propose a novel [[structured sparsity-inducing norm]]s based [[feature learning model]] to integrate the [[multi-dimensional visual descriptor]]s for [[Drosophila]] [[gene expression pattern]]s [[annotation]]s.
[[We]] propose a novel [[unsupervised learning framework]] to [[model activities]] and [[interactions]] in [[crowded]] and [[complicated scene]]s.
[[We]] propose a novel [[worker model]] to characterize the [[annotating behavior]] on [[data batch]]es, and present how to [[train]] the [[worker model]] on [[annotation data set]]s.
[[We]] propose an [[unsupervised learning technique]] for [[extracting information about author]]s and [[topic]]s from [[large text collection]]s.
[[We]] propose a [[principled approach]] to [[filter out]] [[erroneous match]]es based on a [[probabilistic model of price]]s.
[[We]] propose a [[principled]] [[cluster ensemble framework]] for combining [[individual]] [[clustering solutions]] based on the [[consensus partition]] .
[[We]] [[propose a principled method]] for [[adaptive diversification]] of [[query result]]s that [[minimize]]s the [[user effort]] to find the [[desired result]]s, by [[dynamically balancing]] the [[relevance]] and [[diversity]] at each [[query step]] (e.g.
[[We]] propose a [[probabilistic model]], called <i>[[multimodal latent binary embedding (MLBE)]]</i>, to [[learn]] [[hash function]]s from [[multimodal data]] [[automatically]] .
[[We]] propose a [[quadratic optimization approach]] that directly utilizes [[heritability]] as an [[objective]] during the [[derivation of quantitative trait]]s of a [[disease]] .
[[We]] propose a [[robust framework]] to [[jointly]] perform two key [[modeling task]]s involving [[high dimensional data]]: (i) [[learning]] a [[sparse functional mapping]] from multiple [[predictor]]s to multiple [[response]]s while taking advantage of the [[coupling]] among [[response]]s, and (ii) [[estimating]] the [[conditional dependency structure]] among [[response]]s while adjusting for their [[predictor]]s.
[[We]] propose a [[semi-supervised AL]] approach for [[sequence labeling]] where only [[highly uncertain]] [[subsequence]]s are presented to [[human annotator]]s, while all others in the selected sequences are [[automatically labeled]] .
[[We]] propose a simple [[box objectness score]] that measures the [[number of edges]] that exist in the [[box]] minus those that are members of [[contours]] that [[overlap]] the [[box's boundary]] .
[[We]] propose a simple [[mathematical model]] for the generation of [[basic]] [[conversation structures]] and then refine [[this model]] to take into account the [[identities]] of each [[member of the conversation]] .
[[We]] propose a [[stochastic algorithm]] for [[collapsed variational Bayesian inference]] for [[LDA]], which is simpler and more [[efficient]] than the [[state of the art method]] .
[[We]] propose a [[strategy]] to [[train]] a [[binary classifier]] to [[predict]] which [[action]] will be more beneficial for a given [[query]] .
[[We]] propose a [[strongly performing method]] that [[scale]]s to such [[dataset]]s by [[simultaneously learning]] to [[optimize]] [[precision]] at <em>k</em> of the [[ranked list]] of [[annotation]]s for a given [[image]] <em>and</em> [[learning]] a [[low-dimensional joint embedding space]] for both [[image]]s and [[annotation]]s.
[[We]] propose a [[technique]] for [[bias correction]] that significantly improves [[annotation quality]] on two tasks.
[[We]] propose a [[two-phase semiautomatic system]] that [[extracts accurate relational metadata]] while minimizing [[user effort]] .
[[We]] propose a [[unified framework]], [[CoupledLP]], to solve [[the problem]] .
[[We]] propose a unified [[neural network architecture]] and [[learning algorithm]] that can be applied to various [[natural language processing task]]s including [[part-of-speech tagging]], [[chunking]], [[named entity recognition]], and [[semantic role labeling]] .
[[We]] propose a [[uniqueness-based interestingness measure]] for [[one-of-the-few claim]]s that is intuitive for [[non-technical user]]s, and we [[design algorithm]]s for finding all [[interesting claim]]s (across all [[subspace]]s) from a [[dataset]] .
[[We]] propose [[Confluence model]] to formalize the [[effect]]s of [[social conformity]] into a [[probabilistic model]] .
[[We]] proposed an enhancement to the [[association rule mining]] where the [[algorithm]] only [[discover]]s [[association rule]]s with [[ROI]]s that form an [[approximately]] [[contiguous volume]] .
[[We]] propose disconnecting the [[process]] of finding [[shapelet]]s from the [[classification algorithm]] by proposing a [[shapelet transformation]] .
[[We]] propose [[efficient]] [[heuristic algorithm]]s that improve upon [[simple greedy algorithm]]s by incorporating the notion of [[phantom event]]s and by using [[look-ahead estimation]] .
[[We]] propose [[exact algorithm]]s for calculating the [[market confidence]] and the [[expected cost]] with <i>O</i> (<i> n</i> log<sup>2</sup> <i>n </i>) [[time cost]] in a <i>[[Wise Market]]</i> with <i>n</i> [[investor]]s.
[[We]] propose five new [[feature selection strategies]] for [[rank-order space]] and assess their [[selective superiorities]] .
[[We]] propose four new [[methods]] to accurately determine [[word sense]] [[dominance]] using [[raw text]] and a [[published thesaurus]] .
[[We]] propose [[GBASE]], a [[scalable]] and general [[graph management]] and [[mining system]] .
[[We]] propose here a simple [[local-first approach]] to [[community discovery]], able to [[unveil the modular organization]] of [[real complex network]]s.
[[We]] propose [[Information-theoretic Meta-clustering (ITMC)]], a [[formalization]] of [[model-based]] [[clustering]] principled by the [[theory]] of lossy [[data compression]] .
[[We]] propose <i>[[RolX (Role eXtraction)]]</i>, a [[scalable]] ([[linear]] in the [[number of edge]]s), [[unsupervised learning approach]] for automatically [[extracting]] [[structural role]]s from general [[network data]] .
[[We]] propose [[LMMH]], a novel [[log-linear modeling method]] that [[scales]] to [[massive data]] [[applications]] with [[billion]]s of [[training records]] and several [[million]] potential [[predictors]] in a [[map-reduce framework]] .
[[We]] propose [[Marble]], a novel [[sparse non-negative tensor factorization method]] to derive [[phenotype candidate]]s with virtually no [[human supervision]] .
[[We]] [[propose method]]s based on [[Gibbs sampling]] and an [[EM algorithm]] to [[estimate the model's parameter]]s and [[fit]] [[our model]] to [[real social network]]s.
[[We]] propose [[MetricForensics]], a [[scalable framework]] for [[analysis of volatile graphs]] .
[[We]] propose natural, [[simple heuristic]]s and compare their [[performance]] to [[hashing]] and [[METIS]], a fast, [[offline heuristic]] .
[[We]] propose [[network comprehension]] as a key [[problem]] for the [[KDD community]], where the [[goal]] is to [[create explainable representations]] of [[complex]] [[biological networks]] .
[[We]] propose novel [[multi-armed bandit]] [[(explore / exploit) scheme]]s to [[maximize]] [[total clicks]] on a [[content module]] published regularly on [[Yahoo!]]
[[We]] propose [[penalized likelihood methods]] for estimating the [[concentration matrix]] in the [[Gaussian graphical model]] .
[[We]] propose [[Rank-Loss Support Instance Machine]]s, which [[optimize]] a [[regularized]] [[rank-loss objective]] and can be instantiated with different [[aggregation model]]s connecting [[instance-level prediction]]s with [[bag-level prediction]]s.
[[We]] propose [[ReFeX (Recursive Feature eXtraction)]], a novel [[algorithm]], that [[recursive]]ly combines [[local (node-based) feature]]s with [[neighborhood (egonet-based) feature]]s; and [[outputs regional feature]]s - capturing [["behavioral" information]] .
[[We]] propose [[Rubik]], a [[constrained non-negative tensor factorization]] and [[completion method]] for [[phenotyping]] .
[[We]] propose several [[heuristic]]s to deal with each [[case]], aiming to solve [[missing value]]s, when [[partial knowledge]] is available, and to capture the strongest [[semantic evidence]] that results in the most [[accurate]] [[similarity assessment]], when dealing with [[overlapping knowledge]] .
[[We]] propose solutions for scenarios when the [[opponent's strategy]] is [[known]] or [[unknown]] and [[available]] or [[unavailable]] for [[training]] .
[[We]] [[propose solutions]] to this [[problem]] involving [[large-scale]] [[learning methods]] that leverage [[features]] drawn from [[ad]] [[creatives]] in addition to their [[keyword]]s and [[landing page]]s.
[[We]] propose [[SPARFA-Trace]], a new [[machine learning-based framework]] for [[time-varying learning]] and [[content analytics]] for [[educational application]]s.
[[We]] propose [[supervised]] and [[semi-supervised variation]]s of [[this problem]] and postulate a [[constrained tensor decomposition formulation]] and a corresponding [[alternating least squares solver]] that is [[easy to implement]] .
[[We]] propose the [[CISVM method]], a [[support vector machine]], to work with [[cost interval information]] .
[[We]] propose the first [[monitoring algorithm]] for [[multivariate regression model]]s of [[distributed data stream]]s that [[guarantees a bounded model error]] .
[[We]] propose the [[FMA]] as a [[reference ontology]] in [[biomedical informatics]] for correlating different views of anatomy, aligning existing and [[emerging ontologi]]es in [[bioinformatics ontologi]]es and providing a [[structure-based template]] for representing biological [[function]]s.
[[We]] propose the [[notion]] of Ï - <i>[[visible [[MCE]]</i> to achieve this goal and [[design algorithm]]s to realize the notion.
[[We]] propose [[ThermoCast]], a novel [[thermal forecasting model]] to [[predict]] the [[temperature]]s surrounding the [[server]]s in a [[data center]], based on [[continuous stream]]s of [[temperature]] and [[airflow measurement]]s.
[[We]] propose the [[Social Event Organization (SEO) problem]] as one of [[assigning]] a [[set of event]]s for a group of [[user]]s to attend, where the [[user]]s are [[socially connected]] with each other and have [[innate levels of interest]] in those [[event]]s.
[[We]] propose the [[Tree-structured Boltzmann Machine (T-RBM)]], a novel [[two-stage Markov Network]], as [[our solution]] .
[[We]] propose the [[WhoAmI method]], a [[Double Dependent-Variable Factor Graph Model]], to address [[this problem]] by considering not only the effects of [[feature]]s on gender / [[age]], but also the [[interrelation]] between [[gender]] and [[age]] .
[[We]] propose three efficient approximations to solve the problem: a [[greedy algorithm]], an <i>[[approximate popularity maximization]]</i> ([[APM]] for short), and <i>[[approximate annotation-benefit maximization]]</i> ([[AAM]] for short).
[[We]] propose to address these [[incorrect purchase]]s by leveraging the [[huge volume]]s of [[data]] that [[traveller]]s create as they move about the [[city]], by providing, to each of [[them]], [[personalised ticket]] [[recommendation]]s based on their [[estimate]]d [[future travel pattern]]s.
[[We]] propose to apply the [[tree-structural regularized graphical model]] to [[estimate]] the [[mouse brain network]] .
[[We]] propose to augment standard [[keyword search]] with [[user feedback]] on [[latent topic]]s.
[[We]] propose to [[compress]] [[weighted graphs (network)s]], motivated by the [[observation]] that [[large network]]s of [[social]], [[biological]], or other relations can be [[complex]] to handle and [[visualize]] .
[[We]] propose to construct [[binary code]]s for [[user]]s and [[item]]s such that the [[preference of user]]s over [[item]]s can be [[accurate]]ly preserved by the [[Hamming distance]] between their respective [[binary code]]s.
[[We]] propose to develop a [[semantic visualization model]] that [[approximate]]s [[L2-normalized data]] directly.
[[We]] propose to employ the [[accelerated gradient descent]] to efficiently solve the [[optimization problem]] in [[rMTFL]], and show that [[the proposed algorithm]] is [[scalable]] to [[large-size problem]]s.
[[We]] propose to employ the general [[projected gradient scheme]] to efficiently [[solve]] such a [[convex surrogate]]; however, in the [[optimization formulation]], the [[objective function]] is [[non-differentiable]] and the [[feasible domain]] is [[non-trivial]] .
[[We]] propose to [[jointly optimise]] both [[component]]s in an [[EM fashion]] with high [[efficiency]] to help the [[meta-bidder]] successfully catch the [[transient statistical arbitrage]] [[opportuniti]]es in [[RTB]] .
[[We]] propose to leverage the [[machine learning]] and [[statistical method]]s to [[train]] the [[winning price model]] from the [[bidding history]] .
[[We]] propose to optimize a [[larger class]] of [[loss functions for ranking]], based on an [[ordered weighted average (OWA)]] ([[Yager, 1988]]) of the [[classification loss]]es.
[[We]] propose to solve [[this problem]] in its [[dual form]], which is [[convex]] and [[smooth]] .
[[We]] propose treating [[the data]] as indication of [[positive]] and [[negative preference]] associated with vastly varying [[confidence level]]s.
[[We]] propose treating the [[data]] as indication of [[positive]] and [[negative preference]] associated with [[vastly varying confidence level]]s.
[[We]] propose two [[approximation algorithm]]s for identifying [[communiti]]es in [[dynamic social network]]s.
[[We]] propose two [[clustering schemes]] based on [[equivalence class]]es and [[maximal hypergraph clique]]s, and study two [[lattice traversal technique]]s based on [[bottom-up]] and [[hybrid search]] .
[[We]] propose two [[efficient algorithms]], which discover [[frequent patterns]] in <i>[[bottom-up]]</i> and [[<i>top-down</i> manners]] .
[[We]] propose two [[methods for incremental inference]], based respectively on [[sampling]] and [[variational technique]]s.
[[We]] propose two new methods, <i>[[k-regression]]</i> and <i>[[tree-regression]]</i>, to partition the entire [[collection of frequent itemsets]] in order to [[minimize]] the [[restoration error]] .
[[We]] propose two novel [[algorithm]]s for [[large-scale]] [[OCCF]] that allow to [[weight]] the [[unknowns]] .
[[We]] propose two novel [[hierarchical clustering approach]]es which can generate [[block]]s within a specified [[size range]], and we present a [[penalty function]] which allows [[control]] of the [[trade-off]] between [[block quality]] and [[block size]] in the [[clustering process]] .
[[We]] propose two [[novel method]]s for [[topical video representation]] .
[[We]] propose two novel [[modification]]s to standard [[neural net training]] that address [[challenge]]s and exploit [[properti]]es that are peculiar, if not [[exclusive]], to [[medical data]] .
[[We]] propose using [[random walk models]] for [[resolving identity uncertainty]] since they have proven [[effective]] for [[finding points]] which are [[proximately located]] in a [[network]] .
[[We]] prove that <b> [[RecMax]] </b> is not only [[NP-hard]] to [[solve optimally]], it is [[NP-hard]] to even [[approximate]] within any [[reasonable factor]] .
[[We]] prove that, by [[updating]] the [[linear model]] in the [[dual form]], [[the proposed method]] fully utilizes the [[data]] in [[memory]] and [[converge]]s to a [[globally optimal solution]] on the entire [[data]] .
[[We]] prove that, despite the [[bias]] and [[weak guarantee]] from [[ANN query]], [[the proposed algorithm]] has [[global convergence]] to the [[solution defined]] on entire [[dataset]], with [[sublinear complexity]] each [[iteration]] .
[[We]] prove that [[our methods]] meet the new [[utility criterion]]; [[we]] also demonstrate the performance of [[our algorithm]]s through extensive [[experiment]]s on the [[transaction data sets]] from the [[FIMI repository]] .
[[We]] prove that [[prominent streak]]s are a [[subset]] of [[LPS]]s and the number of [[LPS]]s isthe [[length]] of a [[data sequence]], in [[comparison]] with the [[quadratic number]] of [[candidate]]s produced by a [[brute-force]] [[baseline method]] .
[[We]] prove that selecting the [[optimal marginal]]s with the goal of [[minimizing error]] is [[NP-hard]] and, thus, design an [[approximation algorithm]] using an [[integer programming]] [[relaxation]] and the [[constrained concave-convex procedure]] .
[[We]] prove that the [[approximation lead]]s to an [[identical solution]] to the [[exact optimization problem]] but at a [[fraction]] of the [[optimization cost]] .
[[We]] prove that [[this algorithm]] stays localized in a large [[graph]] and has a [[worst-case]] [[constant runtime]] that depends only on the [[parameter]]s of the [[diffusion]], not the [[size of the graph]] .
[[We]] prove that turning [[high degree nodes]] into [[sinks]] results in only a [[small]] [[approximation error]], while greatly improving [[running times]] .
[[We]] prove that two [[classic models]] in [[information diffusion]] and [[text burstiness]] are special cases of [[our model]] under certain [[situations]] .
[[We]] prove that [[well-established]] [[definition]]s of [[cluster]]s in [[2-domain information network]]s such as [[formal concept]]s, [[maximal bi-clique]]s, and [[noisy binary tile]]s can always be [[represented as]] [[Nash equilibrium point]]s.
[[We]] provide a [[benchmark]] of the different [[estimator]]s showing their [[correlation]] with [[business metric]]s observed by [[running online A/B test]]s on a [[large-scale]] [[commercial recommender system]] .
[[We]] provide additional [[experiment]]s on [[benchmark dataset]]s which demonstrate that [[SiGMa]] can outperform [[state-of-the-art]] [[approach]]es both in [[accuracy]] and [[efficiency]] .
[[We]] provide a [[diagnostic tool]] for [[quantitatively]] assessing the [[comprehension burden]] that a [[textbook impose]]s on the reader due to [[non-sequential presentation]] of [[concept]]s.
[[We]] provide algorithms that use this [[dominance relation]] to identify the [[set of]] [[most preferred collection]]s.
[[We]] provide [[analytical result]]s generated over billions of [[sample]]s and demonstrate that adding [[multimedia feature]]s can significantly improve the [[accuracy]] of [[click prediction]] for new [[ad]]s, compared to a [[state-of-the-art]] [[baseline model]] .
[[We]] provide an [[empirical evaluation]] of [[our algorithm]] on several [[real-world matching data set]]s that demonstrates the [[effectiveness]] of [[our approach]] .
[[We]] provide an [[experimental study]] of the role of [[syntactic parsing]] in [[semantic role labeling]] .
[[We]] provide an extensive [[experimental validation]] of [[our approach]] for the following [[applications]]: [[classification]], [[top-k motif search]], and [[top-k nearest-neighbor queries]] .
[[We]] provide a novel [[algorithm]] to [[approximately]] [[factor]] large [[matrice]]s with millions of [[row]]s, millions of [[column]]s, and billions of [[nonzero element]]s.
[[We]] provide a [[theoretical analysis]] on [[the method]]'s [[complexity]] and show how it outperforms [[state-of-the-art]] [[privacy preserving blocking technique]]s with respect to both [[recall]] and [[processing cost]] .
[[We]] provide a thorough [[theoretical analysis]] of [[our approach]] and show [[experimentally]] that, other things being [[equal]], it produces more [[sensitive OEC]] than the [[average]] .
[[We]] provide [[efficient]], [[scalable method]]s with [[theoretical guarantee]]s for [[generating map]]s.
[[We]] provide [[empirical evidence]] on [[text corpora]] to demonstrate the success of [[DTM]] in terms of [[classification accuracy]] and [[robustness to parameters]] compared to [[state-of-the-art]] [[technique]]s.
[[We]] provide [[experimental result]]s on [[real]] [[traffic log]]s that demonstrate the [[effectiveness]] of [[the proposed model]] .
[[We]] provide [[performance bound]]s for [[Active Vote]] in both a [[batch]] and [[on-line model]] of [[active learning]] .
[[We]] provide [[qualitative result]]s that explain how [[CoFactor]] improves the [[quality]] of the [[inferred factor]]s and characterize the [[circumstance]]s where it provides the most significant [[improvement]]s.
[[We]] provide [[results]] from extensive [[experimental studies]] to explore the [[efficacy]] of [[our approach]] and point out potential [[applications]] of such [[signatures]] for many different [[types]] of [[networks]] including [[social]] and [[information networks]] .
[[We]] provide [[rigorous theoretical analysis]] of [[NoNClu]]s in terms of its [[correctness]], [[convergence]] and [[complexity]] .
[[We]] provide strong [[guarantee]]s on the [[performance]] of [[GP-SELECT]] and apply it to three [[real-world]] [[case studi]]es of [[industrial relevance]]: (1) [[Refreshing]] a [[repository]] of [[price]]s in a [[Global Distribution System]] for the [[travel industry]], (2) [[Identifying diverse]], [[binding-affine peptide]]s in a [[vaccine design task]] and (3) [[Maximizing click]]s in a [[web-scale recommender system]] by [[recommending item]]s to [[user]]s.
[[We]] provide surprising [[insights]] into [[group formation]] based on observations in several [[real-world]] [[networks]], showing that [[users]] often join groups for reasons other than their [[friends]] .
[[We]] provide [[theoretical justification]] such as [[estimation consistency]] for [[the proposed estimator]] .
[[We]] provide two [[optimization technique]]s to solve [[this problem]] .
[[We]] pursue a similar approach with a richer kind of [[latent variable]] -- [[latent feature]]s -- using a [[nonparametric Bayesian technique]] to [[simultaneously infer]] the number of [[feature]]s at the same time we [[learn]] which [[entiti]]es have each [[feature]] .
[[We]] [[qualitatively analyze]] the trained [[RNN Encoder–Decoder]] by comparing its [[phrase score]]s with those given by the [[existing]] [[translation model]] .
[[We]] quantify the [[external influence]]s over [[time]] and describe how these [[influence]]s affect the [[information adoption]] .
We quantitatively analyze a number of key [[social principle]]s and [[theori]]es that [[correlate]] with [[social role]]s and [[status]]es.
[[We]] rank each [[point]] on the basis of its [[distance]] to its <i>k<sup>th</sup></i> [[nearest neighbor]] and [[declare]] the top <i>n</i> [[point]]s in this [[ranking]] to be [[outlier]]s.
We [[ranked]] [[annotators]] according to median success in terms of [[inter-annotator agreement]] (as measured by [[Cohen’s (Cohen, 1960) kappa]]) both [[within their groups]] and for a [[paper]] [[common across groups]] .
[[We]] ran numerous [[experiment]]s on [[real graph]]s, [[spanning billion]]s of [[node]]s and [[edge]]s, and [[we]] show that our proposed [[GBASE]] is indeed [[fast]], [[scalable]] and [[nimble]], with significant savings in [[space]] and [[time]] .
We read about objects ranging from [[cell phone]]s and [[video poker]] to [[prosthetic eye]]s, from [[Web site]]s and [[television]] to [[dialysis machine]]s.
[[We]] re-evaluate the [[state of the art]] for [[object recognition]] from [[small image]]s with [[convolutional network]]s, [[questioning]] the necessity of different [[component]]s in the [[pipeline]] .
[[We]] refer to the [[result]] as [[topic dynamics]], permitting a [[hierarchical]], [[expressive]] [[model of bursts]] as [[intervals]] of [[increasing]] [[momentum]] .
[[We]] refer to this [[learning problem]] as [[unsupervised transfer classification]] .
[[We]] refer to [[this problem]] as the [[setwise classification problem]] .
[[We]] [[reformulate]] the existing [[SCVB0 inference]] by using the [[stochastic divergence minimization algorithm]], with which [[convergence]] can be analyzed in terms of [[Martingale convergence theory]] .
[[We]] release a [[dataset]] of 65646 [[StarCraft]] [[replay]]s that contains 1535 million [[frame]]s and 496 million [[player action]]s.
[[We]] release three new [[datasets of lexical vector representation]]s [[trained]] on the [[BNC]] and [[our evaluation dataset for hyponym generation]] .
[[We]] report an extensive [[performance evaluation]] on three [[ML algorithm]]s on varying [[data]] and [[cluster size]]s.
We report encouraging [[experimental result]]s on 3 [[task]]s with different characteristics: [[political affiliation detection]], [[ethnicity identification]] and [[detecting affinity]] for a particular [[business]] .
[[We]] report [[experimental results]] on several [[standard]] [[dense]] and [[sparse datasets]] that validate [[the proposed approach]] .
[[We]] report [[experiment]]s on [[real graph]]s from various [[domain]]s with over 1M [[edge]]s, where [[ReFeX]] outperforms its [[competitor]]s on typical [[graph mining task]]s like [[network classification]] and [[de-anonymization]] .
[[We]] report its performance in terms of [[precision]] and [[recall]] using the [[MusicBrainz]] and [[DBpedia dataset]]s.
[[We]] report on [[experiment]]s in the [[context]] of [[discovering link]]s in [[real biological network]]s, a demonstration of the practical [[usefulness of the approach]] .
We report on experiments over a 9,000,000 [[Web page]] [[corpus]] that compare [[TEXTRUNNER]] with [[KNOWITALL]], a [[state-of-the-art]] [[Web IE system]] .
[[We]] report on our experience with [[this approach]] through simulated and [[real-world annotation]]s in the domain of [[immunogenetics]] for [[NE annotation]]s.
[[We]] report on three applications - an [[e-journal]], a [[hybrid]] [[Web/face-to-face conference]], and a [[national]] [[policy document]] - where we have used [[D3E]] to promote [[critical discourse]] between [[participant]]s.
[[We]] report our findings on several [[real graphs]], including one of the largest [[publicly available]] [[Web graphs]], thanks to [[Yahoo!]], with &asymp; 6,7 billion [[edges]] .
We report results in [[document modeling]], [[text classification]], and [[collaborative filtering]], comparing to a [[mixture]] of [[unigrams model]] and the [[probabilistic LSI model]] .
[[We]] report some [[experiment]]s on [[real-world dataset]]s to demonstrate the superiority of [[BNMTF]] over other related [[matrix factorization method]]s.
[[We]] report [[state-of-the-art result]]s for [[identifying]] [[discourse]] vs. [[non-discourse]] usage and [[human-level performance]] on [[sense disambiguation]] .
[[We]] report the [[Gene Normalization (GN) challenge in BioCreative III]] where participating [[team]]s were asked to [[return]] a [[ranked list of identifier]]s of the [[gene]]s detected in [[full-text article]]s.
[[We]] report the results of multiple [[studi]]es that explore the [[relative utility]] of the different [[information source]]s and [[extraction method]]s.
... We represent the [[probabilistic knowledge base]] as a [[matrix]] with [[entity-entity pair]]s in the [[row]]s and [[relation]]s in the [[column]]s
[[We]] represent these [[event data]] as a [[tensor of count]]s and develop [[Bayesian Poisson tensor factorization]] to [[infer]] a [[low-dimensional]], [[interpretable representation]] of their [[salient pattern]]s.
[[We]] require a [[domain expert]] to specify an <math>L \times L</math> [[matrix]] of [[pairwise probabilistic constraints]] expressing their [[beliefs]] as to whether the <math>L</math> [[classes]] should be kept [[separate]], [[merged]], or [[split]] .
[[We]] require the [[vector field]] to be close to the [[gradient field]] of the [[ranking function]], and the [[vector field]] to be as [[parallel]] as possible.
[[We]] [[review evidence]] indicating that [[attitude]]s and [[abiliti]]es are among the most powerful [[predictor]]s of [[technology use]] .
[[We]] review techniques from many [[disparate]] and [[disconnected application domain]]s that address each of these [[formulation]]s.
[[We]] review [[this setting]] before [[continuing]] with [[our model]] which does not require a given [[tree structure]] .
[[We]] run [[experiment]]s on three [[real-world]] [[social network]]s, showing that [[our algorithm]] is [[effective]] and [[scalable]] .
[[We]] run [[simulation]]s on [[classic benchmark testing graph]]s, and [[quite surprisingly]], [[the results]] show that the [[trial and error dynamics]] even outperforms the [[Louvain method]] (a popular [[modularity maximization approach]]) if [[individual]]s have [[dense connection]]s within [[communiti]]es.
We say two [[synonym set]]s are <i>[[semantically coherent]]</i> if their [[intersection]] is not [[empty]] .
[[We]] see [[Spark SQL]] as an [[evolution]] of both [[SQL-on-Spark]] and of [[Spark itself]], offering richer [[API]]s and [[optimization]]s while keeping the benefits of the [[Spark programming model]] .
[[We]] [[share lesson]]s on why these two qualities are critical, how to [[measure]] these two [[qualities of metrics of interest]], how to develop [[metric]]s with clear [[directionality]] and [[high sensitivity]] by using [[approach]]es based on [[user behavior model]]s and [[data-driven calibration]], and how to choose the right [[goal metric]]s for the [[entire online service]]s.
[[We]] share [[our experience]] building a [[general framework]] at [[Google]] for [[estimating]] the number of [[user]]s behind [[IP]]s, called hereinafter the sizes of the [[IP]]s.
[[We]] show a considerable improvement in [[predictive log-likelihood]] and [[RMSE]] compared to existing [[state-of-the-art method]]s, and finish by using [[dropout]]'s [[uncertainty]] in [[deep reinforcement learning]] .
[[We]] show [[Active Vote]] can achieve an [[order of magnitude]] [[decrease]] in the number of [[labeled instance]]s over various [[passive learning algorithm]]s such as [[Support Vector Machine]]s.
[[We]] show a [[formal recovery result]] for the [[noiseless case]] and present a [[detailed study]] of [[the algorithm]] on [[synthetic data]] and [[Netflix rating]]s.
[[We]] show [[analytical]]ly and [[experimental]]ly that an [[outlier detector]] based on a [[subsample]] [[per se]], besides [[inducing diversity]], can, under certain [[condition]]s, already improve upon the results of the same [[outlier detector]] on the complete [[dataset]] .
[[We]] show a performance comparison of [[supervised HMM]], [[MaxEnt]], [[SVM]], and [[CRF]] for [[this task]] .
[[We]] show both [[analytically]] and [[empirically]] that the performance of [[approximate kernel k-mean]]s is similar to that of the [[kernel k-means algorithm]], but with dramatically reduced [[run-time complexity]] and [[memory requirement]]s.
[[We]] showcase exemplary [[projection]]s of the resulting [[segment]]s to display the [[interpretability]] of the [[solution]]s.
[[We]] showcase the [[capabilities]] of [[BGP-lens]] by identifying surprising [[phenomena]] verified by [[syadmins]], over a [[massive]] [[trace]] of [[BGP]] [[updates]] spanning 2 [[years]], from the [[publicly available]] [[site]] http://datapository.net.
[[We]] showcase the utility of the [[database]] through two [[web-scale application]]s: (a) [[augmentation of image]]s on the [[web]] as [[webpage]]s are browsed and (b) ~an [[image browsing experience]] (similar in spirit to [[web browsing]]) that is enabled by [[interconnecting semantically]] related [[image]]s (which may not be visually related) through [[shared concept]]s in their corresponding [[text snippet]]s.
[[We]] show [[empirically that]], although the [[sample-based algorithm]]s require more [[iteration]]s, their [[lower cost per iteration]] can lead to dramatically faster [[convergence]] in various [[game]]s.
[[We]] show [[empirical]]ly that [[delta-relevance]] leads to a considerable [[reduction]] of the amount of [[returned pattern]]s.
[[We]] show empirically that our [[interpreter]] is able to effectively leverage different [[levels of prior program structure]] and [[learn]] [[complex transduction task]]s such as [[sequence sorting or addition]] with substantially less [[data]] and better [[generalisation]] over [[problem size]]s.
[[We]] show [[experimental results]] that demonstrate the importance of both [[implicit]] [[group relationships]] and [[interaction-based affinity ranking]] in [[suggesting friends]] .
[[We]] show for a [[large number]] of [[public real-world dataset]]s that these new [[algorithm]]s are fast to [[execute]] and their [[pattern collection]]s outperform previous [[approach]]es both in [[unsupervised]] as well as [[supervised context]]s.
[[We]] show how [[CFTP]] can be used to avoid any [[super-linear preprocessing]] and [[memory requirement]]s.
[[We]] show how different [[setting]]s for [[training]] and applying [[the model]]s, as well as the introduction of [[domain knowledge]] may dramatically [[influence]] both the [[absolute and the relative performance]]s of the different [[algorithm]]s.
[[We]] show how many existing and new [[constraint]]s for [[hierarchical clustering]], can be modeled as a [[Horn-SAT problem]] that is easily solvable in [[polynomial time]] and which allows their [[implementation]] in any number of [[declarative language]]s or [[efficient]] [[solver]]s.
[[We]] show how [[NIMBLE]] can be used to realize [[scalable]] [[implementation]]s of [[ML-DM algorithm]]s and present a [[performance evaluation]] .
[[We]] show how such [[signatures]] can be inferred by [[formulating]] the [[data mining problem]] as one of [[feature selection]] in [[rank-order space]] .
[[We]] show how [[the model]] distances used in the [[kernel]] can be calculated [[analytically]] or efficiently [[estimated]] .
[[We]] show how the resulting [[NP-hard]] [[global optimization problem]] could be [[efficiently approximately solved]] via [[spectral relaxation]] and [[semi-definite programming technique]]s.
[[We]] show how these [[problem]]s can be [[mitigated]] if a [[graph of relationship]]s between [[feature]]s is known.
[[We]] show how this [[factorization]] can be seamlessly combined with [[explicit feature]]s or [[side-information]] for [[page]]s and [[ad]]s, which let us combine the benefits of both [[approach]]es.
[[We]] show how to apply [[our method]] to two [[state-of-the-art]] [[recommender model]]s: [[matrix factorization]] and [[adaptive kNN]] .
[[We]] show how to ensure that this [[property]] continues to hold as new [[item]]s are added to the [[set]] thus requiring a richer [[class]] of [[ranking]] [[policies]] .
[[We]] show how to perform [[probabilistic inference]] by [[averaging]] over the [[inference]]s of [[multiple]] [[belief networks]] .
[[We]] show how to [[programmatically model process]]es that humans use when [[extracting]] [[answers to queri]]es (e.g., [[" Who invented typewriter]]?
[[We]] show how to use [[neighborhood communiti]]es to quickly generate a [[small set]] of [[seed]]s.
[[We]] show in extensive [[experiment]]s on [[real]] and [[synthetic data]] that [[CMM]] is a [[robust measure]] for [[stream clustering evaluation]] .
[[We]] show on a [[large collection]] of [[graph dataset]]s that our [[heuristic]]s are a significant [[improvement]], with the best obtaining an [[average gain]] of 76%.
[[We]] show on [[real-world data]] that we can limit the [[ERM]] [[computation]]s to a [[fraction]] of [[instance]]s with comparable [[performance]] .
[[We]] show [[our method]] has a great potential for providing [[in-depth analyse]]s by clearly [[identifying]] [[common]] and [[discriminative topic]]s among [[multiple]] [[document set]]s.
[[We]] show [[our model]] induces a [[stochastic process]] on the [[dyadic space]] with [[kernel]] ([[covariance]]) given by a [[polynomial function]] of [[features]] .
[[We]] show [[our model]] is able to [[detect bursts]] for [[MeSH term]]s [[accurately]] as well as [[efficiently]] .
[[We]] show [[qualitatively]] and [[quantitatively]] that our [[model]] can construct and [[annotate]] [[graphs]] of [[relationships]] and make useful [[predictions]] .
[[We]] show results obtained from applying this [[system]] at two major [[US]] [[health insurance compani]]es indicating significant reduction in [[claim audit cost]]s and [[potential saving]]s of $20-$26 million / year making the [[insurance provider]]s more [[efficient]] and lowering their [[operating cost]]s.
[[We]] show that a [[deep connection]] exists between [[ROC space]] and [[PR space]], such that a [[curve]] dominates in [[ROC space]] if and only if it dominates in [[PR space]] .
[[We]] show that an [[aggregate shock]] induces [[negative correlation]] between [[job creation]] and [[job destruction]] whereas a [[dispersion shock]] induces [[positive correlation]] .
[[We]] show that any [[hierarchical domain knowledge]] can be easily [[represented by]] [[relative constraint]]s.
[[We]] show that [[approximate mining]] yields interesting [[pattern]]s in several [[real-world graph]]s ranging from [[IT]] and [[protein interaction network]]s to [[protein structure]]s.
[[We]] show that [[Bayesian PMF model]]s can be [[efficiently]] [[trained]] using [[Markov chain Monte Carlo method]]s by applying them to the [[Netflix dataset]], which consists of over [[100 million]] [[movie]] [[rating]]s.
[[We]] show that combining [[SAME]] with [[factored sample]] [[representation]] (or [[approximation]]) gives throughput competitive with the [[fast]]est [[symbolic method]]s, but with potentially better [[quality]] .
[[We]] show that compared to [[existing]] [[method]]s, [[our approach]] is able to achieve improved [[AUC]] and [[F1]] with significantly [[lower]] [[running time]] .
[[We]] show that comparing with [[state-of-the-art adaptive]] and [[non-adaptive SA method]]s, [[lower regret]]s and [[faster rate]]s can be achieved under [[low-variation assumption]]s.
[[We]] show that [[computation]] and [[memory]] for [[training]] these [[multi-space model]]s can be efficiently [[parallelized]] over many [[node]]s of a [[cluster]] .
[[We]] show that [[dropout]] improves the performance of [[neural network]]s on [[supervised learning task]]s in [[vision]], [[speech recognition]], [[document classification]] and [[computational biology]], obtaining [[state-of-the-art result]]s on many [[benchmark data set]]s.
[[We]] show that [[easily accessible]] [[digital records of behavior]], [[Facebook Like]]s, can be used to [[automatically and accurately predict]] a range of [[highly sensitive]] [[personal attribute]]s including: [[sexual orientation]], [[ethnicity]], [[religious]] and [[political view]]s, [[personality trait]]s, [[intelligence]], [[happiness]], use of [[addictive substance]]s, [[parental separation]], [[age]], and [[gender]] .
[[We]] show that even though <b> [[RecMax]] </b> is hard to [[approximate]], simple [[natural heuristic]]s may provide [[impressive gain]]s, for [[targeted marketing]] using [[RS]] .
[[We]] show that existing [[fact finder]]s on [[structured data]] can be modeled as specific [[instance]]s of this [[framework]] .
[[We]] show that, for [[sponsored search results]], a substantial portion of the [[change]] in [[CTR]] when [[conditioned]] on [[prior clicks]] is in fact [[due]] to a [[change]] in the [[relevance of results]] for that [[query instance]], not just due to a [[change]] in the [[probability of examination]] .
[[We]] show that [[home advantage]] in [[soccer]] is partly due to the [[conservative strategy]] of the [[away team]] .
[[We]] show that in [[cost]]/[[skew]] [[settings]] where the [[choice]] between [[search]] and [[active labeling]] is equivocal, a [[hybrid strategy]] can combine the [[benefits]] .
[[We]] show that [[inference]] in our [[information extraction scenario]] reduces to solving an [[instance]] of the [[maximum weight subgraph problem]] .
[[We]] show that [[information]] about [[social relationship]]s can be used to improve [[user-level sentiment analysis]] .
[[We]] show that, [[in steady state]], the [[optimal strategy]] can be computed as the [[solution]] of a [[semi-definite program (SDP)]] .
[[We]] show that in three [[real dataset]]s, [[sample]]d from the websites of [[technology]] and [[consulting group]]s and a [[news broadcaster]], [[page repetition]]s occur for the majority as a very specific [[structure]], namely in the form of [[nested palindrome]]s.
[[We]] show that it is able to [[detect change]]s of [[customer group]]s, which correspond to [[change]]s of [[real market environment]]s.
[[We]] show that it is [[NP-complete]] to [[decide the achievability]] of a [[simple upper bound]] on [[maximum reciprocity]], and provide conditions for achieving [[it]] .
[[We]] show that it solves [[problem]]s very effectively in [[tasks]] such as [[information retrieval]], [[regression]] and [[anomaly detection]] .
[[We]] show that [[jointly]] [[parsing]] a [[bitext]] can substantially improve [[parse quality]] on both sides.
[[We]] show that [[label complexity]] of [[our algorithm]] is at [[most log]] <i>n</i> times the [[label complexity]] of the [[black-box]], and also bound the difference in the [[recall]] of [[classifier]] learnt by [[our algorithm]] and the [[recall]] of the [[optimal classifier]] satisfying the [[precision constraint]] .
[[We]] show that [[linearly combining (blending)]] a [[set]] of [[CF algorithm]]s increases the [[accuracy]] and [[outperform]]s any single [[CF algorithm]] .
[[We]] show that [[low level cue]]s such as [[color]] can be used to [[quantify]] [[image similarity]] and also to discriminate among [[product]]s with different [[visual appearance]]s.
[[We]] show that [[<math>\cal Q</math>-learning converge]]s to the [[optimum action-values with probability 1]] so long as all [[action]]s are repeatedly [[sampled]] in all [[state]]s and the [[action-value]]s are [[represented discretely]] .
[[We]] show that [[mobility measure]]s alone yield surprising [[predictive power]], comparable to traditional [[network-based measure]]s.
[[We]] show that [[node]]s with high [[in-degree]] tend to arrive late for new [[trend]]s, while [[user]]s in the top of our [[ranking]] tend to be early [[adopter]]s that also [[influence]] their [[social contact]]s to adopt the new [[trend]] .
[[We]] show that [[our algorithm]] can return [[top-k similar vertice]]s for any [[vertex]] in a [[network]] 300Ã [[fast]]er than the [[state-of-the-art]] [[method]]s.
[[We]] show that [[our algorithm]]s [[outperform]] an array of [[heuristic]]s in terms of their [[effectiveness]] in [[controlling diffusion process]]es, often beating the next best by a significant [[margin]] .
[[We]] show that [[our approach]] performs better than recently proposed [[protein function prediction method]]s on [[composite]] and multiple [[kernel]]s.
[[We]] show that our [[behavior-based approach]] produces [[classifier]]s having performance comparable to that of a [[classifier]] [[trained]] on the [[ground truth data]] .
[[We]] show that our [[congestion model]] is effective in [[modeling]] [[dynamic congestion condition]]s.
[[We]] show that our [[expected-utility]] [[active inference strategy]] also selects [[good example]]s for [[learning]] .
[[We]] show that [[our method]] can [[reliably extract]] [[side-effect]]s and [[filter out]] [[false statement]]s, while [[identifying]] [[trustworthy user]]s that are likely to [[contribute]] valuable [[medical information]] .
[[We]] show that [[our method]]s provide constant [[approximation]]s compared to the [[optimal histogram]]s produced by the [[state-of-the-art]] in the worst case.
[[We]] show that [[our model]] [[reliably predict]]s the [[location]]s and [[dynamics of future human movement]] and gives an [[order of magnitude]] better [[performance than]] present [[model]]s of [[human mobility]] .
[[We]] show that [[our proposed diagnostic]] is effective via an extensive [[empirical evaluation]] on a variety of [[estimator]]s and [[simulated]] and [[real dataset]]s, including a [[real-world query workload]] from [[Conviva, Inc.]] involving 1.7 [[TB]] of [[data]] (i.e., approximately 0.5 billion [[data point]]s).
[[We]] show that our proposed [[DynaMMo]] [[method]] (a) can successFully [[learn]] the [[latent variable]]s and their [[evolution]]; (b) can provide [[high]] [[compression]] for little loss of [[reconstruction accuracy]]; (c) can [[extract]] compact but powerful [[features]] for [[segmentation]], [[interpretation]], and [[forecast]]ing; (d) has [[complexity]] [[linear]] on the duration of [[sequence]]s.
[[We]] show that our proposed [[SPG-GMKL optimizer]] can be an [[order of magnitude]] faster than [[projected gradient descent]] on even [[small]] and [[medium sized dataset]]s.
[[We]] show that [[our system]] results in an [[order of magnitude]] better [[precision (hit rate]]) over [[existing approaches]] which is [[accurate]] enough to potentially result in over $15-25 million in [[savings]] for a typical [[insurer]] .
[[We]] show that [[SEC]] has [[theoretical equivalence]] to [[weighted K-means clustering]] and results in vastly reduced [[algorithmic complexity]] .
[[We]] show that [[seed]]ing can make a [[substantial difference]], if done [[carefully]] .
[[We]] show that simple [[measure]]s of [[communiti]]es can be [[defined]] for [[flow data]] that allow a remarkably effective level of [[intrusion detection]] simply by looking for [[flow]]s that do not respect those [[communiti]]es.
[[We]] show that [[SO-CAL's performance]] is consistent across [[domain]]s and in [[completely unseen data]] .
[[We]] show that some of the [[problem]]s are [[NP-hard]] yet admit [[effective]] [[heuristics]], some of which can exploit [[properties]] of [[social network]]s such as [[link reciprocity]] .
[[We]] show that such a strategy may introduce significant [[bias]] in [[estimate]]s and propose a [[new model]] that ameliorates this issue by [[positing]] <i>[[local]] </i>, [[context-specific factor]]s for [[entiti]]es.
[[We]] show that such [[latent model]]s achieve substantially higher [[accuracy]] than a [[traditional]] [[classification approach]] .
[[We]] show that [[TANGENT]] makes reasonable, yet surprising, [[horizon-broadening]] [[recommendation]]s.
[[We]] show that temporal [[ConvNet]]s can achieve astonishing performance without the [[knowledge of words]], [[phrases]], [[sentence]]s and any other [[syntactic]] or [[semantic structure]]s with regards to a [[human language]] .
[[We]] show that the [[collective classification approach]] of [[RMNs]], and the introduction of [[subgraph patterns]] over [[link labels]], provide significant [[improvements]] in [[accuracy]] over [[flat classification]], which attempts to [[predict each link]] in [[isolation]] .
[[We]] show that the [[dual problem]] of [[the proposed formulation]] is a [[smooth optimization problem]] with a [[piecewise sphere constraint]] .
[[We]] show that the [[fair seed allocation problem]] is [[NP-hard]], and develop an [[efficient algorithm]] called <i>[[Needy Greedy]] </i>.
[[We]] show that [[the inferred latent factor matrices]] capture [[interpretable multilateral relation]]s that both [[conform]] to and [[inform our knowledge]] of [[international a air]]s.
[[We]] show that the [[maximum]] [[reciprocity]] hinges crucially on the [[in-]] and [[out-degree sequence]]s, which may be [[intuitively interpreted]] as [[constraint]]s on some "[[social capacities]]" of [[node]]s and impose fundamental limits on [[achievable]] [[reciprocity]] .
[[We]] show that the [[model]], a [[cascading]] [[non-homogeneous Poisson process]], can be formulated as a [[double-chain]] [[hidden Markov model]], allowing us to use an efficient [[inference algorithm]] to [[estimate]] the [[model parameters]] from [[observed data]] .
[[We]] show that the [[Offset Tree]] is an [[optimal]] [[reduction]] to [[binary classification]] .
[[We]] show that the [[performance]] of [[MA]]s are competitive with the best [[convex]] combination of the [[iterate]]s from the [[base algorithm]]s for [[online]] as well as [[batch convex optimization problem]]s.
[[We]] show that the [[problem]] can be [[formulated]] as an [[integer linear programming (ILP) problem]] .
[[We]] show that the [[problem is intractable]]; therefore, we resort to [[approximation]] and [[heuristic algorithm]]s.
[[We]] show that the problem is [[NP-hard]] and propose a [[randomized algorithm]] with [[approximation guarantee]] [[proportional]] to the [[maximum]] [[degree]] of the [[input]] [[graph]] .
[[We]] show that [[the proposed method]] outperforms [[state-of-the-art method]]s in application to [[density estimation]] and to [[anomaly detection]] .
[[We]] show that the [[risk]] for the two considered [[outcome]]s can be evaluated from [[patients' characteristic]]s and that [[feature]]s of the [[patient-physician match]] improve the [[prediction accuracy]] for the [[treatment's success]] .
[[We]] show that these [[measure]]s are correlated to the [[classical]] [[performance measure]]s of [[constrained clustering algorithm]]s.
[[We]] show that [[these proposed algorithm]]s improve on [[previous automatic procedure]]s and can reach or <i>surpass</i> [[human expert-level optimization]] for many [[algorithm]]s including [[latent Dirichlet allocation]], [[structured SVMs]] and [[convolutional neural networks]] .
[[We]] show that the [[Stackelberg prediction game]] generalizes existing [[prediction model]]s.
[[We]] show that the [[variance]] of [[this estimator]] is controlled by the second [[eigenvalue]] of the [[normalized Laplacian]] of the [[network]] (the [[network structure penalty]]) and the [[correlation]] between [[node degree]]s and the property being measured (the [[effective savings factor]]).
[[We]] show that this [[method]] permits [[classification]] at a [[higher accuracy]] than that obtained with traditional [[voxel selection]], and that the [[set]]s of [[voxel]]s used are more reproducible across [[cross-validation fold]]s than those [[identified]] with [[voxel selection]], and lie in plausible [[brain location]]s.
[[We]] show that [[this model]] significantly improves the [[performance]] over [[MF model]]s on several [[dataset]]s with little additional [[computational overhead]] .
[[We]] show that [[this problem]] is [[NP-Complete]], even if simple [[Naive Bayes Classifier]]s are used for [[tag prediction]] .
[[We]] show that this [[resulting]] [[set]] is very [[small]], both in terms of [[database size]] and in [[number]] of its [[local]] [[relational patterns]] : a reduction of up to [[4]] [[orders of magnitude]] is attained.
[[We]] show that [[TIMECRUNCH]] is able to [[compress]] these [[graph]]s by [[summarizing]] important [[temporal structure]]s and [[finds pattern]]s that agree with [[intuition]] .
[[We]] show that [[two-phase algorithm]] with [[partial correlation-based]] [[CI tests]] can deal with [[continuous data]] following [[arbitrary distributions]] rather than only [[Gaussian distribution]] .
[[We]] show that under extreme [[skew]], even basic [[technique]]s for [[guided learning]] completely dominate [[smart (active) strategies]] for applying [[human resource]]s to [[select cases for labeling]] .
[[We]] show that [[VEWS]] beats [[ClueBot NG]] and [[STiki]], the best known [[algorithm]]s today for [[vandalism detection]] .
[[We]] show that [[WFST]]s provide a common and natural representation for [[hidden Markov models (HMMs)]], [[context-dependency]], [[pronunciation dictionari]]es, [[grammar]]s, and [[alternative recognition output]]s.
[[We]] show that with only some [[information]] about the [[nodes]] themselves in a [[network]], [[we]] get surprisingly high [[accuracy]] of [[label]]s.
[[We]] show the better [[performance]] of [[our algorithm]]s for [[diversity]] and [[coverage maximization]] by running [[experiment]]s on [[real]] ([[Twitter]]) and [[synthetic data]] in the context of [[real-time search]] over [[micro-post]]s.
[[We]] show the correlation between the two [[schemes]], their [[strengths]] and [[weaknesses]] and discuss the benefits of combining a [[rhetorical based analysis]] of the [[papers]] with a [[content]]-based one.
[[We]] show the [[effectiveness]] and [[scalability]] of our [[method]] on [[synthetic]] and [[real-world graph]]s, as compared to both existing [[graph clustering]] and [[outlier detection approach]]es.
[[We]] show the proposed [[probabilistic model]] can be easily adapted for many interesting [[application]]s including [[facet generation]] and [[review annotation]] .
[[We]] show the superiority of [[our algorithm]] with [[performance evaluation]] and comparison with other [[state-of-the-art]] [[learning algorithm]]s.
[[We]] show this [[formulation]] works well in [[controlled experiment]]s where [[supervision is incomplete]], [[superfluous and noisy]] and is able to [[recover]] the underlying [[ground truth network]] .
[[We]] show, through both [[theoretical analysis]] and extensive [[experiment]]s on eleven moderate and [[large benchmark network]]s with various [[sample size]]s, that [[the proposed method]] has much improved [[learning accuracy]] and [[scalability]] compared with ten [[competing algorithm]]s.
[[We]] show through [[experiment]]s that on [[real-world data]] from [[Sina Weibo]], [[our method]] outperforms existing [[off-the-shelf algorithm]]s that do not take [[users' individuality]] or [[social connection]]s into [[account]] .
[[We]] show [[visualization]] results based on [[half a year long]] [[dataset]] of [[Japanese tweet]]s and a [[four months long]] [[collection]] of [[tweet]]s from [[USA]] .
[[We]] [[solve]] the [[problem]] as a [[constrained optimization problem]] over [[multiple]] [[learners]] and show significant [[improvement]] in [[classifying]] [[short]] [[sentence]]s into a [[large]] [[label space of categories]], including previously [[unobserved]] [[categories]] .
[[We]] solve [[this problem]] here, illustrating [[our approach]] with [[data]] collected from the [[portal]]s [[Digg]] (http://digg.com) and [[YouTube]] (http://youtube.com), two well-known examples of popular [[content-sharing-and-filtering service]]s.
[[We]] start by showing that [[MCCFR]] performs the same [[regret update]]s as [[CFR]] on [[expectation]] .
[[We]] start with a [[small number]] of [[exemplar group member]]s -- they may be [[follower]]s of a [[political ideology]] or [[fan]]s of a [[music genre]] -- and need to use those examples to [[discover]] the additional [[member]]s.
[[We]] start with simple [[estimate]]s based on the [[specifications of entities]], and then leverage [[user review]]s and [[product search log]]s to improve the [[estimation]] .
[[Westfall-Young light]] opens the door to significant [[pattern mining]] on [[large dataset]]s that previously involved prohibitive [[runtime]] or [[memory cost]]s.
[[We]] study a large [[subset of data]] from [[CiteULike]], a [[bibliography sharing service]], and show that [[our algorithm]] provides a more [[effective]] [[recommender system]] than traditional [[collaborative filtering]] .
[[We]] study a novel [[clustering problem]] in which the [[pairwise relation]]s between [[object]]s are <i>[[categorical]] </i>.
[[We]] study a [[problem of graph-query reformulation]] enabling [[explorative query-driven discovery]] in [[graph database]]s.
[[We]] study [[discriminative clustering]] for [[market segmentation task]]s.
[[We]] study [[folksonomy learning]] using [[social metadata]] extracted from the [[photo-sharing site]] [[Flickr]], and demonstrate that [[the proposed approach]] addresses the [[challenges]] .
[[We]] [[study]] in [[this paper]] the [[problem of incremental]] [[crawling]] of [[web forums]], which is a very fundamental yet challenging [[step]] in many [[web applications]] .
[[We]] study [[multi-class bandit prediction]], an [[online learning problem]] where the [[learner]] only receives a [[partial feedback]] in each [[trial]] indicating whether the [[predicted class label]] is [[correct]] .
[[We]] study [[online boosting]], the [[task]] of converting any [[weak online learner]] into a [[strong online learner]] .
[[We]] study [[robustness]] with respect to the details of [[graph construction]]s, [[error]]s in [[node labeling]], [[degree variability]], and a variety of other [[real-world heterogeneiti]]es, studying [[these methods]] through a precise [[relationship]] with [[mincut problem]]s.
[[We]] [[study]] several [[variants]] of the [[problem]], show that they are [[NP-hard]], and propose [[efficient]] [[heuristics]] to solve them.
[[We]] [[study]] several [[variant]]s that model different requirements as [[constraint]]s and discuss some of the [[subtleties involved]] .
[[We]] study specifically the power of [[making prediction]]s via a [[hybrid approach]] that combines [[discriminatively trained predictive model]]s with a [[deep neural network]] that models the [[joint statistic]]s of a [[set of]] [[weather-related variable]]s.
[[We]] study the [[accuracy]] of [[evaluation metric]]s used to [[estimate]] the efficacy of [[predictive model]]s.
[[We]] study the impact of [[display advertising]] on [[user search behavior]] using a [[field experiment]] .
[[We]] study the [[interplay]] between a [[dynamic process]] and the [[structure]] of the [[network]] on which it is [[defined]] .
[[We]] study the [[market]] for [[apps on Facebook]], the dominant [[social networking platform]], and make use of a [[rule change]] by [[Facebook]] by which highly [[engaging app]]s were rewarded with further opportunities to engage [[user]]s.
[[We]] study the [[numerical performance]] of a [[limited memory]] [[quasi-Newton method]] for [[large scale optimization]], which we call the [[L-BFGS method]] .
[[We]] study the performance of [[generalized additive models (GAMs)]], which combine [[single-feature model]]s called [[shape function]]s through a [[linear function]] .
[[We]] study the [[performance]] of our [[Manhattan]] and [[Euclidean distance]] (p=1, 2) [[estimator]]s on diverse [[dataset]]s, demonstrating [[scalability]] and [[accuracy]] even when a [[small fraction]] of the [[data]] is [[sampled]] .
[[We]] study the problem of [[active learning]] for [[multilabel classification]] .
[[We]] study the problem of [[approximating]] the [[3-profile]] of a [[large graph]] .
[[We]] study the [[problem of detecting vandals]] on [[Wikipedia]] <i>before</i> any [[human]] or known [[vandalism detection system]] reports flagging potential [[vandal]]s so that such [[user]]s can be presented early to [[Wikipedia administrator]]s.
[[We]] study the [[problem]] of [[learning to predict]] a [[spatio-temporal output sequence]] given an [[input sequence]] .
[[We]] study the problem of [[link prediction]] in <i>[[coupled network]]s</i>, where we have the [[structure information]] of one (source) [[network]] and the [[interaction]]s between this [[network]] and another (target) [[network]] .
[[We]] study the [[problem of selecting]] a [[set]] of [[points of interest (POIs)]] to show on a [[map]] .
[[We]] study this [[interest evolution process]], and the utility accrued by [[recommendation]]s, as a [[function]] of [[the system's recommendation strategy]] .
We study [[this problem]] in a fully [[Bayesian setting]], focusing on the problem of using [[Facebook]] [[user-ID]]s as [[feature]]s, with the [[social network]] giving the [[relationship]] structure.
[[We]] subsequently studied the [[heterogeneous nature]] ([[co-existence]] of both [[online]] and [[offline social interaction]]s) of [[EBSN]]s on two [[challenging problem]]s: [[community detection]] and [[information flow]] .
[[We]] successfully apply [[the proposed method]] to a [[natural language processing (NLP) application]] by improving the [[testing accuracy]] under some [[training]] / [[testing speed requirement]]s.
[[We]] successfully find [[migration motif]]s that confirm existing [[finance theories]] and other [[motif]]s that may lead to new [[financial model]]s.
[[We]] successfully integrate six widely used [[visual descriptor]]s to [[annotate]] the [[Drosophila]] [[gene expression pattern]]s from the [[lateral]], [[dorsal]], and [[ventral]] [[view]]s.
[[We]] suggest that for [[subordinate]], [[noncompetitive group]]s (e.g., [[elderly people]]), the [[positive stereotype of warmth acts jointly]] with the [[negative stereotype]] of [[low competence]] to maintain the advantage of more [[privileged group]]s.
[[We]] suggest that [[micropublication]]s, [[generated by]] useful [[software tool]]s supporting such [[activiti]]es as [[writing]], [[editing]], [[reviewing]], and [[discussion]], will be of great value in improving the [[quality]] and [[tractability]] of [[biomedical communication]]s.
[[We]] summarize the results of an [[empirical study]] that applies our [[method]] to a number of [[2D]] [[synthetic data set]]s, [[consisting of cluster]]s of arbitrary [[shape and scale]], and to [[real multi-dimensional classification example]]s from [[benchmark]]s, including [[image segmentation]] .
[[We]] support our [[position]] by means of a critical [[review of the existing literature]], a substantial [[collection]] of [[carefully]] [[controlled experimental work]], and [[theoretical arguments]] .
[[We]] support this [[power-law assumption]] by providing [[empirical evidence]] that such [[decay law]]s are abundant in [[real-world data set]]s.
[[We]] survey [[technique]]s for [[optimizing]] the various [[step]]s in an [[information extraction pipeline]], adapting to [[dynamic data]], integrating with existing [[entiti]]es and [[handling uncertainty]] in the [[extraction process]] .
[[We]] survey these literature streams: we provide a [[brief introduction]] to the [[historical origin]]s of [[quantitative research]] on [[pricing]] and [[demand estimation]], point to different [[subfield]]s in the area of [[dynamic pricing]], and provide an [[in-depth overview]] of the available [[literature]] on [[dynamic pricing]] and [[learning]] .
[[We]] [[survey]] the use of [[weighted finite-state transducers (WFSTs)]] in [[speech recognition]] .
[[We]] synthesize a [[large feature set]] that includes [[first]] and [[second order time-series feature]]s, [[detrended fluctuation analysis (DFA)]], [[spectral analysis]], [[approximative entropy]], and [[cross-signal feature]]s.
[[We]] synthesize a representative [[feature set]] that describes the [[atmosphere motion]], and apply a [[streaming feature selection algorithm]] to [[online identify]] the [[precipitation precursor]]s from the enormous [[feature space]] .
[[We]] systematically review recent [[empirical research]] on [[factor]]s that influence [[trust in automation]] to present a [[three-layered trust model]] that synthesizes [[existing knowledge]] .
[[We]] systematically study how the [[network characteristic]]s reflect the [[social situation]]s of [[user]]s in an [[online society]] .
[[We]] tackle that challenge in [[this paper]] by first developing [[experiment methodology]] for [[quantifying]] [[long-term user learning]] .
[[We]] tackle the challenging problem of [[mining]] the simplest [[Boolean pattern]]s from [[categorical dataset]]s.
[[We]] tackle these [[challenges]] by developing a [[method]] for [[tracing paths]] of [[diffusion]] and [[influence]] through [[networks]] and [[inferring the networks]] over which [[contagions]] [[propagate]] .
[[We]] tackle this [[issue]] by introducing an additional [[model]] that captures the [[conversion delay]] .
[[We]] [[tested our methods]] on several [[synthetic]] and [[real data sets]], and [[evaluated]] its [[performance]] by [[internal]] or [[external]] [[accuracy indice]]s.
[[We]] [[test]] [[MetricForensics]] on [[three]] [[real-world graphs]]: an [[enterprise]] [[IP trace]], a [[trace]] of [[legitimate]] and [[malicious network traffic]] from a [[research institution]], and the [[MIT Reality Mining proximity sensor data]] .
[[We]] test [[our algorithm]] on [[network]]s from three different [[domain]]s: a [[social network]], a [[network]] of [[English word]]s that appear [[adjacent]]ly in a [[novel]], and a [[marine food web]] .
We test [[our algorithm]]s on [[synthetic]] and [[real data]] from the domains of [[protein-interaction network]]s, [[social media]], and [[bibliometric]]s.
We [[test our approach]] on both [[synthetic]] and [[real world dataset]]s.
[[We]] test [[our implementation]] in several [[experiments scaling]] up to 640 cores on [[Amazon EC2]] .
[[We]] test [[our method]]s on three [[large dataset]]s, including two very [[sparse one]]s, in [[static]] and [[dynamic condition]]s.
We test our [[model]] on three [[data set]]s, performing [[unsupervised topic classification]] and [[link prediction]] .
[[We]] test [[our technique]] on [[part of speech tagging]] and show performance gains for [[varying amounts of source]] and [[target training data]], as well as [[improvement]]s in [[target domain]] [[parsing accuracy]] using our improved [[tagger]] .
We test [[SCRAM]] based on a [[large number]] of [[historical taxi trajectori]]es and [[validate]] the [[recommendation fairness]] and [[driving efficiency of SCRAM]] with [[extensive evaluation]]s.
We [[test]] the [[approach]] on the task of [[recognizing]], in [[PubMed abstracts]], [[experimentally validated]] [[subcellular localization relations]] that have been [[curated]] by [[biomedical researchers]] .
[[We]] test the [[kernel matrix approximation accuracy]] for several different [[bandwidth]]s and [[dataset]]s.
[[We]] [[test the proposed algorithm empirically]] on [[real-world graph]]s and show that it outperforms popular [[approach]]es for [[active learning]] and [[bandit problem]]s as well as [[truncated lookahead]] of a few [[step]]s.
[[We]] test the proposed [[citation-LDA]] on two [[dataset]]s: the [[ACL Anthology Network (AAN)]] of [[natural language research literature]]s and [[PubMed Central (PMC]]) archive of [[biomedical]] and [[life sciences literature]]s, and demonstrate that [[Citation-LDA]] can effectively discover the [[evolution of research theme]]s, with better [[formed topic]]s than (conventional) [[Content-LDA]] .
We [[test]] the resulting [[system]] on four [[datasets]], three of which are [[medium-sized text collection]]s, while the fourth is a [[large-scale]] [[user]]/[[movie]] [[dataset]] .
[[We]] then address the [[question]], can [[we]] [[predict]] [[bounce rate]] by [[analyzing]] the [[features]] of the [[advertisement]]?
[[We]] then analyze how the [[ontologi]]es of a [[sample]] of [[smart city application]]s fit into the overall [[Smart City Ontology]], the [[consistency]] between [[digital space]]s, [[knowledge processe]]s, city [[domain]]s targeted by the [[application]]s, and the types of innovation that determine their impact.
[[We]] then analyze [[sentiment]]s by [[transfer]]ring [[user bias]]es to [[textual feature]]s.
[[We]] then apply [[0-bit CWS]] for [[building]] [[linear classifier]]s to [[approximate]] [[min-max kernel]] [[classifier]]s, as extensively [[validated]] on a wide range of [[public dataset]]s.
[[We]] then apply both the [[bagged logistic model]] and the [[probabilistic model]] to a [[real-world data set]] from a [[multi-channel advertising campaign]] for a well-known [[consumer software]] and [[services brand]] .
[[We]] then apply the model to [[predict]] [[trust]] and [[distrust]] between [[raters]] and [[review contributors]] using a [[real data-set]] .
[[We]] then apply [[this methodology]] to ads shown on [[Google search]], more specifically, to [[determine]] and [[quantify]] the drivers of [[ads blindness]] and [[sightedness]], the [[phenomenon of user]]s changing their [[inherent propensity]] to [[click on or interact with ad]]s.
We then apply this [[model]] to two [[e-mail]] [[data sets]] consisting of [[404]] and [[6,164]] [[users]], respectively, that were collected from two [[universities]] in different [[countries]] and [[years]] .
[[We]] then apply this on the [[data]] from the [[Gutenberg project]] to measure the [[number]] and [[size]] of the obtained [[longest substring]]s.
[[We]] then compare our modifications of [[HUWRS]] (called [[HUWRS.IP]]) to other [[state of the art]] [[algorithm]]s, concluding that [[HUWRS]] .
[[We]] then construct novel features for both [[ad]]s and [[user]]s based on [[our definition]] on [[psychological desire]] and incorporate them into the [[learning framework]] of [[click prediction]] .
[[We]] then define [[weak consistency]] for a [[model construction algorithm]] and show a [[simple algorithm]] that is [[weakly consistent]] .
[[We]] then demonstrate empirically the [[performance]] of [[deterministic leverage score sampling]], which many [[times match]]es or [[outperform]]s the [[state-of-the-art technique]]s.
[[We]] then demonstrate the implementation of [[COPE]] and report [[experimental result]]s running on [[real]] [[commercial platform]] to demonstrate the [[practical value]] of [[this system]] .
[[We]] then derive [[HL-MRF]]s by generalizing this unified [[objective]] .
[[We]] then describe a novel [[friend suggestion algorithm]] that uses a [[user]]'s [[implicit]] [[social graph]] to generate a [[friend group]], given a [[small]] [[seed set]] of [[contacts]] which the [[user]] has [[already labeled]] as [[friends]] .
[[We]] then develop a [[bagged logistic regression model]], which [[we]] show achieves a comparable [[classification accuracy]] as a usual [[logistic regression]], but a much more [[stable estimate]] of [[individual advertising channel]] [[contribution]]s.
[[We]] then develop a [[computationally efficient]] [[updating procedure]] to [[update]] the [[probability simplex vector]] in <i>O</i> (<i> m</i> <sup>2</sup>) time, [[irrespective]] of the [[data size]] <i>n </i>.
[[We]] then [[empirically validate]] [[our model]] using [[economic game experiment]]s.
[[We]] then [[estimate]] empirical [[Type I error rate]]s for each [[bootstrap method]] .
[[We]] then [[evaluate the proposed algorithm]] on a [[large-scale]] [[LBSN dataset]] .
[[We]] then exploit [[this decomposition]] to discover crucial knowledge on how [[pattern]]s in [[GSN]] affect [[aquatic invasion]]s, and then illustrate how such [[knowledge]] can be used to devise [[effective]] and [[economical invasive species management strategi]]es.
[[We]] then extend our [[combinatorial solution]] to any [[convex function]] by solving a [[convex cost]] [[flow]] .
We then introduce an [[optimization framework]] based on <i>[[Factor Conditioning Symmetry]] </i>, and [[we]] propose a [[probabilistic model]] to integrate the [[optimization framework]] on [[local structural information]] as well as [[network influence]] to infer the [[unknown]] [[social role]]s and [[statuses of online user]]s.
We then introduce [[variational method]]s, which exploit [[laws of large numbers]] to transform the original [[graphical model]] into a [[simplified]] [[graphical model]] in which [[inference]] is [[efficient]] .
[[We]] then [[observe]] that the [[privacy]]-[[utility]] [[tradeoff]] in [[data publishing]] is similar to the [[risk]]-[[return]] [[tradeoff]] in [[financial investment]], and propose an integrated [[framework]] for considering [[privacy]]-[[utility]] [[tradeoff]], borrowing [[concept]]s from the [[Modern Portfolio Theory]] for [[financial investment]] .
[[We]] then perform [[supervised binary classification]] on this [[labeled dataset]] in order to discover additional [[identifying]] [[patterns of behavior]] typical of [[labeled positive]]s in the [[population]] .
[[We]] then point out several key [[function]]s that help achieve a [[win-win situation]] between [[job seeker]]s and [[enterprise]]s for a successful [[recruiting system]] .
[[We]] then propose a [[hierarchical optimization algorithm]] for [[learning]] the [[coefficient]]s and [[kernel bandwidth]]s in an integrated way.
[[We]] then propose an [[efficient algorithm]] to solve [[this problem]] by [[block coordinate descent]] .
[[We]] then propose an [[efficient algorithm]] which not only utilizes [[social tags]] as enriched [[semantic features]] for the [[objects]], but also [[infers the categories]] of [[unlabeled objects]] from both [[homogeneous]] and [[heterogeneous]] [[labeled objects]], through the [[implicit]] [[connection]] of [[social tags]] .
[[We]] then propose three new [[user browsing models]], which attribute [[CTR]] [[changes]] solely to [[changes]] in [[relevance]], solely to [[changes]] in [[examination]] (with an [[enhanced model]] of [[user behavior]]), or to both [[changes]] in [[relevance]] and [[examination]] .
[[We]] then prove that with a [[sufficiently large]] [[unlabeled sample]], one is able to [[find classifiers]] with [[low incompatibility]] .
[[We]] then provide a [[fast]] and effective [[near-linear-time]] (in [[node]]s and [[edge]]s) [[algorithm COARSENET]] for the same.
[[We]] then provide a [[no-regret algorithm]] which can quickly [[learn]] a [[user's preference]]s from limited [[feedback]] .
[[We]] then show how [[graph cluster randomization]] admits an [[efficient exact algorithm]] to [[compute]] the [[probabiliti]]es for each [[vertex]] being [[network exposed]] under several of these [[exposure condition]]s.
[[We]] then show how to formulate [[constrained hierarchical clustering]] in a [[flexible manner]] so that any [[number of algorithm]]s, whose [[output]] is a [[dendrogram]], can make use of the [[constraint]]s.
[[We]] then show that for [[real fMRI data]] our approach can reproduce well known results in [[neurology]] regarding the [[default mode network]] in [[resting-state healthy]] and [[Alzheimer]] affected [[individual]]s.
[[We]] then show the [[effectiveness]] of [[our algorithm]]s through exhaustive [[simulation studi]]es on [[real data set]]s.
[[We]] then specialize [[SSGD]] to obtain a new [[matrix-factorization algorithm]], called [[DSGD]], that can be [[fully distributed]] and run on [[web-scale dataset]]s using, e.g., [[MapReduce]] .
[[We]] then [[systematically apply]] and evaluate a [[serie]]s of established [[data mining method]]s, including [[forward feature selection]], [[linear]] and [[nonlinear classification algorithm]]s, and [[exploratory undersampling]] for [[class imbalance]] .
[[We]] then use a modification of the [[NetCover method]] to [[infer]] the [[underlying network]] .
[[We]], then, use a [[similarity query]] [[example]] on [[real]] [[tropical cyclone event]] [[data sequence]]s from [[2000]] to [[2008]] to discuss (i) a [[problem of scientific interest]], and (ii) [[challenges]] and [[issues]] related to the [[weather event]] [[similarity search problem]] .
[[We]] then validate [[the results]] and our claims by conducting [[experiment]]s on [[synthetic]] and [[real industrial data]] obtained from diverse [[domain]]s.
[[We]] [[theoretical]]ly demonstrate that two commonly [[observed properti]]es of [[social network]]s, [[heavy-tailed degree distribution]]s and [[large clustering coefficient]]s, imply the existence of [[vertex neighborhood]]s (also known as [[egonet]]s) that are themselves good [[communiti]]es.
[[We]] [[theoretically prove]] that [[our method]] is [[very accurate]], and [[experimentally confirm]] that it is three [[orders of magnitude]] faster than previous [[method]]s.
[[We]] [[theoretically]] prove that the [[sensitivity]] of such [[inference]] is only [[O (log n)]], where n is the [[number of vertice]]s in a [[network]] .
[[We]] [[theoretically]] show that [[RCM]] has a smaller [[upper bound]] on [[generalization error]] compared to the version without [[regularization]] .
We therefore present [[SparseLDA]], our [[new algorithm]] and [[data structure]] that substantially improves [[sampling]] [[performance]] .
[[We]] therefore propose [[SEC]], an efficient [[Spectral Ensemble Clustering method]] based on [[co-association matrix]] .
[[We]] [[tracked]] 1.6 million [[mainstream media]] [[sites]] and [[blogs]] over a period of three [[months]] with the total of 90 million [[articles]] and [[we]] find a [[set]] of [[novel]] and [[persistent]] [[temporal patterns]] in the [[news]] [[cycle]] .
[[We]] [[trained a large]], [[deep convolutional neural network]] to classify the 1.2 million [[high-resolution image]]s in the [[ImageNet LSVRC-2010 contest]] into the [[1000]] different [[classe]]s.
[[We]] [[train]] [[the model]] to [[learn the translation probability]] of an [[English phrase]] to a corresponding [[French phrase]] .
[[We]] translate our [[intuition]]s and expectations about how [[the system]] should behave into [[invariant]]s, the [[truth of which]] can be [[verified from data]] emitted by [[the system]] .
[[We]] treat this as a [[supervised learning problem]], [[trained]] using [[networks of products]] derived from [[browsing]] and [[co-purchasing log]]s.
[[We]] unite [[three approach]]es from the [[randomized algorithm]]s, [[probabilistic graphical model]]s, and [[fuzzy logic communiti]]es, showing that all three lead to the same [[inference objective]] .
We use a [[Horvitz-Thompson construction]] in [[conjunction]] with a [[scheme]] that <i>[[sample]]s</i> [[arriving edge]]s <i>without</i> [[adjacenci]]es to previously [[sampled edge]]s with [[probability]] <i>p</i> and <i>holds</i> [[edges with adjacencies]] <i>with</i> [[probability]] <i>q </i>.
"[[We]] use a [[large dataset voluntarily contributed]] by the [[member]]s of [[Norton Community Watch]], consisting of [[partial list]]s of the [[file]]s that exist on their [[machine]]s, to identify [[close relationship]]s between [[file]]s that often appear together on machines.
[[We]] use a [[method]] from [[medical decision theory]] for eliciting the [[relative costs]] of [[false positives]] and [[false negatives]] from the [[domain expert]], constructing a [[utility measure]] of [[classification performance]] that integrates the [[expert preference]]s.
We use a [[real-world data set]] and [[employ]] a promising [[research approach]] combining [[econometric]] with [[predictive modeling technique]]s in a [[causal estimation]] [[framework]] that allows for more [[accurate]] [[counterfactual]]s.
[[We]] use a similar [[idea]] to design an [[approximation algorithm]] for the general [[case]] where some [[individuals]] are possibly [[unobserved]] at [[times]], and to show that the [[approximation]] [[factor]] increases twofold but remains a [[constant]] regardless of the [[input]] size.
[[We]] use a [[vertical database layout]] to [[cluster]] [[related transactions]] together.
[[We]] use [[cluster analysis]] to help [[auditor]]s focus their [[effort]]s when [[evaluating]] [[group life insurance]] [[claim]]s.
We use [[collapsed Gibbs sampler]] for [[inference]] to [[recover]] local [[DPM]]s and [[identify]] their [[class association]]s.
We use [[data]] from [[German]] for [[illustration purpose]]s, [[the NEGRA corpu]]s, a [[linguistically annotated collection]] of [[newspaper article]]s from the [[Frankfurter Rundschau]] .
We used the [[CoreSC annotation scheme]] and the [[semantic annotation tool]] [[SAPIENT]] ([[Liakata et al., 2009]]) to construct a [[corpus]] of 265 [[annotated papers]] ([[Liakata and Soldatova, 2009]]) from [[physical chemistry]] and [[biochemistry]] .
[[We]] used [[this model]] to [[predict]] the 5 [[personality factor]]s in a separate [[sample]] of 4, 824 [[Facebook user]]s, [[examining (a) convergence]] with [[self-reports of personality]] at the [[domain - and facet-level]]; (b) [[discriminant validity]] between [[predictions of distinct trait]]s; (c) [[agreement with informant reports of personality]]; (d) [[patterns of correlations with external criteria]] (e.g., [[number of friend]]s, [[political attitude]]s, [[impulsiveness]]); and (e) [[test-retest reliability]] over [[6-month interval]]s.
[[We]] use features extracted from the [[OverFeat network]] as a generic image representation to tackle the diverse range of [[recognition task]]s of [[object image classification]], [[scene recognition]], [[fine grained recognition]], [[attribute detection]] and [[image retrieval]] applied to a [[diverse set]] of [[dataset]]s.
We use <i>[[synonym set]]</i> to represent the [[metadata]] of [[product schema]]s.
[[We]] use [[mean reciprocal rank]] to [[evaluate ShoppingAdvisor]], and show how the [[performance]] increases by more than 50% along the [[path]] from [[root]] to [[leaf]] .
[[We]] use [[Metropolis sampling]] based on [[local]] [[swaps]] to achieve this.
We use [[multi-dimensional]] [[KS distance]] to compare the [[generated graph]]s to the [[observed graph]]s and the results show [[mKPGM]]s are able to produce a closer [[match]] to [[real-world graph]]s (10-90% [[reduction]] in [[KS distance]]), while still providing [[natural variation]] in the [[generated graph]]s.
[[We]] use [[optimal coupling]]s to [[rigorously establish]] that this yields a "[[good]]" [[similarity measure]] in the [[BBS sense]] for two well known [[families of graph]]s.
We use our [[sensor]]s to [[assess the state of the world]], our [[brain]]s to [[think]] and [[choose action]]s to [[achieve objective]]s, and our [[bodi]]es to [[execute]] those [[action]]s.
We use [[policy discontinuiti]]es at [[state border]]s to identify the [[effects of]] [[minimum wages]] on [[earnings]] and [[employment]] in [[restaurant]]s and other [[low-wage sector]]s.
We use [[random walk]] with restart to reduce [[data sparseness]], rely on [[co-clustering]] to group [[queri]]es and [[Web page]]s, and exploit [[page similarity]] to improve [[matching precision]] .
[[We]] use [[Situation Calculus]] to model the [[evolution of the world]] and an [[observation model]] to analyze the [[evolution of intentions]] and [[belief]]s.
[[We]] use [[supervised learning]] to classify the [[HTML table]]s and [[HTML list]]s within a [[web page]] as [[product specification]] or not.
We use the many [[tag]]s present in our [[del.icio.us]] [[dataset]] to [[quantitatively]] demonstrate the new models' [[higher correlation]] with [[human relatedness score]]s over several [[strong]] [[baseline]]s.
[[We]] use these [[feature]]s to identify [[matching product]]s across different [[online shop]]s and [[enrich product ad]]s with the [[extracted data]] .
[[We]] use these [[metric]]s to create [[structured summarie]]s of [[information]], which we call <i>[[metro map]]s </i>.
We use the [[TextRunner system]] [1] to [[extract tuples from text]], and then [[induce]] [[general concepts]] and [[relations]] from them by [[jointly]] [[clustering]] the [[objects]] and [[relational strings]] in the [[tuples]] .
We use [[this algorithm]] to analyze [[abstracts]] from [[PNAS]] by using [[Bayesian model]] [[selection]] to establish the number of [[topics]] .
[[We]] use three [[real dataset]]s describing [[user behavior]]s on [[Facebook]] - [[user response]]s to [[ads]], [[search result]]s, and [[News Feed stori]]es - to generate [[data]] for [[synthetic experiment]]s in which there is no effect of the [[treatment]] on [[average]] by design.
[[We]] utilize the [[Alternating Direction Method of Multipliers]] ([[ADMM]]) [[framework]] to [[tensor factorization]] and [[completion]], which can be easily [[scale]]d through [[parallel computing]] .
[[We]] validate [[our approach]] based on [[traffic data]] from two large [[Asian citi]]es.
[[We]] validate [[our approach]] by comparing a [[descriptive segment]] [[trained]] on [[ground truth]] to one [[trained]] on [[behavioral attribute]]s only.
[[We]] validate [[our approach]] on [[two]] different [[coreference problems]]: [[newswire anaphora resolution]] and [[research paper citation matching]], demonstrating improvements in both [[tasks]] and achieving an [[error reduction]] of up to 62% when compared to a [[method]] that [[reasons]] about [[mentions]] only.
[[We]] validate [[our approach]] using [[label]]ed [[intrusion attempt data]] [[collected]] at a [[large number]] of [[edge network]]s.
[[We]] validate [[our hypothesis]] by extensive [[quantitative evaluation]]s on a [[gold dataset]] of [[similar profile]]s generated from [[recruiting activity log]]s from [[actual recruiter]]s using [[LinkedIn]] .
[[We]] validate [[our model]] using one month of complete [[Twitter]] [[data]] and demonstrate a strong improvement in [[predictive accuracy]] over existing [[approach]]es.
[[We]] validate [[RSC]] using [[real data]] consisting of over 35 million [[posting]]s from [[Twitter]] and [[Reddit]] .
[[We]] [[validate]] [[the]] [[effectiveness]] of [[our approach]] by [[empirical results]] on [[real-world data sets]], with [[applications]] to [[constrained image segmentation]] and [[clustering benchmark data sets]] with both [[binary]] and [[degree-of-belief constraints]] .
[[We]] validate the [[effectiveness]] of [[the proposed model]] on [[synthetic data]] and a [[real-world healthcare survey data set]] .
[[We]] validate [[the framework]] on [[image]] and [[video retrieval tasks]] in which [[tags]] from the [[LabelMe dataset]] are used to improve [[image retrieval performance]] from a [[Flickr dataset]] and video [[retrieval performance]] from a [[YouTube dataset]] .
[[We]] [[validate]] [[the model]] on three different [[types]] of [[real-world data sets]] .
[[We]] validate the [[performance]] of these [[algorithm]]s through [[experiment]]s that demonstrate the [[efficacy]] of [[our method]]s and related [[heuristic]]s.
[[We]] validate the [[quality]] of the [[database]] through a [[large-scale]] [[comparative study]] .
[[We]] validate the [[utility]], [[scalability]] and the effectiveness of the [[approach]] on [[hundreds of machine]]s using [[real]] and [[synthetic dataset]]s.
We [[variabilize]] the [[ground atom]]s in each [[path]], and use them to form [[clauses]], which are evaluated using a [[pseudo]]-[[likelihood measure]] .
[[We]] verified the [[efficiency]] of [[our algorithm]]s by [[experiment]]s in [[large real-world graph]]s.
[[We]] verify, both [[empirically]] and [[theoretically]], that [[dropout]] is effective in regulating the [[learned metric]] to avoid the [[over-fitting problem]] .
[[We]] [[verify our findings]] with [[verify our findings with experiments]] on both [[implicit]] [[user interests]] indicated by the content of [[communications]] or [[Web 2.0]] [[activities]], and [[explicit user interests]] specified in [[user profiles]] .
[[We]] will also evaluate [[our algorithm]] [[experimentally]] on [[real data set]]s and on [[randomly generated input]]s.
[[We]] will also show that by applying this general [[method]], a [[large number]] of [[problem]]s in [[PPDM]] can be solved with [[enhanced security]] .
[[We]] will describe how to [[detect association]]s among [[drug]]s and their [[adverse event]]s several years before an [[alert]] is issued as well as compute the [[true rate]] of [[drug-drug interaction]]s.
[[We]] will discuss [[method]]s that use [[unstructured patient data]] to [[monitor]] for [[adverse]] [[drug event]]s, [[profile specific drug]]s, identify [[off-label drug usage]], [[uncover]] [['natural experiment]]s' and generate [[practice-based evidence]] for [[difficult-to-test]] [[clinical hypothese]]s.
[[We]] will discuss the [[design tradeoff]]s, designer / [[user experience]], and some of the [[pragmatic challenge]]s related to a [[model]] that [[integrates large]], [[complex]], [[heterogeneous data source]]s.
[[We]] will discuss the [[testing]] of a [[clinical hypothesis]] about an association between [[allergic condition]]s and [[chronic uveiti]]s in [[patient]]s with [[juvenile idiopathic arthriti]]s.
[[We]] will discuss two kinds of [[neural network architecture]]s, that can be [[mixed and matched]] - [[feed-forward network]]s and [[Recurrent/Recursive network]]s.
[[We]] will examine the challenges of applying [[machine learning]] to [[non-search advertising]] and in doing so explore the creation of [[business environments]] - [[organization]], [[infrastructure]], [[tools]], [[processes]] (and [[costs considerations]]) - in which [[scientists]] can quickly develop new [[petabyte scale]] [[algorithmic approaches]], migrate them rapidly to [[real-time]] [[production]] and [[deliver]] fully [[customized experiences]] for [[marketers]], [[publishers]] and [[consumers]] alike.
[[We]] will present [[approach]]es to identify novel [[off-label uses of drugs]] using the [[patient feature matrix]] along with [[prior knowledge]] about [[drug]]s, [[disease]]s, and [[known usage]] .
[[We]] will present [[experiment]] results to show the [[effectiveness]] of the [[inference]] .
[[We]] will review a [[natural experiment]] -- where a [[subset of congestive]] [[heart failure patient]]s who were [[prescribed Cilostazol]] despite its [[black box warning]] -- and profile its [[safety]] .
[[We]] will show how broad [[classes]] of [[algorithm]]s can be extended to the [[uncertain data]] [[setting]] .
[[We]] will show that [[DRCC]] can be [[solved]] via alternating [[minimization]], and its [[convergence]] is [[theoretically guaranteed]] .
[[We]] will show that [[XGBoost]] employs a [[boosting algorithm]] which we will [[term Newton boosting]] .
[[We]] will test the [[approach]] on a number of [[real]] and [[synthetic data sets]], and show the [[effectiveness]] of two of our [[approaches]] over competitive [[technique]]s.
[[We]] will then show how [[this approach]], along with a second [[stage classifier]] that identifies [[non-intentional traffic]] at the [[browser level]], is deployed in production at [[Media6Degrees (m6d)]], a [[targeting technology]] [[company]] for [[display advertising]] .
[[We]] will use the term <i>[[semantic lexicon]]</i> to refer to a [[dictionary]] that associates [[word]]s with [[semantic class]]es.
[[We]] witness a growing interest in [[clustering]] a [[social network]] of [[people]] based on both their [[social relationship]]s and their [[participation]]s in [[activity based information network]]s.
What are the [[" pain " point]]s of [[user]]s' In [[this talk]], [[I]] will discuss my own [[experience]] on [[user modeling]] with [[big data]] .
What forms does [[our knowledge take]], across different [[domain]]s and [[task]]s?
What happens when [[machine]]s become [[more intelligent than]] [[human]]s?
[[What-if analysis]] satisfies this need by enabling [[user]]s to [[simulate]] and [[inspect]] [[behavior]] of a [[complex system]] under some given [[hypotheses]], called [[scenario]]s.
What is central to [[our argument]] is that [[knowledge]] is held by [[individual]]s, but is also expressed in regularities by which [[members cooperate]] in a [[social community]] (i.e., [[group]], [[organization]], or [[network]]).
What may not cause wonder or [[surprise]] in [[Zurich]] or [[London]] or [[Pari]]s may, however, be quite problematic in [[Cairo]] or [[Bombay]] or [[Lagos (or Moscow]]), in their challenging struggle to establish the [[norm]]s and [[institution]]s of a [[functioning market economy]] .
When [[aggregating hinge loss]]es, the [[optimization problem]] is similar to the [[SVM]] for [[interdependent output space]]s.
When applied to [[protein remote homology detection]] and [[remote fold recognition]], [[our framework]] achieves comparable [[performance]] to the [[state-of-the-art]] (e.g., [[kernel support vector machine]]s).
When applying [[data mining]] and [[machine learning algorithm]]s on [[high dimensional data]], a [[critical issue]] is known as [[curse of dimensionality]] ([[Hastie et al., 2005]]).
When a [[user]] is going to initiate an [[activity]] by [[issuing]] a corresponding [[query]], he / she needs to consider the [[relationship]] among [[candidate]] [[attendee]]s to find a [[group of mutually close friend]]s.
When compared with the [[method]] that finds the [[subgraph]] of the largest [[average degree]], [[our algorithm]]s [[return denser subgraph]]s with smaller [[diameter]] .
When [[computer]]s are involved, determining [[disparate impact]] (and hence [[bias]]) is [[harder]] .
When [[configured]] to [[identify]] 20x20 [[co-cluster]]s in the [[Netflix]] [[training dataset]], the [[implementation]] [[predicted]] over [[100]] million [[rating]]s in 16.31 minutes and achieved an [[RMSE]] of 0.88846 without any [[fine-tuning]] or [[domain knowledge]] .
When dealing with [[sensitive]] and [[personal user data]], the [[process of record linkage]] raises [[privacy issue]]s.
When [[deciding]] which ad to show to a [[user]], one must calculate [[likelihood-to-convert score]]s for that [[user]] across all potential [[advertiser]]s in [[the system]] .
When [[employee]]s begin [[retiring]] in droves, many [[compani]]es will face severe [[talent shortage]]s.
When faced with a new [[task]], [[we]] first [[mine]] some [[reliable (prior) knowledge]] from the [[past learning]] / [[modeling]] results and then use it to guide the [[model inference]] to [[generate]] more [[coherent topic]]s.
When feasible, [[this capability]] will significantly aid in such [[task]]s as [[terminology extraction]] and [[cross-seller]] [[product offering search]]es.
When [[<i>n</i> is large enough]] this represents the [[average]] of the [[squared error]]s between the two [[distribution]]s weighted by the implicit [[uncertainty]], <math>w(x)</math>, due to the [[estimation]] [[method]] .
When it comes to [[making decisions]] in our lives, [[we]] think we're making [[smart]], [[rational choice]]s.
When [[labeled examples]] are limited and difficult to obtain, [[transfer learning]] employs [[knowledge]] from a [[source domain]] to improve [[learning]] [[accuracy]] in the [[target domain]] .
When [[labor]] [[share]] [[falls]], the [[compensation–productivity gap]] [[widens]] .
When [[labor share]] is [[constant]] or [[rising]], [[worker]]s [[benefit]] from [[economic growth]] .
When making [[recommendation]]s, [[COM]] [[estimate]]s the [[preference]] of a [[group]] to an [[item]] by aggregating the [[preference]]s of the [[group member]]s with different [[weight]]s.
When [[new program]]s and [[grant]]s are initiated, using the [[chart of accounts]] and its defined [[methodologies to determine]] the [[account]]s, and other parameters for its [[operation]]s and [[accounting]] creates an [[opportunity]] for clear communication regarding [[expectation]]s and [[authoriti]]es.
When new [[rating]]s, [[user]]s, or [[item]]s enter the [[system]], we can update the [[factorization]] in [[time independent]] of the [[size of data]] (number of [[user]]s, [[item]]s and [[rating]]s).
When [[patient]]s are [[randomly allocated]] to a [[sequence]] of [[treatment]]s ([[crossover design]]), each [[patient]] serves as his or her own [[control]] .
when [[refining]] the [[query]] or [[view]]ing the [[next page of result]]s).
When [[source]]s of [[complementary information]] are [[integrated]], a better [[profile of a user]] can be built to improve [[online service]]s such as [[verifying]] [[online information]] .
When tackling [[non-stationary]] [[concept]]s, [[ensembles of classifiers]] have several advantages over [[single classifier methods]] : they are easy to [[scale]] and [[parallelize]], they can adapt to change quickly by [[pruning]] [[under-performing]] [[parts]] of the [[ensemble]], and they therefore usually also generate more [[accurate]] [[concept descriptions]] .
When the [[input]] [[graph]] cannot fit into [[main memory]], [[triangle listing]] requires [[random disk access]]es that can incur prohibitively large [[I/O cost]] .
When the [[objective function]] is strongly [[convex]], [[we]] show that [[A2DM2]] has a O ([[1=k2) convergence rate]] .
When there are [[series]] of related [[matrice]]s and [[vector]]s (for example, where each [[matrix]] corresponds to a different [[layer in the network]]), [[superscript]] [[indice]]s are used <math>(W_1, W_2)</math>.
When these [[prediction]]s are supported by a [[high-level representation]], [[we]] refer to these as [[knowledge based]] [[on-line classification task]]s.
When the [[total number]] of [[token]]s is <i>[[N]]</i> and the [[number of topic]]s is <i>[[K]]</i>, the [[CVB0 inference]] requires Î (<i> NK </i>) [[memory]] .
When the [[transitivity]] is [[constant]] and there are more [[edge]]s than [[wedge]]s ([[common properti]]es for [[social network]]s), we can prove that [[our algorithm]] requires <i>O</i> (â <i>n </i>) [[space]] (<i> n</i> is the [[number of vertice]]s) to provide [[accurate estimate]]s.
When [[we]] extract the [[patient-related information]] from the [[noisy data]], we have to remove or correct at least two kinds of [[noise]]: <i>[[explicit noise]] </i>, which includes [[spelling error]]s, [[unfinished sentence]]s, [[omission]] of [[sentence delimiter]]s, and [[variants of terms]], and <i>[[implicit noise]] </i>, which includes [[non-patient information]] and [[patient]]'s [[untrustworthy information]] .
When [[we]] reach the final refined [[graph]], the [[algorithm]] is run to [[convergence]] and the [[high-flow region]]s are [[clustered]] together, with [[region]]s without any [[flow]] forming the natural [[boundaries]] of the [[clusters]] .
When you connect with [[Sony Interactive Entertainment LLC ("SIE LLC")]] online by using [[our websites]], [[the PlayStation Network]] and [[Sony Entertainment Network (collectively "SEN")]], or certain [[hardware]] or [[software]] which utilizes [[SEN]] for [[data collection]] and refers to [[this Privacy Policy]] (all collectively, "[[SIE LLC Network]]"), [[we]] collect and handle [[information]] as described below.
:where <math>\omega_0=\sqrt{k/m}</math> is called the [[angular frequency]] (a.k.a [[natural frequency]]), <math>A=\sqrt{a^2+b^2}</math> is called the [[amplitude]] and <math>\delta=tan^{-1}(b/a)</math> is called the [[phase]] (a.k.a [[phase shift]], [[phase angle]]).
where <math>P(D\vert I)</math> and <math>P(D\vert \sim I)</math> are the [[probability]] of observing the [[data]] (<i>D</i>) [[conditioned on]] the [[genes]] sharing [[benchmark]] [[associations]] (<i>I</i>) and not sharing [[benchmark]] [[associations]] (<i>~I</i>).
where <math>p</math> is the [[probability of success]] (a [[crash]]) and <math>q = (1-p)</math> is the [[probability of failure]] ([[no]] [[crash]]).
::where <math>s_1</math> is the [[standard deviation]] of [[random sample]] drawn from a [[population]] with [[standard deviation]] <math>\sigma_1</math>, <math>s_2</math> is the [[standard deviation]] of an independent [[random sample]] drawn from a [[population]] with [[standard deviation]] <math>\sigma_2</math>.
where <math>α</math> is a [[fixed intercept]], <math>u_c</math>, <math>u_p</math> and <math>u_{cp}</math> are [[random intercept]]s allowing different [[log-odds of failure]] between different [[cluster]]s, [[period]]s and [[cluster-period]]s, respectively, <math>β</math> is the [[treatment effect]] representing the [[log-odds ratio]] of outcome between [[experimental]] and [[control]] and <math>x_{cp}</math> is an [[indicator of the treatment]] given in the [[cluster-period]] <math>cp</math>.
where [[pattern]]s are [[probabilistic function]]s of the [[data]] (thus allowing one to account for [[noise]] and [[error]]s in the [[data mining process]], and allowing one to study [[data mining technique]]s based on [[subsampling]] the [[data]]).
Where will advancing [[technology]], [[job automation]], [[outsourcing]] and [[globalization]] lead?
::where x is the [[sample]] [[mean]], μ is the [[population]] [[mean]], s is the [[standard deviation]] of the [[sample]], and n is the sample size.
Which [[pattern]]s exist in [[real-world dynamic graph]]s, and how can we find and [[rank]] them in [[term]]s of [[importance]]?
Which [[technologi]]es are at [[risk of displacement]] or [[encroachment]]?
While [[Active Learning (AL)]] has already been shown to markedly reduce the [[annotation effort]]s for many [[sequence labeling task]]s compared to [[random selection]], [[AL]] remains unconcerned about the [[internal structure]] of the selected [[sequence]]s (typically, [[sentence]]s).
While [[AI technologi]]es are likely to have a profound [[future impact]] on [[employment]] and [[workplace trend]]s in a typical [[North American city]], it is difficult to [[accurately]] [[assess]] [[current impact]]s, [[positive]] or [[negative]] .
While a substantial amount of [[research]] has already been performed in the [[area]] of [[recommender system]]s, most existing [[approach]]es focus on recommending the most [[relevant item]]s to [[user]]s without taking into account any additional [[contextual information]], such as [[time]], [[location]], or the [[company]] of other people (e.g., for [[watching movies]] or [[dining out]]).
While [[bandit method]]s that seek to find the [[optimal]] [[trade-off]] between explore and exploit have been studied for decades, [[existing solution]]s are not [[satisfactory]] for [[web content publishing application]]s where [[dynamic set of item]]s with [[short lifetime]]s, [[delayed feedback]] and [[non-stationary reward (CTR) distribution]]s are typical.
While [[CANaLI]] is a very powerful [[NL system]], which [[placed first in the 2016]] competition on [[Question Answering over Linked Data QALD-6]], even more powerful [[user-friendly interface]]s are available to [[user]]s who [[enter question]]s and [[queri]]es on [[web-browser]]s.
While changes in the [[properti]]es of [[SEMG signal]]s with respect to [[muscle fatigue]] have been reported in the [[literature]], the large [[variation]] in [[these signal]]s across different [[individual]]s makes the [[task of modeling]] and [[classification]] of [[SEMG signal]]s challenging.
While complete [[semantic understanding]] is still a [[far-distant goal]], [[researcher]]s have taken a [[divide and conquer approach]] and identified several [[sub-task]]s useful for [[application development]] and [[analysis]] .
While [[content term]]s are only generated from [[resource topics]], [[social tags]] are [[generated by]] [[resource topics]] and [[user perspectives]] together.
While [[data]] is often [[multi-faceted]] in its [[very nature]], traditional [[clustering method]]s are restricted to find just a [[single grouping]] .
While [[entiti]]es appear in many [[page]]s, current [[engine]]s only find each [[page]] individually.
While [[existing]] [[weighting approaches]] mitigate the first two [[problem]]s above, a [[sparsity preserving solution]] that would allow to [[efficiently]] utilize [[data sets]] with e.g., hundred thousands of [[users]] and [[item]]s has not yet been [[reported]] .
While former [[MTL]] emphasizes [[shared structure]] among [[model]]s, [[MTC]] aims at [[joint prediction]] to exploit [[inter-output correlation]] .
While [[Gibbs sampling]] has been effectively used for [[static topic model]]s ([[Griffiths and Steyvers, 2004]]), [[nonconjugacy]] makes [[sampling method]]s more difficult for [[this dynamic model]] .
While [[hashing-based similarity search seek]]s to address the [[scalability issue]], [[multimodal similarity search]] deals with [[application]]s in which [[data]] of multiple [[modaliti]]es are available.
While high degrees of [[incompleteness]] in [[plan trace]]s presents [[learning challenge]]s for [[traditional (complete) model]]s, [[capability model]]s can still [[learn to extract useful information]] .
While [[human]]s easily [[recognize relation]]s between [[data]] from different [[domain]]s without any [[supervision]], [[learning to automatically discover]] them is in general very challenging and needs many [[ground-truth pair]]s that illustrate the [[relation]]s.
While [[IBAL]] is definitely significant, [[Poole]]'s [[ICL]] and [[Sato]]'s [[PRISM]] were developed [[much earlier]] and have had a significant impact on [[the field]] .
While, in [[theory]], [[data annotation task]]s are assumed to be attempted by [[worker]]s [[independently]], in practice, [[data annotation task]]s are often grouped into [[batch]]es to be presented and annotated by [[worker]]s together, in order to save on the [[time]] or [[cost overhead]] of providing [[instruction]]s or necessary [[background]] .
While it is often possible to directly [[observe]] when [[nodes]] become [[infected]], [[observing]] [[individual transmissions]] (i.e., who [[infects]] whom or who [[influence]]s whom) is typically very difficult.
While [[kernel-based clustering algorithm]]s can capture the [[non-linear structure]] in [[data]], they do not [[scale]] well in terms of [[speed]] and [[memory requirement]]s when the number of [[object]]s to be [[cluster]]ed exceeds tens of thousands.
While many of the [[algorithm]]s and [[abstraction]]s used by a [[DBMS]] are [[textbook material]], there has been relatively [[sparse coverage]] in the [[literature]] of the [[systems design issue]]s that make a [[DBMS work]] .
While many [[resource]]s for [[networks]] of interesting [[entities]] are emerging, most of these can only [[annotate]] [[connections]] in a limited fashion.
While many [[validation]] [[measures]] have been developed for [[evaluating]] the [[performance]] of [[clustering algorithms]], these [[measures]] often provide inconsistent [[information]] about the [[clustering]] [[performance]] and the best suitable [[measures]] to use in practice remain unknown.
While most present [[model]]s of [[information adoption]] in [[network]]s assume [[information]] only [[pass]]es from a [[node]] to [[node]] via the [[edge]]s of the underlying [[network]], the recent [[availability]] of [[massive]] [[online social media data]] allows us to study this [[process]] in more detail.
While multiple [[approach]]es have been proposed to enrich such [[taxonomi]]es with new [[concept]]s, these [[technique]]s are typically evaluated by [[measuring the accuracy]] at [[identifying relationship]]s between [[word]]s, e.g., that a dog is a [[canine]], rather [[relationship]]s between [[specific concept]]s.
While no [[enterprise]] is affected by all of these [[disclosures]], [[administrators]] commonly face many outstanding [[vulnerabilities]] across the [[software system]]s they [[manage]] .
While [[optimal profile construction]] is [[NP-hard]] for [[pay-per-click advertising]] with [[bid increment]]s, it can be efficiently solved via a [[greedy approximation algorithm]] [[guarantee]]d to provide a [[near-optimal solution]] due to the fact that [[keyword profile utility]] is [[submodular]]: it exhibits the property of diminishing [[return]]s with increasing [[profile size]] .
While [[poverty increased]] and spread in the [[2000s]], it also became more [[concentrated]] in [[high-poverty]] and [[economically distressed neighborhood]]s, and those [[neighborhood]]s were increasingly located in the [[suburb]]s.
While [[predictive modeling]] has been thoroughly [[research]]ed in recent years in the [[digital advertising domain]], the [[attribution problem]] focuses more on [[accurate]] and [[stable interpretation]] of the [[influence]] of each [[user interaction]] to the [[final user decision]] rather than just [[user classification]] .
While previous approaches such as [[ACE]] ([[Doddington et al., 2004]]), [[LCTL]] ([[Simpson et al., 2008]]), [[OntoNotes]] ([[Pradhan et al., 2007]]), [[Machine Reading]] ([[Strassel et al., 2010]]), [[TimeML]] ([[Boguraev and Ando, 2005]]), [[Penn Discourse Treebank]] ([[Prasad et al., 2014]]), and [[Rhetorical Structure Theory]] ([[Mann and Thompson, 1988]]) laid some of the groundwork for this type of resource, the [[DEFT program]] requires annotation of [[complex]] and [[hierarchical event structure]]s that go beyond any of the existing (and partially-overlapping) [[task definition]]s.
While previous [[sampling approach]]es mainly rely on the [[Markov Chain Monte Carlo method]], [[our procedure]]s are [[direct]], i.e., non [[process-simulating]], [[sampling algorithm]]s.
While [[previous work]] has modeled what these [[shared stori]]es say about the [[user]] who [[share]]s them, the [[converse question]] remains unexplored: what can we learn about an [[article]] from the [[identiti]]es of its likely [[reader]]s?
While [[racial dispariti]]es have [[decreased]] since the beginning of the [[Great Recession]], the [[young Black]] and [[Hispanic]] [[unemployment rate]]s still far exceed the [[current rate]] for [[young White]]s (5 percent) and [[Asian]]s (7 percent).
While [[readability]] is characterised by the [[degree of comprehension]] with which a [[text]] is [[read]], the ability of a [[reader]] to [[comprehend a text]] is influenced by the [[text]]'s [[readability]] .
While [[recommender system]]s have been widely studied, this is mostly in [[relation]] to [[leisure product]]s (e.g. [[movie]]s, [[book]]s and [[music]]) with [[non-repeated purchase]]s.
While [[relational patterns]] show how [[attribute values]] [[co-occur]] in detail, their huge [[numbers]] [[hamper]] their [[usage]] in [[data analysis]] .
While [[sequence]]s S1–S4 represent normal [[daily profile]]s of a [[user]], the [[sequence]] S5 is possibly an attempt to [[break into a computer]] by trying different [[password]]s.
While the [[component]]s of [[DHC]] are generic, in [[this work]], [[we]] focus on [[congestive heart failure (CHF)]], a pressing [[chronic condition]] .
While the [[interactive alignment approach]] highlights [[imitative pattern]]s between [[interlocutor]]s, the [[synergy approach]] points to [[structural organization]] at the level of the [[interaction]] - such as [[complementary pattern]]s straddling [[speech turn]]s and [[interlocutor]]s.
While their [[algorithm]] runs in [[cubic time]] (with a [[quadratic time heuristic]]), [[we]] propose a novel [[random projection-based technique]] that is able to [[estimate]] the [[angle-based outlier factor]] for all [[data point]]s in [[time near-linear]] in the [[size of the data]] .
While the need for [[mining critical alert]]s over [[large scale]] [[alert sequence]]s is evident, most [[alert analysis technique]]s stop at [[modeling]] and [[mining]] the [[causal relation]]s among the [[alert]]s.
While [[the paper]] focuses on [[frequent pattern mining]], [[the]] [[technique]]s developed [[here]] are relevant whenever the [[data mining output]] is a [[list of elements]] [[ordered]] according to an appropriately "[[robust]]" [[measure of interest]] .
While [[the paper]] specifically describes the [[experiment system]] and [[experimental processes]] [[we]] have in place at [[Google]], [[we]] believe they can be generalized and applied by any [[entity]] interested in using [[experimentation]] to improve [[search engines]] and other [[web applications]] .
While the [[physiological response of human]]s to [[emotional event]]s or [[stimuli]] is [[well-investigated]] for many [[modaliti]]es (like [[EEG]], [[skin resistance]],...), [[surprisingly little]] is known about the exhalation of so-called [[Volatile Organic Compounds (VOCs)]] at quite [[low concentration]]s in response to such [[stimuli]] .
While there are [[limited air-quality-monitor-station]]s in a [[city]], [[air quality]] varies in urban spaces [[non-linearly]] and depends on multiple [[factor]]s, such as [[meteorology]], [[traffic volume]], and [[land use]]s.
While there are some studies on [[POI]] [[recommendation]]s, it lacks of [[integrated analysis]] of the [[joint effect]] of [[multiple]] [[factor]]s.
While there has been a lot of progress in designing effective [[personalized recommender system]]s, by exploiting [[user interest]]s and [[historical interaction data]] through [[implicit]] ([[item click]]) or [[explicit]] ([[item rating]]) feedback, [[directly optimizing]] for [[users' satisfaction]] with [[the system]] remains challenging.
While these [[network]]s have long been difficult to [[train]] and often contain [[millions]] of [[parameter]]s, recent advances in [[network architecture]]s, [[optimization technique]]s, and [[parallel computation]] have enabled [[large-scale learning]] with [[recurrent net]]s.
While these new [[political order]]s established the [[principle of accountability]], neither [[England]] in [[1689]] nor the [[United States]] in [[1789]] could be considered a [[modern democracy]] .
While these [[technologies]] are both [[real]] and [[important]], and some [[job]]s will [[disappear because of them]], the future of [[job]]s overall isn't nearly as gloomy as many [[prognosticator]]s [[believe]] .
While the [[theoretical aspect]]s of [[offline]] [[controlled experiments]] have been well [[studied]] and [[documented]], the practical aspects of running them in [[online settings]], such as [[web sites]] and [[service]]s, are still being developed.
While the [[transcription]]s used to [[train speech recognition system]]s are [[lexical]], the targets presented to the [[network]]s are usually [[phonetic]] .
While this [[formulation]] admits a [[convex programming solution]], [[we]] strive for more [[robust]] and [[scalable algorithms]] .
While [[this]] is not the first [[work]] to explore [[supervised learning]], many [[factors]] significant in [[influencing]] and [[guiding]] [[classification]] remain unexplored.
While this leads to [[positive outcome]]s for some [[people]], many others do not have the necessary [[experience]], [[knowledge]] and [[instinct]] to make [[good decision]]s.
While [[title]]s are a form of [[unstructured text]], their central role in [[consumer decision making]] and [[search engine rank performance]], [[seller]]s are motivated to ensure that its [[text]] is [[rich in relevant information]] while also being easy to [[read]]/[[parse]] .
While [[we]] believe these [[question]]s to be of [[intrinsic interest]], [[we]] discuss possible applications to [[genetic programming]] and [[developer productivity tool]]s.
While we often search for various [[data "entities"]] (<i>e.g.</i>, [[phone number]], [[paper PDF]], [[date]]), today's [[engine]]s only take us indirectly to [[page]]s.
While [[wiki]]s demonstrated their benefits for [[creating]] and [[sharing knowledge]] in [[open web environment]]s, they are also successfully used in [[compani]]es and [[universiti]]es as [[general knowledge management tool]]s.
Why did the slowdown in [[skill upgrading]] and the erosion in [[labor market institution]]s happen a [[decade]] [[earlier]] in [[the US]] than in [[Germany]]?
[[Wikipedia]] being a [[large]], [[freely available]], [[frequently updated]] and [[community maintained knowledge base]], has been central to much recent [[research]] .
[[Wikipedia]] can support the development of [[automatic method]]s for [[keyword extraction]] and [[word-sense disambiguation]] .
[[Wikipedia]], for example, is a [[free]] [[encyclopedia]] that currently contains 4,208,409 [[English article]]s.
[[Wikipedia]] has become the first [[stop]] for [[celebrity biographies]] and [[facts]] .
[[Wikipedia]] provides a [[knowledge base]] for computing [[word relatedness]] in a more [[structured fashion]] than a [[search engine]] and with more coverage than [[WordNet]] .
[[Wiktionary]] and [[WordNet]] do not categorize [[phrase]]s as continuous or discontinous.
Will [[networked]], [[automated]], [[artificial intelligence (AI) application]]s and [[robotic device]]s have [[displace]]d more [[job]]s than they have [[created]] by [[2025]]?
[[Winning]] a [[competition]] engenders subsequent [[unrelated]] [[unethical behavior]] .
With a [[large number]] of [[sensor]]s (sand) deployed in a designated [[area]], the [[CPS]] is required to [[discover]] all the [[trajectories]] (lines) of passing [[intruder]]s in [[real time]] .
With an [[analytical model]] and [[experiment]]s, we explore different [[setting]]s that balance time ([[query evaluation speed]]) and space ([[memory utilization]]).
With an [[ensemble]] of three [[residual]] and one [[Inception-v4]], we achieve 3.08 percent [[top-5 error]] on the [[test set]] of the [[ImageNet classification (CLS) challenge]] .
With a poorly designed [[COA]], [[straightforward task]]s such as the [[preparation of standard]] reports become [[onerou]]s and often require [[human]] and [[spreadsheet intervention]] .
With a single [[feature]] [[per]] [[entity]], it is equivalent to a [[stochastic blockmodel]] .
With a specific [[model]]ing of "[[background topic]]" and "[[unknown entiti]]es", [[our model]] is able to harvest useful [[evidence]]s out of [[noisy information]] .
With [[English Gigaword Corpus]] ([[Parker et al., 2009]]), [[we]] use the [[skip-gram model]] as implemented in word2vec ([[Mikolov et al., 2013]]) to [[induce embedding]]s.
With enough [[sample]]s the [[node]] can [[estimate]] the [[distribution]] over its [[Markov blanket]] and (roughly speaking) determine its own [[statistics]] .
With experiments on various [[real-world data set]]s, [[we]] demonstrate the [[high potential]] of [[MVGen]] to [[detect multiple]], [[overlapping clustering view]]s in [[subspace projection]]s of the [[data]] .
With [[experiment]]s on various [[real-world data]], we demonstrate [[SMVC]]'s potential to [[detect]] [[multiple]] [[clustering view]]s and its [[capability]] to improve the [[result]] by exploiting [[prior knowledge]] .
With [[high probability]] the [[approximation]] is a [[superset]] of the [[FI]]s, and no [[itemset]] with [[frequency]] much lower than the [[threshold]] is included in [[it]] .
Within a deliberately designed [[offline simulation framework]] we apply [[our algorithm]]s to an [[industry leading performance]] based [[contextual advertising system]] and conduct extensive [[evaluations]] with [[real]] [[online event log data]] .
Within each [[problem formulation]], [[we]] group [[technique]]s into [[categori]]es based on the [[nature of the underlying algorithm]] .
Within forty years, [[Moravec]] believes, [[we]] will achieve [[human equivalence]] in our [[machines]], not only in [[their]] [[capacity to reason]] but also in their [[ability to perceive]], [[interact with]], and [[change]] their [[complex environment]] .
Within [[our framework]], [[we]] show how to produce [[statistically unbiased estimator]]s for various [[graph properti]]es from the [[sample]] .
Within [[this framework]], [[we]] evaluate, both [[intrinsically]] and [[extrinsically]], [[smoothing technique]]s for [[integrating " foreground " model]]s (to capture [[recency]]) and [["background" model]]s (to combat [[sparsity]]), as well as different [[technique]]s for [[retaining history]] .
With large [[initial weight]]s, [[autoencoders]] typically find poor [[local minima]]; with small [[initial weight]]s, the [[gradient]]s in the [[early layer]]s are tiny, making it infeasible to [[train]] [[autoencoders with many hidden layers]] .
With many [[technical]] and [[data challenge]]s ahead of us, [[we]] are committed to utilizing our [[huge]] [[data asset]] well to understand the [[need]], [[intent]], and [[behavior]] of our [[user]]s for the purpose of serving them better.
With [[networked data]], the [[class membership]] of one [[entity]] may have an influence on the [[class membership]] of a [[related]] [[entity]] .
With [[non-uniform sampling]] of [[node]]s and [[non-uniform weighting]] of [[neighbors' response]]s, [[we]] devise an ideal [[unbiased estimator]] .
With [[Ontology Matching]], [[researcher]]s and [[practitioner]]s will find a [[reference book]] which presents currently available work in a [[uniform framework]] .
With [[our contribution]], [[we]] aim to provide [[guidance to practitioner]]s on how to develop and [[scale]] [[continuous experimentation]] in [[software organization]]s with the purpose of becoming [[data-driven at scale]] .
With [[our contribution]], [[we]] aim to provide [[guidance to practitioners]] on how to [[develop]] and [[scale]] [[continuous experimentation]] in [[software organization]]s with the purpose of becoming [[data-driven at scale]] .
With our [[parallel algorithm]] and the resulting [[system]], [[we]] can build above 450 [[BT]]-[[category]] [[model]]s from the entire [[Yahoo]]'s [[user]] base within one [[day]], the scale that one can not even imagine with prior [[systems]] .
Without any [[lookahead search]], the [[neural network]]s play [[Go]] at the level of [[state-of-the-art]] [[Monte Carlo tree search program]]s that simulate thousands of [[random game]]s of [[self-play]] .
Without any [[prior knowledge]], these [[question]]s, named <i>[[no-truth question]]s </i>, are difficult to be distinguished from the [[question]]s that have [[true answer]]s, named <i>[[has-truth question]]s </i>.
Without directly applicable previous [[method]]s, [[we]] modified three [[graphical model]]s as [[baseline]]s.
Without [[partitioning]] the [[graph]], communication quickly becomes a [[limiting factor]] in [[scaling the system]] up.
[[Without tracking]] and [[human labeling effort]], [[our framework]] completes many challenging [[visual surveillance task]]s of [[board interest]] such as: (1) [[discovering]] typical [[atomic activiti]]es and [[interaction]]s; (2) [[segmenting long video sequence]]s into different [[interaction]]s; (3) [[segmenting motion]]s into different [[activiti]]es; (4) [[detecting abnormality]]; and (5) supporting [[high-level queri]]es on [[activiti]]es and [[interaction]]s.
Without using any [[information]] (e.g. [[revert]]s) provided by other [[user]]s, [[these algorithms]] each have over 85% [[classification accuracy]] .
With [[Queripidia]] we want to [[answer questions]] on the [[fringe]] of [[Wikipedia]] .
With respect to [[internal validity]], [[selection bias]], [[information bias]], and [[confounding]] are present to some degree in all [[observational research]] .
With similar [[training cost]], [[our approach]] has shown a significant improvement in terms of [[prediction]] [[accuracy criteria]] over the [[existing approach]]es.
With such a dramatic [[decline]] in [[real earning]]s, it is [[small comfort]] that [[wage]]s in the [[city]] have actually slumpedfor [[young people]] in the rest of the [[country]] .
With that ability, [[we]] [[discovered]] that it's easy to [[incorrectly identify]] the [[winning treatment]] because of [[Simpson's paradox]] .
With the [[advance]]s and increasing [[sophistication]] in [[data collection technique]]s, we are facing with [[large amounts of data]] collected from [[multiple]] [[heterogeneous source]]s in many [[application]]s.
With the [[development]] of [[Web application]]s, [[textual document]]s are not only [[getting richer]], but also ubiquitously [[interconnected]] with [[user]]s and other [[object]]s in various ways, which brings about [[text-rich]] [[heterogeneous information]] [[network]]s.
With the ever increasing [[size]] and [[complexity]] of today's [[event logs]], the [[task of analyzing]] [[event logs]] has become cumbersome to carry out [[manually]] .
With the [[explosion]] of [[mobile device]]s with [[camera]]s, [[online search]] has moved beyond [[text]] to other [[modaliti]]es like [[image]]s, [[voice]], and [[writing]] .
With the [[explosion]] of [[smartphone]]s and [[social network service]]s, [[location-based social networks (LBSNs)]] are increasingly seen as [[tool]]s for [[business]]es (e.g., [[restaurant]]s, [[hotel]]s) to promote their [[product]]s and [[service]]s.
With the explosion of [[software size]], [[checking]] [[conformance of implementation]] to [[specification]] becomes an increasingly important but also [[hard problem]] .
With the [[explosive growth]] of [[social network]]s, many [[application]]s are increasingly harnessing the pulse of [[online crowd]]s for a variety of [[task]]s such as [[marketing]], [[advertising]], and [[opinion mining]] .
With the explosive number of [[self-formed communiti]]es in [[social network]]s, one important [[demand]] is to identify [[magnet communiti]]es for [[user]]s.
With the extracted [[TAS]], which [[represent]] [[sets]] of possible [[frequent execution]]s with their typical [[transition time]]s, a few [[factorizing]] [[operator]]s are built.
With the [[globalisation]] of the [[world's economi]]es and [[ever-evolving financial structure]]s, fraud has become one of the main [[dissipater]]s of [[government wealth]] and [[perhap]]s even a major [[contributor]] in the [[slowing down of economi]]es in general.
With the [[growth]] of [[location-based social network]]s, [[fine grained data]] describing [[user mobility]] and [[popularity]] of places has recently become [[attainable]] .
With the help of the [[influence]] [[analysis]], [[we]] present several important [[applications]] on [[real data sets]] such as 1) what are the [[representative]] [[nodes]] on a given topic?
With the help of [[this network]], [[multi-label learning]] is decomposed into a [[series]] of [[single-label classification problems]], where a [[classifier]] is constructed for each [[label]] by incorporating its [[parental labels]] as additional [[features]] .
With the [[proliferation]] of [[smart phone]]s and [[wireless tablet]]s, professionals who have an [[operational responsibility]] in [[disaster situation]]s are relying on such [[device]]s to maintain [[communication]] .
With the rapid development of advanced [[data acquisition technique]]s such as [[high-throughput]] [[biological experiment]]s and [[wireless sensor network]]s, [[large amount]] of [[graph-structured data]], [[graph data]] for short, have been collected in a wide range of [[application]]s.
With the [[rapid development]] of [[location-based social networks (LBSNs)]], [[spatial item recommendation]] has become an important means to help [[people]] discover attractive and interesting [[venue]]s and [[event]]s, especially when [[user]]s travel out of [[town]] .
With the rapid expansion of [[the Web]], [[learning in-depth knowledge]] about a [[topic]] from [[the Web]] is becoming increasingly [[important]] and [[popular]] .
With the [[rapid growth]] of [[Web 2.0]], a variety of [[content sharing service]]s, such as <i>[[Flickr]], [[YouTube]], [[Blogger]] </i>, and <i>[[TripAdvisor]]</i> etc, have become extremely popular over the [[last decade]] .
With the [[RDs]] discussed in this section, one can compare [[experimental]] and [[control group]]s on (a) [[posttest score]]s, while [[controlling for]] [[pretest difference]]s or (b) [[mean gain score]]s, that is, the difference between the [[posttest mean]] and the [[pretest mean]] .
With the recent advent of inexpensive and [[scalable]] [[online]] [[annotation]] tools, such as [[Amazon]]'s [[Mechanical Turk]], the [[labeling process]] has become more vulnerable to [[noise]] - and without [[prior knowledge]] of the [[accuracy]] of each [[individual]] [[label]]er.
With these [[approach]]es it is necessary to expand the [[ad retrieval system]] or build new [[index]] to handle the [[categori]]es or [[class]]es, and it is not always [[easy to maintain]] the number of [[categori]]es and [[class]]es required for [[business need]]s.
With the [[sitemap]], [[posts]] from the same [[thread]] but [[distributed]] on various [[pages]] can be [[concatenated]] according to their [[timestamps]] .
With the [[support]] of the [[legally-grounded methodology]] of [[situation testing]], [[we]] tackle the problems of [[discrimination discovery]] and [[prevention]] from a [[dataset]] of [[historical decision]]s by adopting a [[variant]] of [[k-NN classification]] .
With the [[technique]], it is possible to [[generalize]] past [[known errors]] and [[mistakes]] to [[capture]] [[failures]] and [[anomalies]] .
With [[this book]], the [[editor]]s introduce [[graduate student]]s and [[advanced professional]]s to this exciting field.
With this in mind, [[we]] introduce a [[well-founded approach]] for [[succinct]]ly [[summarizing data]] with a [[collection of itemset]]s; using a [[probabilistic maximum entropy model]], [[we]] [[iteratively]] find the most [[interesting itemset]], and in turn update [[our model]] of the [[data]] accordingly.
With this [[method]], no [[sensitive information]] of a [[party]] will be [[revealed]] even when all other [[parties]] [[collude]] .
With today's [[large-scale]], constantly expanding [[document collections]], it is useful to be able to [[infer]] [[topic]] [[distributions]] for [[new documents]] without [[retraining the model]] .
With [[Twitter]] being widely used [[around the world]], [[user]]s are facing enormous new [[tweet]]s every day.
With two [[real-world data set]]s, [[we]] demonstrate that <b>[[GB-CENT]]</b> can effectively (i.e. [[fast]] and accurately) achieve better [[accuracy]] than [[state-of-the-art]] [[matrix factorization]], [[decision tree based model]]s and their [[ensemble]] .
[[Women]] are now [[overrepresented]] in [[social science]]s, yet only constitute a [[fraction]] of the [[engineering workforce]] .
[[Word2vec]]: [[We]] created [[word representation]]s using the [[word2vec toolkit]] .
[[Word embeddings]] [[map]] each [[word]] in text to a [[‘k’ dimensional]] (~50) [[real valued vector]] .
[[Words]], [[contexts]], and [[senses]] are represented in [[Word Space]], a [[high-dimensional]], [[real-valued space]] in which [[closeness]] corresponds to [[semantic similarity]] .
[[Word Segmentation Task]], [[Chinese Language]], [[Chinese Word Segmentation]], [[Writing System]] .
[[Word sense disambiguation]] is the [[process]] of [[automatically figuring out]] the [[intended meaning]] of such a [[word]] when used in a [[sentence]] .
[[Word sense disambiguation]] is the [[task]] of [[assigning]] [[sense labels]] to [[occurrences of an ambiguous word]] .
[[Word Sense Disambiguation (WSD)]] and [[Entity Linking (EL)]] are well-known [[problem]]s in the [[Natural Language Processing field]] and both address the [[lexical ambiguity]] of [[language]] .
[[Word sense disambiguation (WSD)]] is the ability to [[identify]] the [[meaning]] of [[words in context]] in a [[computational manner]] .
[[Word]]s in the [[document]]s are assumed to be [[randomly generated]] by particular [[topic assignment]]s and [[topic-to-word probability distribution]]s.
[[Worker]]s in [[flexible job]]s share many [[labour market characteristic]]s (eg, [[lower credential]]s, [[low income]], [[women]], [[immigrant]] and [[non‐white]]) with the [[unemployed]], while themselves experiencing bouts of [[unemployment]], a factor strongly associated with [[adverse health outcome]]s.6,7
Working in [[distributed subgroup]]s, the participants addressed [[near-term]] [[AI development]]s, [[long-term]] [[possibiliti]]es, and [[legal]] and [[ethical concern]]s, and then came together in a [[three-day]] [[meeting]] at [[Asilomar]] to share and discuss their [[finding]]s.
Work on regression trees started with [[RETIS]] ([[Karalic & Cestnik, 1991]]) and [[M5]] ([[Quinlan, 1992]]).
[[World of Warcraft]], [[big five inventory]], [[personality]], [[video games]], [[violence]]
[[Worship]] your [[body and beauty]] and [[sexual allure]] and you will always [[feel ugly]] .
Would [[cash]] or [[token reward]]s (e.g., [[personal gift]]s or [[chocolate]]s) provide a stronger incentive?
Written by an [[internationally recognized expert]] in [[the field]], it draws on a number of [[discipline]]s besides [[linguistics]], including [[philosophy]], [[neuroscience]], [[genetics]], and [[animal behaviour]], and will appeal to a wide range of readers interested in [[language origins]] and [[evolution]] .
Written for [[game scholar]]s, [[game developer]]s, and [[interactive designer]]s, [[Rules of Play]] is a [[textbook]], [[reference book]], and [[theoretical guide]] .
Yet, [[the field]] lacks [[overarching structure]]s that would facilitate [[greater integration]], [[consistency]], and [[understanding]] of [[this body of research]] .
Yet these [[misguided behavior]]s are neither [[random]] nor [[senseles]]s.
Yet, under some [[reasonable conditions]], [[we]] [[show]] both [[theoretically]] and [[empirically]] that a [[memory-efficient algorithm]] exists.
[[Zero-based budgeting]]: A [[budgeting technique]] that begins at [[zero]] and justifies every [[expenditure]] .
